Index: openCVLibrary3411/src/main/java/org/opencv/core/MatOfDouble.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/core/MatOfDouble.java	(date 1605830247338)
+++ openCVLibrary3411/src/main/java/org/opencv/core/MatOfDouble.java	(date 1605830247338)
@@ -0,0 +1,79 @@
+package org.opencv.core;
+
+import java.util.Arrays;
+import java.util.List;
+
+public class MatOfDouble extends Mat {
+    // 64FC(x)
+    private static final int _depth = CvType.CV_64F;
+    private static final int _channels = 1;
+
+    public MatOfDouble() {
+        super();
+    }
+
+    protected MatOfDouble(long addr) {
+        super(addr);
+        if( !empty() && checkVector(_channels, _depth) < 0 )
+            throw new IllegalArgumentException("Incompatible Mat");
+        //FIXME: do we need release() here?
+    }
+
+    public static MatOfDouble fromNativeAddr(long addr) {
+        return new MatOfDouble(addr);
+    }
+
+    public MatOfDouble(Mat m) {
+        super(m, Range.all());
+        if( !empty() && checkVector(_channels, _depth) < 0 )
+            throw new IllegalArgumentException("Incompatible Mat");
+        //FIXME: do we need release() here?
+    }
+
+    public MatOfDouble(double...a) {
+        super();
+        fromArray(a);
+    }
+
+    public void alloc(int elemNumber) {
+        if(elemNumber>0)
+            super.create(elemNumber, 1, CvType.makeType(_depth, _channels));
+    }
+
+    public void fromArray(double...a) {
+        if(a==null || a.length==0)
+            return;
+        int num = a.length / _channels;
+        alloc(num);
+        put(0, 0, a); //TODO: check ret val!
+    }
+
+    public double[] toArray() {
+        int num = checkVector(_channels, _depth);
+        if(num < 0)
+            throw new RuntimeException("Native Mat has unexpected type or size: " + toString());
+        double[] a = new double[num * _channels];
+        if(num == 0)
+            return a;
+        get(0, 0, a); //TODO: check ret val!
+        return a;
+    }
+
+    public void fromList(List<Double> lb) {
+        if(lb==null || lb.size()==0)
+            return;
+        Double ab[] = lb.toArray(new Double[0]);
+        double a[] = new double[ab.length];
+        for(int i=0; i<ab.length; i++)
+            a[i] = ab[i];
+        fromArray(a);
+    }
+
+    public List<Double> toList() {
+        double[] a = toArray();
+        Double ab[] = new Double[a.length];
+        for(int i=0; i<a.length; i++)
+            ab[i] = a[i];
+        return Arrays.asList(ab);
+    }
+}
Index: openCVLibrary3411/src/main/java/org/opencv/core/MatOfDMatch.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/core/MatOfDMatch.java	(date 1605830247336)
+++ openCVLibrary3411/src/main/java/org/opencv/core/MatOfDMatch.java	(date 1605830247336)
@@ -0,0 +1,83 @@
+package org.opencv.core;
+
+import java.util.Arrays;
+import java.util.List;
+
+import org.opencv.core.DMatch;
+
+public class MatOfDMatch extends Mat {
+    // 32FC4
+    private static final int _depth = CvType.CV_32F;
+    private static final int _channels = 4;
+
+    public MatOfDMatch() {
+        super();
+    }
+
+    protected MatOfDMatch(long addr) {
+        super(addr);
+        if( !empty() && checkVector(_channels, _depth) < 0 )
+            throw new IllegalArgumentException("Incompatible Mat: " + toString());
+        //FIXME: do we need release() here?
+    }
+
+    public static MatOfDMatch fromNativeAddr(long addr) {
+        return new MatOfDMatch(addr);
+    }
+
+    public MatOfDMatch(Mat m) {
+        super(m, Range.all());
+        if( !empty() && checkVector(_channels, _depth) < 0 )
+            throw new IllegalArgumentException("Incompatible Mat: " + toString());
+        //FIXME: do we need release() here?
+    }
+
+    public MatOfDMatch(DMatch...ap) {
+        super();
+        fromArray(ap);
+    }
+
+    public void alloc(int elemNumber) {
+        if(elemNumber>0)
+            super.create(elemNumber, 1, CvType.makeType(_depth, _channels));
+    }
+
+
+    public void fromArray(DMatch...a) {
+        if(a==null || a.length==0)
+            return;
+        int num = a.length;
+        alloc(num);
+        float buff[] = new float[num * _channels];
+        for(int i=0; i<num; i++) {
+            DMatch m = a[i];
+            buff[_channels*i+0] = m.queryIdx;
+            buff[_channels*i+1] = m.trainIdx;
+            buff[_channels*i+2] = m.imgIdx;
+            buff[_channels*i+3] = m.distance;
+        }
+        put(0, 0, buff); //TODO: check ret val!
+    }
+
+    public DMatch[] toArray() {
+        int num = (int) total();
+        DMatch[] a = new DMatch[num];
+        if(num == 0)
+            return a;
+        float buff[] = new float[num * _channels];
+        get(0, 0, buff); //TODO: check ret val!
+        for(int i=0; i<num; i++)
+            a[i] = new DMatch((int) buff[_channels*i+0], (int) buff[_channels*i+1], (int) buff[_channels*i+2], buff[_channels*i+3]);
+        return a;
+    }
+
+    public void fromList(List<DMatch> ldm) {
+        DMatch adm[] = ldm.toArray(new DMatch[0]);
+        fromArray(adm);
+    }
+
+    public List<DMatch> toList() {
+        DMatch[] adm = toArray();
+        return Arrays.asList(adm);
+    }
+}
Index: openCVLibrary3411/src/main/java/org/opencv/core/DMatch.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/core/DMatch.java	(date 1605830247299)
+++ openCVLibrary3411/src/main/java/org/opencv/core/DMatch.java	(date 1605830247299)
@@ -0,0 +1,58 @@
+package org.opencv.core;
+
+//C++: class DMatch
+
+/**
+ * Structure for matching: query descriptor index, train descriptor index, train
+ * image index and distance between descriptors.
+ */
+public class DMatch {
+
+    /**
+     * Query descriptor index.
+     */
+    public int queryIdx;
+    /**
+     * Train descriptor index.
+     */
+    public int trainIdx;
+    /**
+     * Train image index.
+     */
+    public int imgIdx;
+
+    // javadoc: DMatch::distance
+    public float distance;
+
+    // javadoc: DMatch::DMatch()
+    public DMatch() {
+        this(-1, -1, Float.MAX_VALUE);
+    }
+
+    // javadoc: DMatch::DMatch(_queryIdx, _trainIdx, _distance)
+    public DMatch(int _queryIdx, int _trainIdx, float _distance) {
+        queryIdx = _queryIdx;
+        trainIdx = _trainIdx;
+        imgIdx = -1;
+        distance = _distance;
+    }
+
+    // javadoc: DMatch::DMatch(_queryIdx, _trainIdx, _imgIdx, _distance)
+    public DMatch(int _queryIdx, int _trainIdx, int _imgIdx, float _distance) {
+        queryIdx = _queryIdx;
+        trainIdx = _trainIdx;
+        imgIdx = _imgIdx;
+        distance = _distance;
+    }
+
+    public boolean lessThan(DMatch it) {
+        return distance < it.distance;
+    }
+
+    @Override
+    public String toString() {
+        return "DMatch [queryIdx=" + queryIdx + ", trainIdx=" + trainIdx
+                + ", imgIdx=" + imgIdx + ", distance=" + distance + "]";
+    }
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/core/KeyPoint.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/core/KeyPoint.java	(date 1605830247317)
+++ openCVLibrary3411/src/main/java/org/opencv/core/KeyPoint.java	(date 1605830247317)
@@ -0,0 +1,77 @@
+package org.opencv.core;
+
+import org.opencv.core.Point;
+
+//javadoc: KeyPoint
+public class KeyPoint {
+
+    /**
+     * Coordinates of the keypoint.
+     */
+    public Point pt;
+    /**
+     * Diameter of the useful keypoint adjacent area.
+     */
+    public float size;
+    /**
+     * Computed orientation of the keypoint (-1 if not applicable).
+     */
+    public float angle;
+    /**
+     * The response, by which the strongest keypoints have been selected. Can
+     * be used for further sorting or subsampling.
+     */
+    public float response;
+    /**
+     * Octave (pyramid layer), from which the keypoint has been extracted.
+     */
+    public int octave;
+    /**
+     * Object ID, that can be used to cluster keypoints by an object they
+     * belong to.
+     */
+    public int class_id;
+
+    // javadoc:KeyPoint::KeyPoint(x,y,_size,_angle,_response,_octave,_class_id)
+    public KeyPoint(float x, float y, float _size, float _angle, float _response, int _octave, int _class_id) {
+        pt = new Point(x, y);
+        size = _size;
+        angle = _angle;
+        response = _response;
+        octave = _octave;
+        class_id = _class_id;
+    }
+
+    // javadoc: KeyPoint::KeyPoint()
+    public KeyPoint() {
+        this(0, 0, 0, -1, 0, 0, -1);
+    }
+
+    // javadoc: KeyPoint::KeyPoint(x, y, _size, _angle, _response, _octave)
+    public KeyPoint(float x, float y, float _size, float _angle, float _response, int _octave) {
+        this(x, y, _size, _angle, _response, _octave, -1);
+    }
+
+    // javadoc: KeyPoint::KeyPoint(x, y, _size, _angle, _response)
+    public KeyPoint(float x, float y, float _size, float _angle, float _response) {
+        this(x, y, _size, _angle, _response, 0, -1);
+    }
+
+    // javadoc: KeyPoint::KeyPoint(x, y, _size, _angle)
+    public KeyPoint(float x, float y, float _size, float _angle) {
+        this(x, y, _size, _angle, 0, 0, -1);
+    }
+
+    // javadoc: KeyPoint::KeyPoint(x, y, _size)
+    public KeyPoint(float x, float y, float _size) {
+        this(x, y, _size, -1, 0, 0, -1);
+    }
+
+    @Override
+    public String toString() {
+        return "KeyPoint [pt=" + pt + ", size=" + size + ", angle=" + angle
+                + ", response=" + response + ", octave=" + octave
+                + ", class_id=" + class_id + "]";
+    }
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/core/Mat.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/core/Mat.java	(date 1605830247321)
+++ openCVLibrary3411/src/main/java/org/opencv/core/Mat.java	(date 1605830247321)
@@ -0,0 +1,1416 @@
+package org.opencv.core;
+
+import java.nio.ByteBuffer;
+
+// C++: class Mat
+//javadoc: Mat
+public class Mat {
+
+    public final long nativeObj;
+
+    public Mat(long addr) {
+        if (addr == 0)
+            throw new UnsupportedOperationException("Native object address is NULL");
+        nativeObj = addr;
+    }
+
+    //
+    // C++: Mat::Mat()
+    //
+
+    // javadoc: Mat::Mat()
+    public Mat() {
+        nativeObj = n_Mat();
+    }
+
+    //
+    // C++: Mat::Mat(int rows, int cols, int type)
+    //
+
+    // javadoc: Mat::Mat(rows, cols, type)
+    public Mat(int rows, int cols, int type) {
+        nativeObj = n_Mat(rows, cols, type);
+    }
+
+    //
+    // C++: Mat::Mat(int rows, int cols, int type, void* data)
+    //
+
+    // javadoc: Mat::Mat(rows, cols, type, data)
+    public Mat(int rows, int cols, int type, ByteBuffer data) {
+        nativeObj = n_Mat(rows, cols, type, data);
+    }
+
+    //
+    // C++: Mat::Mat(int rows, int cols, int type, void* data, size_t step)
+    //
+
+    // javadoc: Mat::Mat(rows, cols, type, data, step)
+    public Mat(int rows, int cols, int type, ByteBuffer data, long step) {
+        nativeObj = n_Mat(rows, cols, type, data, step);
+    }
+
+    //
+    // C++: Mat::Mat(Size size, int type)
+    //
+
+    // javadoc: Mat::Mat(size, type)
+    public Mat(Size size, int type) {
+        nativeObj = n_Mat(size.width, size.height, type);
+    }
+
+    //
+    // C++: Mat::Mat(int ndims, const int* sizes, int type)
+    //
+
+    // javadoc: Mat::Mat(sizes, type)
+    public Mat(int[] sizes, int type) {
+        nativeObj = n_Mat(sizes.length, sizes, type);
+    }
+
+    //
+    // C++: Mat::Mat(int rows, int cols, int type, Scalar s)
+    //
+
+    // javadoc: Mat::Mat(rows, cols, type, s)
+    public Mat(int rows, int cols, int type, Scalar s) {
+        nativeObj = n_Mat(rows, cols, type, s.val[0], s.val[1], s.val[2], s.val[3]);
+    }
+
+    //
+    // C++: Mat::Mat(Size size, int type, Scalar s)
+    //
+
+    // javadoc: Mat::Mat(size, type, s)
+    public Mat(Size size, int type, Scalar s) {
+        nativeObj = n_Mat(size.width, size.height, type, s.val[0], s.val[1], s.val[2], s.val[3]);
+    }
+
+    //
+    // C++: Mat::Mat(int ndims, const int* sizes, int type, Scalar s)
+    //
+
+    // javadoc: Mat::Mat(sizes, type, s)
+    public Mat(int[] sizes, int type, Scalar s) {
+        nativeObj = n_Mat(sizes.length, sizes, type, s.val[0], s.val[1], s.val[2], s.val[3]);
+    }
+
+    //
+    // C++: Mat::Mat(Mat m, Range rowRange, Range colRange = Range::all())
+    //
+
+    // javadoc: Mat::Mat(m, rowRange, colRange)
+    public Mat(Mat m, Range rowRange, Range colRange) {
+        nativeObj = n_Mat(m.nativeObj, rowRange.start, rowRange.end, colRange.start, colRange.end);
+    }
+
+    // javadoc: Mat::Mat(m, rowRange)
+    public Mat(Mat m, Range rowRange) {
+        nativeObj = n_Mat(m.nativeObj, rowRange.start, rowRange.end);
+    }
+
+    //
+    // C++: Mat::Mat(const Mat& m, const std::vector<Range>& ranges)
+    //
+
+    // javadoc: Mat::Mat(m, ranges)
+    public Mat(Mat m, Range[] ranges) {
+        nativeObj = n_Mat(m.nativeObj, ranges);
+    }
+
+    //
+    // C++: Mat::Mat(Mat m, Rect roi)
+    //
+
+    // javadoc: Mat::Mat(m, roi)
+    public Mat(Mat m, Rect roi) {
+        nativeObj = n_Mat(m.nativeObj, roi.y, roi.y + roi.height, roi.x, roi.x + roi.width);
+    }
+
+    //
+    // C++: Mat Mat::adjustROI(int dtop, int dbottom, int dleft, int dright)
+    //
+
+    // javadoc: Mat::adjustROI(dtop, dbottom, dleft, dright)
+    public Mat adjustROI(int dtop, int dbottom, int dleft, int dright) {
+        return new Mat(n_adjustROI(nativeObj, dtop, dbottom, dleft, dright));
+    }
+
+    //
+    // C++: void Mat::assignTo(Mat m, int type = -1)
+    //
+
+    // javadoc: Mat::assignTo(m, type)
+    public void assignTo(Mat m, int type) {
+        n_assignTo(nativeObj, m.nativeObj, type);
+    }
+
+    // javadoc: Mat::assignTo(m)
+    public void assignTo(Mat m) {
+        n_assignTo(nativeObj, m.nativeObj);
+    }
+
+    //
+    // C++: int Mat::channels()
+    //
+
+    // javadoc: Mat::channels()
+    public int channels() {
+        return n_channels(nativeObj);
+    }
+
+    //
+    // C++: int Mat::checkVector(int elemChannels, int depth = -1, bool
+    // requireContinuous = true)
+    //
+
+    // javadoc: Mat::checkVector(elemChannels, depth, requireContinuous)
+    public int checkVector(int elemChannels, int depth, boolean requireContinuous) {
+        return n_checkVector(nativeObj, elemChannels, depth, requireContinuous);
+    }
+
+    // javadoc: Mat::checkVector(elemChannels, depth)
+    public int checkVector(int elemChannels, int depth) {
+        return n_checkVector(nativeObj, elemChannels, depth);
+    }
+
+    // javadoc: Mat::checkVector(elemChannels)
+    public int checkVector(int elemChannels) {
+        return n_checkVector(nativeObj, elemChannels);
+    }
+
+    //
+    // C++: Mat Mat::clone()
+    //
+
+    // javadoc: Mat::clone()
+    public Mat clone() {
+        return new Mat(n_clone(nativeObj));
+    }
+
+    //
+    // C++: Mat Mat::col(int x)
+    //
+
+    // javadoc: Mat::col(x)
+    public Mat col(int x) {
+        return new Mat(n_col(nativeObj, x));
+    }
+
+    //
+    // C++: Mat Mat::colRange(int startcol, int endcol)
+    //
+
+    // javadoc: Mat::colRange(startcol, endcol)
+    public Mat colRange(int startcol, int endcol) {
+        return new Mat(n_colRange(nativeObj, startcol, endcol));
+    }
+
+    //
+    // C++: Mat Mat::colRange(Range r)
+    //
+
+    // javadoc: Mat::colRange(r)
+    public Mat colRange(Range r) {
+        return new Mat(n_colRange(nativeObj, r.start, r.end));
+    }
+
+    //
+    // C++: int Mat::dims()
+    //
+
+    // javadoc: Mat::dims()
+    public int dims() {
+        return n_dims(nativeObj);
+    }
+
+    //
+    // C++: int Mat::cols()
+    //
+
+    // javadoc: Mat::cols()
+    public int cols() {
+        return n_cols(nativeObj);
+    }
+
+    //
+    // C++: void Mat::convertTo(Mat& m, int rtype, double alpha = 1, double beta
+    // = 0)
+    //
+
+    // javadoc: Mat::convertTo(m, rtype, alpha, beta)
+    public void convertTo(Mat m, int rtype, double alpha, double beta) {
+        n_convertTo(nativeObj, m.nativeObj, rtype, alpha, beta);
+    }
+
+    // javadoc: Mat::convertTo(m, rtype, alpha)
+    public void convertTo(Mat m, int rtype, double alpha) {
+        n_convertTo(nativeObj, m.nativeObj, rtype, alpha);
+    }
+
+    // javadoc: Mat::convertTo(m, rtype)
+    public void convertTo(Mat m, int rtype) {
+        n_convertTo(nativeObj, m.nativeObj, rtype);
+    }
+
+    //
+    // C++: void Mat::copyTo(Mat& m)
+    //
+
+    // javadoc: Mat::copyTo(m)
+    public void copyTo(Mat m) {
+        n_copyTo(nativeObj, m.nativeObj);
+    }
+
+    //
+    // C++: void Mat::copyTo(Mat& m, Mat mask)
+    //
+
+    // javadoc: Mat::copyTo(m, mask)
+    public void copyTo(Mat m, Mat mask) {
+        n_copyTo(nativeObj, m.nativeObj, mask.nativeObj);
+    }
+
+    //
+    // C++: void Mat::create(int rows, int cols, int type)
+    //
+
+    // javadoc: Mat::create(rows, cols, type)
+    public void create(int rows, int cols, int type) {
+        n_create(nativeObj, rows, cols, type);
+    }
+
+    //
+    // C++: void Mat::create(Size size, int type)
+    //
+
+    // javadoc: Mat::create(size, type)
+    public void create(Size size, int type) {
+        n_create(nativeObj, size.width, size.height, type);
+    }
+
+    //
+    // C++: void Mat::create(int ndims, const int* sizes, int type)
+    //
+
+    // javadoc: Mat::create(sizes, type)
+    public void create(int[] sizes, int type) {
+        n_create(nativeObj, sizes.length, sizes, type);
+    }
+
+    //
+    // C++: void Mat::copySize(const Mat& m);
+    //
+
+    // javadoc: Mat::copySize(m)
+    public void copySize(Mat m) {
+        n_copySize(nativeObj, m.nativeObj);
+    }
+
+    //
+    // C++: Mat Mat::cross(Mat m)
+    //
+
+    // javadoc: Mat::cross(m)
+    public Mat cross(Mat m) {
+        return new Mat(n_cross(nativeObj, m.nativeObj));
+    }
+
+    //
+    // C++: long Mat::dataAddr()
+    //
+
+    // javadoc: Mat::dataAddr()
+    public long dataAddr() {
+        return n_dataAddr(nativeObj);
+    }
+
+    //
+    // C++: int Mat::depth()
+    //
+
+    // javadoc: Mat::depth()
+    public int depth() {
+        return n_depth(nativeObj);
+    }
+
+    //
+    // C++: Mat Mat::diag(int d = 0)
+    //
+
+    // javadoc: Mat::diag(d)
+    public Mat diag(int d) {
+        return new Mat(n_diag(nativeObj, d));
+    }
+
+    // javadoc: Mat::diag()
+    public Mat diag() {
+        return new Mat(n_diag(nativeObj, 0));
+    }
+
+    //
+    // C++: static Mat Mat::diag(Mat d)
+    //
+
+    // javadoc: Mat::diag(d)
+    public static Mat diag(Mat d) {
+        return new Mat(n_diag(d.nativeObj));
+    }
+
+    //
+    // C++: double Mat::dot(Mat m)
+    //
+
+    // javadoc: Mat::dot(m)
+    public double dot(Mat m) {
+        return n_dot(nativeObj, m.nativeObj);
+    }
+
+    //
+    // C++: size_t Mat::elemSize()
+    //
+
+    // javadoc: Mat::elemSize()
+    public long elemSize() {
+        return n_elemSize(nativeObj);
+    }
+
+    //
+    // C++: size_t Mat::elemSize1()
+    //
+
+    // javadoc: Mat::elemSize1()
+    public long elemSize1() {
+        return n_elemSize1(nativeObj);
+    }
+
+    //
+    // C++: bool Mat::empty()
+    //
+
+    // javadoc: Mat::empty()
+    public boolean empty() {
+        return n_empty(nativeObj);
+    }
+
+    //
+    // C++: static Mat Mat::eye(int rows, int cols, int type)
+    //
+
+    // javadoc: Mat::eye(rows, cols, type)
+    public static Mat eye(int rows, int cols, int type) {
+        return new Mat(n_eye(rows, cols, type));
+    }
+
+    //
+    // C++: static Mat Mat::eye(Size size, int type)
+    //
+
+    // javadoc: Mat::eye(size, type)
+    public static Mat eye(Size size, int type) {
+        return new Mat(n_eye(size.width, size.height, type));
+    }
+
+    //
+    // C++: Mat Mat::inv(int method = DECOMP_LU)
+    //
+
+    // javadoc: Mat::inv(method)
+    public Mat inv(int method) {
+        return new Mat(n_inv(nativeObj, method));
+    }
+
+    // javadoc: Mat::inv()
+    public Mat inv() {
+        return new Mat(n_inv(nativeObj));
+    }
+
+    //
+    // C++: bool Mat::isContinuous()
+    //
+
+    // javadoc: Mat::isContinuous()
+    public boolean isContinuous() {
+        return n_isContinuous(nativeObj);
+    }
+
+    //
+    // C++: bool Mat::isSubmatrix()
+    //
+
+    // javadoc: Mat::isSubmatrix()
+    public boolean isSubmatrix() {
+        return n_isSubmatrix(nativeObj);
+    }
+
+    //
+    // C++: void Mat::locateROI(Size wholeSize, Point ofs)
+    //
+
+    // javadoc: Mat::locateROI(wholeSize, ofs)
+    public void locateROI(Size wholeSize, Point ofs) {
+        double[] wholeSize_out = new double[2];
+        double[] ofs_out = new double[2];
+        locateROI_0(nativeObj, wholeSize_out, ofs_out);
+        if (wholeSize != null) {
+            wholeSize.width = wholeSize_out[0];
+            wholeSize.height = wholeSize_out[1];
+        }
+        if (ofs != null) {
+            ofs.x = ofs_out[0];
+            ofs.y = ofs_out[1];
+        }
+    }
+
+    //
+    // C++: Mat Mat::mul(Mat m, double scale = 1)
+    //
+
+    // javadoc: Mat::mul(m, scale)
+    public Mat mul(Mat m, double scale) {
+        return new Mat(n_mul(nativeObj, m.nativeObj, scale));
+    }
+
+    // javadoc: Mat::mul(m)
+    public Mat mul(Mat m) {
+        return new Mat(n_mul(nativeObj, m.nativeObj));
+    }
+
+    //
+    // C++: static Mat Mat::ones(int rows, int cols, int type)
+    //
+
+    // javadoc: Mat::ones(rows, cols, type)
+    public static Mat ones(int rows, int cols, int type) {
+        return new Mat(n_ones(rows, cols, type));
+    }
+
+    //
+    // C++: static Mat Mat::ones(Size size, int type)
+    //
+
+    // javadoc: Mat::ones(size, type)
+    public static Mat ones(Size size, int type) {
+        return new Mat(n_ones(size.width, size.height, type));
+    }
+
+    //
+    // C++: static Mat Mat::ones(int ndims, const int* sizes, int type)
+    //
+
+    // javadoc: Mat::ones(sizes, type)
+    public static Mat ones(int[] sizes, int type) {
+        return new Mat(n_ones(sizes.length, sizes, type));
+    }
+
+    //
+    // C++: void Mat::push_back(Mat m)
+    //
+
+    // javadoc: Mat::push_back(m)
+    public void push_back(Mat m) {
+        n_push_back(nativeObj, m.nativeObj);
+    }
+
+    //
+    // C++: void Mat::release()
+    //
+
+    // javadoc: Mat::release()
+    public void release() {
+        n_release(nativeObj);
+    }
+
+    //
+    // C++: Mat Mat::reshape(int cn, int rows = 0)
+    //
+
+    // javadoc: Mat::reshape(cn, rows)
+    public Mat reshape(int cn, int rows) {
+        return new Mat(n_reshape(nativeObj, cn, rows));
+    }
+
+    // javadoc: Mat::reshape(cn)
+    public Mat reshape(int cn) {
+        return new Mat(n_reshape(nativeObj, cn));
+    }
+
+    //
+    // C++: Mat Mat::reshape(int cn, int newndims, const int* newsz)
+    //
+
+    // javadoc: Mat::reshape(cn, newshape)
+    public Mat reshape(int cn, int[] newshape) {
+        return new Mat(n_reshape_1(nativeObj, cn, newshape.length, newshape));
+    }
+
+    //
+    // C++: Mat Mat::row(int y)
+    //
+
+    // javadoc: Mat::row(y)
+    public Mat row(int y) {
+        return new Mat(n_row(nativeObj, y));
+    }
+
+    //
+    // C++: Mat Mat::rowRange(int startrow, int endrow)
+    //
+
+    // javadoc: Mat::rowRange(startrow, endrow)
+    public Mat rowRange(int startrow, int endrow) {
+        return new Mat(n_rowRange(nativeObj, startrow, endrow));
+    }
+
+    //
+    // C++: Mat Mat::rowRange(Range r)
+    //
+
+    // javadoc: Mat::rowRange(r)
+    public Mat rowRange(Range r) {
+        return new Mat(n_rowRange(nativeObj, r.start, r.end));
+    }
+
+    //
+    // C++: int Mat::rows()
+    //
+
+    // javadoc: Mat::rows()
+    public int rows() {
+        return n_rows(nativeObj);
+    }
+
+    //
+    // C++: Mat Mat::operator =(Scalar s)
+    //
+
+    // javadoc: Mat::operator =(s)
+    public Mat setTo(Scalar s) {
+        return new Mat(n_setTo(nativeObj, s.val[0], s.val[1], s.val[2], s.val[3]));
+    }
+
+    //
+    // C++: Mat Mat::setTo(Scalar value, Mat mask = Mat())
+    //
+
+    // javadoc: Mat::setTo(value, mask)
+    public Mat setTo(Scalar value, Mat mask) {
+        return new Mat(n_setTo(nativeObj, value.val[0], value.val[1], value.val[2], value.val[3], mask.nativeObj));
+    }
+
+    //
+    // C++: Mat Mat::setTo(Mat value, Mat mask = Mat())
+    //
+
+    // javadoc: Mat::setTo(value, mask)
+    public Mat setTo(Mat value, Mat mask) {
+        return new Mat(n_setTo(nativeObj, value.nativeObj, mask.nativeObj));
+    }
+
+    // javadoc: Mat::setTo(value)
+    public Mat setTo(Mat value) {
+        return new Mat(n_setTo(nativeObj, value.nativeObj));
+    }
+
+    //
+    // C++: Size Mat::size()
+    //
+
+    // javadoc: Mat::size()
+    public Size size() {
+        return new Size(n_size(nativeObj));
+    }
+
+    //
+    // C++: int Mat::size(int i)
+    //
+
+    // javadoc: Mat::size(int i)
+    public int size(int i) {
+        return n_size_i(nativeObj, i);
+    }
+
+    //
+    // C++: size_t Mat::step1(int i = 0)
+    //
+
+    // javadoc: Mat::step1(i)
+    public long step1(int i) {
+        return n_step1(nativeObj, i);
+    }
+
+    // javadoc: Mat::step1()
+    public long step1() {
+        return n_step1(nativeObj);
+    }
+
+    //
+    // C++: Mat Mat::operator()(int rowStart, int rowEnd, int colStart, int
+    // colEnd)
+    //
+
+    // javadoc: Mat::operator()(rowStart, rowEnd, colStart, colEnd)
+    public Mat submat(int rowStart, int rowEnd, int colStart, int colEnd) {
+        return new Mat(n_submat_rr(nativeObj, rowStart, rowEnd, colStart, colEnd));
+    }
+
+    //
+    // C++: Mat Mat::operator()(Range rowRange, Range colRange)
+    //
+
+    // javadoc: Mat::operator()(rowRange, colRange)
+    public Mat submat(Range rowRange, Range colRange) {
+        return new Mat(n_submat_rr(nativeObj, rowRange.start, rowRange.end, colRange.start, colRange.end));
+    }
+
+    //
+    // C++: Mat Mat::operator()(const std::vector<Range>& ranges)
+    //
+
+    // javadoc: Mat::operator()(ranges[])
+    public Mat submat(Range[] ranges) {
+        return new Mat(n_submat_ranges(nativeObj, ranges));
+    }
+
+    //
+    // C++: Mat Mat::operator()(Rect roi)
+    //
+
+    // javadoc: Mat::operator()(roi)
+    public Mat submat(Rect roi) {
+        return new Mat(n_submat(nativeObj, roi.x, roi.y, roi.width, roi.height));
+    }
+
+    //
+    // C++: Mat Mat::t()
+    //
+
+    // javadoc: Mat::t()
+    public Mat t() {
+        return new Mat(n_t(nativeObj));
+    }
+
+    //
+    // C++: size_t Mat::total()
+    //
+
+    // javadoc: Mat::total()
+    public long total() {
+        return n_total(nativeObj);
+    }
+
+    //
+    // C++: int Mat::type()
+    //
+
+    // javadoc: Mat::type()
+    public int type() {
+        return n_type(nativeObj);
+    }
+
+    //
+    // C++: static Mat Mat::zeros(int rows, int cols, int type)
+    //
+
+    // javadoc: Mat::zeros(rows, cols, type)
+    public static Mat zeros(int rows, int cols, int type) {
+        return new Mat(n_zeros(rows, cols, type));
+    }
+
+    //
+    // C++: static Mat Mat::zeros(Size size, int type)
+    //
+
+    // javadoc: Mat::zeros(size, type)
+    public static Mat zeros(Size size, int type) {
+        return new Mat(n_zeros(size.width, size.height, type));
+    }
+
+    //
+    // C++: static Mat Mat::zeros(int ndims, const int* sizes, int type)
+    //
+
+    // javadoc: Mat::zeros(sizes, type)
+    public static Mat zeros(int[] sizes, int type) {
+        return new Mat(n_zeros(sizes.length, sizes, type));
+    }
+
+    @Override
+    protected void finalize() throws Throwable {
+        n_delete(nativeObj);
+        super.finalize();
+    }
+
+    // javadoc:Mat::toString()
+    @Override
+    public String toString() {
+        String _dims = (dims() > 0) ? "" : "-1*-1*";
+        for (int i=0; i<dims(); i++) {
+            _dims += size(i) + "*";
+        }
+        return "Mat [ " + _dims + CvType.typeToString(type()) +
+                ", isCont=" + isContinuous() + ", isSubmat=" + isSubmatrix() +
+                ", nativeObj=0x" + Long.toHexString(nativeObj) +
+                ", dataAddr=0x" + Long.toHexString(dataAddr()) +
+                " ]";
+    }
+
+    // javadoc:Mat::dump()
+    public String dump() {
+        return nDump(nativeObj);
+    }
+
+    // javadoc:Mat::put(row,col,data)
+    public int put(int row, int col, double... data) {
+        int t = type();
+        if (data == null || data.length % CvType.channels(t) != 0)
+            throw new UnsupportedOperationException(
+                    "Provided data element number (" +
+                            (data == null ? 0 : data.length) +
+                            ") should be multiple of the Mat channels count (" +
+                            CvType.channels(t) + ")");
+        return nPutD(nativeObj, row, col, data.length, data);
+    }
+
+    // javadoc:Mat::put(idx,data)
+    public int put(int[] idx, double... data) {
+        int t = type();
+        if (data == null || data.length % CvType.channels(t) != 0)
+            throw new UnsupportedOperationException(
+                    "Provided data element number (" +
+                            (data == null ? 0 : data.length) +
+                            ") should be multiple of the Mat channels count (" +
+                            CvType.channels(t) + ")");
+        if (idx.length != dims())
+            throw new IllegalArgumentException("Incorrect number of indices");
+        return nPutDIdx(nativeObj, idx, data.length, data);
+    }
+
+    // javadoc:Mat::put(row,col,data)
+    public int put(int row, int col, float[] data) {
+        int t = type();
+        if (data == null || data.length % CvType.channels(t) != 0)
+            throw new UnsupportedOperationException(
+                    "Provided data element number (" +
+                            (data == null ? 0 : data.length) +
+                            ") should be multiple of the Mat channels count (" +
+                            CvType.channels(t) + ")");
+        if (CvType.depth(t) == CvType.CV_32F) {
+            return nPutF(nativeObj, row, col, data.length, data);
+        }
+        throw new UnsupportedOperationException("Mat data type is not compatible: " + t);
+    }
+
+    // javadoc:Mat::put(idx,data)
+    public int put(int[] idx, float[] data) {
+        int t = type();
+        if (data == null || data.length % CvType.channels(t) != 0)
+            throw new UnsupportedOperationException(
+                    "Provided data element number (" +
+                            (data == null ? 0 : data.length) +
+                            ") should be multiple of the Mat channels count (" +
+                            CvType.channels(t) + ")");
+        if (idx.length != dims())
+            throw new IllegalArgumentException("Incorrect number of indices");
+        if (CvType.depth(t) == CvType.CV_32F) {
+            return nPutFIdx(nativeObj, idx, data.length, data);
+        }
+        throw new UnsupportedOperationException("Mat data type is not compatible: " + t);
+    }
+
+    // javadoc:Mat::put(row,col,data)
+    public int put(int row, int col, int[] data) {
+        int t = type();
+        if (data == null || data.length % CvType.channels(t) != 0)
+            throw new UnsupportedOperationException(
+                    "Provided data element number (" +
+                            (data == null ? 0 : data.length) +
+                            ") should be multiple of the Mat channels count (" +
+                            CvType.channels(t) + ")");
+        if (CvType.depth(t) == CvType.CV_32S) {
+            return nPutI(nativeObj, row, col, data.length, data);
+        }
+        throw new UnsupportedOperationException("Mat data type is not compatible: " + t);
+    }
+
+    // javadoc:Mat::put(idx,data)
+    public int put(int[] idx, int[] data) {
+        int t = type();
+        if (data == null || data.length % CvType.channels(t) != 0)
+            throw new UnsupportedOperationException(
+                    "Provided data element number (" +
+                            (data == null ? 0 : data.length) +
+                            ") should be multiple of the Mat channels count (" +
+                            CvType.channels(t) + ")");
+        if (idx.length != dims())
+            throw new IllegalArgumentException("Incorrect number of indices");
+        if (CvType.depth(t) == CvType.CV_32S) {
+            return nPutIIdx(nativeObj, idx, data.length, data);
+        }
+        throw new UnsupportedOperationException("Mat data type is not compatible: " + t);
+    }
+
+    // javadoc:Mat::put(row,col,data)
+    public int put(int row, int col, short[] data) {
+        int t = type();
+        if (data == null || data.length % CvType.channels(t) != 0)
+            throw new UnsupportedOperationException(
+                    "Provided data element number (" +
+                            (data == null ? 0 : data.length) +
+                            ") should be multiple of the Mat channels count (" +
+                            CvType.channels(t) + ")");
+        if (CvType.depth(t) == CvType.CV_16U || CvType.depth(t) == CvType.CV_16S) {
+            return nPutS(nativeObj, row, col, data.length, data);
+        }
+        throw new UnsupportedOperationException("Mat data type is not compatible: " + t);
+    }
+
+    // javadoc:Mat::put(idx,data)
+    public int put(int[] idx, short[] data) {
+        int t = type();
+        if (data == null || data.length % CvType.channels(t) != 0)
+            throw new UnsupportedOperationException(
+                    "Provided data element number (" +
+                            (data == null ? 0 : data.length) +
+                            ") should be multiple of the Mat channels count (" +
+                            CvType.channels(t) + ")");
+        if (idx.length != dims())
+            throw new IllegalArgumentException("Incorrect number of indices");
+        if (CvType.depth(t) == CvType.CV_16U || CvType.depth(t) == CvType.CV_16S) {
+            return nPutSIdx(nativeObj, idx, data.length, data);
+        }
+        throw new UnsupportedOperationException("Mat data type is not compatible: " + t);
+    }
+
+    // javadoc:Mat::put(row,col,data)
+    public int put(int row, int col, byte[] data) {
+        int t = type();
+        if (data == null || data.length % CvType.channels(t) != 0)
+            throw new UnsupportedOperationException(
+                    "Provided data element number (" +
+                            (data == null ? 0 : data.length) +
+                            ") should be multiple of the Mat channels count (" +
+                            CvType.channels(t) + ")");
+        if (CvType.depth(t) == CvType.CV_8U || CvType.depth(t) == CvType.CV_8S) {
+            return nPutB(nativeObj, row, col, data.length, data);
+        }
+        throw new UnsupportedOperationException("Mat data type is not compatible: " + t);
+    }
+
+    // javadoc:Mat::put(idx,data)
+    public int put(int[] idx, byte[] data) {
+        int t = type();
+        if (data == null || data.length % CvType.channels(t) != 0)
+            throw new UnsupportedOperationException(
+                    "Provided data element number (" +
+                            (data == null ? 0 : data.length) +
+                            ") should be multiple of the Mat channels count (" +
+                            CvType.channels(t) + ")");
+        if (idx.length != dims())
+            throw new IllegalArgumentException("Incorrect number of indices");
+        if (CvType.depth(t) == CvType.CV_8U || CvType.depth(t) == CvType.CV_8S) {
+            return nPutBIdx(nativeObj, idx, data.length, data);
+        }
+        throw new UnsupportedOperationException("Mat data type is not compatible: " + t);
+    }
+
+    // javadoc:Mat::put(row,col,data,offset,length)
+    public int put(int row, int col, byte[] data, int offset, int length) {
+        int t = type();
+        if (data == null || length % CvType.channels(t) != 0)
+            throw new UnsupportedOperationException(
+                    "Provided data element number (" +
+                            (data == null ? 0 : data.length) +
+                            ") should be multiple of the Mat channels count (" +
+                            CvType.channels(t) + ")");
+        if (CvType.depth(t) == CvType.CV_8U || CvType.depth(t) == CvType.CV_8S) {
+            return nPutBwOffset(nativeObj, row, col, length, offset, data);
+        }
+        throw new UnsupportedOperationException("Mat data type is not compatible: " + t);
+    }
+
+    // javadoc:Mat::put(idx,data,offset,length)
+    public int put(int[] idx, byte[] data, int offset, int length) {
+        int t = type();
+        if (data == null || length % CvType.channels(t) != 0)
+            throw new UnsupportedOperationException(
+                    "Provided data element number (" +
+                            (data == null ? 0 : data.length) +
+                            ") should be multiple of the Mat channels count (" +
+                            CvType.channels(t) + ")");
+        if (idx.length != dims())
+            throw new IllegalArgumentException("Incorrect number of indices");
+        if (CvType.depth(t) == CvType.CV_8U || CvType.depth(t) == CvType.CV_8S) {
+            return nPutBwIdxOffset(nativeObj, idx, length, offset, data);
+        }
+        throw new UnsupportedOperationException("Mat data type is not compatible: " + t);
+    }
+
+    // javadoc:Mat::get(row,col,data)
+    public int get(int row, int col, byte[] data) {
+        int t = type();
+        if (data == null || data.length % CvType.channels(t) != 0)
+            throw new UnsupportedOperationException(
+                    "Provided data element number (" +
+                            (data == null ? 0 : data.length) +
+                            ") should be multiple of the Mat channels count (" +
+                            CvType.channels(t) + ")");
+        if (CvType.depth(t) == CvType.CV_8U || CvType.depth(t) == CvType.CV_8S) {
+            return nGetB(nativeObj, row, col, data.length, data);
+        }
+        throw new UnsupportedOperationException("Mat data type is not compatible: " + t);
+    }
+
+    // javadoc:Mat::get(idx,data)
+    public int get(int[] idx, byte[] data) {
+        int t = type();
+        if (data == null || data.length % CvType.channels(t) != 0)
+            throw new UnsupportedOperationException(
+                    "Provided data element number (" +
+                            (data == null ? 0 : data.length) +
+                            ") should be multiple of the Mat channels count (" +
+                            CvType.channels(t) + ")");
+        if (idx.length != dims())
+            throw new IllegalArgumentException("Incorrect number of indices");
+        if (CvType.depth(t) == CvType.CV_8U || CvType.depth(t) == CvType.CV_8S) {
+            return nGetBIdx(nativeObj, idx, data.length, data);
+        }
+        throw new UnsupportedOperationException("Mat data type is not compatible: " + t);
+    }
+
+    // javadoc:Mat::get(row,col,data)
+    public int get(int row, int col, short[] data) {
+        int t = type();
+        if (data == null || data.length % CvType.channels(t) != 0)
+            throw new UnsupportedOperationException(
+                    "Provided data element number (" +
+                            (data == null ? 0 : data.length) +
+                            ") should be multiple of the Mat channels count (" +
+                            CvType.channels(t) + ")");
+        if (CvType.depth(t) == CvType.CV_16U || CvType.depth(t) == CvType.CV_16S) {
+            return nGetS(nativeObj, row, col, data.length, data);
+        }
+        throw new UnsupportedOperationException("Mat data type is not compatible: " + t);
+    }
+
+    // javadoc:Mat::get(idx,data)
+    public int get(int[] idx, short[] data) {
+        int t = type();
+        if (data == null || data.length % CvType.channels(t) != 0)
+            throw new UnsupportedOperationException(
+                    "Provided data element number (" +
+                            (data == null ? 0 : data.length) +
+                            ") should be multiple of the Mat channels count (" +
+                            CvType.channels(t) + ")");
+        if (idx.length != dims())
+            throw new IllegalArgumentException("Incorrect number of indices");
+        if (CvType.depth(t) == CvType.CV_16U || CvType.depth(t) == CvType.CV_16S) {
+            return nGetSIdx(nativeObj, idx, data.length, data);
+        }
+        throw new UnsupportedOperationException("Mat data type is not compatible: " + t);
+    }
+
+    // javadoc:Mat::get(row,col,data)
+    public int get(int row, int col, int[] data) {
+        int t = type();
+        if (data == null || data.length % CvType.channels(t) != 0)
+            throw new UnsupportedOperationException(
+                    "Provided data element number (" +
+                            (data == null ? 0 : data.length) +
+                            ") should be multiple of the Mat channels count (" +
+                            CvType.channels(t) + ")");
+        if (CvType.depth(t) == CvType.CV_32S) {
+            return nGetI(nativeObj, row, col, data.length, data);
+        }
+        throw new UnsupportedOperationException("Mat data type is not compatible: " + t);
+    }
+
+    // javadoc:Mat::get(idx,data)
+    public int get(int[] idx, int[] data) {
+        int t = type();
+        if (data == null || data.length % CvType.channels(t) != 0)
+            throw new UnsupportedOperationException(
+                    "Provided data element number (" +
+                            (data == null ? 0 : data.length) +
+                            ") should be multiple of the Mat channels count (" +
+                            CvType.channels(t) + ")");
+        if (idx.length != dims())
+            throw new IllegalArgumentException("Incorrect number of indices");
+        if (CvType.depth(t) == CvType.CV_32S) {
+            return nGetIIdx(nativeObj, idx, data.length, data);
+        }
+        throw new UnsupportedOperationException("Mat data type is not compatible: " + t);
+    }
+
+    // javadoc:Mat::get(row,col,data)
+    public int get(int row, int col, float[] data) {
+        int t = type();
+        if (data == null || data.length % CvType.channels(t) != 0)
+            throw new UnsupportedOperationException(
+                    "Provided data element number (" +
+                            (data == null ? 0 : data.length) +
+                            ") should be multiple of the Mat channels count (" +
+                            CvType.channels(t) + ")");
+        if (CvType.depth(t) == CvType.CV_32F) {
+            return nGetF(nativeObj, row, col, data.length, data);
+        }
+        throw new UnsupportedOperationException("Mat data type is not compatible: " + t);
+    }
+
+    // javadoc:Mat::get(idx,data)
+    public int get(int[] idx, float[] data) {
+        int t = type();
+        if (data == null || data.length % CvType.channels(t) != 0)
+            throw new UnsupportedOperationException(
+                    "Provided data element number (" +
+                            (data == null ? 0 : data.length) +
+                            ") should be multiple of the Mat channels count (" +
+                            CvType.channels(t) + ")");
+        if (idx.length != dims())
+            throw new IllegalArgumentException("Incorrect number of indices");
+        if (CvType.depth(t) == CvType.CV_32F) {
+            return nGetFIdx(nativeObj, idx, data.length, data);
+        }
+        throw new UnsupportedOperationException("Mat data type is not compatible: " + t);
+    }
+
+    // javadoc:Mat::get(row,col,data)
+    public int get(int row, int col, double[] data) {
+        int t = type();
+        if (data == null || data.length % CvType.channels(t) != 0)
+            throw new UnsupportedOperationException(
+                    "Provided data element number (" +
+                            (data == null ? 0 : data.length) +
+                            ") should be multiple of the Mat channels count (" +
+                            CvType.channels(t) + ")");
+        if (CvType.depth(t) == CvType.CV_64F) {
+            return nGetD(nativeObj, row, col, data.length, data);
+        }
+        throw new UnsupportedOperationException("Mat data type is not compatible: " + t);
+    }
+
+    // javadoc:Mat::get(idx,data)
+    public int get(int[] idx, double[] data) {
+        int t = type();
+        if (data == null || data.length % CvType.channels(t) != 0)
+            throw new UnsupportedOperationException(
+                    "Provided data element number (" +
+                            (data == null ? 0 : data.length) +
+                            ") should be multiple of the Mat channels count (" +
+                            CvType.channels(t) + ")");
+        if (idx.length != dims())
+            throw new IllegalArgumentException("Incorrect number of indices");
+        if (CvType.depth(t) == CvType.CV_64F) {
+            return nGetDIdx(nativeObj, idx, data.length, data);
+        }
+        throw new UnsupportedOperationException("Mat data type is not compatible: " + t);
+    }
+
+    // javadoc:Mat::get(row,col)
+    public double[] get(int row, int col) {
+        return nGet(nativeObj, row, col);
+    }
+
+    // javadoc:Mat::get(idx)
+    public double[] get(int[] idx) {
+        if (idx.length != dims())
+            throw new IllegalArgumentException("Incorrect number of indices");
+        return nGetIdx(nativeObj, idx);
+    }
+
+    // javadoc:Mat::height()
+    public int height() {
+        return rows();
+    }
+
+    // javadoc:Mat::width()
+    public int width() {
+        return cols();
+    }
+
+    // javadoc:Mat::getNativeObjAddr()
+    public long getNativeObjAddr() {
+        return nativeObj;
+    }
+
+    // C++: Mat::Mat()
+    private static native long n_Mat();
+
+    // C++: Mat::Mat(int rows, int cols, int type)
+    private static native long n_Mat(int rows, int cols, int type);
+
+    // C++: Mat::Mat(int ndims, const int* sizes, int type)
+    private static native long n_Mat(int ndims, int[] sizes, int type);
+
+    // C++: Mat::Mat(int rows, int cols, int type, void* data)
+    private static native long n_Mat(int rows, int cols, int type, ByteBuffer data);
+
+    // C++: Mat::Mat(int rows, int cols, int type, void* data, size_t step)
+    private static native long n_Mat(int rows, int cols, int type, ByteBuffer data, long step);
+
+    // C++: Mat::Mat(Size size, int type)
+    private static native long n_Mat(double size_width, double size_height, int type);
+
+    // C++: Mat::Mat(int rows, int cols, int type, Scalar s)
+    private static native long n_Mat(int rows, int cols, int type, double s_val0, double s_val1, double s_val2, double s_val3);
+
+    // C++: Mat::Mat(Size size, int type, Scalar s)
+    private static native long n_Mat(double size_width, double size_height, int type, double s_val0, double s_val1, double s_val2, double s_val3);
+
+    // C++: Mat::Mat(int ndims, const int* sizes, int type, Scalar s)
+    private static native long n_Mat(int ndims, int[] sizes, int type, double s_val0, double s_val1, double s_val2, double s_val3);
+
+    // C++: Mat::Mat(Mat m, Range rowRange, Range colRange = Range::all())
+    private static native long n_Mat(long m_nativeObj, int rowRange_start, int rowRange_end, int colRange_start, int colRange_end);
+
+    private static native long n_Mat(long m_nativeObj, int rowRange_start, int rowRange_end);
+
+    // C++: Mat::Mat(const Mat& m, const std::vector<Range>& ranges)
+    private static native long n_Mat(long m_nativeObj, Range[] ranges);
+
+    // C++: Mat Mat::adjustROI(int dtop, int dbottom, int dleft, int dright)
+    private static native long n_adjustROI(long nativeObj, int dtop, int dbottom, int dleft, int dright);
+
+    // C++: void Mat::assignTo(Mat m, int type = -1)
+    private static native void n_assignTo(long nativeObj, long m_nativeObj, int type);
+
+    private static native void n_assignTo(long nativeObj, long m_nativeObj);
+
+    // C++: int Mat::channels()
+    private static native int n_channels(long nativeObj);
+
+    // C++: int Mat::checkVector(int elemChannels, int depth = -1, bool
+    // requireContinuous = true)
+    private static native int n_checkVector(long nativeObj, int elemChannels, int depth, boolean requireContinuous);
+
+    private static native int n_checkVector(long nativeObj, int elemChannels, int depth);
+
+    private static native int n_checkVector(long nativeObj, int elemChannels);
+
+    // C++: Mat Mat::clone()
+    private static native long n_clone(long nativeObj);
+
+    // C++: Mat Mat::col(int x)
+    private static native long n_col(long nativeObj, int x);
+
+    // C++: Mat Mat::colRange(int startcol, int endcol)
+    private static native long n_colRange(long nativeObj, int startcol, int endcol);
+
+    // C++: int Mat::dims()
+    private static native int n_dims(long nativeObj);
+
+    // C++: int Mat::cols()
+    private static native int n_cols(long nativeObj);
+
+    // C++: void Mat::convertTo(Mat& m, int rtype, double alpha = 1, double beta
+    // = 0)
+    private static native void n_convertTo(long nativeObj, long m_nativeObj, int rtype, double alpha, double beta);
+
+    private static native void n_convertTo(long nativeObj, long m_nativeObj, int rtype, double alpha);
+
+    private static native void n_convertTo(long nativeObj, long m_nativeObj, int rtype);
+
+    // C++: void Mat::copyTo(Mat& m)
+    private static native void n_copyTo(long nativeObj, long m_nativeObj);
+
+    // C++: void Mat::copyTo(Mat& m, Mat mask)
+    private static native void n_copyTo(long nativeObj, long m_nativeObj, long mask_nativeObj);
+
+    // C++: void Mat::create(int rows, int cols, int type)
+    private static native void n_create(long nativeObj, int rows, int cols, int type);
+
+    // C++: void Mat::create(Size size, int type)
+    private static native void n_create(long nativeObj, double size_width, double size_height, int type);
+
+    // C++: void Mat::create(int ndims, const int* sizes, int type)
+    private static native void n_create(long nativeObj, int ndims, int[] sizes, int type);
+
+    // C++: void Mat::copySize(const Mat& m)
+    private static native void n_copySize(long nativeObj, long m_nativeObj);
+
+    // C++: Mat Mat::cross(Mat m)
+    private static native long n_cross(long nativeObj, long m_nativeObj);
+
+    // C++: long Mat::dataAddr()
+    private static native long n_dataAddr(long nativeObj);
+
+    // C++: int Mat::depth()
+    private static native int n_depth(long nativeObj);
+
+    // C++: Mat Mat::diag(int d = 0)
+    private static native long n_diag(long nativeObj, int d);
+
+    // C++: static Mat Mat::diag(Mat d)
+    private static native long n_diag(long d_nativeObj);
+
+    // C++: double Mat::dot(Mat m)
+    private static native double n_dot(long nativeObj, long m_nativeObj);
+
+    // C++: size_t Mat::elemSize()
+    private static native long n_elemSize(long nativeObj);
+
+    // C++: size_t Mat::elemSize1()
+    private static native long n_elemSize1(long nativeObj);
+
+    // C++: bool Mat::empty()
+    private static native boolean n_empty(long nativeObj);
+
+    // C++: static Mat Mat::eye(int rows, int cols, int type)
+    private static native long n_eye(int rows, int cols, int type);
+
+    // C++: static Mat Mat::eye(Size size, int type)
+    private static native long n_eye(double size_width, double size_height, int type);
+
+    // C++: Mat Mat::inv(int method = DECOMP_LU)
+    private static native long n_inv(long nativeObj, int method);
+
+    private static native long n_inv(long nativeObj);
+
+    // C++: bool Mat::isContinuous()
+    private static native boolean n_isContinuous(long nativeObj);
+
+    // C++: bool Mat::isSubmatrix()
+    private static native boolean n_isSubmatrix(long nativeObj);
+
+    // C++: void Mat::locateROI(Size wholeSize, Point ofs)
+    private static native void locateROI_0(long nativeObj, double[] wholeSize_out, double[] ofs_out);
+
+    // C++: Mat Mat::mul(Mat m, double scale = 1)
+    private static native long n_mul(long nativeObj, long m_nativeObj, double scale);
+
+    private static native long n_mul(long nativeObj, long m_nativeObj);
+
+    // C++: static Mat Mat::ones(int rows, int cols, int type)
+    private static native long n_ones(int rows, int cols, int type);
+
+    // C++: static Mat Mat::ones(Size size, int type)
+    private static native long n_ones(double size_width, double size_height, int type);
+
+    // C++: static Mat Mat::ones(int ndims, const int* sizes, int type)
+    private static native long n_ones(int ndims, int[] sizes, int type);
+
+    // C++: void Mat::push_back(Mat m)
+    private static native void n_push_back(long nativeObj, long m_nativeObj);
+
+    // C++: void Mat::release()
+    private static native void n_release(long nativeObj);
+
+    // C++: Mat Mat::reshape(int cn, int rows = 0)
+    private static native long n_reshape(long nativeObj, int cn, int rows);
+
+    private static native long n_reshape(long nativeObj, int cn);
+
+    // C++: Mat Mat::reshape(int cn, int newndims, const int* newsz)
+    private static native long n_reshape_1(long nativeObj, int cn, int newndims, int[] newsz);
+
+    // C++: Mat Mat::row(int y)
+    private static native long n_row(long nativeObj, int y);
+
+    // C++: Mat Mat::rowRange(int startrow, int endrow)
+    private static native long n_rowRange(long nativeObj, int startrow, int endrow);
+
+    // C++: int Mat::rows()
+    private static native int n_rows(long nativeObj);
+
+    // C++: Mat Mat::operator =(Scalar s)
+    private static native long n_setTo(long nativeObj, double s_val0, double s_val1, double s_val2, double s_val3);
+
+    // C++: Mat Mat::setTo(Scalar value, Mat mask = Mat())
+    private static native long n_setTo(long nativeObj, double s_val0, double s_val1, double s_val2, double s_val3, long mask_nativeObj);
+
+    // C++: Mat Mat::setTo(Mat value, Mat mask = Mat())
+    private static native long n_setTo(long nativeObj, long value_nativeObj, long mask_nativeObj);
+
+    private static native long n_setTo(long nativeObj, long value_nativeObj);
+
+    // C++: Size Mat::size()
+    private static native double[] n_size(long nativeObj);
+
+    // C++: int Mat::size(int i)
+    private static native int n_size_i(long nativeObj, int i);
+
+    // C++: size_t Mat::step1(int i = 0)
+    private static native long n_step1(long nativeObj, int i);
+
+    private static native long n_step1(long nativeObj);
+
+    // C++: Mat Mat::operator()(Range rowRange, Range colRange)
+    private static native long n_submat_rr(long nativeObj, int rowRange_start, int rowRange_end, int colRange_start, int colRange_end);
+
+    // C++: Mat Mat::operator()(const std::vector<Range>& ranges)
+    private static native long n_submat_ranges(long nativeObj, Range[] ranges);
+
+    // C++: Mat Mat::operator()(Rect roi)
+    private static native long n_submat(long nativeObj, int roi_x, int roi_y, int roi_width, int roi_height);
+
+    // C++: Mat Mat::t()
+    private static native long n_t(long nativeObj);
+
+    // C++: size_t Mat::total()
+    private static native long n_total(long nativeObj);
+
+    // C++: int Mat::type()
+    private static native int n_type(long nativeObj);
+
+    // C++: static Mat Mat::zeros(int rows, int cols, int type)
+    private static native long n_zeros(int rows, int cols, int type);
+
+    // C++: static Mat Mat::zeros(Size size, int type)
+    private static native long n_zeros(double size_width, double size_height, int type);
+
+    // C++: static Mat Mat::zeros(int ndims, const int* sizes, int type)
+    private static native long n_zeros(int ndims, int[] sizes, int type);
+
+    // native support for java finalize()
+    private static native void n_delete(long nativeObj);
+
+    private static native int nPutD(long self, int row, int col, int count, double[] data);
+
+    private static native int nPutDIdx(long self, int[] idx, int count, double[] data);
+
+    private static native int nPutF(long self, int row, int col, int count, float[] data);
+
+    private static native int nPutFIdx(long self, int[] idx, int count, float[] data);
+
+    private static native int nPutI(long self, int row, int col, int count, int[] data);
+
+    private static native int nPutIIdx(long self, int[] idx, int count, int[] data);
+
+    private static native int nPutS(long self, int row, int col, int count, short[] data);
+
+    private static native int nPutSIdx(long self, int[] idx, int count, short[] data);
+
+    private static native int nPutB(long self, int row, int col, int count, byte[] data);
+
+    private static native int nPutBIdx(long self, int[] idx, int count, byte[] data);
+
+    private static native int nPutBwOffset(long self, int row, int col, int count, int offset, byte[] data);
+
+    private static native int nPutBwIdxOffset(long self, int[] idx, int count, int offset, byte[] data);
+
+    private static native int nGetB(long self, int row, int col, int count, byte[] vals);
+
+    private static native int nGetBIdx(long self, int[] idx, int count, byte[] vals);
+
+    private static native int nGetS(long self, int row, int col, int count, short[] vals);
+
+    private static native int nGetSIdx(long self, int[] idx, int count, short[] vals);
+
+    private static native int nGetI(long self, int row, int col, int count, int[] vals);
+
+    private static native int nGetIIdx(long self, int[] idx, int count, int[] vals);
+
+    private static native int nGetF(long self, int row, int col, int count, float[] vals);
+
+    private static native int nGetFIdx(long self, int[] idx, int count, float[] vals);
+
+    private static native int nGetD(long self, int row, int col, int count, double[] vals);
+
+    private static native int nGetDIdx(long self, int[] idx, int count, double[] vals);
+
+    private static native double[] nGet(long self, int row, int col);
+
+    private static native double[] nGetIdx(long self, int[] idx);
+
+    private static native String nDump(long self);
+}
Index: openCVLibrary3411/src/main/java/org/opencv/core/MatOfByte.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/core/MatOfByte.java	(date 1605830247322)
+++ openCVLibrary3411/src/main/java/org/opencv/core/MatOfByte.java	(date 1605830247322)
@@ -0,0 +1,98 @@
+package org.opencv.core;
+
+import java.util.Arrays;
+import java.util.List;
+
+public class MatOfByte extends Mat {
+    // 8UC(x)
+    private static final int _depth = CvType.CV_8U;
+    private static final int _channels = 1;
+
+    public MatOfByte() {
+        super();
+    }
+
+    protected MatOfByte(long addr) {
+        super(addr);
+        if( !empty() && checkVector(_channels, _depth) < 0 )
+            throw new IllegalArgumentException("Incompatible Mat");
+        //FIXME: do we need release() here?
+    }
+
+    public static MatOfByte fromNativeAddr(long addr) {
+        return new MatOfByte(addr);
+    }
+
+    public MatOfByte(Mat m) {
+        super(m, Range.all());
+        if( !empty() && checkVector(_channels, _depth) < 0 )
+            throw new IllegalArgumentException("Incompatible Mat");
+        //FIXME: do we need release() here?
+    }
+
+    public MatOfByte(byte...a) {
+        super();
+        fromArray(a);
+    }
+
+    public MatOfByte(int offset, int length, byte...a) {
+        super();
+        fromArray(offset, length, a);
+    }
+
+    public void alloc(int elemNumber) {
+        if(elemNumber>0)
+            super.create(elemNumber, 1, CvType.makeType(_depth, _channels));
+    }
+
+    public void fromArray(byte...a) {
+        if(a==null || a.length==0)
+            return;
+        int num = a.length / _channels;
+        alloc(num);
+        put(0, 0, a); //TODO: check ret val!
+    }
+
+    public void fromArray(int offset, int length, byte...a) {
+        if (offset < 0)
+            throw new IllegalArgumentException("offset < 0");
+        if (a == null)
+            throw new NullPointerException();
+        if (length < 0 || length + offset > a.length)
+            throw new IllegalArgumentException("invalid 'length' parameter: " + Integer.toString(length));
+        if (a.length == 0)
+            return;
+        int num = length / _channels;
+        alloc(num);
+        put(0, 0, a, offset, length); //TODO: check ret val!
+    }
+
+    public byte[] toArray() {
+        int num = checkVector(_channels, _depth);
+        if(num < 0)
+            throw new RuntimeException("Native Mat has unexpected type or size: " + toString());
+        byte[] a = new byte[num * _channels];
+        if(num == 0)
+            return a;
+        get(0, 0, a); //TODO: check ret val!
+        return a;
+    }
+
+    public void fromList(List<Byte> lb) {
+        if(lb==null || lb.size()==0)
+            return;
+        Byte ab[] = lb.toArray(new Byte[0]);
+        byte a[] = new byte[ab.length];
+        for(int i=0; i<ab.length; i++)
+            a[i] = ab[i];
+        fromArray(a);
+    }
+
+    public List<Byte> toList() {
+        byte[] a = toArray();
+        Byte ab[] = new Byte[a.length];
+        for(int i=0; i<a.length; i++)
+            ab[i] = a[i];
+        return Arrays.asList(ab);
+    }
+}
Index: app/src/main/java/com/maksym/findthis/CameraSearch.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- app/src/main/java/com/maksym/findthis/CameraSearch.java	(revision cc086539aa8e7367bf4575d32b97aa049d6faad9)
+++ app/src/main/java/com/maksym/findthis/CameraSearch.java	(date 1607977832387)
@@ -32,6 +32,7 @@
 import org.opencv.android.Utils;
 import org.opencv.core.Core;
 import org.opencv.core.Mat;
+import org.opencv.core.Rect;
 
 public class CameraSearch extends AppCompatActivity implements CustomCameraBridgeViewBase.CvCameraViewListener, MatchObjectsCallback {
 
@@ -65,6 +66,8 @@
     private ObjectEntity objectEntity;
     private Bitmap objectPhoto;
     private boolean detectorOperating, objectReceived = false;
+    private Mat inputFrameWithObject;
+    private DetectionEngine detectionEngine = DetectionEngine.getInstance();
 
     @Override
     public void onCreate(Bundle savedInstanceState) {
@@ -145,20 +148,41 @@
     @Override
     public Mat onCameraFrame(Mat inputFrame) {
         //Log.d(TAG,"on camera frame");
-        // DetectionEngine.getInstance().drawKeypoints(Constants.FAST_DETECTOR_ID, inputFrame);
-        //DetectionEngine.getInstance().drawKeypoints(Constants.BRISK_DETECTOR_ID, inputFrame);
-        // DetectionEngine.getInstance().drawKeypoints(Constants.SIFT_DETECTOR_ID, inputFrame);
-        // DetectionEngine.getInstance().drawKeypoints(Constants.AKAZE_DETECTOR_ID, inputFrame);
-        // DetectionEngine.getInstance().drawKeypoints(Constants.GFTT_DETECTOR_ID, inputFrame);
+//        DetectionEngine.getInstance().drawKeypoints(Constants.AKAZE_DETECTOR_ID, inputFrame);
+//        DetectionEngine.getInstance().drawKeypoints(Constants.BRISK_DETECTOR_ID, inputFrame);
+//        DetectionEngine.getInstance().drawKeypoints(Constants.FAST_DETECTOR_ID, inputFrame);
+//        DetectionEngine.getInstance().drawKeypoints(Constants.GFTT_DETECTOR_ID, inputFrame);
+//        DetectionEngine.getInstance().drawKeypoints(Constants.KAZE_DETECTOR_ID, inputFrame);
+//        DetectionEngine.getInstance().drawKeypoints(Constants.MSER_DETECTOR_ID, inputFrame);
+//        DetectionEngine.getInstance().drawKeypoints(Constants.ORB_DETECTOR_ID, inputFrame);
+//        DetectionEngine.getInstance().drawKeypoints(Constants.SIFT_DETECTOR_ID, inputFrame);
+
+        long start = System.nanoTime();
         if (!detectorOperating){
             detectorOperating = true;
             Mat objectMat = new Mat();
-            DetectionEngine detectionEngine = DetectionEngine.getInstance();
-
             Utils.bitmapToMat(objectPhoto, objectMat);
-//            detectionEngine.matchObjects(objectMat, inputFrame, detectionEngine.selectDetector(objectEntity.getDetectorType()), this);
             detectionEngine.matchObjectsThread(objectMat, inputFrame, detectionEngine.selectDetector(objectEntity.getDetectorType()), this);
         }
+        if (objectReceived) {
+
+            detectionEngine.matchObjects(inputFrameWithObject, inputFrame, detectionEngine.selectTracker(Constants.ORB_DETECTOR_ID));
+
+        }
+
+//        Mat objectMat = new Mat();
+//        Utils.bitmapToMat(objectPhoto, objectMat);
+//        Utils.bitmapToMat(objectPhoto, objectMat);
+//        detectionEngine.matchObjects(objectMat, inputFrame, detectionEngine.selectDetector(objectEntity.getDetectorType()));
+
+        long end = System.nanoTime();
+        double frameDelivered = (double) (end - start)/1000000000;
+        double framesPerSecond = 1/frameDelivered;
+
+        if (framesPerSecond < 200) {
+            Log.d("FPS", "Frame in: " + frameDelivered);
+            Log.d("FPS", "Frames per second: " + framesPerSecond);
+        }
         return inputFrame;
     }
 
@@ -176,13 +200,51 @@
     }
 
     @Override
-    public void matchObjectsCallback(Mat homography, boolean found) {
+    public void matchObjectsCallback(float [] sceneCornersData, Mat frame, boolean found) {
         if (found) {
             Log.d(TAG, "match found!");
-            Log.d(TAG, "Homography from callback: " + homography.toString());
+
+            int rightTopX = Math.round(sceneCornersData[2]);
+            int rightTopY = Math.round(sceneCornersData[3]);
+            int letBotX =  Math.round(sceneCornersData[6]);
+            int letBotY =  Math.round(sceneCornersData[7]);
+
+            //if out of frame set to 0
+            if (rightTopX < 0)
+                rightTopX = 0;
+            if (rightTopY < 0)
+                rightTopY = 0;
+            if (letBotX < 0)
+                letBotX = 0;
+            if (letBotY < 0)
+                letBotY = 0;
+
+            int width = letBotX - rightTopX;
+            int height = letBotY - rightTopY;
+
+            // check if trying to crop out of bounds
+            if (width + rightTopX > frame.cols())
+                width = frame.cols()-rightTopX;
+            if (height + rightTopY > frame.rows() - rightTopY)
+                height = frame.rows()-rightTopY;
+
+            Log.d(TAG, "Frame dimensions: "+frame.cols()+"*"+frame.rows());
+            Log.d(TAG, "r t x: "+rightTopX);
+            Log.d(TAG, "r t y: "+rightTopY);
+            Log.d(TAG, "l b x: "+letBotX);
+            Log.d(TAG, "l b y: "+letBotY);
+            Log.d(TAG, "width: "+width);
+            Log.d(TAG, "height: "+height);
+
+
+            Rect roi = new Rect(rightTopX, rightTopY, width, height);
+            inputFrameWithObject = new Mat(frame, roi);
+
+            objectReceived = true;
         }
         else {
             Log.d(TAG, "No match!");
+            objectReceived = false;
         }
         detectorOperating = false;
     }
Index: openCVLibrary3411/src/main/java/org/opencv/core/Algorithm.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/core/Algorithm.java	(date 1605830247260)
+++ openCVLibrary3411/src/main/java/org/opencv/core/Algorithm.java	(date 1605830247260)
@@ -0,0 +1,120 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.core;
+
+
+
+// C++: class Algorithm
+/**
+ * This is a base class for all more or less complex algorithms in OpenCV
+ *
+ * especially for classes of algorithms, for which there can be multiple implementations. The examples
+ * are stereo correspondence (for which there are algorithms like block matching, semi-global block
+ * matching, graph-cut etc.), background subtraction (which can be done using mixture-of-gaussians
+ * models, codebook-based algorithm etc.), optical flow (block matching, Lucas-Kanade, Horn-Schunck
+ * etc.).
+ *
+ * Here is example of SimpleBlobDetector use in your application via Algorithm interface:
+ * SNIPPET: snippets/core_various.cpp Algorithm
+ */
+public class Algorithm {
+
+    protected final long nativeObj;
+    protected Algorithm(long addr) { nativeObj = addr; }
+
+    public long getNativeObjAddr() { return nativeObj; }
+
+    // internal usage only
+    public static Algorithm __fromPtr__(long addr) { return new Algorithm(addr); }
+
+    //
+    // C++:  String cv::Algorithm::getDefaultName()
+    //
+
+    /**
+     * Returns the algorithm string identifier.
+     * This string is used as top level xml/yml node tag when the object is saved to a file or string.
+     * @return automatically generated
+     */
+    public String getDefaultName() {
+        return getDefaultName_0(nativeObj);
+    }
+
+
+    //
+    // C++:  bool cv::Algorithm::empty()
+    //
+
+    /**
+     * Returns true if the Algorithm is empty (e.g. in the very beginning or after unsuccessful read
+     * @return automatically generated
+     */
+    public boolean empty() {
+        return empty_0(nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::Algorithm::clear()
+    //
+
+    /**
+     * Clears the algorithm state
+     */
+    public void clear() {
+        clear_0(nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::Algorithm::read(FileNode fn)
+    //
+
+    // Unknown type 'FileNode' (I), skipping the function
+
+
+    //
+    // C++:  void cv::Algorithm::save(String filename)
+    //
+
+    /**
+     * Saves the algorithm to a file.
+     * In order to make this method work, the derived class must implement Algorithm::write(FileStorage&amp; fs).
+     * @param filename automatically generated
+     */
+    public void save(String filename) {
+        save_0(nativeObj, filename);
+    }
+
+
+    //
+    // C++:  void cv::Algorithm::write(Ptr_FileStorage fs, String name = String())
+    //
+
+    // Unknown type 'Ptr_FileStorage' (I), skipping the function
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++:  String cv::Algorithm::getDefaultName()
+    private static native String getDefaultName_0(long nativeObj);
+
+    // C++:  bool cv::Algorithm::empty()
+    private static native boolean empty_0(long nativeObj);
+
+    // C++:  void cv::Algorithm::clear()
+    private static native void clear_0(long nativeObj);
+
+    // C++:  void cv::Algorithm::save(String filename)
+    private static native void save_0(long nativeObj, String filename);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/core/Core.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/core/Core.java	(date 1605830247279)
+++ openCVLibrary3411/src/main/java/org/opencv/core/Core.java	(date 1605830247279)
@@ -0,0 +1,6123 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.core;
+
+import java.util.ArrayList;
+import java.util.List;
+import org.opencv.core.Mat;
+import org.opencv.core.MatOfDouble;
+import org.opencv.core.MatOfInt;
+import org.opencv.core.Scalar;
+import org.opencv.core.TermCriteria;
+import org.opencv.utils.Converters;
+
+// C++: class Core
+
+public class Core {
+    // these constants are wrapped inside functions to prevent inlining
+    private static String getVersion() { return "3.4.11"; }
+    private static String getNativeLibraryName() { return "opencv_java3411"; }
+    private static int getVersionMajorJ() { return 3; }
+    private static int getVersionMinorJ() { return 4; }
+    private static int getVersionRevisionJ() { return 11; }
+    private static String getVersionStatusJ() { return ""; }
+
+    public static final String VERSION = getVersion();
+    public static final String NATIVE_LIBRARY_NAME = getNativeLibraryName();
+    public static final int VERSION_MAJOR = getVersionMajorJ();
+    public static final int VERSION_MINOR = getVersionMinorJ();
+    public static final int VERSION_REVISION = getVersionRevisionJ();
+    public static final String VERSION_STATUS = getVersionStatusJ();
+
+    private static final int
+            CV_8U = 0,
+            CV_8S = 1,
+            CV_16U = 2,
+            CV_16S = 3,
+            CV_32S = 4,
+            CV_32F = 5,
+            CV_64F = 6,
+            CV_USRTYPE1 = 7;
+
+
+    // C++: enum DecompTypes
+    public static final int
+            DECOMP_LU = 0,
+            DECOMP_SVD = 1,
+            DECOMP_EIG = 2,
+            DECOMP_CHOLESKY = 3,
+            DECOMP_QR = 4,
+            DECOMP_NORMAL = 16;
+
+
+    // C++: enum HersheyFonts
+    public static final int
+            FONT_HERSHEY_SIMPLEX = 0,
+            FONT_HERSHEY_PLAIN = 1,
+            FONT_HERSHEY_DUPLEX = 2,
+            FONT_HERSHEY_COMPLEX = 3,
+            FONT_HERSHEY_TRIPLEX = 4,
+            FONT_HERSHEY_COMPLEX_SMALL = 5,
+            FONT_HERSHEY_SCRIPT_SIMPLEX = 6,
+            FONT_HERSHEY_SCRIPT_COMPLEX = 7,
+            FONT_ITALIC = 16;
+
+
+    // C++: enum <unnamed>
+    public static final int
+            SVD_MODIFY_A = 1,
+            SVD_NO_UV = 2,
+            SVD_FULL_UV = 4,
+            FILLED = -1,
+            REDUCE_SUM = 0,
+            REDUCE_AVG = 1,
+            REDUCE_MAX = 2,
+            REDUCE_MIN = 3,
+            Hamming_normType = 6,
+            RNG_UNIFORM = 0,
+            RNG_NORMAL = 1,
+            Formatter_FMT_DEFAULT = 0,
+            Formatter_FMT_MATLAB = 1,
+            Formatter_FMT_CSV = 2,
+            Formatter_FMT_PYTHON = 3,
+            Formatter_FMT_NUMPY = 4,
+            Formatter_FMT_C = 5,
+            Param_INT = 0,
+            Param_BOOLEAN = 1,
+            Param_REAL = 2,
+            Param_STRING = 3,
+            Param_MAT = 4,
+            Param_MAT_VECTOR = 5,
+            Param_ALGORITHM = 6,
+            Param_FLOAT = 7,
+            Param_UNSIGNED_INT = 8,
+            Param_UINT64 = 9,
+            Param_UCHAR = 11,
+            Param_SCALAR = 12;
+
+
+    // C++: enum LineTypes
+    public static final int
+            LINE_4 = 4,
+            LINE_8 = 8,
+            LINE_AA = 16;
+
+
+    // C++: enum BorderTypes
+    public static final int
+            BORDER_CONSTANT = 0,
+            BORDER_REPLICATE = 1,
+            BORDER_REFLECT = 2,
+            BORDER_WRAP = 3,
+            BORDER_REFLECT_101 = 4,
+            BORDER_TRANSPARENT = 5,
+            BORDER_REFLECT101 = 4,
+            BORDER_DEFAULT = 4,
+            BORDER_ISOLATED = 16;
+
+
+    // C++: enum GemmFlags
+    public static final int
+            GEMM_1_T = 1,
+            GEMM_2_T = 2,
+            GEMM_3_T = 4;
+
+
+    // C++: enum KmeansFlags
+    public static final int
+            KMEANS_RANDOM_CENTERS = 0,
+            KMEANS_PP_CENTERS = 2,
+            KMEANS_USE_INITIAL_LABELS = 1;
+
+
+    // C++: enum CmpTypes
+    public static final int
+            CMP_EQ = 0,
+            CMP_GT = 1,
+            CMP_GE = 2,
+            CMP_LT = 3,
+            CMP_LE = 4,
+            CMP_NE = 5;
+
+
+    // C++: enum Flags
+    public static final int
+            PCA_DATA_AS_ROW = 0,
+            PCA_DATA_AS_COL = 1,
+            PCA_USE_AVG = 2;
+
+
+    // C++: enum DftFlags
+    public static final int
+            DFT_INVERSE = 1,
+            DFT_SCALE = 2,
+            DFT_ROWS = 4,
+            DFT_COMPLEX_OUTPUT = 16,
+            DFT_REAL_OUTPUT = 32,
+            DFT_COMPLEX_INPUT = 64,
+            DCT_INVERSE = 1,
+            DCT_ROWS = 4;
+
+
+    // C++: enum CovarFlags
+    public static final int
+            COVAR_SCRAMBLED = 0,
+            COVAR_NORMAL = 1,
+            COVAR_USE_AVG = 2,
+            COVAR_SCALE = 4,
+            COVAR_ROWS = 8,
+            COVAR_COLS = 16;
+
+
+    // C++: enum SortFlags
+    public static final int
+            SORT_EVERY_ROW = 0,
+            SORT_EVERY_COLUMN = 1,
+            SORT_ASCENDING = 0,
+            SORT_DESCENDING = 16;
+
+
+    // C++: enum NormTypes
+    public static final int
+            NORM_INF = 1,
+            NORM_L1 = 2,
+            NORM_L2 = 4,
+            NORM_L2SQR = 5,
+            NORM_HAMMING = 6,
+            NORM_HAMMING2 = 7,
+            NORM_TYPE_MASK = 7,
+            NORM_RELATIVE = 8,
+            NORM_MINMAX = 32;
+
+
+    // C++: enum RotateFlags
+    public static final int
+            ROTATE_90_CLOCKWISE = 0,
+            ROTATE_180 = 1,
+            ROTATE_90_COUNTERCLOCKWISE = 2;
+
+
+    // C++: enum Code
+    public static final int
+            StsOk = 0,
+            StsBackTrace = -1,
+            StsError = -2,
+            StsInternal = -3,
+            StsNoMem = -4,
+            StsBadArg = -5,
+            StsBadFunc = -6,
+            StsNoConv = -7,
+            StsAutoTrace = -8,
+            HeaderIsNull = -9,
+            BadImageSize = -10,
+            BadOffset = -11,
+            BadDataPtr = -12,
+            BadStep = -13,
+            BadModelOrChSeq = -14,
+            BadNumChannels = -15,
+            BadNumChannel1U = -16,
+            BadDepth = -17,
+            BadAlphaChannel = -18,
+            BadOrder = -19,
+            BadOrigin = -20,
+            BadAlign = -21,
+            BadCallBack = -22,
+            BadTileSize = -23,
+            BadCOI = -24,
+            BadROISize = -25,
+            MaskIsTiled = -26,
+            StsNullPtr = -27,
+            StsVecLengthErr = -28,
+            StsFilterStructContentErr = -29,
+            StsKernelStructContentErr = -30,
+            StsFilterOffsetErr = -31,
+            StsBadSize = -201,
+            StsDivByZero = -202,
+            StsInplaceNotSupported = -203,
+            StsObjectNotFound = -204,
+            StsUnmatchedFormats = -205,
+            StsBadFlag = -206,
+            StsBadPoint = -207,
+            StsBadMask = -208,
+            StsUnmatchedSizes = -209,
+            StsUnsupportedFormat = -210,
+            StsOutOfRange = -211,
+            StsParseError = -212,
+            StsNotImplemented = -213,
+            StsBadMemBlock = -214,
+            StsAssert = -215,
+            GpuNotSupported = -216,
+            GpuApiCallError = -217,
+            OpenGlNotSupported = -218,
+            OpenGlApiCallError = -219,
+            OpenCLApiCallError = -220,
+            OpenCLDoubleNotSupported = -221,
+            OpenCLInitError = -222,
+            OpenCLNoAMDBlasFft = -223;
+
+
+    //
+    // C++:  Scalar cv::mean(Mat src, Mat mask = Mat())
+    //
+
+    /**
+     * Calculates an average (mean) of array elements.
+     *
+     * The function cv::mean calculates the mean value M of array elements,
+     * independently for each channel, and return it:
+     * \(\begin{array}{l} N =  \sum _{I: \; \texttt{mask} (I) \ne 0} 1 \\ M_c =  \left ( \sum _{I: \; \texttt{mask} (I) \ne 0}{ \texttt{mtx} (I)_c} \right )/N \end{array}\)
+     * When all the mask elements are 0's, the function returns Scalar::all(0)
+     * @param src input array that should have from 1 to 4 channels so that the result can be stored in
+     * Scalar_ .
+     * @param mask optional operation mask.
+     * SEE:  countNonZero, meanStdDev, norm, minMaxLoc
+     * @return automatically generated
+     */
+    public static Scalar mean(Mat src, Mat mask) {
+        return new Scalar(mean_0(src.nativeObj, mask.nativeObj));
+    }
+
+    /**
+     * Calculates an average (mean) of array elements.
+     *
+     * The function cv::mean calculates the mean value M of array elements,
+     * independently for each channel, and return it:
+     * \(\begin{array}{l} N =  \sum _{I: \; \texttt{mask} (I) \ne 0} 1 \\ M_c =  \left ( \sum _{I: \; \texttt{mask} (I) \ne 0}{ \texttt{mtx} (I)_c} \right )/N \end{array}\)
+     * When all the mask elements are 0's, the function returns Scalar::all(0)
+     * @param src input array that should have from 1 to 4 channels so that the result can be stored in
+     * Scalar_ .
+     * SEE:  countNonZero, meanStdDev, norm, minMaxLoc
+     * @return automatically generated
+     */
+    public static Scalar mean(Mat src) {
+        return new Scalar(mean_1(src.nativeObj));
+    }
+
+
+    //
+    // C++:  Scalar cv::sum(Mat src)
+    //
+
+    /**
+     * Calculates the sum of array elements.
+     *
+     * The function cv::sum calculates and returns the sum of array elements,
+     * independently for each channel.
+     * @param src input array that must have from 1 to 4 channels.
+     * SEE:  countNonZero, mean, meanStdDev, norm, minMaxLoc, reduce
+     * @return automatically generated
+     */
+    public static Scalar sumElems(Mat src) {
+        return new Scalar(sumElems_0(src.nativeObj));
+    }
+
+
+    //
+    // C++:  Scalar cv::trace(Mat mtx)
+    //
+
+    /**
+     * Returns the trace of a matrix.
+     *
+     * The function cv::trace returns the sum of the diagonal elements of the
+     * matrix mtx .
+     * \(\mathrm{tr} ( \texttt{mtx} ) =  \sum _i  \texttt{mtx} (i,i)\)
+     * @param mtx input matrix.
+     * @return automatically generated
+     */
+    public static Scalar trace(Mat mtx) {
+        return new Scalar(trace_0(mtx.nativeObj));
+    }
+
+
+    //
+    // C++:  String cv::getBuildInformation()
+    //
+
+    /**
+     * Returns full configuration time cmake output.
+     *
+     * Returned value is raw cmake output including version control system revision, compiler version,
+     * compiler flags, enabled modules and third party libraries, etc. Output format depends on target
+     * architecture.
+     * @return automatically generated
+     */
+    public static String getBuildInformation() {
+        return getBuildInformation_0();
+    }
+
+
+    //
+    // C++:  String cv::getHardwareFeatureName(int feature)
+    //
+
+    /**
+     * Returns feature name by ID
+     *
+     * Returns empty string if feature is not defined
+     * @param feature automatically generated
+     * @return automatically generated
+     */
+    public static String getHardwareFeatureName(int feature) {
+        return getHardwareFeatureName_0(feature);
+    }
+
+
+    //
+    // C++:  String cv::getVersionString()
+    //
+
+    /**
+     * Returns library version string
+     *
+     * For example "3.4.1-dev".
+     *
+     * SEE: getMajorVersion, getMinorVersion, getRevisionVersion
+     * @return automatically generated
+     */
+    public static String getVersionString() {
+        return getVersionString_0();
+    }
+
+
+    //
+    // C++:  String cv::ipp::getIppVersion()
+    //
+
+    public static String getIppVersion() {
+        return getIppVersion_0();
+    }
+
+
+    //
+    // C++:  String cv::samples::findFile(String relative_path, bool required = true, bool silentMode = false)
+    //
+
+    /**
+     * Try to find requested data file
+     *
+     * Search directories:
+     *
+     * 1. Directories passed via {@code addSamplesDataSearchPath()}
+     * 2. OPENCV_SAMPLES_DATA_PATH_HINT environment variable
+     * 3. OPENCV_SAMPLES_DATA_PATH environment variable
+     *    If parameter value is not empty and nothing is found then stop searching.
+     * 4. Detects build/install path based on:
+     *    a. current working directory (CWD)
+     *    b. and/or binary module location (opencv_core/opencv_world, doesn't work with static linkage)
+     * 5. Scan {@code &lt;source&gt;/{,data,samples/data}} directories if build directory is detected or the current directory is in source tree.
+     * 6. Scan {@code &lt;install&gt;/share/OpenCV} directory if install directory is detected.
+     *
+     * SEE: cv::utils::findDataFile
+     *
+     * @param relative_path Relative path to data file
+     * @param required Specify "file not found" handling.
+     *        If true, function prints information message and raises cv::Exception.
+     *        If false, function returns empty result
+     * @param silentMode Disables messages
+     * @return Returns path (absolute or relative to the current directory) or empty string if file is not found
+     */
+    public static String findFile(String relative_path, boolean required, boolean silentMode) {
+        return findFile_0(relative_path, required, silentMode);
+    }
+
+    /**
+     * Try to find requested data file
+     *
+     * Search directories:
+     *
+     * 1. Directories passed via {@code addSamplesDataSearchPath()}
+     * 2. OPENCV_SAMPLES_DATA_PATH_HINT environment variable
+     * 3. OPENCV_SAMPLES_DATA_PATH environment variable
+     *    If parameter value is not empty and nothing is found then stop searching.
+     * 4. Detects build/install path based on:
+     *    a. current working directory (CWD)
+     *    b. and/or binary module location (opencv_core/opencv_world, doesn't work with static linkage)
+     * 5. Scan {@code &lt;source&gt;/{,data,samples/data}} directories if build directory is detected or the current directory is in source tree.
+     * 6. Scan {@code &lt;install&gt;/share/OpenCV} directory if install directory is detected.
+     *
+     * SEE: cv::utils::findDataFile
+     *
+     * @param relative_path Relative path to data file
+     * @param required Specify "file not found" handling.
+     *        If true, function prints information message and raises cv::Exception.
+     *        If false, function returns empty result
+     * @return Returns path (absolute or relative to the current directory) or empty string if file is not found
+     */
+    public static String findFile(String relative_path, boolean required) {
+        return findFile_1(relative_path, required);
+    }
+
+    /**
+     * Try to find requested data file
+     *
+     * Search directories:
+     *
+     * 1. Directories passed via {@code addSamplesDataSearchPath()}
+     * 2. OPENCV_SAMPLES_DATA_PATH_HINT environment variable
+     * 3. OPENCV_SAMPLES_DATA_PATH environment variable
+     *    If parameter value is not empty and nothing is found then stop searching.
+     * 4. Detects build/install path based on:
+     *    a. current working directory (CWD)
+     *    b. and/or binary module location (opencv_core/opencv_world, doesn't work with static linkage)
+     * 5. Scan {@code &lt;source&gt;/{,data,samples/data}} directories if build directory is detected or the current directory is in source tree.
+     * 6. Scan {@code &lt;install&gt;/share/OpenCV} directory if install directory is detected.
+     *
+     * SEE: cv::utils::findDataFile
+     *
+     * @param relative_path Relative path to data file
+     *        If true, function prints information message and raises cv::Exception.
+     *        If false, function returns empty result
+     * @return Returns path (absolute or relative to the current directory) or empty string if file is not found
+     */
+    public static String findFile(String relative_path) {
+        return findFile_2(relative_path);
+    }
+
+
+    //
+    // C++:  String cv::samples::findFileOrKeep(String relative_path, bool silentMode = false)
+    //
+
+    public static String findFileOrKeep(String relative_path, boolean silentMode) {
+        return findFileOrKeep_0(relative_path, silentMode);
+    }
+
+    public static String findFileOrKeep(String relative_path) {
+        return findFileOrKeep_1(relative_path);
+    }
+
+
+    //
+    // C++:  bool cv::checkRange(Mat a, bool quiet = true,  _hidden_ * pos = 0, double minVal = -DBL_MAX, double maxVal = DBL_MAX)
+    //
+
+    /**
+     * Checks every element of an input array for invalid values.
+     *
+     * The function cv::checkRange checks that every array element is neither NaN nor infinite. When minVal &gt;
+     * <ul>
+     *   <li>
+     * DBL_MAX and maxVal &lt; DBL_MAX, the function also checks that each value is between minVal and
+     * maxVal. In case of multi-channel arrays, each channel is processed independently. If some values
+     * are out of range, position of the first outlier is stored in pos (when pos != NULL). Then, the
+     * function either returns false (when quiet=true) or throws an exception.
+     * @param a input array.
+     * @param quiet a flag, indicating whether the functions quietly return false when the array elements
+     * are out of range or they throw an exception.
+     * elements.
+     * @param minVal inclusive lower boundary of valid values range.
+     * @param maxVal exclusive upper boundary of valid values range.
+     *   </li>
+     * </ul>
+     * @return automatically generated
+     */
+    public static boolean checkRange(Mat a, boolean quiet, double minVal, double maxVal) {
+        return checkRange_0(a.nativeObj, quiet, minVal, maxVal);
+    }
+
+    /**
+     * Checks every element of an input array for invalid values.
+     *
+     * The function cv::checkRange checks that every array element is neither NaN nor infinite. When minVal &gt;
+     * <ul>
+     *   <li>
+     * DBL_MAX and maxVal &lt; DBL_MAX, the function also checks that each value is between minVal and
+     * maxVal. In case of multi-channel arrays, each channel is processed independently. If some values
+     * are out of range, position of the first outlier is stored in pos (when pos != NULL). Then, the
+     * function either returns false (when quiet=true) or throws an exception.
+     * @param a input array.
+     * @param quiet a flag, indicating whether the functions quietly return false when the array elements
+     * are out of range or they throw an exception.
+     * elements.
+     * @param minVal inclusive lower boundary of valid values range.
+     *   </li>
+     * </ul>
+     * @return automatically generated
+     */
+    public static boolean checkRange(Mat a, boolean quiet, double minVal) {
+        return checkRange_1(a.nativeObj, quiet, minVal);
+    }
+
+    /**
+     * Checks every element of an input array for invalid values.
+     *
+     * The function cv::checkRange checks that every array element is neither NaN nor infinite. When minVal &gt;
+     * <ul>
+     *   <li>
+     * DBL_MAX and maxVal &lt; DBL_MAX, the function also checks that each value is between minVal and
+     * maxVal. In case of multi-channel arrays, each channel is processed independently. If some values
+     * are out of range, position of the first outlier is stored in pos (when pos != NULL). Then, the
+     * function either returns false (when quiet=true) or throws an exception.
+     * @param a input array.
+     * @param quiet a flag, indicating whether the functions quietly return false when the array elements
+     * are out of range or they throw an exception.
+     * elements.
+     *   </li>
+     * </ul>
+     * @return automatically generated
+     */
+    public static boolean checkRange(Mat a, boolean quiet) {
+        return checkRange_2(a.nativeObj, quiet);
+    }
+
+    /**
+     * Checks every element of an input array for invalid values.
+     *
+     * The function cv::checkRange checks that every array element is neither NaN nor infinite. When minVal &gt;
+     * <ul>
+     *   <li>
+     * DBL_MAX and maxVal &lt; DBL_MAX, the function also checks that each value is between minVal and
+     * maxVal. In case of multi-channel arrays, each channel is processed independently. If some values
+     * are out of range, position of the first outlier is stored in pos (when pos != NULL). Then, the
+     * function either returns false (when quiet=true) or throws an exception.
+     * @param a input array.
+     * are out of range or they throw an exception.
+     * elements.
+     *   </li>
+     * </ul>
+     * @return automatically generated
+     */
+    public static boolean checkRange(Mat a) {
+        return checkRange_4(a.nativeObj);
+    }
+
+
+    //
+    // C++:  bool cv::eigen(Mat src, Mat& eigenvalues, Mat& eigenvectors = Mat())
+    //
+
+    /**
+     * Calculates eigenvalues and eigenvectors of a symmetric matrix.
+     *
+     * The function cv::eigen calculates just eigenvalues, or eigenvalues and eigenvectors of the symmetric
+     * matrix src:
+     * <code>
+     *     src*eigenvectors.row(i).t() = eigenvalues.at&lt;srcType&gt;(i)*eigenvectors.row(i).t()
+     * </code>
+     *
+     * <b>Note:</b> Use cv::eigenNonSymmetric for calculation of real eigenvalues and eigenvectors of non-symmetric matrix.
+     *
+     * @param src input matrix that must have CV_32FC1 or CV_64FC1 type, square size and be symmetrical
+     * (src ^T^ == src).
+     * @param eigenvalues output vector of eigenvalues of the same type as src; the eigenvalues are stored
+     * in the descending order.
+     * @param eigenvectors output matrix of eigenvectors; it has the same size and type as src; the
+     * eigenvectors are stored as subsequent matrix rows, in the same order as the corresponding
+     * eigenvalues.
+     * SEE: eigenNonSymmetric, completeSymm , PCA
+     * @return automatically generated
+     */
+    public static boolean eigen(Mat src, Mat eigenvalues, Mat eigenvectors) {
+        return eigen_0(src.nativeObj, eigenvalues.nativeObj, eigenvectors.nativeObj);
+    }
+
+    /**
+     * Calculates eigenvalues and eigenvectors of a symmetric matrix.
+     *
+     * The function cv::eigen calculates just eigenvalues, or eigenvalues and eigenvectors of the symmetric
+     * matrix src:
+     * <code>
+     *     src*eigenvectors.row(i).t() = eigenvalues.at&lt;srcType&gt;(i)*eigenvectors.row(i).t()
+     * </code>
+     *
+     * <b>Note:</b> Use cv::eigenNonSymmetric for calculation of real eigenvalues and eigenvectors of non-symmetric matrix.
+     *
+     * @param src input matrix that must have CV_32FC1 or CV_64FC1 type, square size and be symmetrical
+     * (src ^T^ == src).
+     * @param eigenvalues output vector of eigenvalues of the same type as src; the eigenvalues are stored
+     * in the descending order.
+     * eigenvectors are stored as subsequent matrix rows, in the same order as the corresponding
+     * eigenvalues.
+     * SEE: eigenNonSymmetric, completeSymm , PCA
+     * @return automatically generated
+     */
+    public static boolean eigen(Mat src, Mat eigenvalues) {
+        return eigen_1(src.nativeObj, eigenvalues.nativeObj);
+    }
+
+
+    //
+    // C++:  bool cv::solve(Mat src1, Mat src2, Mat& dst, int flags = DECOMP_LU)
+    //
+
+    /**
+     * Solves one or more linear systems or least-squares problems.
+     *
+     * The function cv::solve solves a linear system or least-squares problem (the
+     * latter is possible with SVD or QR methods, or by specifying the flag
+     * #DECOMP_NORMAL ):
+     * \(\texttt{dst} =  \arg \min _X \| \texttt{src1} \cdot \texttt{X} -  \texttt{src2} \|\)
+     *
+     * If #DECOMP_LU or #DECOMP_CHOLESKY method is used, the function returns 1
+     * if src1 (or \(\texttt{src1}^T\texttt{src1}\) ) is non-singular. Otherwise,
+     * it returns 0. In the latter case, dst is not valid. Other methods find a
+     * pseudo-solution in case of a singular left-hand side part.
+     *
+     * <b>Note:</b> If you want to find a unity-norm solution of an under-defined
+     * singular system \(\texttt{src1}\cdot\texttt{dst}=0\) , the function solve
+     * will not do the work. Use SVD::solveZ instead.
+     *
+     * @param src1 input matrix on the left-hand side of the system.
+     * @param src2 input matrix on the right-hand side of the system.
+     * @param dst output solution.
+     * @param flags solution (matrix inversion) method (#DecompTypes)
+     * SEE: invert, SVD, eigen
+     * @return automatically generated
+     */
+    public static boolean solve(Mat src1, Mat src2, Mat dst, int flags) {
+        return solve_0(src1.nativeObj, src2.nativeObj, dst.nativeObj, flags);
+    }
+
+    /**
+     * Solves one or more linear systems or least-squares problems.
+     *
+     * The function cv::solve solves a linear system or least-squares problem (the
+     * latter is possible with SVD or QR methods, or by specifying the flag
+     * #DECOMP_NORMAL ):
+     * \(\texttt{dst} =  \arg \min _X \| \texttt{src1} \cdot \texttt{X} -  \texttt{src2} \|\)
+     *
+     * If #DECOMP_LU or #DECOMP_CHOLESKY method is used, the function returns 1
+     * if src1 (or \(\texttt{src1}^T\texttt{src1}\) ) is non-singular. Otherwise,
+     * it returns 0. In the latter case, dst is not valid. Other methods find a
+     * pseudo-solution in case of a singular left-hand side part.
+     *
+     * <b>Note:</b> If you want to find a unity-norm solution of an under-defined
+     * singular system \(\texttt{src1}\cdot\texttt{dst}=0\) , the function solve
+     * will not do the work. Use SVD::solveZ instead.
+     *
+     * @param src1 input matrix on the left-hand side of the system.
+     * @param src2 input matrix on the right-hand side of the system.
+     * @param dst output solution.
+     * SEE: invert, SVD, eigen
+     * @return automatically generated
+     */
+    public static boolean solve(Mat src1, Mat src2, Mat dst) {
+        return solve_1(src1.nativeObj, src2.nativeObj, dst.nativeObj);
+    }
+
+
+    //
+    // C++:  bool cv::ipp::useIPP()
+    //
+
+    /**
+     * proxy for hal::Cholesky
+     * @return automatically generated
+     */
+    public static boolean useIPP() {
+        return useIPP_0();
+    }
+
+
+    //
+    // C++:  bool cv::ipp::useIPP_NE()
+    //
+
+    public static boolean useIPP_NE() {
+        return useIPP_NE_0();
+    }
+
+
+    //
+    // C++:  bool cv::ipp::useIPP_NotExact()
+    //
+
+    public static boolean useIPP_NotExact() {
+        return useIPP_NotExact_0();
+    }
+
+
+    //
+    // C++:  double cv::Mahalanobis(Mat v1, Mat v2, Mat icovar)
+    //
+
+    /**
+     * Calculates the Mahalanobis distance between two vectors.
+     *
+     * The function cv::Mahalanobis calculates and returns the weighted distance between two vectors:
+     * \(d( \texttt{vec1} , \texttt{vec2} )= \sqrt{\sum_{i,j}{\texttt{icovar(i,j)}\cdot(\texttt{vec1}(I)-\texttt{vec2}(I))\cdot(\texttt{vec1(j)}-\texttt{vec2(j)})} }\)
+     * The covariance matrix may be calculated using the #calcCovarMatrix function and then inverted using
+     * the invert function (preferably using the #DECOMP_SVD method, as the most accurate).
+     * @param v1 first 1D input vector.
+     * @param v2 second 1D input vector.
+     * @param icovar inverse covariance matrix.
+     * @return automatically generated
+     */
+    public static double Mahalanobis(Mat v1, Mat v2, Mat icovar) {
+        return Mahalanobis_0(v1.nativeObj, v2.nativeObj, icovar.nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::PSNR(Mat src1, Mat src2)
+    //
+
+    /**
+     * Computes the Peak Signal-to-Noise Ratio (PSNR) image quality metric.
+     *
+     * This function calculates the Peak Signal-to-Noise Ratio (PSNR) image quality metric in decibels (dB), between two input arrays src1 and src2. Arrays must have depth CV_8U.
+     *
+     * The PSNR is calculated as follows:
+     *
+     * \(
+     * \texttt{PSNR} = 10 \cdot \log_{10}{\left( \frac{R^2}{MSE} \right) }
+     * \)
+     *
+     * where R is the maximum integer value of depth CV_8U (255) and MSE is the mean squared error between the two arrays.
+     *
+     * @param src1 first input array.
+     * @param src2 second input array of the same size as src1.
+     * @return automatically generated
+     */
+    public static double PSNR(Mat src1, Mat src2) {
+        return PSNR_0(src1.nativeObj, src2.nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::determinant(Mat mtx)
+    //
+
+    /**
+     * Returns the determinant of a square floating-point matrix.
+     *
+     * The function cv::determinant calculates and returns the determinant of the
+     * specified matrix. For small matrices ( mtx.cols=mtx.rows&lt;=3 ), the
+     * direct method is used. For larger matrices, the function uses LU
+     * factorization with partial pivoting.
+     *
+     * For symmetric positively-determined matrices, it is also possible to use
+     * eigen decomposition to calculate the determinant.
+     * @param mtx input matrix that must have CV_32FC1 or CV_64FC1 type and
+     * square size.
+     * SEE: trace, invert, solve, eigen, REF: MatrixExpressions
+     * @return automatically generated
+     */
+    public static double determinant(Mat mtx) {
+        return determinant_0(mtx.nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::getTickFrequency()
+    //
+
+    /**
+     * Returns the number of ticks per second.
+     *
+     * The function returns the number of ticks per second. That is, the following code computes the
+     * execution time in seconds:
+     * <code>
+     *     double t = (double)getTickCount();
+     *     // do something ...
+     *     t = ((double)getTickCount() - t)/getTickFrequency();
+     * </code>
+     * SEE: getTickCount, TickMeter
+     * @return automatically generated
+     */
+    public static double getTickFrequency() {
+        return getTickFrequency_0();
+    }
+
+
+    //
+    // C++:  double cv::invert(Mat src, Mat& dst, int flags = DECOMP_LU)
+    //
+
+    /**
+     * Finds the inverse or pseudo-inverse of a matrix.
+     *
+     * The function cv::invert inverts the matrix src and stores the result in dst
+     * . When the matrix src is singular or non-square, the function calculates
+     * the pseudo-inverse matrix (the dst matrix) so that norm(src\*dst - I) is
+     * minimal, where I is an identity matrix.
+     *
+     * In case of the #DECOMP_LU method, the function returns non-zero value if
+     * the inverse has been successfully calculated and 0 if src is singular.
+     *
+     * In case of the #DECOMP_SVD method, the function returns the inverse
+     * condition number of src (the ratio of the smallest singular value to the
+     * largest singular value) and 0 if src is singular. The SVD method
+     * calculates a pseudo-inverse matrix if src is singular.
+     *
+     * Similarly to #DECOMP_LU, the method #DECOMP_CHOLESKY works only with
+     * non-singular square matrices that should also be symmetrical and
+     * positively defined. In this case, the function stores the inverted
+     * matrix in dst and returns non-zero. Otherwise, it returns 0.
+     *
+     * @param src input floating-point M x N matrix.
+     * @param dst output matrix of N x M size and the same type as src.
+     * @param flags inversion method (cv::DecompTypes)
+     * SEE: solve, SVD
+     * @return automatically generated
+     */
+    public static double invert(Mat src, Mat dst, int flags) {
+        return invert_0(src.nativeObj, dst.nativeObj, flags);
+    }
+
+    /**
+     * Finds the inverse or pseudo-inverse of a matrix.
+     *
+     * The function cv::invert inverts the matrix src and stores the result in dst
+     * . When the matrix src is singular or non-square, the function calculates
+     * the pseudo-inverse matrix (the dst matrix) so that norm(src\*dst - I) is
+     * minimal, where I is an identity matrix.
+     *
+     * In case of the #DECOMP_LU method, the function returns non-zero value if
+     * the inverse has been successfully calculated and 0 if src is singular.
+     *
+     * In case of the #DECOMP_SVD method, the function returns the inverse
+     * condition number of src (the ratio of the smallest singular value to the
+     * largest singular value) and 0 if src is singular. The SVD method
+     * calculates a pseudo-inverse matrix if src is singular.
+     *
+     * Similarly to #DECOMP_LU, the method #DECOMP_CHOLESKY works only with
+     * non-singular square matrices that should also be symmetrical and
+     * positively defined. In this case, the function stores the inverted
+     * matrix in dst and returns non-zero. Otherwise, it returns 0.
+     *
+     * @param src input floating-point M x N matrix.
+     * @param dst output matrix of N x M size and the same type as src.
+     * SEE: solve, SVD
+     * @return automatically generated
+     */
+    public static double invert(Mat src, Mat dst) {
+        return invert_1(src.nativeObj, dst.nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::kmeans(Mat data, int K, Mat& bestLabels, TermCriteria criteria, int attempts, int flags, Mat& centers = Mat())
+    //
+
+    /**
+     * Finds centers of clusters and groups input samples around the clusters.
+     *
+     * The function kmeans implements a k-means algorithm that finds the centers of cluster_count clusters
+     * and groups the input samples around the clusters. As an output, \(\texttt{bestLabels}_i\) contains a
+     * 0-based cluster index for the sample stored in the \(i^{th}\) row of the samples matrix.
+     *
+     * <b>Note:</b>
+     * <ul>
+     *   <li>
+     *    (Python) An example on K-means clustering can be found at
+     *     opencv_source_code/samples/python/kmeans.py
+     * @param data Data for clustering. An array of N-Dimensional points with float coordinates is needed.
+     * Examples of this array can be:
+     *   </li>
+     *   <li>
+     *    Mat points(count, 2, CV_32F);
+     *   </li>
+     *   <li>
+     *    Mat points(count, 1, CV_32FC2);
+     *   </li>
+     *   <li>
+     *    Mat points(1, count, CV_32FC2);
+     *   </li>
+     *   <li>
+     *    std::vector&lt;cv::Point2f&gt; points(sampleCount);
+     * @param K Number of clusters to split the set by.
+     * @param bestLabels Input/output integer array that stores the cluster indices for every sample.
+     * @param criteria The algorithm termination criteria, that is, the maximum number of iterations and/or
+     * the desired accuracy. The accuracy is specified as criteria.epsilon. As soon as each of the cluster
+     * centers moves by less than criteria.epsilon on some iteration, the algorithm stops.
+     * @param attempts Flag to specify the number of times the algorithm is executed using different
+     * initial labellings. The algorithm returns the labels that yield the best compactness (see the last
+     * function parameter).
+     * @param flags Flag that can take values of cv::KmeansFlags
+     * @param centers Output matrix of the cluster centers, one row per each cluster center.
+     * @return The function returns the compactness measure that is computed as
+     * \(\sum _i  \| \texttt{samples} _i -  \texttt{centers} _{ \texttt{labels} _i} \| ^2\)
+     * after every attempt. The best (minimum) value is chosen and the corresponding labels and the
+     * compactness value are returned by the function. Basically, you can use only the core of the
+     * function, set the number of attempts to 1, initialize labels each time using a custom algorithm,
+     * pass them with the ( flags = #KMEANS_USE_INITIAL_LABELS ) flag, and then choose the best
+     * (most-compact) clustering.
+     *   </li>
+     * </ul>
+     */
+    public static double kmeans(Mat data, int K, Mat bestLabels, TermCriteria criteria, int attempts, int flags, Mat centers) {
+        return kmeans_0(data.nativeObj, K, bestLabels.nativeObj, criteria.type, criteria.maxCount, criteria.epsilon, attempts, flags, centers.nativeObj);
+    }
+
+    /**
+     * Finds centers of clusters and groups input samples around the clusters.
+     *
+     * The function kmeans implements a k-means algorithm that finds the centers of cluster_count clusters
+     * and groups the input samples around the clusters. As an output, \(\texttt{bestLabels}_i\) contains a
+     * 0-based cluster index for the sample stored in the \(i^{th}\) row of the samples matrix.
+     *
+     * <b>Note:</b>
+     * <ul>
+     *   <li>
+     *    (Python) An example on K-means clustering can be found at
+     *     opencv_source_code/samples/python/kmeans.py
+     * @param data Data for clustering. An array of N-Dimensional points with float coordinates is needed.
+     * Examples of this array can be:
+     *   </li>
+     *   <li>
+     *    Mat points(count, 2, CV_32F);
+     *   </li>
+     *   <li>
+     *    Mat points(count, 1, CV_32FC2);
+     *   </li>
+     *   <li>
+     *    Mat points(1, count, CV_32FC2);
+     *   </li>
+     *   <li>
+     *    std::vector&lt;cv::Point2f&gt; points(sampleCount);
+     * @param K Number of clusters to split the set by.
+     * @param bestLabels Input/output integer array that stores the cluster indices for every sample.
+     * @param criteria The algorithm termination criteria, that is, the maximum number of iterations and/or
+     * the desired accuracy. The accuracy is specified as criteria.epsilon. As soon as each of the cluster
+     * centers moves by less than criteria.epsilon on some iteration, the algorithm stops.
+     * @param attempts Flag to specify the number of times the algorithm is executed using different
+     * initial labellings. The algorithm returns the labels that yield the best compactness (see the last
+     * function parameter).
+     * @param flags Flag that can take values of cv::KmeansFlags
+     * @return The function returns the compactness measure that is computed as
+     * \(\sum _i  \| \texttt{samples} _i -  \texttt{centers} _{ \texttt{labels} _i} \| ^2\)
+     * after every attempt. The best (minimum) value is chosen and the corresponding labels and the
+     * compactness value are returned by the function. Basically, you can use only the core of the
+     * function, set the number of attempts to 1, initialize labels each time using a custom algorithm,
+     * pass them with the ( flags = #KMEANS_USE_INITIAL_LABELS ) flag, and then choose the best
+     * (most-compact) clustering.
+     *   </li>
+     * </ul>
+     */
+    public static double kmeans(Mat data, int K, Mat bestLabels, TermCriteria criteria, int attempts, int flags) {
+        return kmeans_1(data.nativeObj, K, bestLabels.nativeObj, criteria.type, criteria.maxCount, criteria.epsilon, attempts, flags);
+    }
+
+
+    //
+    // C++:  double cv::norm(Mat src1, Mat src2, int normType = NORM_L2, Mat mask = Mat())
+    //
+
+    /**
+     * Calculates an absolute difference norm or a relative difference norm.
+     *
+     * This version of cv::norm calculates the absolute difference norm
+     * or the relative difference norm of arrays src1 and src2.
+     * The type of norm to calculate is specified using #NormTypes.
+     *
+     * @param src1 first input array.
+     * @param src2 second input array of the same size and the same type as src1.
+     * @param normType type of the norm (see #NormTypes).
+     * @param mask optional operation mask; it must have the same size as src1 and CV_8UC1 type.
+     * @return automatically generated
+     */
+    public static double norm(Mat src1, Mat src2, int normType, Mat mask) {
+        return norm_0(src1.nativeObj, src2.nativeObj, normType, mask.nativeObj);
+    }
+
+    /**
+     * Calculates an absolute difference norm or a relative difference norm.
+     *
+     * This version of cv::norm calculates the absolute difference norm
+     * or the relative difference norm of arrays src1 and src2.
+     * The type of norm to calculate is specified using #NormTypes.
+     *
+     * @param src1 first input array.
+     * @param src2 second input array of the same size and the same type as src1.
+     * @param normType type of the norm (see #NormTypes).
+     * @return automatically generated
+     */
+    public static double norm(Mat src1, Mat src2, int normType) {
+        return norm_1(src1.nativeObj, src2.nativeObj, normType);
+    }
+
+    /**
+     * Calculates an absolute difference norm or a relative difference norm.
+     *
+     * This version of cv::norm calculates the absolute difference norm
+     * or the relative difference norm of arrays src1 and src2.
+     * The type of norm to calculate is specified using #NormTypes.
+     *
+     * @param src1 first input array.
+     * @param src2 second input array of the same size and the same type as src1.
+     * @return automatically generated
+     */
+    public static double norm(Mat src1, Mat src2) {
+        return norm_2(src1.nativeObj, src2.nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::norm(Mat src1, int normType = NORM_L2, Mat mask = Mat())
+    //
+
+    /**
+     * Calculates the  absolute norm of an array.
+     *
+     * This version of #norm calculates the absolute norm of src1. The type of norm to calculate is specified using #NormTypes.
+     *
+     * As example for one array consider the function \(r(x)= \begin{pmatrix} x \\ 1-x \end{pmatrix}, x \in [-1;1]\).
+     * The \( L_{1}, L_{2} \) and \( L_{\infty} \) norm for the sample value \(r(-1) = \begin{pmatrix} -1 \\ 2 \end{pmatrix}\)
+     * is calculated as follows
+     * \(align*}
+     *     \| r(-1) \|_{L_1} &amp;= |-1| + |2| = 3 \\
+     *     \| r(-1) \|_{L_2} &amp;= \sqrt{(-1)^{2} + (2)^{2}} = \sqrt{5} \\
+     *     \| r(-1) \|_{L_\infty} &amp;= \max(|-1|,|2|) = 2
+     * \)
+     * and for \(r(0.5) = \begin{pmatrix} 0.5 \\ 0.5 \end{pmatrix}\) the calculation is
+     * \(align*}
+     *     \| r(0.5) \|_{L_1} &amp;= |0.5| + |0.5| = 1 \\
+     *     \| r(0.5) \|_{L_2} &amp;= \sqrt{(0.5)^{2} + (0.5)^{2}} = \sqrt{0.5} \\
+     *     \| r(0.5) \|_{L_\infty} &amp;= \max(|0.5|,|0.5|) = 0.5.
+     * \)
+     * The following graphic shows all values for the three norm functions \(\| r(x) \|_{L_1}, \| r(x) \|_{L_2}\) and \(\| r(x) \|_{L_\infty}\).
+     * It is notable that the \( L_{1} \) norm forms the upper and the \( L_{\infty} \) norm forms the lower border for the example function \( r(x) \).
+     * ![Graphs for the different norm functions from the above example](pics/NormTypes_OneArray_1-2-INF.png)
+     *
+     * When the mask parameter is specified and it is not empty, the norm is
+     *
+     * If normType is not specified, #NORM_L2 is used.
+     * calculated only over the region specified by the mask.
+     *
+     * Multi-channel input arrays are treated as single-channel arrays, that is,
+     * the results for all channels are combined.
+     *
+     * Hamming norms can only be calculated with CV_8U depth arrays.
+     *
+     * @param src1 first input array.
+     * @param normType type of the norm (see #NormTypes).
+     * @param mask optional operation mask; it must have the same size as src1 and CV_8UC1 type.
+     * @return automatically generated
+     */
+    public static double norm(Mat src1, int normType, Mat mask) {
+        return norm_3(src1.nativeObj, normType, mask.nativeObj);
+    }
+
+    /**
+     * Calculates the  absolute norm of an array.
+     *
+     * This version of #norm calculates the absolute norm of src1. The type of norm to calculate is specified using #NormTypes.
+     *
+     * As example for one array consider the function \(r(x)= \begin{pmatrix} x \\ 1-x \end{pmatrix}, x \in [-1;1]\).
+     * The \( L_{1}, L_{2} \) and \( L_{\infty} \) norm for the sample value \(r(-1) = \begin{pmatrix} -1 \\ 2 \end{pmatrix}\)
+     * is calculated as follows
+     * \(align*}
+     *     \| r(-1) \|_{L_1} &amp;= |-1| + |2| = 3 \\
+     *     \| r(-1) \|_{L_2} &amp;= \sqrt{(-1)^{2} + (2)^{2}} = \sqrt{5} \\
+     *     \| r(-1) \|_{L_\infty} &amp;= \max(|-1|,|2|) = 2
+     * \)
+     * and for \(r(0.5) = \begin{pmatrix} 0.5 \\ 0.5 \end{pmatrix}\) the calculation is
+     * \(align*}
+     *     \| r(0.5) \|_{L_1} &amp;= |0.5| + |0.5| = 1 \\
+     *     \| r(0.5) \|_{L_2} &amp;= \sqrt{(0.5)^{2} + (0.5)^{2}} = \sqrt{0.5} \\
+     *     \| r(0.5) \|_{L_\infty} &amp;= \max(|0.5|,|0.5|) = 0.5.
+     * \)
+     * The following graphic shows all values for the three norm functions \(\| r(x) \|_{L_1}, \| r(x) \|_{L_2}\) and \(\| r(x) \|_{L_\infty}\).
+     * It is notable that the \( L_{1} \) norm forms the upper and the \( L_{\infty} \) norm forms the lower border for the example function \( r(x) \).
+     * ![Graphs for the different norm functions from the above example](pics/NormTypes_OneArray_1-2-INF.png)
+     *
+     * When the mask parameter is specified and it is not empty, the norm is
+     *
+     * If normType is not specified, #NORM_L2 is used.
+     * calculated only over the region specified by the mask.
+     *
+     * Multi-channel input arrays are treated as single-channel arrays, that is,
+     * the results for all channels are combined.
+     *
+     * Hamming norms can only be calculated with CV_8U depth arrays.
+     *
+     * @param src1 first input array.
+     * @param normType type of the norm (see #NormTypes).
+     * @return automatically generated
+     */
+    public static double norm(Mat src1, int normType) {
+        return norm_4(src1.nativeObj, normType);
+    }
+
+    /**
+     * Calculates the  absolute norm of an array.
+     *
+     * This version of #norm calculates the absolute norm of src1. The type of norm to calculate is specified using #NormTypes.
+     *
+     * As example for one array consider the function \(r(x)= \begin{pmatrix} x \\ 1-x \end{pmatrix}, x \in [-1;1]\).
+     * The \( L_{1}, L_{2} \) and \( L_{\infty} \) norm for the sample value \(r(-1) = \begin{pmatrix} -1 \\ 2 \end{pmatrix}\)
+     * is calculated as follows
+     * \(align*}
+     *     \| r(-1) \|_{L_1} &amp;= |-1| + |2| = 3 \\
+     *     \| r(-1) \|_{L_2} &amp;= \sqrt{(-1)^{2} + (2)^{2}} = \sqrt{5} \\
+     *     \| r(-1) \|_{L_\infty} &amp;= \max(|-1|,|2|) = 2
+     * \)
+     * and for \(r(0.5) = \begin{pmatrix} 0.5 \\ 0.5 \end{pmatrix}\) the calculation is
+     * \(align*}
+     *     \| r(0.5) \|_{L_1} &amp;= |0.5| + |0.5| = 1 \\
+     *     \| r(0.5) \|_{L_2} &amp;= \sqrt{(0.5)^{2} + (0.5)^{2}} = \sqrt{0.5} \\
+     *     \| r(0.5) \|_{L_\infty} &amp;= \max(|0.5|,|0.5|) = 0.5.
+     * \)
+     * The following graphic shows all values for the three norm functions \(\| r(x) \|_{L_1}, \| r(x) \|_{L_2}\) and \(\| r(x) \|_{L_\infty}\).
+     * It is notable that the \( L_{1} \) norm forms the upper and the \( L_{\infty} \) norm forms the lower border for the example function \( r(x) \).
+     * ![Graphs for the different norm functions from the above example](pics/NormTypes_OneArray_1-2-INF.png)
+     *
+     * When the mask parameter is specified and it is not empty, the norm is
+     *
+     * If normType is not specified, #NORM_L2 is used.
+     * calculated only over the region specified by the mask.
+     *
+     * Multi-channel input arrays are treated as single-channel arrays, that is,
+     * the results for all channels are combined.
+     *
+     * Hamming norms can only be calculated with CV_8U depth arrays.
+     *
+     * @param src1 first input array.
+     * @return automatically generated
+     */
+    public static double norm(Mat src1) {
+        return norm_5(src1.nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::solvePoly(Mat coeffs, Mat& roots, int maxIters = 300)
+    //
+
+    /**
+     * Finds the real or complex roots of a polynomial equation.
+     *
+     * The function cv::solvePoly finds real and complex roots of a polynomial equation:
+     * \(\texttt{coeffs} [n] x^{n} +  \texttt{coeffs} [n-1] x^{n-1} + ... +  \texttt{coeffs} [1] x +  \texttt{coeffs} [0] = 0\)
+     * @param coeffs array of polynomial coefficients.
+     * @param roots output (complex) array of roots.
+     * @param maxIters maximum number of iterations the algorithm does.
+     * @return automatically generated
+     */
+    public static double solvePoly(Mat coeffs, Mat roots, int maxIters) {
+        return solvePoly_0(coeffs.nativeObj, roots.nativeObj, maxIters);
+    }
+
+    /**
+     * Finds the real or complex roots of a polynomial equation.
+     *
+     * The function cv::solvePoly finds real and complex roots of a polynomial equation:
+     * \(\texttt{coeffs} [n] x^{n} +  \texttt{coeffs} [n-1] x^{n-1} + ... +  \texttt{coeffs} [1] x +  \texttt{coeffs} [0] = 0\)
+     * @param coeffs array of polynomial coefficients.
+     * @param roots output (complex) array of roots.
+     * @return automatically generated
+     */
+    public static double solvePoly(Mat coeffs, Mat roots) {
+        return solvePoly_1(coeffs.nativeObj, roots.nativeObj);
+    }
+
+
+    //
+    // C++:  float cv::cubeRoot(float val)
+    //
+
+    /**
+     * Computes the cube root of an argument.
+     *
+     *  The function cubeRoot computes \(\sqrt[3]{\texttt{val}}\). Negative arguments are handled correctly.
+     *  NaN and Inf are not handled. The accuracy approaches the maximum possible accuracy for
+     *  single-precision data.
+     *  @param val A function argument.
+     * @return automatically generated
+     */
+    public static float cubeRoot(float val) {
+        return cubeRoot_0(val);
+    }
+
+
+    //
+    // C++:  float cv::fastAtan2(float y, float x)
+    //
+
+    /**
+     * Calculates the angle of a 2D vector in degrees.
+     *
+     *  The function fastAtan2 calculates the full-range angle of an input 2D vector. The angle is measured
+     *  in degrees and varies from 0 to 360 degrees. The accuracy is about 0.3 degrees.
+     *  @param x x-coordinate of the vector.
+     *  @param y y-coordinate of the vector.
+     * @return automatically generated
+     */
+    public static float fastAtan2(float y, float x) {
+        return fastAtan2_0(y, x);
+    }
+
+
+    //
+    // C++:  int cv::borderInterpolate(int p, int len, int borderType)
+    //
+
+    /**
+     * Computes the source location of an extrapolated pixel.
+     *
+     * The function computes and returns the coordinate of a donor pixel corresponding to the specified
+     * extrapolated pixel when using the specified extrapolation border mode. For example, if you use
+     * cv::BORDER_WRAP mode in the horizontal direction, cv::BORDER_REFLECT_101 in the vertical direction and
+     * want to compute value of the "virtual" pixel Point(-5, 100) in a floating-point image img , it
+     * looks like:
+     * <code>
+     *     float val = img.at&lt;float&gt;(borderInterpolate(100, img.rows, cv::BORDER_REFLECT_101),
+     *                               borderInterpolate(-5, img.cols, cv::BORDER_WRAP));
+     * </code>
+     * Normally, the function is not called directly. It is used inside filtering functions and also in
+     * copyMakeBorder.
+     * @param p 0-based coordinate of the extrapolated pixel along one of the axes, likely &lt;0 or &gt;= len
+     * @param len Length of the array along the corresponding axis.
+     * @param borderType Border type, one of the #BorderTypes, except for #BORDER_TRANSPARENT and
+     * #BORDER_ISOLATED . When borderType==#BORDER_CONSTANT , the function always returns -1, regardless
+     * of p and len.
+     *
+     * SEE: copyMakeBorder
+     * @return automatically generated
+     */
+    public static int borderInterpolate(int p, int len, int borderType) {
+        return borderInterpolate_0(p, len, borderType);
+    }
+
+
+    //
+    // C++:  int cv::countNonZero(Mat src)
+    //
+
+    /**
+     * Counts non-zero array elements.
+     *
+     * The function returns the number of non-zero elements in src :
+     * \(\sum _{I: \; \texttt{src} (I) \ne0 } 1\)
+     * @param src single-channel array.
+     * SEE:  mean, meanStdDev, norm, minMaxLoc, calcCovarMatrix
+     * @return automatically generated
+     */
+    public static int countNonZero(Mat src) {
+        return countNonZero_0(src.nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::getNumThreads()
+    //
+
+    /**
+     * Returns the number of threads used by OpenCV for parallel regions.
+     *
+     * Always returns 1 if OpenCV is built without threading support.
+     *
+     * The exact meaning of return value depends on the threading framework used by OpenCV library:
+     * <ul>
+     *   <li>
+     *  {@code TBB} - The number of threads, that OpenCV will try to use for parallel regions. If there is
+     *   any tbb::thread_scheduler_init in user code conflicting with OpenCV, then function returns
+     *   default number of threads used by TBB library.
+     *   </li>
+     *   <li>
+     *  {@code OpenMP} - An upper bound on the number of threads that could be used to form a new team.
+     *   </li>
+     *   <li>
+     *  {@code Concurrency} - The number of threads, that OpenCV will try to use for parallel regions.
+     *   </li>
+     *   <li>
+     *  {@code GCD} - Unsupported; returns the GCD thread pool limit (512) for compatibility.
+     *   </li>
+     *   <li>
+     *  {@code C=} - The number of threads, that OpenCV will try to use for parallel regions, if before
+     *   called setNumThreads with threads &gt; 0, otherwise returns the number of logical CPUs,
+     *   available for the process.
+     * SEE: setNumThreads, getThreadNum
+     *   </li>
+     * </ul>
+     * @return automatically generated
+     */
+    public static int getNumThreads() {
+        return getNumThreads_0();
+    }
+
+
+    //
+    // C++:  int cv::getNumberOfCPUs()
+    //
+
+    /**
+     * Returns the number of logical CPUs available for the process.
+     * @return automatically generated
+     */
+    public static int getNumberOfCPUs() {
+        return getNumberOfCPUs_0();
+    }
+
+
+    //
+    // C++:  int cv::getOptimalDFTSize(int vecsize)
+    //
+
+    /**
+     * Returns the optimal DFT size for a given vector size.
+     *
+     * DFT performance is not a monotonic function of a vector size. Therefore, when you calculate
+     * convolution of two arrays or perform the spectral analysis of an array, it usually makes sense to
+     * pad the input data with zeros to get a bit larger array that can be transformed much faster than the
+     * original one. Arrays whose size is a power-of-two (2, 4, 8, 16, 32, ...) are the fastest to process.
+     * Though, the arrays whose size is a product of 2's, 3's, and 5's (for example, 300 = 5\*5\*3\*2\*2)
+     * are also processed quite efficiently.
+     *
+     * The function cv::getOptimalDFTSize returns the minimum number N that is greater than or equal to vecsize
+     * so that the DFT of a vector of size N can be processed efficiently. In the current implementation N
+     * = 2 ^p^ \* 3 ^q^ \* 5 ^r^ for some integer p, q, r.
+     *
+     * The function returns a negative number if vecsize is too large (very close to INT_MAX ).
+     *
+     * While the function cannot be used directly to estimate the optimal vector size for DCT transform
+     * (since the current DCT implementation supports only even-size vectors), it can be easily processed
+     * as getOptimalDFTSize((vecsize+1)/2)\*2.
+     * @param vecsize vector size.
+     * SEE: dft , dct , idft , idct , mulSpectrums
+     * @return automatically generated
+     */
+    public static int getOptimalDFTSize(int vecsize) {
+        return getOptimalDFTSize_0(vecsize);
+    }
+
+
+    //
+    // C++:  int cv::getThreadNum()
+    //
+
+    /**
+     * Returns the index of the currently executed thread within the current parallel region. Always
+     * returns 0 if called outside of parallel region.
+     *
+     * @deprecated Current implementation doesn't corresponding to this documentation.
+     *
+     * The exact meaning of the return value depends on the threading framework used by OpenCV library:
+     * <ul>
+     *   <li>
+     *  {@code TBB} - Unsupported with current 4.1 TBB release. Maybe will be supported in future.
+     *   </li>
+     *   <li>
+     *  {@code OpenMP} - The thread number, within the current team, of the calling thread.
+     *   </li>
+     *   <li>
+     *  {@code Concurrency} - An ID for the virtual processor that the current context is executing on (0
+     *   for master thread and unique number for others, but not necessary 1,2,3,...).
+     *   </li>
+     *   <li>
+     *  {@code GCD} - System calling thread's ID. Never returns 0 inside parallel region.
+     *   </li>
+     *   <li>
+     *  {@code C=} - The index of the current parallel task.
+     * SEE: setNumThreads, getNumThreads
+     *   </li>
+     * </ul>
+     * @return automatically generated
+     */
+    @Deprecated
+    public static int getThreadNum() {
+        return getThreadNum_0();
+    }
+
+
+    //
+    // C++:  int cv::getVersionMajor()
+    //
+
+    /**
+     * Returns major library version
+     * @return automatically generated
+     */
+    public static int getVersionMajor() {
+        return getVersionMajor_0();
+    }
+
+
+    //
+    // C++:  int cv::getVersionMinor()
+    //
+
+    /**
+     * Returns minor library version
+     * @return automatically generated
+     */
+    public static int getVersionMinor() {
+        return getVersionMinor_0();
+    }
+
+
+    //
+    // C++:  int cv::getVersionRevision()
+    //
+
+    /**
+     * Returns revision field of the library version
+     * @return automatically generated
+     */
+    public static int getVersionRevision() {
+        return getVersionRevision_0();
+    }
+
+
+    //
+    // C++:  int cv::solveCubic(Mat coeffs, Mat& roots)
+    //
+
+    /**
+     * Finds the real roots of a cubic equation.
+     *
+     * The function solveCubic finds the real roots of a cubic equation:
+     * <ul>
+     *   <li>
+     *    if coeffs is a 4-element vector:
+     * \(\texttt{coeffs} [0] x^3 +  \texttt{coeffs} [1] x^2 +  \texttt{coeffs} [2] x +  \texttt{coeffs} [3] = 0\)
+     *   </li>
+     *   <li>
+     *    if coeffs is a 3-element vector:
+     * \(x^3 +  \texttt{coeffs} [0] x^2 +  \texttt{coeffs} [1] x +  \texttt{coeffs} [2] = 0\)
+     *   </li>
+     * </ul>
+     *
+     * The roots are stored in the roots array.
+     * @param coeffs equation coefficients, an array of 3 or 4 elements.
+     * @param roots output array of real roots that has 1 or 3 elements.
+     * @return number of real roots. It can be 0, 1 or 2.
+     */
+    public static int solveCubic(Mat coeffs, Mat roots) {
+        return solveCubic_0(coeffs.nativeObj, roots.nativeObj);
+    }
+
+
+    //
+    // C++:  int64 cv::getCPUTickCount()
+    //
+
+    /**
+     * Returns the number of CPU ticks.
+     *
+     * The function returns the current number of CPU ticks on some architectures (such as x86, x64,
+     * PowerPC). On other platforms the function is equivalent to getTickCount. It can also be used for
+     * very accurate time measurements, as well as for RNG initialization. Note that in case of multi-CPU
+     * systems a thread, from which getCPUTickCount is called, can be suspended and resumed at another CPU
+     * with its own counter. So, theoretically (and practically) the subsequent calls to the function do
+     * not necessary return the monotonously increasing values. Also, since a modern CPU varies the CPU
+     * frequency depending on the load, the number of CPU clocks spent in some code cannot be directly
+     * converted to time units. Therefore, getTickCount is generally a preferable solution for measuring
+     * execution time.
+     * @return automatically generated
+     */
+    public static long getCPUTickCount() {
+        return getCPUTickCount_0();
+    }
+
+
+    //
+    // C++:  int64 cv::getTickCount()
+    //
+
+    /**
+     * Returns the number of ticks.
+     *
+     * The function returns the number of ticks after the certain event (for example, when the machine was
+     * turned on). It can be used to initialize RNG or to measure a function execution time by reading the
+     * tick count before and after the function call.
+     * SEE: getTickFrequency, TickMeter
+     * @return automatically generated
+     */
+    public static long getTickCount() {
+        return getTickCount_0();
+    }
+
+
+    //
+    // C++:  string cv::getCPUFeaturesLine()
+    //
+
+    // Return type 'string' is not supported, skipping the function
+
+
+    //
+    // C++:  void cv::LUT(Mat src, Mat lut, Mat& dst)
+    //
+
+    /**
+     * Performs a look-up table transform of an array.
+     *
+     * The function LUT fills the output array with values from the look-up table. Indices of the entries
+     * are taken from the input array. That is, the function processes each element of src as follows:
+     * \(\texttt{dst} (I)  \leftarrow \texttt{lut(src(I) + d)}\)
+     * where
+     * \(d =  \fork{0}{if \(\texttt{src}\) has depth \(\texttt{CV_8U}\)}{128}{if \(\texttt{src}\) has depth \(\texttt{CV_8S}\)}\)
+     * @param src input array of 8-bit elements.
+     * @param lut look-up table of 256 elements; in case of multi-channel input array, the table should
+     * either have a single channel (in this case the same table is used for all channels) or the same
+     * number of channels as in the input array.
+     * @param dst output array of the same size and number of channels as src, and the same depth as lut.
+     * SEE:  convertScaleAbs, Mat::convertTo
+     */
+    public static void LUT(Mat src, Mat lut, Mat dst) {
+        LUT_0(src.nativeObj, lut.nativeObj, dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::PCABackProject(Mat data, Mat mean, Mat eigenvectors, Mat& result)
+    //
+
+    /**
+     * wrap PCA::backProject
+     * @param data automatically generated
+     * @param mean automatically generated
+     * @param eigenvectors automatically generated
+     * @param result automatically generated
+     */
+    public static void PCABackProject(Mat data, Mat mean, Mat eigenvectors, Mat result) {
+        PCABackProject_0(data.nativeObj, mean.nativeObj, eigenvectors.nativeObj, result.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::PCACompute(Mat data, Mat& mean, Mat& eigenvectors, Mat& eigenvalues, double retainedVariance)
+    //
+
+    /**
+     * wrap PCA::operator() and add eigenvalues output parameter
+     * @param data automatically generated
+     * @param mean automatically generated
+     * @param eigenvectors automatically generated
+     * @param eigenvalues automatically generated
+     * @param retainedVariance automatically generated
+     */
+    public static void PCACompute2(Mat data, Mat mean, Mat eigenvectors, Mat eigenvalues, double retainedVariance) {
+        PCACompute2_0(data.nativeObj, mean.nativeObj, eigenvectors.nativeObj, eigenvalues.nativeObj, retainedVariance);
+    }
+
+
+    //
+    // C++:  void cv::PCACompute(Mat data, Mat& mean, Mat& eigenvectors, Mat& eigenvalues, int maxComponents = 0)
+    //
+
+    /**
+     * wrap PCA::operator() and add eigenvalues output parameter
+     * @param data automatically generated
+     * @param mean automatically generated
+     * @param eigenvectors automatically generated
+     * @param eigenvalues automatically generated
+     * @param maxComponents automatically generated
+     */
+    public static void PCACompute2(Mat data, Mat mean, Mat eigenvectors, Mat eigenvalues, int maxComponents) {
+        PCACompute2_1(data.nativeObj, mean.nativeObj, eigenvectors.nativeObj, eigenvalues.nativeObj, maxComponents);
+    }
+
+    /**
+     * wrap PCA::operator() and add eigenvalues output parameter
+     * @param data automatically generated
+     * @param mean automatically generated
+     * @param eigenvectors automatically generated
+     * @param eigenvalues automatically generated
+     */
+    public static void PCACompute2(Mat data, Mat mean, Mat eigenvectors, Mat eigenvalues) {
+        PCACompute2_2(data.nativeObj, mean.nativeObj, eigenvectors.nativeObj, eigenvalues.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::PCACompute(Mat data, Mat& mean, Mat& eigenvectors, double retainedVariance)
+    //
+
+    /**
+     * wrap PCA::operator()
+     * @param data automatically generated
+     * @param mean automatically generated
+     * @param eigenvectors automatically generated
+     * @param retainedVariance automatically generated
+     */
+    public static void PCACompute(Mat data, Mat mean, Mat eigenvectors, double retainedVariance) {
+        PCACompute_0(data.nativeObj, mean.nativeObj, eigenvectors.nativeObj, retainedVariance);
+    }
+
+
+    //
+    // C++:  void cv::PCACompute(Mat data, Mat& mean, Mat& eigenvectors, int maxComponents = 0)
+    //
+
+    /**
+     * wrap PCA::operator()
+     * @param data automatically generated
+     * @param mean automatically generated
+     * @param eigenvectors automatically generated
+     * @param maxComponents automatically generated
+     */
+    public static void PCACompute(Mat data, Mat mean, Mat eigenvectors, int maxComponents) {
+        PCACompute_1(data.nativeObj, mean.nativeObj, eigenvectors.nativeObj, maxComponents);
+    }
+
+    /**
+     * wrap PCA::operator()
+     * @param data automatically generated
+     * @param mean automatically generated
+     * @param eigenvectors automatically generated
+     */
+    public static void PCACompute(Mat data, Mat mean, Mat eigenvectors) {
+        PCACompute_2(data.nativeObj, mean.nativeObj, eigenvectors.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::PCAProject(Mat data, Mat mean, Mat eigenvectors, Mat& result)
+    //
+
+    /**
+     * wrap PCA::project
+     * @param data automatically generated
+     * @param mean automatically generated
+     * @param eigenvectors automatically generated
+     * @param result automatically generated
+     */
+    public static void PCAProject(Mat data, Mat mean, Mat eigenvectors, Mat result) {
+        PCAProject_0(data.nativeObj, mean.nativeObj, eigenvectors.nativeObj, result.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::SVBackSubst(Mat w, Mat u, Mat vt, Mat rhs, Mat& dst)
+    //
+
+    /**
+     * wrap SVD::backSubst
+     * @param w automatically generated
+     * @param u automatically generated
+     * @param vt automatically generated
+     * @param rhs automatically generated
+     * @param dst automatically generated
+     */
+    public static void SVBackSubst(Mat w, Mat u, Mat vt, Mat rhs, Mat dst) {
+        SVBackSubst_0(w.nativeObj, u.nativeObj, vt.nativeObj, rhs.nativeObj, dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::SVDecomp(Mat src, Mat& w, Mat& u, Mat& vt, int flags = 0)
+    //
+
+    /**
+     * wrap SVD::compute
+     * @param src automatically generated
+     * @param w automatically generated
+     * @param u automatically generated
+     * @param vt automatically generated
+     * @param flags automatically generated
+     */
+    public static void SVDecomp(Mat src, Mat w, Mat u, Mat vt, int flags) {
+        SVDecomp_0(src.nativeObj, w.nativeObj, u.nativeObj, vt.nativeObj, flags);
+    }
+
+    /**
+     * wrap SVD::compute
+     * @param src automatically generated
+     * @param w automatically generated
+     * @param u automatically generated
+     * @param vt automatically generated
+     */
+    public static void SVDecomp(Mat src, Mat w, Mat u, Mat vt) {
+        SVDecomp_1(src.nativeObj, w.nativeObj, u.nativeObj, vt.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::absdiff(Mat src1, Mat src2, Mat& dst)
+    //
+
+    /**
+     * Calculates the per-element absolute difference between two arrays or between an array and a scalar.
+     *
+     * The function cv::absdiff calculates:
+     * Absolute difference between two arrays when they have the same
+     *     size and type:
+     *     \(\texttt{dst}(I) =  \texttt{saturate} (| \texttt{src1}(I) -  \texttt{src2}(I)|)\)
+     * Absolute difference between an array and a scalar when the second
+     *     array is constructed from Scalar or has as many elements as the
+     *     number of channels in {@code src1}:
+     *     \(\texttt{dst}(I) =  \texttt{saturate} (| \texttt{src1}(I) -  \texttt{src2} |)\)
+     * Absolute difference between a scalar and an array when the first
+     *     array is constructed from Scalar or has as many elements as the
+     *     number of channels in {@code src2}:
+     *     \(\texttt{dst}(I) =  \texttt{saturate} (| \texttt{src1} -  \texttt{src2}(I) |)\)
+     *     where I is a multi-dimensional index of array elements. In case of
+     *     multi-channel arrays, each channel is processed independently.
+     * <b>Note:</b> Saturation is not applied when the arrays have the depth CV_32S.
+     * You may even get a negative value in the case of overflow.
+     * @param src1 first input array or a scalar.
+     * @param src2 second input array or a scalar.
+     * @param dst output array that has the same size and type as input arrays.
+     * SEE: cv::abs(const Mat&amp;)
+     */
+    public static void absdiff(Mat src1, Mat src2, Mat dst) {
+        absdiff_0(src1.nativeObj, src2.nativeObj, dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::absdiff(Mat src1, Scalar src2, Mat& dst)
+    //
+
+    public static void absdiff(Mat src1, Scalar src2, Mat dst) {
+        absdiff_1(src1.nativeObj, src2.val[0], src2.val[1], src2.val[2], src2.val[3], dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::add(Mat src1, Mat src2, Mat& dst, Mat mask = Mat(), int dtype = -1)
+    //
+
+    /**
+     * Calculates the per-element sum of two arrays or an array and a scalar.
+     *
+     * The function add calculates:
+     * <ul>
+     *   <li>
+     *  Sum of two arrays when both input arrays have the same size and the same number of channels:
+     * \(\texttt{dst}(I) =  \texttt{saturate} ( \texttt{src1}(I) +  \texttt{src2}(I)) \quad \texttt{if mask}(I) \ne0\)
+     *   </li>
+     *   <li>
+     *  Sum of an array and a scalar when src2 is constructed from Scalar or has the same number of
+     * elements as {@code src1.channels()}:
+     * \(\texttt{dst}(I) =  \texttt{saturate} ( \texttt{src1}(I) +  \texttt{src2} ) \quad \texttt{if mask}(I) \ne0\)
+     *   </li>
+     *   <li>
+     *  Sum of a scalar and an array when src1 is constructed from Scalar or has the same number of
+     * elements as {@code src2.channels()}:
+     * \(\texttt{dst}(I) =  \texttt{saturate} ( \texttt{src1} +  \texttt{src2}(I) ) \quad \texttt{if mask}(I) \ne0\)
+     * where {@code I} is a multi-dimensional index of array elements. In case of multi-channel arrays, each
+     * channel is processed independently.
+     *   </li>
+     * </ul>
+     *
+     * The first function in the list above can be replaced with matrix expressions:
+     * <code>
+     *     dst = src1 + src2;
+     *     dst += src1; // equivalent to add(dst, src1, dst);
+     * </code>
+     * The input arrays and the output array can all have the same or different depths. For example, you
+     * can add a 16-bit unsigned array to a 8-bit signed array and store the sum as a 32-bit
+     * floating-point array. Depth of the output array is determined by the dtype parameter. In the second
+     * and third cases above, as well as in the first case, when src1.depth() == src2.depth(), dtype can
+     * be set to the default -1. In this case, the output array will have the same depth as the input
+     * array, be it src1, src2 or both.
+     * <b>Note:</b> Saturation is not applied when the output array has the depth CV_32S. You may even get
+     * result of an incorrect sign in the case of overflow.
+     * @param src1 first input array or a scalar.
+     * @param src2 second input array or a scalar.
+     * @param dst output array that has the same size and number of channels as the input array(s); the
+     * depth is defined by dtype or src1/src2.
+     * @param mask optional operation mask - 8-bit single channel array, that specifies elements of the
+     * output array to be changed.
+     * @param dtype optional depth of the output array (see the discussion below).
+     * SEE: subtract, addWeighted, scaleAdd, Mat::convertTo
+     */
+    public static void add(Mat src1, Mat src2, Mat dst, Mat mask, int dtype) {
+        add_0(src1.nativeObj, src2.nativeObj, dst.nativeObj, mask.nativeObj, dtype);
+    }
+
+    /**
+     * Calculates the per-element sum of two arrays or an array and a scalar.
+     *
+     * The function add calculates:
+     * <ul>
+     *   <li>
+     *  Sum of two arrays when both input arrays have the same size and the same number of channels:
+     * \(\texttt{dst}(I) =  \texttt{saturate} ( \texttt{src1}(I) +  \texttt{src2}(I)) \quad \texttt{if mask}(I) \ne0\)
+     *   </li>
+     *   <li>
+     *  Sum of an array and a scalar when src2 is constructed from Scalar or has the same number of
+     * elements as {@code src1.channels()}:
+     * \(\texttt{dst}(I) =  \texttt{saturate} ( \texttt{src1}(I) +  \texttt{src2} ) \quad \texttt{if mask}(I) \ne0\)
+     *   </li>
+     *   <li>
+     *  Sum of a scalar and an array when src1 is constructed from Scalar or has the same number of
+     * elements as {@code src2.channels()}:
+     * \(\texttt{dst}(I) =  \texttt{saturate} ( \texttt{src1} +  \texttt{src2}(I) ) \quad \texttt{if mask}(I) \ne0\)
+     * where {@code I} is a multi-dimensional index of array elements. In case of multi-channel arrays, each
+     * channel is processed independently.
+     *   </li>
+     * </ul>
+     *
+     * The first function in the list above can be replaced with matrix expressions:
+     * <code>
+     *     dst = src1 + src2;
+     *     dst += src1; // equivalent to add(dst, src1, dst);
+     * </code>
+     * The input arrays and the output array can all have the same or different depths. For example, you
+     * can add a 16-bit unsigned array to a 8-bit signed array and store the sum as a 32-bit
+     * floating-point array. Depth of the output array is determined by the dtype parameter. In the second
+     * and third cases above, as well as in the first case, when src1.depth() == src2.depth(), dtype can
+     * be set to the default -1. In this case, the output array will have the same depth as the input
+     * array, be it src1, src2 or both.
+     * <b>Note:</b> Saturation is not applied when the output array has the depth CV_32S. You may even get
+     * result of an incorrect sign in the case of overflow.
+     * @param src1 first input array or a scalar.
+     * @param src2 second input array or a scalar.
+     * @param dst output array that has the same size and number of channels as the input array(s); the
+     * depth is defined by dtype or src1/src2.
+     * @param mask optional operation mask - 8-bit single channel array, that specifies elements of the
+     * output array to be changed.
+     * SEE: subtract, addWeighted, scaleAdd, Mat::convertTo
+     */
+    public static void add(Mat src1, Mat src2, Mat dst, Mat mask) {
+        add_1(src1.nativeObj, src2.nativeObj, dst.nativeObj, mask.nativeObj);
+    }
+
+    /**
+     * Calculates the per-element sum of two arrays or an array and a scalar.
+     *
+     * The function add calculates:
+     * <ul>
+     *   <li>
+     *  Sum of two arrays when both input arrays have the same size and the same number of channels:
+     * \(\texttt{dst}(I) =  \texttt{saturate} ( \texttt{src1}(I) +  \texttt{src2}(I)) \quad \texttt{if mask}(I) \ne0\)
+     *   </li>
+     *   <li>
+     *  Sum of an array and a scalar when src2 is constructed from Scalar or has the same number of
+     * elements as {@code src1.channels()}:
+     * \(\texttt{dst}(I) =  \texttt{saturate} ( \texttt{src1}(I) +  \texttt{src2} ) \quad \texttt{if mask}(I) \ne0\)
+     *   </li>
+     *   <li>
+     *  Sum of a scalar and an array when src1 is constructed from Scalar or has the same number of
+     * elements as {@code src2.channels()}:
+     * \(\texttt{dst}(I) =  \texttt{saturate} ( \texttt{src1} +  \texttt{src2}(I) ) \quad \texttt{if mask}(I) \ne0\)
+     * where {@code I} is a multi-dimensional index of array elements. In case of multi-channel arrays, each
+     * channel is processed independently.
+     *   </li>
+     * </ul>
+     *
+     * The first function in the list above can be replaced with matrix expressions:
+     * <code>
+     *     dst = src1 + src2;
+     *     dst += src1; // equivalent to add(dst, src1, dst);
+     * </code>
+     * The input arrays and the output array can all have the same or different depths. For example, you
+     * can add a 16-bit unsigned array to a 8-bit signed array and store the sum as a 32-bit
+     * floating-point array. Depth of the output array is determined by the dtype parameter. In the second
+     * and third cases above, as well as in the first case, when src1.depth() == src2.depth(), dtype can
+     * be set to the default -1. In this case, the output array will have the same depth as the input
+     * array, be it src1, src2 or both.
+     * <b>Note:</b> Saturation is not applied when the output array has the depth CV_32S. You may even get
+     * result of an incorrect sign in the case of overflow.
+     * @param src1 first input array or a scalar.
+     * @param src2 second input array or a scalar.
+     * @param dst output array that has the same size and number of channels as the input array(s); the
+     * depth is defined by dtype or src1/src2.
+     * output array to be changed.
+     * SEE: subtract, addWeighted, scaleAdd, Mat::convertTo
+     */
+    public static void add(Mat src1, Mat src2, Mat dst) {
+        add_2(src1.nativeObj, src2.nativeObj, dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::add(Mat src1, Scalar src2, Mat& dst, Mat mask = Mat(), int dtype = -1)
+    //
+
+    public static void add(Mat src1, Scalar src2, Mat dst, Mat mask, int dtype) {
+        add_3(src1.nativeObj, src2.val[0], src2.val[1], src2.val[2], src2.val[3], dst.nativeObj, mask.nativeObj, dtype);
+    }
+
+    public static void add(Mat src1, Scalar src2, Mat dst, Mat mask) {
+        add_4(src1.nativeObj, src2.val[0], src2.val[1], src2.val[2], src2.val[3], dst.nativeObj, mask.nativeObj);
+    }
+
+    public static void add(Mat src1, Scalar src2, Mat dst) {
+        add_5(src1.nativeObj, src2.val[0], src2.val[1], src2.val[2], src2.val[3], dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::addWeighted(Mat src1, double alpha, Mat src2, double beta, double gamma, Mat& dst, int dtype = -1)
+    //
+
+    /**
+     * Calculates the weighted sum of two arrays.
+     *
+     * The function addWeighted calculates the weighted sum of two arrays as follows:
+     * \(\texttt{dst} (I)= \texttt{saturate} ( \texttt{src1} (I)* \texttt{alpha} +  \texttt{src2} (I)* \texttt{beta} +  \texttt{gamma} )\)
+     * where I is a multi-dimensional index of array elements. In case of multi-channel arrays, each
+     * channel is processed independently.
+     * The function can be replaced with a matrix expression:
+     * <code>
+     *     dst = src1*alpha + src2*beta + gamma;
+     * </code>
+     * <b>Note:</b> Saturation is not applied when the output array has the depth CV_32S. You may even get
+     * result of an incorrect sign in the case of overflow.
+     * @param src1 first input array.
+     * @param alpha weight of the first array elements.
+     * @param src2 second input array of the same size and channel number as src1.
+     * @param beta weight of the second array elements.
+     * @param gamma scalar added to each sum.
+     * @param dst output array that has the same size and number of channels as the input arrays.
+     * @param dtype optional depth of the output array; when both input arrays have the same depth, dtype
+     * can be set to -1, which will be equivalent to src1.depth().
+     * SEE:  add, subtract, scaleAdd, Mat::convertTo
+     */
+    public static void addWeighted(Mat src1, double alpha, Mat src2, double beta, double gamma, Mat dst, int dtype) {
+        addWeighted_0(src1.nativeObj, alpha, src2.nativeObj, beta, gamma, dst.nativeObj, dtype);
+    }
+
+    /**
+     * Calculates the weighted sum of two arrays.
+     *
+     * The function addWeighted calculates the weighted sum of two arrays as follows:
+     * \(\texttt{dst} (I)= \texttt{saturate} ( \texttt{src1} (I)* \texttt{alpha} +  \texttt{src2} (I)* \texttt{beta} +  \texttt{gamma} )\)
+     * where I is a multi-dimensional index of array elements. In case of multi-channel arrays, each
+     * channel is processed independently.
+     * The function can be replaced with a matrix expression:
+     * <code>
+     *     dst = src1*alpha + src2*beta + gamma;
+     * </code>
+     * <b>Note:</b> Saturation is not applied when the output array has the depth CV_32S. You may even get
+     * result of an incorrect sign in the case of overflow.
+     * @param src1 first input array.
+     * @param alpha weight of the first array elements.
+     * @param src2 second input array of the same size and channel number as src1.
+     * @param beta weight of the second array elements.
+     * @param gamma scalar added to each sum.
+     * @param dst output array that has the same size and number of channels as the input arrays.
+     * can be set to -1, which will be equivalent to src1.depth().
+     * SEE:  add, subtract, scaleAdd, Mat::convertTo
+     */
+    public static void addWeighted(Mat src1, double alpha, Mat src2, double beta, double gamma, Mat dst) {
+        addWeighted_1(src1.nativeObj, alpha, src2.nativeObj, beta, gamma, dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::batchDistance(Mat src1, Mat src2, Mat& dist, int dtype, Mat& nidx, int normType = NORM_L2, int K = 0, Mat mask = Mat(), int update = 0, bool crosscheck = false)
+    //
+
+    /**
+     * naive nearest neighbor finder
+     *
+     * see http://en.wikipedia.org/wiki/Nearest_neighbor_search
+     * TODO: document
+     * @param src1 automatically generated
+     * @param src2 automatically generated
+     * @param dist automatically generated
+     * @param dtype automatically generated
+     * @param nidx automatically generated
+     * @param normType automatically generated
+     * @param K automatically generated
+     * @param mask automatically generated
+     * @param update automatically generated
+     * @param crosscheck automatically generated
+     */
+    public static void batchDistance(Mat src1, Mat src2, Mat dist, int dtype, Mat nidx, int normType, int K, Mat mask, int update, boolean crosscheck) {
+        batchDistance_0(src1.nativeObj, src2.nativeObj, dist.nativeObj, dtype, nidx.nativeObj, normType, K, mask.nativeObj, update, crosscheck);
+    }
+
+    /**
+     * naive nearest neighbor finder
+     *
+     * see http://en.wikipedia.org/wiki/Nearest_neighbor_search
+     * TODO: document
+     * @param src1 automatically generated
+     * @param src2 automatically generated
+     * @param dist automatically generated
+     * @param dtype automatically generated
+     * @param nidx automatically generated
+     * @param normType automatically generated
+     * @param K automatically generated
+     * @param mask automatically generated
+     * @param update automatically generated
+     */
+    public static void batchDistance(Mat src1, Mat src2, Mat dist, int dtype, Mat nidx, int normType, int K, Mat mask, int update) {
+        batchDistance_1(src1.nativeObj, src2.nativeObj, dist.nativeObj, dtype, nidx.nativeObj, normType, K, mask.nativeObj, update);
+    }
+
+    /**
+     * naive nearest neighbor finder
+     *
+     * see http://en.wikipedia.org/wiki/Nearest_neighbor_search
+     * TODO: document
+     * @param src1 automatically generated
+     * @param src2 automatically generated
+     * @param dist automatically generated
+     * @param dtype automatically generated
+     * @param nidx automatically generated
+     * @param normType automatically generated
+     * @param K automatically generated
+     * @param mask automatically generated
+     */
+    public static void batchDistance(Mat src1, Mat src2, Mat dist, int dtype, Mat nidx, int normType, int K, Mat mask) {
+        batchDistance_2(src1.nativeObj, src2.nativeObj, dist.nativeObj, dtype, nidx.nativeObj, normType, K, mask.nativeObj);
+    }
+
+    /**
+     * naive nearest neighbor finder
+     *
+     * see http://en.wikipedia.org/wiki/Nearest_neighbor_search
+     * TODO: document
+     * @param src1 automatically generated
+     * @param src2 automatically generated
+     * @param dist automatically generated
+     * @param dtype automatically generated
+     * @param nidx automatically generated
+     * @param normType automatically generated
+     * @param K automatically generated
+     */
+    public static void batchDistance(Mat src1, Mat src2, Mat dist, int dtype, Mat nidx, int normType, int K) {
+        batchDistance_3(src1.nativeObj, src2.nativeObj, dist.nativeObj, dtype, nidx.nativeObj, normType, K);
+    }
+
+    /**
+     * naive nearest neighbor finder
+     *
+     * see http://en.wikipedia.org/wiki/Nearest_neighbor_search
+     * TODO: document
+     * @param src1 automatically generated
+     * @param src2 automatically generated
+     * @param dist automatically generated
+     * @param dtype automatically generated
+     * @param nidx automatically generated
+     * @param normType automatically generated
+     */
+    public static void batchDistance(Mat src1, Mat src2, Mat dist, int dtype, Mat nidx, int normType) {
+        batchDistance_4(src1.nativeObj, src2.nativeObj, dist.nativeObj, dtype, nidx.nativeObj, normType);
+    }
+
+    /**
+     * naive nearest neighbor finder
+     *
+     * see http://en.wikipedia.org/wiki/Nearest_neighbor_search
+     * TODO: document
+     * @param src1 automatically generated
+     * @param src2 automatically generated
+     * @param dist automatically generated
+     * @param dtype automatically generated
+     * @param nidx automatically generated
+     */
+    public static void batchDistance(Mat src1, Mat src2, Mat dist, int dtype, Mat nidx) {
+        batchDistance_5(src1.nativeObj, src2.nativeObj, dist.nativeObj, dtype, nidx.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::bitwise_and(Mat src1, Mat src2, Mat& dst, Mat mask = Mat())
+    //
+
+    /**
+     * computes bitwise conjunction of the two arrays (dst = src1 &amp; src2)
+     * Calculates the per-element bit-wise conjunction of two arrays or an
+     * array and a scalar.
+     *
+     * The function cv::bitwise_and calculates the per-element bit-wise logical conjunction for:
+     * Two arrays when src1 and src2 have the same size:
+     *     \(\texttt{dst} (I) =  \texttt{src1} (I)  \wedge \texttt{src2} (I) \quad \texttt{if mask} (I) \ne0\)
+     * An array and a scalar when src2 is constructed from Scalar or has
+     *     the same number of elements as {@code src1.channels()}:
+     *     \(\texttt{dst} (I) =  \texttt{src1} (I)  \wedge \texttt{src2} \quad \texttt{if mask} (I) \ne0\)
+     * A scalar and an array when src1 is constructed from Scalar or has
+     *     the same number of elements as {@code src2.channels()}:
+     *     \(\texttt{dst} (I) =  \texttt{src1}  \wedge \texttt{src2} (I) \quad \texttt{if mask} (I) \ne0\)
+     * In case of floating-point arrays, their machine-specific bit
+     * representations (usually IEEE754-compliant) are used for the operation.
+     * In case of multi-channel arrays, each channel is processed
+     * independently. In the second and third cases above, the scalar is first
+     * converted to the array type.
+     * @param src1 first input array or a scalar.
+     * @param src2 second input array or a scalar.
+     * @param dst output array that has the same size and type as the input
+     * arrays.
+     * @param mask optional operation mask, 8-bit single channel array, that
+     * specifies elements of the output array to be changed.
+     */
+    public static void bitwise_and(Mat src1, Mat src2, Mat dst, Mat mask) {
+        bitwise_and_0(src1.nativeObj, src2.nativeObj, dst.nativeObj, mask.nativeObj);
+    }
+
+    /**
+     * computes bitwise conjunction of the two arrays (dst = src1 &amp; src2)
+     * Calculates the per-element bit-wise conjunction of two arrays or an
+     * array and a scalar.
+     *
+     * The function cv::bitwise_and calculates the per-element bit-wise logical conjunction for:
+     * Two arrays when src1 and src2 have the same size:
+     *     \(\texttt{dst} (I) =  \texttt{src1} (I)  \wedge \texttt{src2} (I) \quad \texttt{if mask} (I) \ne0\)
+     * An array and a scalar when src2 is constructed from Scalar or has
+     *     the same number of elements as {@code src1.channels()}:
+     *     \(\texttt{dst} (I) =  \texttt{src1} (I)  \wedge \texttt{src2} \quad \texttt{if mask} (I) \ne0\)
+     * A scalar and an array when src1 is constructed from Scalar or has
+     *     the same number of elements as {@code src2.channels()}:
+     *     \(\texttt{dst} (I) =  \texttt{src1}  \wedge \texttt{src2} (I) \quad \texttt{if mask} (I) \ne0\)
+     * In case of floating-point arrays, their machine-specific bit
+     * representations (usually IEEE754-compliant) are used for the operation.
+     * In case of multi-channel arrays, each channel is processed
+     * independently. In the second and third cases above, the scalar is first
+     * converted to the array type.
+     * @param src1 first input array or a scalar.
+     * @param src2 second input array or a scalar.
+     * @param dst output array that has the same size and type as the input
+     * arrays.
+     * specifies elements of the output array to be changed.
+     */
+    public static void bitwise_and(Mat src1, Mat src2, Mat dst) {
+        bitwise_and_1(src1.nativeObj, src2.nativeObj, dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::bitwise_not(Mat src, Mat& dst, Mat mask = Mat())
+    //
+
+    /**
+     *  Inverts every bit of an array.
+     *
+     * The function cv::bitwise_not calculates per-element bit-wise inversion of the input
+     * array:
+     * \(\texttt{dst} (I) =  \neg \texttt{src} (I)\)
+     * In case of a floating-point input array, its machine-specific bit
+     * representation (usually IEEE754-compliant) is used for the operation. In
+     * case of multi-channel arrays, each channel is processed independently.
+     * @param src input array.
+     * @param dst output array that has the same size and type as the input
+     * array.
+     * @param mask optional operation mask, 8-bit single channel array, that
+     * specifies elements of the output array to be changed.
+     */
+    public static void bitwise_not(Mat src, Mat dst, Mat mask) {
+        bitwise_not_0(src.nativeObj, dst.nativeObj, mask.nativeObj);
+    }
+
+    /**
+     *  Inverts every bit of an array.
+     *
+     * The function cv::bitwise_not calculates per-element bit-wise inversion of the input
+     * array:
+     * \(\texttt{dst} (I) =  \neg \texttt{src} (I)\)
+     * In case of a floating-point input array, its machine-specific bit
+     * representation (usually IEEE754-compliant) is used for the operation. In
+     * case of multi-channel arrays, each channel is processed independently.
+     * @param src input array.
+     * @param dst output array that has the same size and type as the input
+     * array.
+     * specifies elements of the output array to be changed.
+     */
+    public static void bitwise_not(Mat src, Mat dst) {
+        bitwise_not_1(src.nativeObj, dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::bitwise_or(Mat src1, Mat src2, Mat& dst, Mat mask = Mat())
+    //
+
+    /**
+     * Calculates the per-element bit-wise disjunction of two arrays or an
+     * array and a scalar.
+     *
+     * The function cv::bitwise_or calculates the per-element bit-wise logical disjunction for:
+     * Two arrays when src1 and src2 have the same size:
+     *     \(\texttt{dst} (I) =  \texttt{src1} (I)  \vee \texttt{src2} (I) \quad \texttt{if mask} (I) \ne0\)
+     * An array and a scalar when src2 is constructed from Scalar or has
+     *     the same number of elements as {@code src1.channels()}:
+     *     \(\texttt{dst} (I) =  \texttt{src1} (I)  \vee \texttt{src2} \quad \texttt{if mask} (I) \ne0\)
+     * A scalar and an array when src1 is constructed from Scalar or has
+     *     the same number of elements as {@code src2.channels()}:
+     *     \(\texttt{dst} (I) =  \texttt{src1}  \vee \texttt{src2} (I) \quad \texttt{if mask} (I) \ne0\)
+     * In case of floating-point arrays, their machine-specific bit
+     * representations (usually IEEE754-compliant) are used for the operation.
+     * In case of multi-channel arrays, each channel is processed
+     * independently. In the second and third cases above, the scalar is first
+     * converted to the array type.
+     * @param src1 first input array or a scalar.
+     * @param src2 second input array or a scalar.
+     * @param dst output array that has the same size and type as the input
+     * arrays.
+     * @param mask optional operation mask, 8-bit single channel array, that
+     * specifies elements of the output array to be changed.
+     */
+    public static void bitwise_or(Mat src1, Mat src2, Mat dst, Mat mask) {
+        bitwise_or_0(src1.nativeObj, src2.nativeObj, dst.nativeObj, mask.nativeObj);
+    }
+
+    /**
+     * Calculates the per-element bit-wise disjunction of two arrays or an
+     * array and a scalar.
+     *
+     * The function cv::bitwise_or calculates the per-element bit-wise logical disjunction for:
+     * Two arrays when src1 and src2 have the same size:
+     *     \(\texttt{dst} (I) =  \texttt{src1} (I)  \vee \texttt{src2} (I) \quad \texttt{if mask} (I) \ne0\)
+     * An array and a scalar when src2 is constructed from Scalar or has
+     *     the same number of elements as {@code src1.channels()}:
+     *     \(\texttt{dst} (I) =  \texttt{src1} (I)  \vee \texttt{src2} \quad \texttt{if mask} (I) \ne0\)
+     * A scalar and an array when src1 is constructed from Scalar or has
+     *     the same number of elements as {@code src2.channels()}:
+     *     \(\texttt{dst} (I) =  \texttt{src1}  \vee \texttt{src2} (I) \quad \texttt{if mask} (I) \ne0\)
+     * In case of floating-point arrays, their machine-specific bit
+     * representations (usually IEEE754-compliant) are used for the operation.
+     * In case of multi-channel arrays, each channel is processed
+     * independently. In the second and third cases above, the scalar is first
+     * converted to the array type.
+     * @param src1 first input array or a scalar.
+     * @param src2 second input array or a scalar.
+     * @param dst output array that has the same size and type as the input
+     * arrays.
+     * specifies elements of the output array to be changed.
+     */
+    public static void bitwise_or(Mat src1, Mat src2, Mat dst) {
+        bitwise_or_1(src1.nativeObj, src2.nativeObj, dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::bitwise_xor(Mat src1, Mat src2, Mat& dst, Mat mask = Mat())
+    //
+
+    /**
+     * Calculates the per-element bit-wise "exclusive or" operation on two
+     * arrays or an array and a scalar.
+     *
+     * The function cv::bitwise_xor calculates the per-element bit-wise logical "exclusive-or"
+     * operation for:
+     * Two arrays when src1 and src2 have the same size:
+     *     \(\texttt{dst} (I) =  \texttt{src1} (I)  \oplus \texttt{src2} (I) \quad \texttt{if mask} (I) \ne0\)
+     * An array and a scalar when src2 is constructed from Scalar or has
+     *     the same number of elements as {@code src1.channels()}:
+     *     \(\texttt{dst} (I) =  \texttt{src1} (I)  \oplus \texttt{src2} \quad \texttt{if mask} (I) \ne0\)
+     * A scalar and an array when src1 is constructed from Scalar or has
+     *     the same number of elements as {@code src2.channels()}:
+     *     \(\texttt{dst} (I) =  \texttt{src1}  \oplus \texttt{src2} (I) \quad \texttt{if mask} (I) \ne0\)
+     * In case of floating-point arrays, their machine-specific bit
+     * representations (usually IEEE754-compliant) are used for the operation.
+     * In case of multi-channel arrays, each channel is processed
+     * independently. In the 2nd and 3rd cases above, the scalar is first
+     * converted to the array type.
+     * @param src1 first input array or a scalar.
+     * @param src2 second input array or a scalar.
+     * @param dst output array that has the same size and type as the input
+     * arrays.
+     * @param mask optional operation mask, 8-bit single channel array, that
+     * specifies elements of the output array to be changed.
+     */
+    public static void bitwise_xor(Mat src1, Mat src2, Mat dst, Mat mask) {
+        bitwise_xor_0(src1.nativeObj, src2.nativeObj, dst.nativeObj, mask.nativeObj);
+    }
+
+    /**
+     * Calculates the per-element bit-wise "exclusive or" operation on two
+     * arrays or an array and a scalar.
+     *
+     * The function cv::bitwise_xor calculates the per-element bit-wise logical "exclusive-or"
+     * operation for:
+     * Two arrays when src1 and src2 have the same size:
+     *     \(\texttt{dst} (I) =  \texttt{src1} (I)  \oplus \texttt{src2} (I) \quad \texttt{if mask} (I) \ne0\)
+     * An array and a scalar when src2 is constructed from Scalar or has
+     *     the same number of elements as {@code src1.channels()}:
+     *     \(\texttt{dst} (I) =  \texttt{src1} (I)  \oplus \texttt{src2} \quad \texttt{if mask} (I) \ne0\)
+     * A scalar and an array when src1 is constructed from Scalar or has
+     *     the same number of elements as {@code src2.channels()}:
+     *     \(\texttt{dst} (I) =  \texttt{src1}  \oplus \texttt{src2} (I) \quad \texttt{if mask} (I) \ne0\)
+     * In case of floating-point arrays, their machine-specific bit
+     * representations (usually IEEE754-compliant) are used for the operation.
+     * In case of multi-channel arrays, each channel is processed
+     * independently. In the 2nd and 3rd cases above, the scalar is first
+     * converted to the array type.
+     * @param src1 first input array or a scalar.
+     * @param src2 second input array or a scalar.
+     * @param dst output array that has the same size and type as the input
+     * arrays.
+     * specifies elements of the output array to be changed.
+     */
+    public static void bitwise_xor(Mat src1, Mat src2, Mat dst) {
+        bitwise_xor_1(src1.nativeObj, src2.nativeObj, dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::calcCovarMatrix(Mat samples, Mat& covar, Mat& mean, int flags, int ctype = CV_64F)
+    //
+
+    /**
+     *
+     * <b>Note:</b> use #COVAR_ROWS or #COVAR_COLS flag
+     * @param samples samples stored as rows/columns of a single matrix.
+     * @param covar output covariance matrix of the type ctype and square size.
+     * @param mean input or output (depending on the flags) array as the average value of the input vectors.
+     * @param flags operation flags as a combination of #CovarFlags
+     * @param ctype type of the matrixl; it equals 'CV_64F' by default.
+     */
+    public static void calcCovarMatrix(Mat samples, Mat covar, Mat mean, int flags, int ctype) {
+        calcCovarMatrix_0(samples.nativeObj, covar.nativeObj, mean.nativeObj, flags, ctype);
+    }
+
+    /**
+     *
+     * <b>Note:</b> use #COVAR_ROWS or #COVAR_COLS flag
+     * @param samples samples stored as rows/columns of a single matrix.
+     * @param covar output covariance matrix of the type ctype and square size.
+     * @param mean input or output (depending on the flags) array as the average value of the input vectors.
+     * @param flags operation flags as a combination of #CovarFlags
+     */
+    public static void calcCovarMatrix(Mat samples, Mat covar, Mat mean, int flags) {
+        calcCovarMatrix_1(samples.nativeObj, covar.nativeObj, mean.nativeObj, flags);
+    }
+
+
+    //
+    // C++:  void cv::cartToPolar(Mat x, Mat y, Mat& magnitude, Mat& angle, bool angleInDegrees = false)
+    //
+
+    /**
+     * Calculates the magnitude and angle of 2D vectors.
+     *
+     * The function cv::cartToPolar calculates either the magnitude, angle, or both
+     * for every 2D vector (x(I),y(I)):
+     * \(\begin{array}{l} \texttt{magnitude} (I)= \sqrt{\texttt{x}(I)^2+\texttt{y}(I)^2} , \\ \texttt{angle} (I)= \texttt{atan2} ( \texttt{y} (I), \texttt{x} (I))[ \cdot180 / \pi ] \end{array}\)
+     *
+     * The angles are calculated with accuracy about 0.3 degrees. For the point
+     * (0,0), the angle is set to 0.
+     * @param x array of x-coordinates; this must be a single-precision or
+     * double-precision floating-point array.
+     * @param y array of y-coordinates, that must have the same size and same type as x.
+     * @param magnitude output array of magnitudes of the same size and type as x.
+     * @param angle output array of angles that has the same size and type as
+     * x; the angles are measured in radians (from 0 to 2\*Pi) or in degrees (0 to 360 degrees).
+     * @param angleInDegrees a flag, indicating whether the angles are measured
+     * in radians (which is by default), or in degrees.
+     * SEE: Sobel, Scharr
+     */
+    public static void cartToPolar(Mat x, Mat y, Mat magnitude, Mat angle, boolean angleInDegrees) {
+        cartToPolar_0(x.nativeObj, y.nativeObj, magnitude.nativeObj, angle.nativeObj, angleInDegrees);
+    }
+
+    /**
+     * Calculates the magnitude and angle of 2D vectors.
+     *
+     * The function cv::cartToPolar calculates either the magnitude, angle, or both
+     * for every 2D vector (x(I),y(I)):
+     * \(\begin{array}{l} \texttt{magnitude} (I)= \sqrt{\texttt{x}(I)^2+\texttt{y}(I)^2} , \\ \texttt{angle} (I)= \texttt{atan2} ( \texttt{y} (I), \texttt{x} (I))[ \cdot180 / \pi ] \end{array}\)
+     *
+     * The angles are calculated with accuracy about 0.3 degrees. For the point
+     * (0,0), the angle is set to 0.
+     * @param x array of x-coordinates; this must be a single-precision or
+     * double-precision floating-point array.
+     * @param y array of y-coordinates, that must have the same size and same type as x.
+     * @param magnitude output array of magnitudes of the same size and type as x.
+     * @param angle output array of angles that has the same size and type as
+     * x; the angles are measured in radians (from 0 to 2\*Pi) or in degrees (0 to 360 degrees).
+     * in radians (which is by default), or in degrees.
+     * SEE: Sobel, Scharr
+     */
+    public static void cartToPolar(Mat x, Mat y, Mat magnitude, Mat angle) {
+        cartToPolar_1(x.nativeObj, y.nativeObj, magnitude.nativeObj, angle.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::compare(Mat src1, Mat src2, Mat& dst, int cmpop)
+    //
+
+    /**
+     * Performs the per-element comparison of two arrays or an array and scalar value.
+     *
+     * The function compares:
+     * Elements of two arrays when src1 and src2 have the same size:
+     *     \(\texttt{dst} (I) =  \texttt{src1} (I)  \,\texttt{cmpop}\, \texttt{src2} (I)\)
+     * Elements of src1 with a scalar src2 when src2 is constructed from
+     *     Scalar or has a single element:
+     *     \(\texttt{dst} (I) =  \texttt{src1}(I) \,\texttt{cmpop}\,  \texttt{src2}\)
+     * src1 with elements of src2 when src1 is constructed from Scalar or
+     *     has a single element:
+     *     \(\texttt{dst} (I) =  \texttt{src1}  \,\texttt{cmpop}\, \texttt{src2} (I)\)
+     * When the comparison result is true, the corresponding element of output
+     * array is set to 255. The comparison operations can be replaced with the
+     * equivalent matrix expressions:
+     * <code>
+     *     Mat dst1 = src1 &gt;= src2;
+     *     Mat dst2 = src1 &lt; 8;
+     *     ...
+     * </code>
+     * @param src1 first input array or a scalar; when it is an array, it must have a single channel.
+     * @param src2 second input array or a scalar; when it is an array, it must have a single channel.
+     * @param dst output array of type ref CV_8U that has the same size and the same number of channels as
+     *     the input arrays.
+     * @param cmpop a flag, that specifies correspondence between the arrays (cv::CmpTypes)
+     * SEE: checkRange, min, max, threshold
+     */
+    public static void compare(Mat src1, Mat src2, Mat dst, int cmpop) {
+        compare_0(src1.nativeObj, src2.nativeObj, dst.nativeObj, cmpop);
+    }
+
+
+    //
+    // C++:  void cv::compare(Mat src1, Scalar src2, Mat& dst, int cmpop)
+    //
+
+    public static void compare(Mat src1, Scalar src2, Mat dst, int cmpop) {
+        compare_1(src1.nativeObj, src2.val[0], src2.val[1], src2.val[2], src2.val[3], dst.nativeObj, cmpop);
+    }
+
+
+    //
+    // C++:  void cv::completeSymm(Mat& m, bool lowerToUpper = false)
+    //
+
+    /**
+     * Copies the lower or the upper half of a square matrix to its another half.
+     *
+     * The function cv::completeSymm copies the lower or the upper half of a square matrix to
+     * its another half. The matrix diagonal remains unchanged:
+     * <ul>
+     *   <li>
+     *   \(\texttt{m}_{ij}=\texttt{m}_{ji}\) for \(i &gt; j\) if
+     *     lowerToUpper=false
+     *   </li>
+     *   <li>
+     *   \(\texttt{m}_{ij}=\texttt{m}_{ji}\) for \(i &lt; j\) if
+     *     lowerToUpper=true
+     *   </li>
+     * </ul>
+     *
+     * @param m input-output floating-point square matrix.
+     * @param lowerToUpper operation flag; if true, the lower half is copied to
+     * the upper half. Otherwise, the upper half is copied to the lower half.
+     * SEE: flip, transpose
+     */
+    public static void completeSymm(Mat m, boolean lowerToUpper) {
+        completeSymm_0(m.nativeObj, lowerToUpper);
+    }
+
+    /**
+     * Copies the lower or the upper half of a square matrix to its another half.
+     *
+     * The function cv::completeSymm copies the lower or the upper half of a square matrix to
+     * its another half. The matrix diagonal remains unchanged:
+     * <ul>
+     *   <li>
+     *   \(\texttt{m}_{ij}=\texttt{m}_{ji}\) for \(i &gt; j\) if
+     *     lowerToUpper=false
+     *   </li>
+     *   <li>
+     *   \(\texttt{m}_{ij}=\texttt{m}_{ji}\) for \(i &lt; j\) if
+     *     lowerToUpper=true
+     *   </li>
+     * </ul>
+     *
+     * @param m input-output floating-point square matrix.
+     * the upper half. Otherwise, the upper half is copied to the lower half.
+     * SEE: flip, transpose
+     */
+    public static void completeSymm(Mat m) {
+        completeSymm_1(m.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::convertFp16(Mat src, Mat& dst)
+    //
+
+    /**
+     * Converts an array to half precision floating number.
+     *
+     * This function converts FP32 (single precision floating point) from/to FP16 (half precision floating point). CV_16S format is used to represent FP16 data.
+     * There are two use modes (src -&gt; dst): CV_32F -&gt; CV_16S and CV_16S -&gt; CV_32F. The input array has to have type of CV_32F or
+     * CV_16S to represent the bit depth. If the input array is neither of them, the function will raise an error.
+     * The format of half precision floating point is defined in IEEE 754-2008.
+     *
+     * @param src input array.
+     * @param dst output array.
+     */
+    public static void convertFp16(Mat src, Mat dst) {
+        convertFp16_0(src.nativeObj, dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::convertScaleAbs(Mat src, Mat& dst, double alpha = 1, double beta = 0)
+    //
+
+    /**
+     * Scales, calculates absolute values, and converts the result to 8-bit.
+     *
+     * On each element of the input array, the function convertScaleAbs
+     * performs three operations sequentially: scaling, taking an absolute
+     * value, conversion to an unsigned 8-bit type:
+     * \(\texttt{dst} (I)= \texttt{saturate\_cast&lt;uchar&gt;} (| \texttt{src} (I)* \texttt{alpha} +  \texttt{beta} |)\)
+     * In case of multi-channel arrays, the function processes each channel
+     * independently. When the output is not 8-bit, the operation can be
+     * emulated by calling the Mat::convertTo method (or by using matrix
+     * expressions) and then by calculating an absolute value of the result.
+     * For example:
+     * <code>
+     *     Mat_&lt;float&gt; A(30,30);
+     *     randu(A, Scalar(-100), Scalar(100));
+     *     Mat_&lt;float&gt; B = A*5 + 3;
+     *     B = abs(B);
+     *     // Mat_&lt;float&gt; B = abs(A*5+3) will also do the job,
+     *     // but it will allocate a temporary matrix
+     * </code>
+     * @param src input array.
+     * @param dst output array.
+     * @param alpha optional scale factor.
+     * @param beta optional delta added to the scaled values.
+     * SEE:  Mat::convertTo, cv::abs(const Mat&amp;)
+     */
+    public static void convertScaleAbs(Mat src, Mat dst, double alpha, double beta) {
+        convertScaleAbs_0(src.nativeObj, dst.nativeObj, alpha, beta);
+    }
+
+    /**
+     * Scales, calculates absolute values, and converts the result to 8-bit.
+     *
+     * On each element of the input array, the function convertScaleAbs
+     * performs three operations sequentially: scaling, taking an absolute
+     * value, conversion to an unsigned 8-bit type:
+     * \(\texttt{dst} (I)= \texttt{saturate\_cast&lt;uchar&gt;} (| \texttt{src} (I)* \texttt{alpha} +  \texttt{beta} |)\)
+     * In case of multi-channel arrays, the function processes each channel
+     * independently. When the output is not 8-bit, the operation can be
+     * emulated by calling the Mat::convertTo method (or by using matrix
+     * expressions) and then by calculating an absolute value of the result.
+     * For example:
+     * <code>
+     *     Mat_&lt;float&gt; A(30,30);
+     *     randu(A, Scalar(-100), Scalar(100));
+     *     Mat_&lt;float&gt; B = A*5 + 3;
+     *     B = abs(B);
+     *     // Mat_&lt;float&gt; B = abs(A*5+3) will also do the job,
+     *     // but it will allocate a temporary matrix
+     * </code>
+     * @param src input array.
+     * @param dst output array.
+     * @param alpha optional scale factor.
+     * SEE:  Mat::convertTo, cv::abs(const Mat&amp;)
+     */
+    public static void convertScaleAbs(Mat src, Mat dst, double alpha) {
+        convertScaleAbs_1(src.nativeObj, dst.nativeObj, alpha);
+    }
+
+    /**
+     * Scales, calculates absolute values, and converts the result to 8-bit.
+     *
+     * On each element of the input array, the function convertScaleAbs
+     * performs three operations sequentially: scaling, taking an absolute
+     * value, conversion to an unsigned 8-bit type:
+     * \(\texttt{dst} (I)= \texttt{saturate\_cast&lt;uchar&gt;} (| \texttt{src} (I)* \texttt{alpha} +  \texttt{beta} |)\)
+     * In case of multi-channel arrays, the function processes each channel
+     * independently. When the output is not 8-bit, the operation can be
+     * emulated by calling the Mat::convertTo method (or by using matrix
+     * expressions) and then by calculating an absolute value of the result.
+     * For example:
+     * <code>
+     *     Mat_&lt;float&gt; A(30,30);
+     *     randu(A, Scalar(-100), Scalar(100));
+     *     Mat_&lt;float&gt; B = A*5 + 3;
+     *     B = abs(B);
+     *     // Mat_&lt;float&gt; B = abs(A*5+3) will also do the job,
+     *     // but it will allocate a temporary matrix
+     * </code>
+     * @param src input array.
+     * @param dst output array.
+     * SEE:  Mat::convertTo, cv::abs(const Mat&amp;)
+     */
+    public static void convertScaleAbs(Mat src, Mat dst) {
+        convertScaleAbs_2(src.nativeObj, dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::copyMakeBorder(Mat src, Mat& dst, int top, int bottom, int left, int right, int borderType, Scalar value = Scalar())
+    //
+
+    /**
+     * Forms a border around an image.
+     *
+     * The function copies the source image into the middle of the destination image. The areas to the
+     * left, to the right, above and below the copied source image will be filled with extrapolated
+     * pixels. This is not what filtering functions based on it do (they extrapolate pixels on-fly), but
+     * what other more complex functions, including your own, may do to simplify image boundary handling.
+     *
+     * The function supports the mode when src is already in the middle of dst . In this case, the
+     * function does not copy src itself but simply constructs the border, for example:
+     *
+     * <code>
+     *     // let border be the same in all directions
+     *     int border=2;
+     *     // constructs a larger image to fit both the image and the border
+     *     Mat gray_buf(rgb.rows + border*2, rgb.cols + border*2, rgb.depth());
+     *     // select the middle part of it w/o copying data
+     *     Mat gray(gray_canvas, Rect(border, border, rgb.cols, rgb.rows));
+     *     // convert image from RGB to grayscale
+     *     cvtColor(rgb, gray, COLOR_RGB2GRAY);
+     *     // form a border in-place
+     *     copyMakeBorder(gray, gray_buf, border, border,
+     *                    border, border, BORDER_REPLICATE);
+     *     // now do some custom filtering ...
+     *     ...
+     * </code>
+     * <b>Note:</b> When the source image is a part (ROI) of a bigger image, the function will try to use the
+     * pixels outside of the ROI to form a border. To disable this feature and always do extrapolation, as
+     * if src was not a ROI, use borderType | #BORDER_ISOLATED.
+     *
+     * @param src Source image.
+     * @param dst Destination image of the same type as src and the size Size(src.cols+left+right,
+     * src.rows+top+bottom) .
+     * @param top the top pixels
+     * @param bottom the bottom pixels
+     * @param left the left pixels
+     * @param right Parameter specifying how many pixels in each direction from the source image rectangle
+     * to extrapolate. For example, top=1, bottom=1, left=1, right=1 mean that 1 pixel-wide border needs
+     * to be built.
+     * @param borderType Border type. See borderInterpolate for details.
+     * @param value Border value if borderType==BORDER_CONSTANT .
+     *
+     * SEE:  borderInterpolate
+     */
+    public static void copyMakeBorder(Mat src, Mat dst, int top, int bottom, int left, int right, int borderType, Scalar value) {
+        copyMakeBorder_0(src.nativeObj, dst.nativeObj, top, bottom, left, right, borderType, value.val[0], value.val[1], value.val[2], value.val[3]);
+    }
+
+    /**
+     * Forms a border around an image.
+     *
+     * The function copies the source image into the middle of the destination image. The areas to the
+     * left, to the right, above and below the copied source image will be filled with extrapolated
+     * pixels. This is not what filtering functions based on it do (they extrapolate pixels on-fly), but
+     * what other more complex functions, including your own, may do to simplify image boundary handling.
+     *
+     * The function supports the mode when src is already in the middle of dst . In this case, the
+     * function does not copy src itself but simply constructs the border, for example:
+     *
+     * <code>
+     *     // let border be the same in all directions
+     *     int border=2;
+     *     // constructs a larger image to fit both the image and the border
+     *     Mat gray_buf(rgb.rows + border*2, rgb.cols + border*2, rgb.depth());
+     *     // select the middle part of it w/o copying data
+     *     Mat gray(gray_canvas, Rect(border, border, rgb.cols, rgb.rows));
+     *     // convert image from RGB to grayscale
+     *     cvtColor(rgb, gray, COLOR_RGB2GRAY);
+     *     // form a border in-place
+     *     copyMakeBorder(gray, gray_buf, border, border,
+     *                    border, border, BORDER_REPLICATE);
+     *     // now do some custom filtering ...
+     *     ...
+     * </code>
+     * <b>Note:</b> When the source image is a part (ROI) of a bigger image, the function will try to use the
+     * pixels outside of the ROI to form a border. To disable this feature and always do extrapolation, as
+     * if src was not a ROI, use borderType | #BORDER_ISOLATED.
+     *
+     * @param src Source image.
+     * @param dst Destination image of the same type as src and the size Size(src.cols+left+right,
+     * src.rows+top+bottom) .
+     * @param top the top pixels
+     * @param bottom the bottom pixels
+     * @param left the left pixels
+     * @param right Parameter specifying how many pixels in each direction from the source image rectangle
+     * to extrapolate. For example, top=1, bottom=1, left=1, right=1 mean that 1 pixel-wide border needs
+     * to be built.
+     * @param borderType Border type. See borderInterpolate for details.
+     *
+     * SEE:  borderInterpolate
+     */
+    public static void copyMakeBorder(Mat src, Mat dst, int top, int bottom, int left, int right, int borderType) {
+        copyMakeBorder_1(src.nativeObj, dst.nativeObj, top, bottom, left, right, borderType);
+    }
+
+
+    //
+    // C++:  void cv::dct(Mat src, Mat& dst, int flags = 0)
+    //
+
+    /**
+     * Performs a forward or inverse discrete Cosine transform of 1D or 2D array.
+     *
+     * The function cv::dct performs a forward or inverse discrete Cosine transform (DCT) of a 1D or 2D
+     * floating-point array:
+     * <ul>
+     *   <li>
+     *    Forward Cosine transform of a 1D vector of N elements:
+     *     \(Y = C^{(N)}  \cdot X\)
+     *     where
+     *     \(C^{(N)}_{jk}= \sqrt{\alpha_j/N} \cos \left ( \frac{\pi(2k+1)j}{2N} \right )\)
+     *     and
+     *     \(\alpha_0=1\), \(\alpha_j=2\) for *j &gt; 0*.
+     *   </li>
+     *   <li>
+     *    Inverse Cosine transform of a 1D vector of N elements:
+     *     \(X =  \left (C^{(N)} \right )^{-1}  \cdot Y =  \left (C^{(N)} \right )^T  \cdot Y\)
+     *     (since \(C^{(N)}\) is an orthogonal matrix, \(C^{(N)} \cdot \left(C^{(N)}\right)^T = I\) )
+     *   </li>
+     *   <li>
+     *    Forward 2D Cosine transform of M x N matrix:
+     *     \(Y = C^{(N)}  \cdot X  \cdot \left (C^{(N)} \right )^T\)
+     *   </li>
+     *   <li>
+     *    Inverse 2D Cosine transform of M x N matrix:
+     *     \(X =  \left (C^{(N)} \right )^T  \cdot X  \cdot C^{(N)}\)
+     *   </li>
+     * </ul>
+     *
+     * The function chooses the mode of operation by looking at the flags and size of the input array:
+     * <ul>
+     *   <li>
+     *    If (flags &amp; #DCT_INVERSE) == 0 , the function does a forward 1D or 2D transform. Otherwise, it
+     *     is an inverse 1D or 2D transform.
+     *   </li>
+     *   <li>
+     *    If (flags &amp; #DCT_ROWS) != 0 , the function performs a 1D transform of each row.
+     *   </li>
+     *   <li>
+     *    If the array is a single column or a single row, the function performs a 1D transform.
+     *   </li>
+     *   <li>
+     *    If none of the above is true, the function performs a 2D transform.
+     *   </li>
+     * </ul>
+     *
+     * <b>Note:</b> Currently dct supports even-size arrays (2, 4, 6 ...). For data analysis and approximation, you
+     * can pad the array when necessary.
+     * Also, the function performance depends very much, and not monotonically, on the array size (see
+     * getOptimalDFTSize ). In the current implementation DCT of a vector of size N is calculated via DFT
+     * of a vector of size N/2 . Thus, the optimal DCT size N1 &gt;= N can be calculated as:
+     * <code>
+     *     size_t getOptimalDCTSize(size_t N) { return 2*getOptimalDFTSize((N+1)/2); }
+     *     N1 = getOptimalDCTSize(N);
+     * </code>
+     * @param src input floating-point array.
+     * @param dst output array of the same size and type as src .
+     * @param flags transformation flags as a combination of cv::DftFlags (DCT_*)
+     * SEE: dft , getOptimalDFTSize , idct
+     */
+    public static void dct(Mat src, Mat dst, int flags) {
+        dct_0(src.nativeObj, dst.nativeObj, flags);
+    }
+
+    /**
+     * Performs a forward or inverse discrete Cosine transform of 1D or 2D array.
+     *
+     * The function cv::dct performs a forward or inverse discrete Cosine transform (DCT) of a 1D or 2D
+     * floating-point array:
+     * <ul>
+     *   <li>
+     *    Forward Cosine transform of a 1D vector of N elements:
+     *     \(Y = C^{(N)}  \cdot X\)
+     *     where
+     *     \(C^{(N)}_{jk}= \sqrt{\alpha_j/N} \cos \left ( \frac{\pi(2k+1)j}{2N} \right )\)
+     *     and
+     *     \(\alpha_0=1\), \(\alpha_j=2\) for *j &gt; 0*.
+     *   </li>
+     *   <li>
+     *    Inverse Cosine transform of a 1D vector of N elements:
+     *     \(X =  \left (C^{(N)} \right )^{-1}  \cdot Y =  \left (C^{(N)} \right )^T  \cdot Y\)
+     *     (since \(C^{(N)}\) is an orthogonal matrix, \(C^{(N)} \cdot \left(C^{(N)}\right)^T = I\) )
+     *   </li>
+     *   <li>
+     *    Forward 2D Cosine transform of M x N matrix:
+     *     \(Y = C^{(N)}  \cdot X  \cdot \left (C^{(N)} \right )^T\)
+     *   </li>
+     *   <li>
+     *    Inverse 2D Cosine transform of M x N matrix:
+     *     \(X =  \left (C^{(N)} \right )^T  \cdot X  \cdot C^{(N)}\)
+     *   </li>
+     * </ul>
+     *
+     * The function chooses the mode of operation by looking at the flags and size of the input array:
+     * <ul>
+     *   <li>
+     *    If (flags &amp; #DCT_INVERSE) == 0 , the function does a forward 1D or 2D transform. Otherwise, it
+     *     is an inverse 1D or 2D transform.
+     *   </li>
+     *   <li>
+     *    If (flags &amp; #DCT_ROWS) != 0 , the function performs a 1D transform of each row.
+     *   </li>
+     *   <li>
+     *    If the array is a single column or a single row, the function performs a 1D transform.
+     *   </li>
+     *   <li>
+     *    If none of the above is true, the function performs a 2D transform.
+     *   </li>
+     * </ul>
+     *
+     * <b>Note:</b> Currently dct supports even-size arrays (2, 4, 6 ...). For data analysis and approximation, you
+     * can pad the array when necessary.
+     * Also, the function performance depends very much, and not monotonically, on the array size (see
+     * getOptimalDFTSize ). In the current implementation DCT of a vector of size N is calculated via DFT
+     * of a vector of size N/2 . Thus, the optimal DCT size N1 &gt;= N can be calculated as:
+     * <code>
+     *     size_t getOptimalDCTSize(size_t N) { return 2*getOptimalDFTSize((N+1)/2); }
+     *     N1 = getOptimalDCTSize(N);
+     * </code>
+     * @param src input floating-point array.
+     * @param dst output array of the same size and type as src .
+     * SEE: dft , getOptimalDFTSize , idct
+     */
+    public static void dct(Mat src, Mat dst) {
+        dct_1(src.nativeObj, dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::dft(Mat src, Mat& dst, int flags = 0, int nonzeroRows = 0)
+    //
+
+    /**
+     * Performs a forward or inverse Discrete Fourier transform of a 1D or 2D floating-point array.
+     *
+     * The function cv::dft performs one of the following:
+     * <ul>
+     *   <li>
+     *    Forward the Fourier transform of a 1D vector of N elements:
+     *     \(Y = F^{(N)}  \cdot X,\)
+     *     where \(F^{(N)}_{jk}=\exp(-2\pi i j k/N)\) and \(i=\sqrt{-1}\)
+     *   </li>
+     *   <li>
+     *    Inverse the Fourier transform of a 1D vector of N elements:
+     *     \(\begin{array}{l} X'=  \left (F^{(N)} \right )^{-1}  \cdot Y =  \left (F^{(N)} \right )^*  \cdot y  \\ X = (1/N)  \cdot X, \end{array}\)
+     *     where \(F^*=\left(\textrm{Re}(F^{(N)})-\textrm{Im}(F^{(N)})\right)^T\)
+     *   </li>
+     *   <li>
+     *    Forward the 2D Fourier transform of a M x N matrix:
+     *     \(Y = F^{(M)}  \cdot X  \cdot F^{(N)}\)
+     *   </li>
+     *   <li>
+     *    Inverse the 2D Fourier transform of a M x N matrix:
+     *     \(\begin{array}{l} X'=  \left (F^{(M)} \right )^*  \cdot Y  \cdot \left (F^{(N)} \right )^* \\ X =  \frac{1}{M \cdot N} \cdot X' \end{array}\)
+     *   </li>
+     * </ul>
+     *
+     * In case of real (single-channel) data, the output spectrum of the forward Fourier transform or input
+     * spectrum of the inverse Fourier transform can be represented in a packed format called *CCS*
+     * (complex-conjugate-symmetrical). It was borrowed from IPL (Intel\* Image Processing Library). Here
+     * is how 2D *CCS* spectrum looks:
+     * \(\begin{bmatrix} Re Y_{0,0} &amp; Re Y_{0,1} &amp; Im Y_{0,1} &amp; Re Y_{0,2} &amp; Im Y_{0,2} &amp;  \cdots &amp; Re Y_{0,N/2-1} &amp; Im Y_{0,N/2-1} &amp; Re Y_{0,N/2}  \\ Re Y_{1,0} &amp; Re Y_{1,1} &amp; Im Y_{1,1} &amp; Re Y_{1,2} &amp; Im Y_{1,2} &amp;  \cdots &amp; Re Y_{1,N/2-1} &amp; Im Y_{1,N/2-1} &amp; Re Y_{1,N/2}  \\ Im Y_{1,0} &amp; Re Y_{2,1} &amp; Im Y_{2,1} &amp; Re Y_{2,2} &amp; Im Y_{2,2} &amp;  \cdots &amp; Re Y_{2,N/2-1} &amp; Im Y_{2,N/2-1} &amp; Im Y_{1,N/2}  \\ \hdotsfor{9} \\ Re Y_{M/2-1,0} &amp;  Re Y_{M-3,1}  &amp; Im Y_{M-3,1} &amp;  \hdotsfor{3} &amp; Re Y_{M-3,N/2-1} &amp; Im Y_{M-3,N/2-1}&amp; Re Y_{M/2-1,N/2}  \\ Im Y_{M/2-1,0} &amp;  Re Y_{M-2,1}  &amp; Im Y_{M-2,1} &amp;  \hdotsfor{3} &amp; Re Y_{M-2,N/2-1} &amp; Im Y_{M-2,N/2-1}&amp; Im Y_{M/2-1,N/2}  \\ Re Y_{M/2,0}  &amp;  Re Y_{M-1,1} &amp;  Im Y_{M-1,1} &amp;  \hdotsfor{3} &amp; Re Y_{M-1,N/2-1} &amp; Im Y_{M-1,N/2-1}&amp; Re Y_{M/2,N/2} \end{bmatrix}\)
+     *
+     * In case of 1D transform of a real vector, the output looks like the first row of the matrix above.
+     *
+     * So, the function chooses an operation mode depending on the flags and size of the input array:
+     * <ul>
+     *   <li>
+     *    If #DFT_ROWS is set or the input array has a single row or single column, the function
+     *     performs a 1D forward or inverse transform of each row of a matrix when #DFT_ROWS is set.
+     *     Otherwise, it performs a 2D transform.
+     *   </li>
+     *   <li>
+     *    If the input array is real and #DFT_INVERSE is not set, the function performs a forward 1D or
+     *     2D transform:
+     *   <ul>
+     *     <li>
+     *        When #DFT_COMPLEX_OUTPUT is set, the output is a complex matrix of the same size as
+     *         input.
+     *     </li>
+     *     <li>
+     *        When #DFT_COMPLEX_OUTPUT is not set, the output is a real matrix of the same size as
+     *         input. In case of 2D transform, it uses the packed format as shown above. In case of a
+     *         single 1D transform, it looks like the first row of the matrix above. In case of
+     *         multiple 1D transforms (when using the #DFT_ROWS flag), each row of the output matrix
+     *         looks like the first row of the matrix above.
+     *     </li>
+     *   </ul>
+     *   <li>
+     *    If the input array is complex and either #DFT_INVERSE or #DFT_REAL_OUTPUT are not set, the
+     *     output is a complex array of the same size as input. The function performs a forward or
+     *     inverse 1D or 2D transform of the whole input array or each row of the input array
+     *     independently, depending on the flags DFT_INVERSE and DFT_ROWS.
+     *   </li>
+     *   <li>
+     *    When #DFT_INVERSE is set and the input array is real, or it is complex but #DFT_REAL_OUTPUT
+     *     is set, the output is a real array of the same size as input. The function performs a 1D or 2D
+     *     inverse transformation of the whole input array or each individual row, depending on the flags
+     *     #DFT_INVERSE and #DFT_ROWS.
+     *   </li>
+     * </ul>
+     *
+     * If #DFT_SCALE is set, the scaling is done after the transformation.
+     *
+     * Unlike dct , the function supports arrays of arbitrary size. But only those arrays are processed
+     * efficiently, whose sizes can be factorized in a product of small prime numbers (2, 3, and 5 in the
+     * current implementation). Such an efficient DFT size can be calculated using the getOptimalDFTSize
+     * method.
+     *
+     * The sample below illustrates how to calculate a DFT-based convolution of two 2D real arrays:
+     * <code>
+     *     void convolveDFT(InputArray A, InputArray B, OutputArray C)
+     *     {
+     *         // reallocate the output array if needed
+     *         C.create(abs(A.rows - B.rows)+1, abs(A.cols - B.cols)+1, A.type());
+     *         Size dftSize;
+     *         // calculate the size of DFT transform
+     *         dftSize.width = getOptimalDFTSize(A.cols + B.cols - 1);
+     *         dftSize.height = getOptimalDFTSize(A.rows + B.rows - 1);
+     *
+     *         // allocate temporary buffers and initialize them with 0's
+     *         Mat tempA(dftSize, A.type(), Scalar::all(0));
+     *         Mat tempB(dftSize, B.type(), Scalar::all(0));
+     *
+     *         // copy A and B to the top-left corners of tempA and tempB, respectively
+     *         Mat roiA(tempA, Rect(0,0,A.cols,A.rows));
+     *         A.copyTo(roiA);
+     *         Mat roiB(tempB, Rect(0,0,B.cols,B.rows));
+     *         B.copyTo(roiB);
+     *
+     *         // now transform the padded A &amp; B in-place;
+     *         // use "nonzeroRows" hint for faster processing
+     *         dft(tempA, tempA, 0, A.rows);
+     *         dft(tempB, tempB, 0, B.rows);
+     *
+     *         // multiply the spectrums;
+     *         // the function handles packed spectrum representations well
+     *         mulSpectrums(tempA, tempB, tempA);
+     *
+     *         // transform the product back from the frequency domain.
+     *         // Even though all the result rows will be non-zero,
+     *         // you need only the first C.rows of them, and thus you
+     *         // pass nonzeroRows == C.rows
+     *         dft(tempA, tempA, DFT_INVERSE + DFT_SCALE, C.rows);
+     *
+     *         // now copy the result back to C.
+     *         tempA(Rect(0, 0, C.cols, C.rows)).copyTo(C);
+     *
+     *         // all the temporary buffers will be deallocated automatically
+     *     }
+     * </code>
+     * To optimize this sample, consider the following approaches:
+     * <ul>
+     *   <li>
+     *    Since nonzeroRows != 0 is passed to the forward transform calls and since A and B are copied to
+     *     the top-left corners of tempA and tempB, respectively, it is not necessary to clear the whole
+     *     tempA and tempB. It is only necessary to clear the tempA.cols - A.cols ( tempB.cols - B.cols)
+     *     rightmost columns of the matrices.
+     *   </li>
+     *   <li>
+     *    This DFT-based convolution does not have to be applied to the whole big arrays, especially if B
+     *     is significantly smaller than A or vice versa. Instead, you can calculate convolution by parts.
+     *     To do this, you need to split the output array C into multiple tiles. For each tile, estimate
+     *     which parts of A and B are required to calculate convolution in this tile. If the tiles in C are
+     *     too small, the speed will decrease a lot because of repeated work. In the ultimate case, when
+     *     each tile in C is a single pixel, the algorithm becomes equivalent to the naive convolution
+     *     algorithm. If the tiles are too big, the temporary arrays tempA and tempB become too big and
+     *     there is also a slowdown because of bad cache locality. So, there is an optimal tile size
+     *     somewhere in the middle.
+     *   </li>
+     *   <li>
+     *    If different tiles in C can be calculated in parallel and, thus, the convolution is done by
+     *     parts, the loop can be threaded.
+     *   </li>
+     * </ul>
+     *
+     * All of the above improvements have been implemented in #matchTemplate and #filter2D . Therefore, by
+     * using them, you can get the performance even better than with the above theoretically optimal
+     * implementation. Though, those two functions actually calculate cross-correlation, not convolution,
+     * so you need to "flip" the second convolution operand B vertically and horizontally using flip .
+     * <b>Note:</b>
+     * <ul>
+     *   <li>
+     *    An example using the discrete fourier transform can be found at
+     *     opencv_source_code/samples/cpp/dft.cpp
+     *   </li>
+     *   <li>
+     *    (Python) An example using the dft functionality to perform Wiener deconvolution can be found
+     *     at opencv_source/samples/python/deconvolution.py
+     *   </li>
+     *   <li>
+     *    (Python) An example rearranging the quadrants of a Fourier image can be found at
+     *     opencv_source/samples/python/dft.py
+     * @param src input array that could be real or complex.
+     * @param dst output array whose size and type depends on the flags .
+     * @param flags transformation flags, representing a combination of the #DftFlags
+     * @param nonzeroRows when the parameter is not zero, the function assumes that only the first
+     * nonzeroRows rows of the input array (#DFT_INVERSE is not set) or only the first nonzeroRows of the
+     * output array (#DFT_INVERSE is set) contain non-zeros, thus, the function can handle the rest of the
+     * rows more efficiently and save some time; this technique is very useful for calculating array
+     * cross-correlation or convolution using DFT.
+     * SEE: dct , getOptimalDFTSize , mulSpectrums, filter2D , matchTemplate , flip , cartToPolar ,
+     * magnitude , phase
+     *   </li>
+     * </ul>
+     */
+    public static void dft(Mat src, Mat dst, int flags, int nonzeroRows) {
+        dft_0(src.nativeObj, dst.nativeObj, flags, nonzeroRows);
+    }
+
+    /**
+     * Performs a forward or inverse Discrete Fourier transform of a 1D or 2D floating-point array.
+     *
+     * The function cv::dft performs one of the following:
+     * <ul>
+     *   <li>
+     *    Forward the Fourier transform of a 1D vector of N elements:
+     *     \(Y = F^{(N)}  \cdot X,\)
+     *     where \(F^{(N)}_{jk}=\exp(-2\pi i j k/N)\) and \(i=\sqrt{-1}\)
+     *   </li>
+     *   <li>
+     *    Inverse the Fourier transform of a 1D vector of N elements:
+     *     \(\begin{array}{l} X'=  \left (F^{(N)} \right )^{-1}  \cdot Y =  \left (F^{(N)} \right )^*  \cdot y  \\ X = (1/N)  \cdot X, \end{array}\)
+     *     where \(F^*=\left(\textrm{Re}(F^{(N)})-\textrm{Im}(F^{(N)})\right)^T\)
+     *   </li>
+     *   <li>
+     *    Forward the 2D Fourier transform of a M x N matrix:
+     *     \(Y = F^{(M)}  \cdot X  \cdot F^{(N)}\)
+     *   </li>
+     *   <li>
+     *    Inverse the 2D Fourier transform of a M x N matrix:
+     *     \(\begin{array}{l} X'=  \left (F^{(M)} \right )^*  \cdot Y  \cdot \left (F^{(N)} \right )^* \\ X =  \frac{1}{M \cdot N} \cdot X' \end{array}\)
+     *   </li>
+     * </ul>
+     *
+     * In case of real (single-channel) data, the output spectrum of the forward Fourier transform or input
+     * spectrum of the inverse Fourier transform can be represented in a packed format called *CCS*
+     * (complex-conjugate-symmetrical). It was borrowed from IPL (Intel\* Image Processing Library). Here
+     * is how 2D *CCS* spectrum looks:
+     * \(\begin{bmatrix} Re Y_{0,0} &amp; Re Y_{0,1} &amp; Im Y_{0,1} &amp; Re Y_{0,2} &amp; Im Y_{0,2} &amp;  \cdots &amp; Re Y_{0,N/2-1} &amp; Im Y_{0,N/2-1} &amp; Re Y_{0,N/2}  \\ Re Y_{1,0} &amp; Re Y_{1,1} &amp; Im Y_{1,1} &amp; Re Y_{1,2} &amp; Im Y_{1,2} &amp;  \cdots &amp; Re Y_{1,N/2-1} &amp; Im Y_{1,N/2-1} &amp; Re Y_{1,N/2}  \\ Im Y_{1,0} &amp; Re Y_{2,1} &amp; Im Y_{2,1} &amp; Re Y_{2,2} &amp; Im Y_{2,2} &amp;  \cdots &amp; Re Y_{2,N/2-1} &amp; Im Y_{2,N/2-1} &amp; Im Y_{1,N/2}  \\ \hdotsfor{9} \\ Re Y_{M/2-1,0} &amp;  Re Y_{M-3,1}  &amp; Im Y_{M-3,1} &amp;  \hdotsfor{3} &amp; Re Y_{M-3,N/2-1} &amp; Im Y_{M-3,N/2-1}&amp; Re Y_{M/2-1,N/2}  \\ Im Y_{M/2-1,0} &amp;  Re Y_{M-2,1}  &amp; Im Y_{M-2,1} &amp;  \hdotsfor{3} &amp; Re Y_{M-2,N/2-1} &amp; Im Y_{M-2,N/2-1}&amp; Im Y_{M/2-1,N/2}  \\ Re Y_{M/2,0}  &amp;  Re Y_{M-1,1} &amp;  Im Y_{M-1,1} &amp;  \hdotsfor{3} &amp; Re Y_{M-1,N/2-1} &amp; Im Y_{M-1,N/2-1}&amp; Re Y_{M/2,N/2} \end{bmatrix}\)
+     *
+     * In case of 1D transform of a real vector, the output looks like the first row of the matrix above.
+     *
+     * So, the function chooses an operation mode depending on the flags and size of the input array:
+     * <ul>
+     *   <li>
+     *    If #DFT_ROWS is set or the input array has a single row or single column, the function
+     *     performs a 1D forward or inverse transform of each row of a matrix when #DFT_ROWS is set.
+     *     Otherwise, it performs a 2D transform.
+     *   </li>
+     *   <li>
+     *    If the input array is real and #DFT_INVERSE is not set, the function performs a forward 1D or
+     *     2D transform:
+     *   <ul>
+     *     <li>
+     *        When #DFT_COMPLEX_OUTPUT is set, the output is a complex matrix of the same size as
+     *         input.
+     *     </li>
+     *     <li>
+     *        When #DFT_COMPLEX_OUTPUT is not set, the output is a real matrix of the same size as
+     *         input. In case of 2D transform, it uses the packed format as shown above. In case of a
+     *         single 1D transform, it looks like the first row of the matrix above. In case of
+     *         multiple 1D transforms (when using the #DFT_ROWS flag), each row of the output matrix
+     *         looks like the first row of the matrix above.
+     *     </li>
+     *   </ul>
+     *   <li>
+     *    If the input array is complex and either #DFT_INVERSE or #DFT_REAL_OUTPUT are not set, the
+     *     output is a complex array of the same size as input. The function performs a forward or
+     *     inverse 1D or 2D transform of the whole input array or each row of the input array
+     *     independently, depending on the flags DFT_INVERSE and DFT_ROWS.
+     *   </li>
+     *   <li>
+     *    When #DFT_INVERSE is set and the input array is real, or it is complex but #DFT_REAL_OUTPUT
+     *     is set, the output is a real array of the same size as input. The function performs a 1D or 2D
+     *     inverse transformation of the whole input array or each individual row, depending on the flags
+     *     #DFT_INVERSE and #DFT_ROWS.
+     *   </li>
+     * </ul>
+     *
+     * If #DFT_SCALE is set, the scaling is done after the transformation.
+     *
+     * Unlike dct , the function supports arrays of arbitrary size. But only those arrays are processed
+     * efficiently, whose sizes can be factorized in a product of small prime numbers (2, 3, and 5 in the
+     * current implementation). Such an efficient DFT size can be calculated using the getOptimalDFTSize
+     * method.
+     *
+     * The sample below illustrates how to calculate a DFT-based convolution of two 2D real arrays:
+     * <code>
+     *     void convolveDFT(InputArray A, InputArray B, OutputArray C)
+     *     {
+     *         // reallocate the output array if needed
+     *         C.create(abs(A.rows - B.rows)+1, abs(A.cols - B.cols)+1, A.type());
+     *         Size dftSize;
+     *         // calculate the size of DFT transform
+     *         dftSize.width = getOptimalDFTSize(A.cols + B.cols - 1);
+     *         dftSize.height = getOptimalDFTSize(A.rows + B.rows - 1);
+     *
+     *         // allocate temporary buffers and initialize them with 0's
+     *         Mat tempA(dftSize, A.type(), Scalar::all(0));
+     *         Mat tempB(dftSize, B.type(), Scalar::all(0));
+     *
+     *         // copy A and B to the top-left corners of tempA and tempB, respectively
+     *         Mat roiA(tempA, Rect(0,0,A.cols,A.rows));
+     *         A.copyTo(roiA);
+     *         Mat roiB(tempB, Rect(0,0,B.cols,B.rows));
+     *         B.copyTo(roiB);
+     *
+     *         // now transform the padded A &amp; B in-place;
+     *         // use "nonzeroRows" hint for faster processing
+     *         dft(tempA, tempA, 0, A.rows);
+     *         dft(tempB, tempB, 0, B.rows);
+     *
+     *         // multiply the spectrums;
+     *         // the function handles packed spectrum representations well
+     *         mulSpectrums(tempA, tempB, tempA);
+     *
+     *         // transform the product back from the frequency domain.
+     *         // Even though all the result rows will be non-zero,
+     *         // you need only the first C.rows of them, and thus you
+     *         // pass nonzeroRows == C.rows
+     *         dft(tempA, tempA, DFT_INVERSE + DFT_SCALE, C.rows);
+     *
+     *         // now copy the result back to C.
+     *         tempA(Rect(0, 0, C.cols, C.rows)).copyTo(C);
+     *
+     *         // all the temporary buffers will be deallocated automatically
+     *     }
+     * </code>
+     * To optimize this sample, consider the following approaches:
+     * <ul>
+     *   <li>
+     *    Since nonzeroRows != 0 is passed to the forward transform calls and since A and B are copied to
+     *     the top-left corners of tempA and tempB, respectively, it is not necessary to clear the whole
+     *     tempA and tempB. It is only necessary to clear the tempA.cols - A.cols ( tempB.cols - B.cols)
+     *     rightmost columns of the matrices.
+     *   </li>
+     *   <li>
+     *    This DFT-based convolution does not have to be applied to the whole big arrays, especially if B
+     *     is significantly smaller than A or vice versa. Instead, you can calculate convolution by parts.
+     *     To do this, you need to split the output array C into multiple tiles. For each tile, estimate
+     *     which parts of A and B are required to calculate convolution in this tile. If the tiles in C are
+     *     too small, the speed will decrease a lot because of repeated work. In the ultimate case, when
+     *     each tile in C is a single pixel, the algorithm becomes equivalent to the naive convolution
+     *     algorithm. If the tiles are too big, the temporary arrays tempA and tempB become too big and
+     *     there is also a slowdown because of bad cache locality. So, there is an optimal tile size
+     *     somewhere in the middle.
+     *   </li>
+     *   <li>
+     *    If different tiles in C can be calculated in parallel and, thus, the convolution is done by
+     *     parts, the loop can be threaded.
+     *   </li>
+     * </ul>
+     *
+     * All of the above improvements have been implemented in #matchTemplate and #filter2D . Therefore, by
+     * using them, you can get the performance even better than with the above theoretically optimal
+     * implementation. Though, those two functions actually calculate cross-correlation, not convolution,
+     * so you need to "flip" the second convolution operand B vertically and horizontally using flip .
+     * <b>Note:</b>
+     * <ul>
+     *   <li>
+     *    An example using the discrete fourier transform can be found at
+     *     opencv_source_code/samples/cpp/dft.cpp
+     *   </li>
+     *   <li>
+     *    (Python) An example using the dft functionality to perform Wiener deconvolution can be found
+     *     at opencv_source/samples/python/deconvolution.py
+     *   </li>
+     *   <li>
+     *    (Python) An example rearranging the quadrants of a Fourier image can be found at
+     *     opencv_source/samples/python/dft.py
+     * @param src input array that could be real or complex.
+     * @param dst output array whose size and type depends on the flags .
+     * @param flags transformation flags, representing a combination of the #DftFlags
+     * nonzeroRows rows of the input array (#DFT_INVERSE is not set) or only the first nonzeroRows of the
+     * output array (#DFT_INVERSE is set) contain non-zeros, thus, the function can handle the rest of the
+     * rows more efficiently and save some time; this technique is very useful for calculating array
+     * cross-correlation or convolution using DFT.
+     * SEE: dct , getOptimalDFTSize , mulSpectrums, filter2D , matchTemplate , flip , cartToPolar ,
+     * magnitude , phase
+     *   </li>
+     * </ul>
+     */
+    public static void dft(Mat src, Mat dst, int flags) {
+        dft_1(src.nativeObj, dst.nativeObj, flags);
+    }
+
+    /**
+     * Performs a forward or inverse Discrete Fourier transform of a 1D or 2D floating-point array.
+     *
+     * The function cv::dft performs one of the following:
+     * <ul>
+     *   <li>
+     *    Forward the Fourier transform of a 1D vector of N elements:
+     *     \(Y = F^{(N)}  \cdot X,\)
+     *     where \(F^{(N)}_{jk}=\exp(-2\pi i j k/N)\) and \(i=\sqrt{-1}\)
+     *   </li>
+     *   <li>
+     *    Inverse the Fourier transform of a 1D vector of N elements:
+     *     \(\begin{array}{l} X'=  \left (F^{(N)} \right )^{-1}  \cdot Y =  \left (F^{(N)} \right )^*  \cdot y  \\ X = (1/N)  \cdot X, \end{array}\)
+     *     where \(F^*=\left(\textrm{Re}(F^{(N)})-\textrm{Im}(F^{(N)})\right)^T\)
+     *   </li>
+     *   <li>
+     *    Forward the 2D Fourier transform of a M x N matrix:
+     *     \(Y = F^{(M)}  \cdot X  \cdot F^{(N)}\)
+     *   </li>
+     *   <li>
+     *    Inverse the 2D Fourier transform of a M x N matrix:
+     *     \(\begin{array}{l} X'=  \left (F^{(M)} \right )^*  \cdot Y  \cdot \left (F^{(N)} \right )^* \\ X =  \frac{1}{M \cdot N} \cdot X' \end{array}\)
+     *   </li>
+     * </ul>
+     *
+     * In case of real (single-channel) data, the output spectrum of the forward Fourier transform or input
+     * spectrum of the inverse Fourier transform can be represented in a packed format called *CCS*
+     * (complex-conjugate-symmetrical). It was borrowed from IPL (Intel\* Image Processing Library). Here
+     * is how 2D *CCS* spectrum looks:
+     * \(\begin{bmatrix} Re Y_{0,0} &amp; Re Y_{0,1} &amp; Im Y_{0,1} &amp; Re Y_{0,2} &amp; Im Y_{0,2} &amp;  \cdots &amp; Re Y_{0,N/2-1} &amp; Im Y_{0,N/2-1} &amp; Re Y_{0,N/2}  \\ Re Y_{1,0} &amp; Re Y_{1,1} &amp; Im Y_{1,1} &amp; Re Y_{1,2} &amp; Im Y_{1,2} &amp;  \cdots &amp; Re Y_{1,N/2-1} &amp; Im Y_{1,N/2-1} &amp; Re Y_{1,N/2}  \\ Im Y_{1,0} &amp; Re Y_{2,1} &amp; Im Y_{2,1} &amp; Re Y_{2,2} &amp; Im Y_{2,2} &amp;  \cdots &amp; Re Y_{2,N/2-1} &amp; Im Y_{2,N/2-1} &amp; Im Y_{1,N/2}  \\ \hdotsfor{9} \\ Re Y_{M/2-1,0} &amp;  Re Y_{M-3,1}  &amp; Im Y_{M-3,1} &amp;  \hdotsfor{3} &amp; Re Y_{M-3,N/2-1} &amp; Im Y_{M-3,N/2-1}&amp; Re Y_{M/2-1,N/2}  \\ Im Y_{M/2-1,0} &amp;  Re Y_{M-2,1}  &amp; Im Y_{M-2,1} &amp;  \hdotsfor{3} &amp; Re Y_{M-2,N/2-1} &amp; Im Y_{M-2,N/2-1}&amp; Im Y_{M/2-1,N/2}  \\ Re Y_{M/2,0}  &amp;  Re Y_{M-1,1} &amp;  Im Y_{M-1,1} &amp;  \hdotsfor{3} &amp; Re Y_{M-1,N/2-1} &amp; Im Y_{M-1,N/2-1}&amp; Re Y_{M/2,N/2} \end{bmatrix}\)
+     *
+     * In case of 1D transform of a real vector, the output looks like the first row of the matrix above.
+     *
+     * So, the function chooses an operation mode depending on the flags and size of the input array:
+     * <ul>
+     *   <li>
+     *    If #DFT_ROWS is set or the input array has a single row or single column, the function
+     *     performs a 1D forward or inverse transform of each row of a matrix when #DFT_ROWS is set.
+     *     Otherwise, it performs a 2D transform.
+     *   </li>
+     *   <li>
+     *    If the input array is real and #DFT_INVERSE is not set, the function performs a forward 1D or
+     *     2D transform:
+     *   <ul>
+     *     <li>
+     *        When #DFT_COMPLEX_OUTPUT is set, the output is a complex matrix of the same size as
+     *         input.
+     *     </li>
+     *     <li>
+     *        When #DFT_COMPLEX_OUTPUT is not set, the output is a real matrix of the same size as
+     *         input. In case of 2D transform, it uses the packed format as shown above. In case of a
+     *         single 1D transform, it looks like the first row of the matrix above. In case of
+     *         multiple 1D transforms (when using the #DFT_ROWS flag), each row of the output matrix
+     *         looks like the first row of the matrix above.
+     *     </li>
+     *   </ul>
+     *   <li>
+     *    If the input array is complex and either #DFT_INVERSE or #DFT_REAL_OUTPUT are not set, the
+     *     output is a complex array of the same size as input. The function performs a forward or
+     *     inverse 1D or 2D transform of the whole input array or each row of the input array
+     *     independently, depending on the flags DFT_INVERSE and DFT_ROWS.
+     *   </li>
+     *   <li>
+     *    When #DFT_INVERSE is set and the input array is real, or it is complex but #DFT_REAL_OUTPUT
+     *     is set, the output is a real array of the same size as input. The function performs a 1D or 2D
+     *     inverse transformation of the whole input array or each individual row, depending on the flags
+     *     #DFT_INVERSE and #DFT_ROWS.
+     *   </li>
+     * </ul>
+     *
+     * If #DFT_SCALE is set, the scaling is done after the transformation.
+     *
+     * Unlike dct , the function supports arrays of arbitrary size. But only those arrays are processed
+     * efficiently, whose sizes can be factorized in a product of small prime numbers (2, 3, and 5 in the
+     * current implementation). Such an efficient DFT size can be calculated using the getOptimalDFTSize
+     * method.
+     *
+     * The sample below illustrates how to calculate a DFT-based convolution of two 2D real arrays:
+     * <code>
+     *     void convolveDFT(InputArray A, InputArray B, OutputArray C)
+     *     {
+     *         // reallocate the output array if needed
+     *         C.create(abs(A.rows - B.rows)+1, abs(A.cols - B.cols)+1, A.type());
+     *         Size dftSize;
+     *         // calculate the size of DFT transform
+     *         dftSize.width = getOptimalDFTSize(A.cols + B.cols - 1);
+     *         dftSize.height = getOptimalDFTSize(A.rows + B.rows - 1);
+     *
+     *         // allocate temporary buffers and initialize them with 0's
+     *         Mat tempA(dftSize, A.type(), Scalar::all(0));
+     *         Mat tempB(dftSize, B.type(), Scalar::all(0));
+     *
+     *         // copy A and B to the top-left corners of tempA and tempB, respectively
+     *         Mat roiA(tempA, Rect(0,0,A.cols,A.rows));
+     *         A.copyTo(roiA);
+     *         Mat roiB(tempB, Rect(0,0,B.cols,B.rows));
+     *         B.copyTo(roiB);
+     *
+     *         // now transform the padded A &amp; B in-place;
+     *         // use "nonzeroRows" hint for faster processing
+     *         dft(tempA, tempA, 0, A.rows);
+     *         dft(tempB, tempB, 0, B.rows);
+     *
+     *         // multiply the spectrums;
+     *         // the function handles packed spectrum representations well
+     *         mulSpectrums(tempA, tempB, tempA);
+     *
+     *         // transform the product back from the frequency domain.
+     *         // Even though all the result rows will be non-zero,
+     *         // you need only the first C.rows of them, and thus you
+     *         // pass nonzeroRows == C.rows
+     *         dft(tempA, tempA, DFT_INVERSE + DFT_SCALE, C.rows);
+     *
+     *         // now copy the result back to C.
+     *         tempA(Rect(0, 0, C.cols, C.rows)).copyTo(C);
+     *
+     *         // all the temporary buffers will be deallocated automatically
+     *     }
+     * </code>
+     * To optimize this sample, consider the following approaches:
+     * <ul>
+     *   <li>
+     *    Since nonzeroRows != 0 is passed to the forward transform calls and since A and B are copied to
+     *     the top-left corners of tempA and tempB, respectively, it is not necessary to clear the whole
+     *     tempA and tempB. It is only necessary to clear the tempA.cols - A.cols ( tempB.cols - B.cols)
+     *     rightmost columns of the matrices.
+     *   </li>
+     *   <li>
+     *    This DFT-based convolution does not have to be applied to the whole big arrays, especially if B
+     *     is significantly smaller than A or vice versa. Instead, you can calculate convolution by parts.
+     *     To do this, you need to split the output array C into multiple tiles. For each tile, estimate
+     *     which parts of A and B are required to calculate convolution in this tile. If the tiles in C are
+     *     too small, the speed will decrease a lot because of repeated work. In the ultimate case, when
+     *     each tile in C is a single pixel, the algorithm becomes equivalent to the naive convolution
+     *     algorithm. If the tiles are too big, the temporary arrays tempA and tempB become too big and
+     *     there is also a slowdown because of bad cache locality. So, there is an optimal tile size
+     *     somewhere in the middle.
+     *   </li>
+     *   <li>
+     *    If different tiles in C can be calculated in parallel and, thus, the convolution is done by
+     *     parts, the loop can be threaded.
+     *   </li>
+     * </ul>
+     *
+     * All of the above improvements have been implemented in #matchTemplate and #filter2D . Therefore, by
+     * using them, you can get the performance even better than with the above theoretically optimal
+     * implementation. Though, those two functions actually calculate cross-correlation, not convolution,
+     * so you need to "flip" the second convolution operand B vertically and horizontally using flip .
+     * <b>Note:</b>
+     * <ul>
+     *   <li>
+     *    An example using the discrete fourier transform can be found at
+     *     opencv_source_code/samples/cpp/dft.cpp
+     *   </li>
+     *   <li>
+     *    (Python) An example using the dft functionality to perform Wiener deconvolution can be found
+     *     at opencv_source/samples/python/deconvolution.py
+     *   </li>
+     *   <li>
+     *    (Python) An example rearranging the quadrants of a Fourier image can be found at
+     *     opencv_source/samples/python/dft.py
+     * @param src input array that could be real or complex.
+     * @param dst output array whose size and type depends on the flags .
+     * nonzeroRows rows of the input array (#DFT_INVERSE is not set) or only the first nonzeroRows of the
+     * output array (#DFT_INVERSE is set) contain non-zeros, thus, the function can handle the rest of the
+     * rows more efficiently and save some time; this technique is very useful for calculating array
+     * cross-correlation or convolution using DFT.
+     * SEE: dct , getOptimalDFTSize , mulSpectrums, filter2D , matchTemplate , flip , cartToPolar ,
+     * magnitude , phase
+     *   </li>
+     * </ul>
+     */
+    public static void dft(Mat src, Mat dst) {
+        dft_2(src.nativeObj, dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::divide(Mat src1, Mat src2, Mat& dst, double scale = 1, int dtype = -1)
+    //
+
+    /**
+     * Performs per-element division of two arrays or a scalar by an array.
+     *
+     * The function cv::divide divides one array by another:
+     * \(\texttt{dst(I) = saturate(src1(I)*scale/src2(I))}\)
+     * or a scalar by an array when there is no src1 :
+     * \(\texttt{dst(I) = saturate(scale/src2(I))}\)
+     *
+     * When src2(I) is zero, dst(I) will also be zero. Different channels of
+     * multi-channel arrays are processed independently.
+     *
+     * <b>Note:</b> Saturation is not applied when the output array has the depth CV_32S. You may even get
+     * result of an incorrect sign in the case of overflow.
+     * @param src1 first input array.
+     * @param src2 second input array of the same size and type as src1.
+     * @param scale scalar factor.
+     * @param dst output array of the same size and type as src2.
+     * @param dtype optional depth of the output array; if -1, dst will have depth src2.depth(), but in
+     * case of an array-by-array division, you can only pass -1 when src1.depth()==src2.depth().
+     * SEE:  multiply, add, subtract
+     */
+    public static void divide(Mat src1, Mat src2, Mat dst, double scale, int dtype) {
+        divide_0(src1.nativeObj, src2.nativeObj, dst.nativeObj, scale, dtype);
+    }
+
+    /**
+     * Performs per-element division of two arrays or a scalar by an array.
+     *
+     * The function cv::divide divides one array by another:
+     * \(\texttt{dst(I) = saturate(src1(I)*scale/src2(I))}\)
+     * or a scalar by an array when there is no src1 :
+     * \(\texttt{dst(I) = saturate(scale/src2(I))}\)
+     *
+     * When src2(I) is zero, dst(I) will also be zero. Different channels of
+     * multi-channel arrays are processed independently.
+     *
+     * <b>Note:</b> Saturation is not applied when the output array has the depth CV_32S. You may even get
+     * result of an incorrect sign in the case of overflow.
+     * @param src1 first input array.
+     * @param src2 second input array of the same size and type as src1.
+     * @param scale scalar factor.
+     * @param dst output array of the same size and type as src2.
+     * case of an array-by-array division, you can only pass -1 when src1.depth()==src2.depth().
+     * SEE:  multiply, add, subtract
+     */
+    public static void divide(Mat src1, Mat src2, Mat dst, double scale) {
+        divide_1(src1.nativeObj, src2.nativeObj, dst.nativeObj, scale);
+    }
+
+    /**
+     * Performs per-element division of two arrays or a scalar by an array.
+     *
+     * The function cv::divide divides one array by another:
+     * \(\texttt{dst(I) = saturate(src1(I)*scale/src2(I))}\)
+     * or a scalar by an array when there is no src1 :
+     * \(\texttt{dst(I) = saturate(scale/src2(I))}\)
+     *
+     * When src2(I) is zero, dst(I) will also be zero. Different channels of
+     * multi-channel arrays are processed independently.
+     *
+     * <b>Note:</b> Saturation is not applied when the output array has the depth CV_32S. You may even get
+     * result of an incorrect sign in the case of overflow.
+     * @param src1 first input array.
+     * @param src2 second input array of the same size and type as src1.
+     * @param dst output array of the same size and type as src2.
+     * case of an array-by-array division, you can only pass -1 when src1.depth()==src2.depth().
+     * SEE:  multiply, add, subtract
+     */
+    public static void divide(Mat src1, Mat src2, Mat dst) {
+        divide_2(src1.nativeObj, src2.nativeObj, dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::divide(Mat src1, Scalar src2, Mat& dst, double scale = 1, int dtype = -1)
+    //
+
+    public static void divide(Mat src1, Scalar src2, Mat dst, double scale, int dtype) {
+        divide_3(src1.nativeObj, src2.val[0], src2.val[1], src2.val[2], src2.val[3], dst.nativeObj, scale, dtype);
+    }
+
+    public static void divide(Mat src1, Scalar src2, Mat dst, double scale) {
+        divide_4(src1.nativeObj, src2.val[0], src2.val[1], src2.val[2], src2.val[3], dst.nativeObj, scale);
+    }
+
+    public static void divide(Mat src1, Scalar src2, Mat dst) {
+        divide_5(src1.nativeObj, src2.val[0], src2.val[1], src2.val[2], src2.val[3], dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::divide(double scale, Mat src2, Mat& dst, int dtype = -1)
+    //
+
+    public static void divide(double scale, Mat src2, Mat dst, int dtype) {
+        divide_6(scale, src2.nativeObj, dst.nativeObj, dtype);
+    }
+
+    public static void divide(double scale, Mat src2, Mat dst) {
+        divide_7(scale, src2.nativeObj, dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::eigenNonSymmetric(Mat src, Mat& eigenvalues, Mat& eigenvectors)
+    //
+
+    /**
+     * Calculates eigenvalues and eigenvectors of a non-symmetric matrix (real eigenvalues only).
+     *
+     * <b>Note:</b> Assumes real eigenvalues.
+     *
+     * The function calculates eigenvalues and eigenvectors (optional) of the square matrix src:
+     * <code>
+     *     src*eigenvectors.row(i).t() = eigenvalues.at&lt;srcType&gt;(i)*eigenvectors.row(i).t()
+     * </code>
+     *
+     * @param src input matrix (CV_32FC1 or CV_64FC1 type).
+     * @param eigenvalues output vector of eigenvalues (type is the same type as src).
+     * @param eigenvectors output matrix of eigenvectors (type is the same type as src). The eigenvectors are stored as subsequent matrix rows, in the same order as the corresponding eigenvalues.
+     * SEE: eigen
+     */
+    public static void eigenNonSymmetric(Mat src, Mat eigenvalues, Mat eigenvectors) {
+        eigenNonSymmetric_0(src.nativeObj, eigenvalues.nativeObj, eigenvectors.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::exp(Mat src, Mat& dst)
+    //
+
+    /**
+     * Calculates the exponent of every array element.
+     *
+     * The function cv::exp calculates the exponent of every element of the input
+     * array:
+     * \(\texttt{dst} [I] = e^{ src(I) }\)
+     *
+     * The maximum relative error is about 7e-6 for single-precision input and
+     * less than 1e-10 for double-precision input. Currently, the function
+     * converts denormalized values to zeros on output. Special values (NaN,
+     * Inf) are not handled.
+     * @param src input array.
+     * @param dst output array of the same size and type as src.
+     * SEE: log , cartToPolar , polarToCart , phase , pow , sqrt , magnitude
+     */
+    public static void exp(Mat src, Mat dst) {
+        exp_0(src.nativeObj, dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::extractChannel(Mat src, Mat& dst, int coi)
+    //
+
+    /**
+     * Extracts a single channel from src (coi is 0-based index)
+     * @param src input array
+     * @param dst output array
+     * @param coi index of channel to extract
+     * SEE: mixChannels, split
+     */
+    public static void extractChannel(Mat src, Mat dst, int coi) {
+        extractChannel_0(src.nativeObj, dst.nativeObj, coi);
+    }
+
+
+    //
+    // C++:  void cv::findNonZero(Mat src, Mat& idx)
+    //
+
+    /**
+     * Returns the list of locations of non-zero pixels
+     *
+     * Given a binary matrix (likely returned from an operation such
+     * as threshold(), compare(), &gt;, ==, etc, return all of
+     * the non-zero indices as a cv::Mat or std::vector&lt;cv::Point&gt; (x,y)
+     * For example:
+     * <code>
+     *     cv::Mat binaryImage; // input, binary image
+     *     cv::Mat locations;   // output, locations of non-zero pixels
+     *     cv::findNonZero(binaryImage, locations);
+     *
+     *     // access pixel coordinates
+     *     Point pnt = locations.at&lt;Point&gt;(i);
+     * </code>
+     * or
+     * <code>
+     *     cv::Mat binaryImage; // input, binary image
+     *     vector&lt;Point&gt; locations;   // output, locations of non-zero pixels
+     *     cv::findNonZero(binaryImage, locations);
+     *
+     *     // access pixel coordinates
+     *     Point pnt = locations[i];
+     * </code>
+     * @param src single-channel array (type CV_8UC1)
+     * @param idx the output array, type of cv::Mat or std::vector&lt;Point&gt;, corresponding to non-zero indices in the input
+     */
+    public static void findNonZero(Mat src, Mat idx) {
+        findNonZero_0(src.nativeObj, idx.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::flip(Mat src, Mat& dst, int flipCode)
+    //
+
+    /**
+     * Flips a 2D array around vertical, horizontal, or both axes.
+     *
+     * The function cv::flip flips the array in one of three different ways (row
+     * and column indices are 0-based):
+     * \(\texttt{dst} _{ij} =
+     * \left\{
+     * \begin{array}{l l}
+     * \texttt{src} _{\texttt{src.rows}-i-1,j} &amp; if\;  \texttt{flipCode} = 0 \\
+     * \texttt{src} _{i, \texttt{src.cols} -j-1} &amp; if\;  \texttt{flipCode} &gt; 0 \\
+     * \texttt{src} _{ \texttt{src.rows} -i-1, \texttt{src.cols} -j-1} &amp; if\; \texttt{flipCode} &lt; 0 \\
+     * \end{array}
+     * \right.\)
+     * The example scenarios of using the function are the following:
+     * Vertical flipping of the image (flipCode == 0) to switch between
+     *     top-left and bottom-left image origin. This is a typical operation
+     *     in video processing on Microsoft Windows\* OS.
+     * Horizontal flipping of the image with the subsequent horizontal
+     *     shift and absolute difference calculation to check for a
+     *     vertical-axis symmetry (flipCode &gt; 0).
+     * Simultaneous horizontal and vertical flipping of the image with
+     *     the subsequent shift and absolute difference calculation to check
+     *     for a central symmetry (flipCode &lt; 0).
+     * Reversing the order of point arrays (flipCode &gt; 0 or
+     *     flipCode == 0).
+     * @param src input array.
+     * @param dst output array of the same size and type as src.
+     * @param flipCode a flag to specify how to flip the array; 0 means
+     * flipping around the x-axis and positive value (for example, 1) means
+     * flipping around y-axis. Negative value (for example, -1) means flipping
+     * around both axes.
+     * SEE: transpose , repeat , completeSymm
+     */
+    public static void flip(Mat src, Mat dst, int flipCode) {
+        flip_0(src.nativeObj, dst.nativeObj, flipCode);
+    }
+
+
+    //
+    // C++:  void cv::gemm(Mat src1, Mat src2, double alpha, Mat src3, double beta, Mat& dst, int flags = 0)
+    //
+
+    /**
+     * Performs generalized matrix multiplication.
+     *
+     * The function cv::gemm performs generalized matrix multiplication similar to the
+     * gemm functions in BLAS level 3. For example,
+     * {@code gemm(src1, src2, alpha, src3, beta, dst, GEMM_1_T + GEMM_3_T)}
+     * corresponds to
+     * \(\texttt{dst} =  \texttt{alpha} \cdot \texttt{src1} ^T  \cdot \texttt{src2} +  \texttt{beta} \cdot \texttt{src3} ^T\)
+     *
+     * In case of complex (two-channel) data, performed a complex matrix
+     * multiplication.
+     *
+     * The function can be replaced with a matrix expression. For example, the
+     * above call can be replaced with:
+     * <code>
+     *     dst = alpha*src1.t()*src2 + beta*src3.t();
+     * </code>
+     * @param src1 first multiplied input matrix that could be real(CV_32FC1,
+     * CV_64FC1) or complex(CV_32FC2, CV_64FC2).
+     * @param src2 second multiplied input matrix of the same type as src1.
+     * @param alpha weight of the matrix product.
+     * @param src3 third optional delta matrix added to the matrix product; it
+     * should have the same type as src1 and src2.
+     * @param beta weight of src3.
+     * @param dst output matrix; it has the proper size and the same type as
+     * input matrices.
+     * @param flags operation flags (cv::GemmFlags)
+     * SEE: mulTransposed , transform
+     */
+    public static void gemm(Mat src1, Mat src2, double alpha, Mat src3, double beta, Mat dst, int flags) {
+        gemm_0(src1.nativeObj, src2.nativeObj, alpha, src3.nativeObj, beta, dst.nativeObj, flags);
+    }
+
+    /**
+     * Performs generalized matrix multiplication.
+     *
+     * The function cv::gemm performs generalized matrix multiplication similar to the
+     * gemm functions in BLAS level 3. For example,
+     * {@code gemm(src1, src2, alpha, src3, beta, dst, GEMM_1_T + GEMM_3_T)}
+     * corresponds to
+     * \(\texttt{dst} =  \texttt{alpha} \cdot \texttt{src1} ^T  \cdot \texttt{src2} +  \texttt{beta} \cdot \texttt{src3} ^T\)
+     *
+     * In case of complex (two-channel) data, performed a complex matrix
+     * multiplication.
+     *
+     * The function can be replaced with a matrix expression. For example, the
+     * above call can be replaced with:
+     * <code>
+     *     dst = alpha*src1.t()*src2 + beta*src3.t();
+     * </code>
+     * @param src1 first multiplied input matrix that could be real(CV_32FC1,
+     * CV_64FC1) or complex(CV_32FC2, CV_64FC2).
+     * @param src2 second multiplied input matrix of the same type as src1.
+     * @param alpha weight of the matrix product.
+     * @param src3 third optional delta matrix added to the matrix product; it
+     * should have the same type as src1 and src2.
+     * @param beta weight of src3.
+     * @param dst output matrix; it has the proper size and the same type as
+     * input matrices.
+     * SEE: mulTransposed , transform
+     */
+    public static void gemm(Mat src1, Mat src2, double alpha, Mat src3, double beta, Mat dst) {
+        gemm_1(src1.nativeObj, src2.nativeObj, alpha, src3.nativeObj, beta, dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::hconcat(vector_Mat src, Mat& dst)
+    //
+
+    /**
+     *
+     *  <code>
+     *     std::vector&lt;cv::Mat&gt; matrices = { cv::Mat(4, 1, CV_8UC1, cv::Scalar(1)),
+     *                                       cv::Mat(4, 1, CV_8UC1, cv::Scalar(2)),
+     *                                       cv::Mat(4, 1, CV_8UC1, cv::Scalar(3)),};
+     *
+     *     cv::Mat out;
+     *     cv::hconcat( matrices, out );
+     *     //out:
+     *     //[1, 2, 3;
+     *     // 1, 2, 3;
+     *     // 1, 2, 3;
+     *     // 1, 2, 3]
+     *  </code>
+     *  @param src input array or vector of matrices. all of the matrices must have the same number of rows and the same depth.
+     *  @param dst output array. It has the same number of rows and depth as the src, and the sum of cols of the src.
+     * same depth.
+     */
+    public static void hconcat(List<Mat> src, Mat dst) {
+        Mat src_mat = Converters.vector_Mat_to_Mat(src);
+        hconcat_0(src_mat.nativeObj, dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::idct(Mat src, Mat& dst, int flags = 0)
+    //
+
+    /**
+     * Calculates the inverse Discrete Cosine Transform of a 1D or 2D array.
+     *
+     * idct(src, dst, flags) is equivalent to dct(src, dst, flags | DCT_INVERSE).
+     * @param src input floating-point single-channel array.
+     * @param dst output array of the same size and type as src.
+     * @param flags operation flags.
+     * SEE:  dct, dft, idft, getOptimalDFTSize
+     */
+    public static void idct(Mat src, Mat dst, int flags) {
+        idct_0(src.nativeObj, dst.nativeObj, flags);
+    }
+
+    /**
+     * Calculates the inverse Discrete Cosine Transform of a 1D or 2D array.
+     *
+     * idct(src, dst, flags) is equivalent to dct(src, dst, flags | DCT_INVERSE).
+     * @param src input floating-point single-channel array.
+     * @param dst output array of the same size and type as src.
+     * SEE:  dct, dft, idft, getOptimalDFTSize
+     */
+    public static void idct(Mat src, Mat dst) {
+        idct_1(src.nativeObj, dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::idft(Mat src, Mat& dst, int flags = 0, int nonzeroRows = 0)
+    //
+
+    /**
+     * Calculates the inverse Discrete Fourier Transform of a 1D or 2D array.
+     *
+     * idft(src, dst, flags) is equivalent to dft(src, dst, flags | #DFT_INVERSE) .
+     * <b>Note:</b> None of dft and idft scales the result by default. So, you should pass #DFT_SCALE to one of
+     * dft or idft explicitly to make these transforms mutually inverse.
+     * SEE: dft, dct, idct, mulSpectrums, getOptimalDFTSize
+     * @param src input floating-point real or complex array.
+     * @param dst output array whose size and type depend on the flags.
+     * @param flags operation flags (see dft and #DftFlags).
+     * @param nonzeroRows number of dst rows to process; the rest of the rows have undefined content (see
+     * the convolution sample in dft description.
+     */
+    public static void idft(Mat src, Mat dst, int flags, int nonzeroRows) {
+        idft_0(src.nativeObj, dst.nativeObj, flags, nonzeroRows);
+    }
+
+    /**
+     * Calculates the inverse Discrete Fourier Transform of a 1D or 2D array.
+     *
+     * idft(src, dst, flags) is equivalent to dft(src, dst, flags | #DFT_INVERSE) .
+     * <b>Note:</b> None of dft and idft scales the result by default. So, you should pass #DFT_SCALE to one of
+     * dft or idft explicitly to make these transforms mutually inverse.
+     * SEE: dft, dct, idct, mulSpectrums, getOptimalDFTSize
+     * @param src input floating-point real or complex array.
+     * @param dst output array whose size and type depend on the flags.
+     * @param flags operation flags (see dft and #DftFlags).
+     * the convolution sample in dft description.
+     */
+    public static void idft(Mat src, Mat dst, int flags) {
+        idft_1(src.nativeObj, dst.nativeObj, flags);
+    }
+
+    /**
+     * Calculates the inverse Discrete Fourier Transform of a 1D or 2D array.
+     *
+     * idft(src, dst, flags) is equivalent to dft(src, dst, flags | #DFT_INVERSE) .
+     * <b>Note:</b> None of dft and idft scales the result by default. So, you should pass #DFT_SCALE to one of
+     * dft or idft explicitly to make these transforms mutually inverse.
+     * SEE: dft, dct, idct, mulSpectrums, getOptimalDFTSize
+     * @param src input floating-point real or complex array.
+     * @param dst output array whose size and type depend on the flags.
+     * the convolution sample in dft description.
+     */
+    public static void idft(Mat src, Mat dst) {
+        idft_2(src.nativeObj, dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::inRange(Mat src, Scalar lowerb, Scalar upperb, Mat& dst)
+    //
+
+    /**
+     *  Checks if array elements lie between the elements of two other arrays.
+     *
+     * The function checks the range as follows:
+     * <ul>
+     *   <li>
+     *    For every element of a single-channel input array:
+     *     \(\texttt{dst} (I)= \texttt{lowerb} (I)_0  \leq \texttt{src} (I)_0 \leq  \texttt{upperb} (I)_0\)
+     *   </li>
+     *   <li>
+     *    For two-channel arrays:
+     *     \(\texttt{dst} (I)= \texttt{lowerb} (I)_0  \leq \texttt{src} (I)_0 \leq  \texttt{upperb} (I)_0  \land \texttt{lowerb} (I)_1  \leq \texttt{src} (I)_1 \leq  \texttt{upperb} (I)_1\)
+     *   </li>
+     *   <li>
+     *    and so forth.
+     *   </li>
+     * </ul>
+     *
+     * That is, dst (I) is set to 255 (all 1 -bits) if src (I) is within the
+     * specified 1D, 2D, 3D, ... box and 0 otherwise.
+     *
+     * When the lower and/or upper boundary parameters are scalars, the indexes
+     * (I) at lowerb and upperb in the above formulas should be omitted.
+     * @param src first input array.
+     * @param lowerb inclusive lower boundary array or a scalar.
+     * @param upperb inclusive upper boundary array or a scalar.
+     * @param dst output array of the same size as src and CV_8U type.
+     */
+    public static void inRange(Mat src, Scalar lowerb, Scalar upperb, Mat dst) {
+        inRange_0(src.nativeObj, lowerb.val[0], lowerb.val[1], lowerb.val[2], lowerb.val[3], upperb.val[0], upperb.val[1], upperb.val[2], upperb.val[3], dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::insertChannel(Mat src, Mat& dst, int coi)
+    //
+
+    /**
+     * Inserts a single channel to dst (coi is 0-based index)
+     * @param src input array
+     * @param dst output array
+     * @param coi index of channel for insertion
+     * SEE: mixChannels, merge
+     */
+    public static void insertChannel(Mat src, Mat dst, int coi) {
+        insertChannel_0(src.nativeObj, dst.nativeObj, coi);
+    }
+
+
+    //
+    // C++:  void cv::log(Mat src, Mat& dst)
+    //
+
+    /**
+     * Calculates the natural logarithm of every array element.
+     *
+     * The function cv::log calculates the natural logarithm of every element of the input array:
+     * \(\texttt{dst} (I) =  \log (\texttt{src}(I)) \)
+     *
+     * Output on zero, negative and special (NaN, Inf) values is undefined.
+     *
+     * @param src input array.
+     * @param dst output array of the same size and type as src .
+     * SEE: exp, cartToPolar, polarToCart, phase, pow, sqrt, magnitude
+     */
+    public static void log(Mat src, Mat dst) {
+        log_0(src.nativeObj, dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::magnitude(Mat x, Mat y, Mat& magnitude)
+    //
+
+    /**
+     * Calculates the magnitude of 2D vectors.
+     *
+     * The function cv::magnitude calculates the magnitude of 2D vectors formed
+     * from the corresponding elements of x and y arrays:
+     * \(\texttt{dst} (I) =  \sqrt{\texttt{x}(I)^2 + \texttt{y}(I)^2}\)
+     * @param x floating-point array of x-coordinates of the vectors.
+     * @param y floating-point array of y-coordinates of the vectors; it must
+     * have the same size as x.
+     * @param magnitude output array of the same size and type as x.
+     * SEE: cartToPolar, polarToCart, phase, sqrt
+     */
+    public static void magnitude(Mat x, Mat y, Mat magnitude) {
+        magnitude_0(x.nativeObj, y.nativeObj, magnitude.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::max(Mat src1, Mat src2, Mat& dst)
+    //
+
+    /**
+     * Calculates per-element maximum of two arrays or an array and a scalar.
+     *
+     * The function cv::max calculates the per-element maximum of two arrays:
+     * \(\texttt{dst} (I)= \max ( \texttt{src1} (I), \texttt{src2} (I))\)
+     * or array and a scalar:
+     * \(\texttt{dst} (I)= \max ( \texttt{src1} (I), \texttt{value} )\)
+     * @param src1 first input array.
+     * @param src2 second input array of the same size and type as src1 .
+     * @param dst output array of the same size and type as src1.
+     * SEE:  min, compare, inRange, minMaxLoc, REF: MatrixExpressions
+     */
+    public static void max(Mat src1, Mat src2, Mat dst) {
+        max_0(src1.nativeObj, src2.nativeObj, dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::max(Mat src1, Scalar src2, Mat& dst)
+    //
+
+    public static void max(Mat src1, Scalar src2, Mat dst) {
+        max_1(src1.nativeObj, src2.val[0], src2.val[1], src2.val[2], src2.val[3], dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::meanStdDev(Mat src, vector_double& mean, vector_double& stddev, Mat mask = Mat())
+    //
+
+    /**
+     * Calculates a mean and standard deviation of array elements.
+     *
+     * The function cv::meanStdDev calculates the mean and the standard deviation M
+     * of array elements independently for each channel and returns it via the
+     * output parameters:
+     * \(\begin{array}{l} N =  \sum _{I, \texttt{mask} (I)  \ne 0} 1 \\ \texttt{mean} _c =  \frac{\sum_{ I: \; \texttt{mask}(I) \ne 0} \texttt{src} (I)_c}{N} \\ \texttt{stddev} _c =  \sqrt{\frac{\sum_{ I: \; \texttt{mask}(I) \ne 0} \left ( \texttt{src} (I)_c -  \texttt{mean} _c \right )^2}{N}} \end{array}\)
+     * When all the mask elements are 0's, the function returns
+     * mean=stddev=Scalar::all(0).
+     * <b>Note:</b> The calculated standard deviation is only the diagonal of the
+     * complete normalized covariance matrix. If the full matrix is needed, you
+     * can reshape the multi-channel array M x N to the single-channel array
+     * M\*N x mtx.channels() (only possible when the matrix is continuous) and
+     * then pass the matrix to calcCovarMatrix .
+     * @param src input array that should have from 1 to 4 channels so that the results can be stored in
+     * Scalar_ 's.
+     * @param mean output parameter: calculated mean value.
+     * @param stddev output parameter: calculated standard deviation.
+     * @param mask optional operation mask.
+     * SEE:  countNonZero, mean, norm, minMaxLoc, calcCovarMatrix
+     */
+    public static void meanStdDev(Mat src, MatOfDouble mean, MatOfDouble stddev, Mat mask) {
+        Mat mean_mat = mean;
+        Mat stddev_mat = stddev;
+        meanStdDev_0(src.nativeObj, mean_mat.nativeObj, stddev_mat.nativeObj, mask.nativeObj);
+    }
+
+    /**
+     * Calculates a mean and standard deviation of array elements.
+     *
+     * The function cv::meanStdDev calculates the mean and the standard deviation M
+     * of array elements independently for each channel and returns it via the
+     * output parameters:
+     * \(\begin{array}{l} N =  \sum _{I, \texttt{mask} (I)  \ne 0} 1 \\ \texttt{mean} _c =  \frac{\sum_{ I: \; \texttt{mask}(I) \ne 0} \texttt{src} (I)_c}{N} \\ \texttt{stddev} _c =  \sqrt{\frac{\sum_{ I: \; \texttt{mask}(I) \ne 0} \left ( \texttt{src} (I)_c -  \texttt{mean} _c \right )^2}{N}} \end{array}\)
+     * When all the mask elements are 0's, the function returns
+     * mean=stddev=Scalar::all(0).
+     * <b>Note:</b> The calculated standard deviation is only the diagonal of the
+     * complete normalized covariance matrix. If the full matrix is needed, you
+     * can reshape the multi-channel array M x N to the single-channel array
+     * M\*N x mtx.channels() (only possible when the matrix is continuous) and
+     * then pass the matrix to calcCovarMatrix .
+     * @param src input array that should have from 1 to 4 channels so that the results can be stored in
+     * Scalar_ 's.
+     * @param mean output parameter: calculated mean value.
+     * @param stddev output parameter: calculated standard deviation.
+     * SEE:  countNonZero, mean, norm, minMaxLoc, calcCovarMatrix
+     */
+    public static void meanStdDev(Mat src, MatOfDouble mean, MatOfDouble stddev) {
+        Mat mean_mat = mean;
+        Mat stddev_mat = stddev;
+        meanStdDev_1(src.nativeObj, mean_mat.nativeObj, stddev_mat.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::merge(vector_Mat mv, Mat& dst)
+    //
+
+    /**
+     *
+     * @param mv input vector of matrices to be merged; all the matrices in mv must have the same
+     * size and the same depth.
+     * @param dst output array of the same size and the same depth as mv[0]; The number of channels will
+     * be the total number of channels in the matrix array.
+     */
+    public static void merge(List<Mat> mv, Mat dst) {
+        Mat mv_mat = Converters.vector_Mat_to_Mat(mv);
+        merge_0(mv_mat.nativeObj, dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::min(Mat src1, Mat src2, Mat& dst)
+    //
+
+    /**
+     * Calculates per-element minimum of two arrays or an array and a scalar.
+     *
+     * The function cv::min calculates the per-element minimum of two arrays:
+     * \(\texttt{dst} (I)= \min ( \texttt{src1} (I), \texttt{src2} (I))\)
+     * or array and a scalar:
+     * \(\texttt{dst} (I)= \min ( \texttt{src1} (I), \texttt{value} )\)
+     * @param src1 first input array.
+     * @param src2 second input array of the same size and type as src1.
+     * @param dst output array of the same size and type as src1.
+     * SEE: max, compare, inRange, minMaxLoc
+     */
+    public static void min(Mat src1, Mat src2, Mat dst) {
+        min_0(src1.nativeObj, src2.nativeObj, dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::min(Mat src1, Scalar src2, Mat& dst)
+    //
+
+    public static void min(Mat src1, Scalar src2, Mat dst) {
+        min_1(src1.nativeObj, src2.val[0], src2.val[1], src2.val[2], src2.val[3], dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::mixChannels(vector_Mat src, vector_Mat dst, vector_int fromTo)
+    //
+
+    /**
+     *
+     * @param src input array or vector of matrices; all of the matrices must have the same size and the
+     * same depth.
+     * @param dst output array or vector of matrices; all the matrices <b>must be allocated</b>; their size and
+     * depth must be the same as in src[0].
+     * @param fromTo array of index pairs specifying which channels are copied and where; fromTo[k\*2] is
+     * a 0-based index of the input channel in src, fromTo[k\*2+1] is an index of the output channel in
+     * dst; the continuous channel numbering is used: the first input image channels are indexed from 0 to
+     * src[0].channels()-1, the second input image channels are indexed from src[0].channels() to
+     * src[0].channels() + src[1].channels()-1, and so on, the same scheme is used for the output image
+     * channels; as a special case, when fromTo[k\*2] is negative, the corresponding output channel is
+     * filled with zero .
+     */
+    public static void mixChannels(List<Mat> src, List<Mat> dst, MatOfInt fromTo) {
+        Mat src_mat = Converters.vector_Mat_to_Mat(src);
+        Mat dst_mat = Converters.vector_Mat_to_Mat(dst);
+        Mat fromTo_mat = fromTo;
+        mixChannels_0(src_mat.nativeObj, dst_mat.nativeObj, fromTo_mat.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::mulSpectrums(Mat a, Mat b, Mat& c, int flags, bool conjB = false)
+    //
+
+    /**
+     * Performs the per-element multiplication of two Fourier spectrums.
+     *
+     * The function cv::mulSpectrums performs the per-element multiplication of the two CCS-packed or complex
+     * matrices that are results of a real or complex Fourier transform.
+     *
+     * The function, together with dft and idft , may be used to calculate convolution (pass conjB=false )
+     * or correlation (pass conjB=true ) of two arrays rapidly. When the arrays are complex, they are
+     * simply multiplied (per element) with an optional conjugation of the second-array elements. When the
+     * arrays are real, they are assumed to be CCS-packed (see dft for details).
+     * @param a first input array.
+     * @param b second input array of the same size and type as src1 .
+     * @param c output array of the same size and type as src1 .
+     * @param flags operation flags; currently, the only supported flag is cv::DFT_ROWS, which indicates that
+     * each row of src1 and src2 is an independent 1D Fourier spectrum. If you do not want to use this flag, then simply add a {@code 0} as value.
+     * @param conjB optional flag that conjugates the second input array before the multiplication (true)
+     * or not (false).
+     */
+    public static void mulSpectrums(Mat a, Mat b, Mat c, int flags, boolean conjB) {
+        mulSpectrums_0(a.nativeObj, b.nativeObj, c.nativeObj, flags, conjB);
+    }
+
+    /**
+     * Performs the per-element multiplication of two Fourier spectrums.
+     *
+     * The function cv::mulSpectrums performs the per-element multiplication of the two CCS-packed or complex
+     * matrices that are results of a real or complex Fourier transform.
+     *
+     * The function, together with dft and idft , may be used to calculate convolution (pass conjB=false )
+     * or correlation (pass conjB=true ) of two arrays rapidly. When the arrays are complex, they are
+     * simply multiplied (per element) with an optional conjugation of the second-array elements. When the
+     * arrays are real, they are assumed to be CCS-packed (see dft for details).
+     * @param a first input array.
+     * @param b second input array of the same size and type as src1 .
+     * @param c output array of the same size and type as src1 .
+     * @param flags operation flags; currently, the only supported flag is cv::DFT_ROWS, which indicates that
+     * each row of src1 and src2 is an independent 1D Fourier spectrum. If you do not want to use this flag, then simply add a {@code 0} as value.
+     * or not (false).
+     */
+    public static void mulSpectrums(Mat a, Mat b, Mat c, int flags) {
+        mulSpectrums_1(a.nativeObj, b.nativeObj, c.nativeObj, flags);
+    }
+
+
+    //
+    // C++:  void cv::mulTransposed(Mat src, Mat& dst, bool aTa, Mat delta = Mat(), double scale = 1, int dtype = -1)
+    //
+
+    /**
+     * Calculates the product of a matrix and its transposition.
+     *
+     * The function cv::mulTransposed calculates the product of src and its
+     * transposition:
+     * \(\texttt{dst} = \texttt{scale} ( \texttt{src} - \texttt{delta} )^T ( \texttt{src} - \texttt{delta} )\)
+     * if aTa=true , and
+     * \(\texttt{dst} = \texttt{scale} ( \texttt{src} - \texttt{delta} ) ( \texttt{src} - \texttt{delta} )^T\)
+     * otherwise. The function is used to calculate the covariance matrix. With
+     * zero delta, it can be used as a faster substitute for general matrix
+     * product A\*B when B=A'
+     * @param src input single-channel matrix. Note that unlike gemm, the
+     * function can multiply not only floating-point matrices.
+     * @param dst output square matrix.
+     * @param aTa Flag specifying the multiplication ordering. See the
+     * description below.
+     * @param delta Optional delta matrix subtracted from src before the
+     * multiplication. When the matrix is empty ( delta=noArray() ), it is
+     * assumed to be zero, that is, nothing is subtracted. If it has the same
+     * size as src , it is simply subtracted. Otherwise, it is "repeated" (see
+     * repeat ) to cover the full src and then subtracted. Type of the delta
+     * matrix, when it is not empty, must be the same as the type of created
+     * output matrix. See the dtype parameter description below.
+     * @param scale Optional scale factor for the matrix product.
+     * @param dtype Optional type of the output matrix. When it is negative,
+     * the output matrix will have the same type as src . Otherwise, it will be
+     * type=CV_MAT_DEPTH(dtype) that should be either CV_32F or CV_64F .
+     * SEE: calcCovarMatrix, gemm, repeat, reduce
+     */
+    public static void mulTransposed(Mat src, Mat dst, boolean aTa, Mat delta, double scale, int dtype) {
+        mulTransposed_0(src.nativeObj, dst.nativeObj, aTa, delta.nativeObj, scale, dtype);
+    }
+
+    /**
+     * Calculates the product of a matrix and its transposition.
+     *
+     * The function cv::mulTransposed calculates the product of src and its
+     * transposition:
+     * \(\texttt{dst} = \texttt{scale} ( \texttt{src} - \texttt{delta} )^T ( \texttt{src} - \texttt{delta} )\)
+     * if aTa=true , and
+     * \(\texttt{dst} = \texttt{scale} ( \texttt{src} - \texttt{delta} ) ( \texttt{src} - \texttt{delta} )^T\)
+     * otherwise. The function is used to calculate the covariance matrix. With
+     * zero delta, it can be used as a faster substitute for general matrix
+     * product A\*B when B=A'
+     * @param src input single-channel matrix. Note that unlike gemm, the
+     * function can multiply not only floating-point matrices.
+     * @param dst output square matrix.
+     * @param aTa Flag specifying the multiplication ordering. See the
+     * description below.
+     * @param delta Optional delta matrix subtracted from src before the
+     * multiplication. When the matrix is empty ( delta=noArray() ), it is
+     * assumed to be zero, that is, nothing is subtracted. If it has the same
+     * size as src , it is simply subtracted. Otherwise, it is "repeated" (see
+     * repeat ) to cover the full src and then subtracted. Type of the delta
+     * matrix, when it is not empty, must be the same as the type of created
+     * output matrix. See the dtype parameter description below.
+     * @param scale Optional scale factor for the matrix product.
+     * the output matrix will have the same type as src . Otherwise, it will be
+     * type=CV_MAT_DEPTH(dtype) that should be either CV_32F or CV_64F .
+     * SEE: calcCovarMatrix, gemm, repeat, reduce
+     */
+    public static void mulTransposed(Mat src, Mat dst, boolean aTa, Mat delta, double scale) {
+        mulTransposed_1(src.nativeObj, dst.nativeObj, aTa, delta.nativeObj, scale);
+    }
+
+    /**
+     * Calculates the product of a matrix and its transposition.
+     *
+     * The function cv::mulTransposed calculates the product of src and its
+     * transposition:
+     * \(\texttt{dst} = \texttt{scale} ( \texttt{src} - \texttt{delta} )^T ( \texttt{src} - \texttt{delta} )\)
+     * if aTa=true , and
+     * \(\texttt{dst} = \texttt{scale} ( \texttt{src} - \texttt{delta} ) ( \texttt{src} - \texttt{delta} )^T\)
+     * otherwise. The function is used to calculate the covariance matrix. With
+     * zero delta, it can be used as a faster substitute for general matrix
+     * product A\*B when B=A'
+     * @param src input single-channel matrix. Note that unlike gemm, the
+     * function can multiply not only floating-point matrices.
+     * @param dst output square matrix.
+     * @param aTa Flag specifying the multiplication ordering. See the
+     * description below.
+     * @param delta Optional delta matrix subtracted from src before the
+     * multiplication. When the matrix is empty ( delta=noArray() ), it is
+     * assumed to be zero, that is, nothing is subtracted. If it has the same
+     * size as src , it is simply subtracted. Otherwise, it is "repeated" (see
+     * repeat ) to cover the full src and then subtracted. Type of the delta
+     * matrix, when it is not empty, must be the same as the type of created
+     * output matrix. See the dtype parameter description below.
+     * the output matrix will have the same type as src . Otherwise, it will be
+     * type=CV_MAT_DEPTH(dtype) that should be either CV_32F or CV_64F .
+     * SEE: calcCovarMatrix, gemm, repeat, reduce
+     */
+    public static void mulTransposed(Mat src, Mat dst, boolean aTa, Mat delta) {
+        mulTransposed_2(src.nativeObj, dst.nativeObj, aTa, delta.nativeObj);
+    }
+
+    /**
+     * Calculates the product of a matrix and its transposition.
+     *
+     * The function cv::mulTransposed calculates the product of src and its
+     * transposition:
+     * \(\texttt{dst} = \texttt{scale} ( \texttt{src} - \texttt{delta} )^T ( \texttt{src} - \texttt{delta} )\)
+     * if aTa=true , and
+     * \(\texttt{dst} = \texttt{scale} ( \texttt{src} - \texttt{delta} ) ( \texttt{src} - \texttt{delta} )^T\)
+     * otherwise. The function is used to calculate the covariance matrix. With
+     * zero delta, it can be used as a faster substitute for general matrix
+     * product A\*B when B=A'
+     * @param src input single-channel matrix. Note that unlike gemm, the
+     * function can multiply not only floating-point matrices.
+     * @param dst output square matrix.
+     * @param aTa Flag specifying the multiplication ordering. See the
+     * description below.
+     * multiplication. When the matrix is empty ( delta=noArray() ), it is
+     * assumed to be zero, that is, nothing is subtracted. If it has the same
+     * size as src , it is simply subtracted. Otherwise, it is "repeated" (see
+     * repeat ) to cover the full src and then subtracted. Type of the delta
+     * matrix, when it is not empty, must be the same as the type of created
+     * output matrix. See the dtype parameter description below.
+     * the output matrix will have the same type as src . Otherwise, it will be
+     * type=CV_MAT_DEPTH(dtype) that should be either CV_32F or CV_64F .
+     * SEE: calcCovarMatrix, gemm, repeat, reduce
+     */
+    public static void mulTransposed(Mat src, Mat dst, boolean aTa) {
+        mulTransposed_3(src.nativeObj, dst.nativeObj, aTa);
+    }
+
+
+    //
+    // C++:  void cv::multiply(Mat src1, Mat src2, Mat& dst, double scale = 1, int dtype = -1)
+    //
+
+    /**
+     * Calculates the per-element scaled product of two arrays.
+     *
+     * The function multiply calculates the per-element product of two arrays:
+     *
+     * \(\texttt{dst} (I)= \texttt{saturate} ( \texttt{scale} \cdot \texttt{src1} (I)  \cdot \texttt{src2} (I))\)
+     *
+     * There is also a REF: MatrixExpressions -friendly variant of the first function. See Mat::mul .
+     *
+     * For a not-per-element matrix product, see gemm .
+     *
+     * <b>Note:</b> Saturation is not applied when the output array has the depth
+     * CV_32S. You may even get result of an incorrect sign in the case of
+     * overflow.
+     * @param src1 first input array.
+     * @param src2 second input array of the same size and the same type as src1.
+     * @param dst output array of the same size and type as src1.
+     * @param scale optional scale factor.
+     * @param dtype optional depth of the output array
+     * SEE: add, subtract, divide, scaleAdd, addWeighted, accumulate, accumulateProduct, accumulateSquare,
+     * Mat::convertTo
+     */
+    public static void multiply(Mat src1, Mat src2, Mat dst, double scale, int dtype) {
+        multiply_0(src1.nativeObj, src2.nativeObj, dst.nativeObj, scale, dtype);
+    }
+
+    /**
+     * Calculates the per-element scaled product of two arrays.
+     *
+     * The function multiply calculates the per-element product of two arrays:
+     *
+     * \(\texttt{dst} (I)= \texttt{saturate} ( \texttt{scale} \cdot \texttt{src1} (I)  \cdot \texttt{src2} (I))\)
+     *
+     * There is also a REF: MatrixExpressions -friendly variant of the first function. See Mat::mul .
+     *
+     * For a not-per-element matrix product, see gemm .
+     *
+     * <b>Note:</b> Saturation is not applied when the output array has the depth
+     * CV_32S. You may even get result of an incorrect sign in the case of
+     * overflow.
+     * @param src1 first input array.
+     * @param src2 second input array of the same size and the same type as src1.
+     * @param dst output array of the same size and type as src1.
+     * @param scale optional scale factor.
+     * SEE: add, subtract, divide, scaleAdd, addWeighted, accumulate, accumulateProduct, accumulateSquare,
+     * Mat::convertTo
+     */
+    public static void multiply(Mat src1, Mat src2, Mat dst, double scale) {
+        multiply_1(src1.nativeObj, src2.nativeObj, dst.nativeObj, scale);
+    }
+
+    /**
+     * Calculates the per-element scaled product of two arrays.
+     *
+     * The function multiply calculates the per-element product of two arrays:
+     *
+     * \(\texttt{dst} (I)= \texttt{saturate} ( \texttt{scale} \cdot \texttt{src1} (I)  \cdot \texttt{src2} (I))\)
+     *
+     * There is also a REF: MatrixExpressions -friendly variant of the first function. See Mat::mul .
+     *
+     * For a not-per-element matrix product, see gemm .
+     *
+     * <b>Note:</b> Saturation is not applied when the output array has the depth
+     * CV_32S. You may even get result of an incorrect sign in the case of
+     * overflow.
+     * @param src1 first input array.
+     * @param src2 second input array of the same size and the same type as src1.
+     * @param dst output array of the same size and type as src1.
+     * SEE: add, subtract, divide, scaleAdd, addWeighted, accumulate, accumulateProduct, accumulateSquare,
+     * Mat::convertTo
+     */
+    public static void multiply(Mat src1, Mat src2, Mat dst) {
+        multiply_2(src1.nativeObj, src2.nativeObj, dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::multiply(Mat src1, Scalar src2, Mat& dst, double scale = 1, int dtype = -1)
+    //
+
+    public static void multiply(Mat src1, Scalar src2, Mat dst, double scale, int dtype) {
+        multiply_3(src1.nativeObj, src2.val[0], src2.val[1], src2.val[2], src2.val[3], dst.nativeObj, scale, dtype);
+    }
+
+    public static void multiply(Mat src1, Scalar src2, Mat dst, double scale) {
+        multiply_4(src1.nativeObj, src2.val[0], src2.val[1], src2.val[2], src2.val[3], dst.nativeObj, scale);
+    }
+
+    public static void multiply(Mat src1, Scalar src2, Mat dst) {
+        multiply_5(src1.nativeObj, src2.val[0], src2.val[1], src2.val[2], src2.val[3], dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::normalize(Mat src, Mat& dst, double alpha = 1, double beta = 0, int norm_type = NORM_L2, int dtype = -1, Mat mask = Mat())
+    //
+
+    /**
+     * Normalizes the norm or value range of an array.
+     *
+     * The function cv::normalize normalizes scale and shift the input array elements so that
+     * \(\| \texttt{dst} \| _{L_p}= \texttt{alpha}\)
+     * (where p=Inf, 1 or 2) when normType=NORM_INF, NORM_L1, or NORM_L2, respectively; or so that
+     * \(\min _I  \texttt{dst} (I)= \texttt{alpha} , \, \, \max _I  \texttt{dst} (I)= \texttt{beta}\)
+     *
+     * when normType=NORM_MINMAX (for dense arrays only). The optional mask specifies a sub-array to be
+     * normalized. This means that the norm or min-n-max are calculated over the sub-array, and then this
+     * sub-array is modified to be normalized. If you want to only use the mask to calculate the norm or
+     * min-max but modify the whole array, you can use norm and Mat::convertTo.
+     *
+     * In case of sparse matrices, only the non-zero values are analyzed and transformed. Because of this,
+     * the range transformation for sparse matrices is not allowed since it can shift the zero level.
+     *
+     * Possible usage with some positive example data:
+     * <code>
+     *     vector&lt;double&gt; positiveData = { 2.0, 8.0, 10.0 };
+     *     vector&lt;double&gt; normalizedData_l1, normalizedData_l2, normalizedData_inf, normalizedData_minmax;
+     *
+     *     // Norm to probability (total count)
+     *     // sum(numbers) = 20.0
+     *     // 2.0      0.1     (2.0/20.0)
+     *     // 8.0      0.4     (8.0/20.0)
+     *     // 10.0     0.5     (10.0/20.0)
+     *     normalize(positiveData, normalizedData_l1, 1.0, 0.0, NORM_L1);
+     *
+     *     // Norm to unit vector: ||positiveData|| = 1.0
+     *     // 2.0      0.15
+     *     // 8.0      0.62
+     *     // 10.0     0.77
+     *     normalize(positiveData, normalizedData_l2, 1.0, 0.0, NORM_L2);
+     *
+     *     // Norm to max element
+     *     // 2.0      0.2     (2.0/10.0)
+     *     // 8.0      0.8     (8.0/10.0)
+     *     // 10.0     1.0     (10.0/10.0)
+     *     normalize(positiveData, normalizedData_inf, 1.0, 0.0, NORM_INF);
+     *
+     *     // Norm to range [0.0;1.0]
+     *     // 2.0      0.0     (shift to left border)
+     *     // 8.0      0.75    (6.0/8.0)
+     *     // 10.0     1.0     (shift to right border)
+     *     normalize(positiveData, normalizedData_minmax, 1.0, 0.0, NORM_MINMAX);
+     * </code>
+     *
+     * @param src input array.
+     * @param dst output array of the same size as src .
+     * @param alpha norm value to normalize to or the lower range boundary in case of the range
+     * normalization.
+     * @param beta upper range boundary in case of the range normalization; it is not used for the norm
+     * normalization.
+     * @param norm_type normalization type (see cv::NormTypes).
+     * @param dtype when negative, the output array has the same type as src; otherwise, it has the same
+     * number of channels as src and the depth =CV_MAT_DEPTH(dtype).
+     * @param mask optional operation mask.
+     * SEE: norm, Mat::convertTo, SparseMat::convertTo
+     */
+    public static void normalize(Mat src, Mat dst, double alpha, double beta, int norm_type, int dtype, Mat mask) {
+        normalize_0(src.nativeObj, dst.nativeObj, alpha, beta, norm_type, dtype, mask.nativeObj);
+    }
+
+    /**
+     * Normalizes the norm or value range of an array.
+     *
+     * The function cv::normalize normalizes scale and shift the input array elements so that
+     * \(\| \texttt{dst} \| _{L_p}= \texttt{alpha}\)
+     * (where p=Inf, 1 or 2) when normType=NORM_INF, NORM_L1, or NORM_L2, respectively; or so that
+     * \(\min _I  \texttt{dst} (I)= \texttt{alpha} , \, \, \max _I  \texttt{dst} (I)= \texttt{beta}\)
+     *
+     * when normType=NORM_MINMAX (for dense arrays only). The optional mask specifies a sub-array to be
+     * normalized. This means that the norm or min-n-max are calculated over the sub-array, and then this
+     * sub-array is modified to be normalized. If you want to only use the mask to calculate the norm or
+     * min-max but modify the whole array, you can use norm and Mat::convertTo.
+     *
+     * In case of sparse matrices, only the non-zero values are analyzed and transformed. Because of this,
+     * the range transformation for sparse matrices is not allowed since it can shift the zero level.
+     *
+     * Possible usage with some positive example data:
+     * <code>
+     *     vector&lt;double&gt; positiveData = { 2.0, 8.0, 10.0 };
+     *     vector&lt;double&gt; normalizedData_l1, normalizedData_l2, normalizedData_inf, normalizedData_minmax;
+     *
+     *     // Norm to probability (total count)
+     *     // sum(numbers) = 20.0
+     *     // 2.0      0.1     (2.0/20.0)
+     *     // 8.0      0.4     (8.0/20.0)
+     *     // 10.0     0.5     (10.0/20.0)
+     *     normalize(positiveData, normalizedData_l1, 1.0, 0.0, NORM_L1);
+     *
+     *     // Norm to unit vector: ||positiveData|| = 1.0
+     *     // 2.0      0.15
+     *     // 8.0      0.62
+     *     // 10.0     0.77
+     *     normalize(positiveData, normalizedData_l2, 1.0, 0.0, NORM_L2);
+     *
+     *     // Norm to max element
+     *     // 2.0      0.2     (2.0/10.0)
+     *     // 8.0      0.8     (8.0/10.0)
+     *     // 10.0     1.0     (10.0/10.0)
+     *     normalize(positiveData, normalizedData_inf, 1.0, 0.0, NORM_INF);
+     *
+     *     // Norm to range [0.0;1.0]
+     *     // 2.0      0.0     (shift to left border)
+     *     // 8.0      0.75    (6.0/8.0)
+     *     // 10.0     1.0     (shift to right border)
+     *     normalize(positiveData, normalizedData_minmax, 1.0, 0.0, NORM_MINMAX);
+     * </code>
+     *
+     * @param src input array.
+     * @param dst output array of the same size as src .
+     * @param alpha norm value to normalize to or the lower range boundary in case of the range
+     * normalization.
+     * @param beta upper range boundary in case of the range normalization; it is not used for the norm
+     * normalization.
+     * @param norm_type normalization type (see cv::NormTypes).
+     * @param dtype when negative, the output array has the same type as src; otherwise, it has the same
+     * number of channels as src and the depth =CV_MAT_DEPTH(dtype).
+     * SEE: norm, Mat::convertTo, SparseMat::convertTo
+     */
+    public static void normalize(Mat src, Mat dst, double alpha, double beta, int norm_type, int dtype) {
+        normalize_1(src.nativeObj, dst.nativeObj, alpha, beta, norm_type, dtype);
+    }
+
+    /**
+     * Normalizes the norm or value range of an array.
+     *
+     * The function cv::normalize normalizes scale and shift the input array elements so that
+     * \(\| \texttt{dst} \| _{L_p}= \texttt{alpha}\)
+     * (where p=Inf, 1 or 2) when normType=NORM_INF, NORM_L1, or NORM_L2, respectively; or so that
+     * \(\min _I  \texttt{dst} (I)= \texttt{alpha} , \, \, \max _I  \texttt{dst} (I)= \texttt{beta}\)
+     *
+     * when normType=NORM_MINMAX (for dense arrays only). The optional mask specifies a sub-array to be
+     * normalized. This means that the norm or min-n-max are calculated over the sub-array, and then this
+     * sub-array is modified to be normalized. If you want to only use the mask to calculate the norm or
+     * min-max but modify the whole array, you can use norm and Mat::convertTo.
+     *
+     * In case of sparse matrices, only the non-zero values are analyzed and transformed. Because of this,
+     * the range transformation for sparse matrices is not allowed since it can shift the zero level.
+     *
+     * Possible usage with some positive example data:
+     * <code>
+     *     vector&lt;double&gt; positiveData = { 2.0, 8.0, 10.0 };
+     *     vector&lt;double&gt; normalizedData_l1, normalizedData_l2, normalizedData_inf, normalizedData_minmax;
+     *
+     *     // Norm to probability (total count)
+     *     // sum(numbers) = 20.0
+     *     // 2.0      0.1     (2.0/20.0)
+     *     // 8.0      0.4     (8.0/20.0)
+     *     // 10.0     0.5     (10.0/20.0)
+     *     normalize(positiveData, normalizedData_l1, 1.0, 0.0, NORM_L1);
+     *
+     *     // Norm to unit vector: ||positiveData|| = 1.0
+     *     // 2.0      0.15
+     *     // 8.0      0.62
+     *     // 10.0     0.77
+     *     normalize(positiveData, normalizedData_l2, 1.0, 0.0, NORM_L2);
+     *
+     *     // Norm to max element
+     *     // 2.0      0.2     (2.0/10.0)
+     *     // 8.0      0.8     (8.0/10.0)
+     *     // 10.0     1.0     (10.0/10.0)
+     *     normalize(positiveData, normalizedData_inf, 1.0, 0.0, NORM_INF);
+     *
+     *     // Norm to range [0.0;1.0]
+     *     // 2.0      0.0     (shift to left border)
+     *     // 8.0      0.75    (6.0/8.0)
+     *     // 10.0     1.0     (shift to right border)
+     *     normalize(positiveData, normalizedData_minmax, 1.0, 0.0, NORM_MINMAX);
+     * </code>
+     *
+     * @param src input array.
+     * @param dst output array of the same size as src .
+     * @param alpha norm value to normalize to or the lower range boundary in case of the range
+     * normalization.
+     * @param beta upper range boundary in case of the range normalization; it is not used for the norm
+     * normalization.
+     * @param norm_type normalization type (see cv::NormTypes).
+     * number of channels as src and the depth =CV_MAT_DEPTH(dtype).
+     * SEE: norm, Mat::convertTo, SparseMat::convertTo
+     */
+    public static void normalize(Mat src, Mat dst, double alpha, double beta, int norm_type) {
+        normalize_2(src.nativeObj, dst.nativeObj, alpha, beta, norm_type);
+    }
+
+    /**
+     * Normalizes the norm or value range of an array.
+     *
+     * The function cv::normalize normalizes scale and shift the input array elements so that
+     * \(\| \texttt{dst} \| _{L_p}= \texttt{alpha}\)
+     * (where p=Inf, 1 or 2) when normType=NORM_INF, NORM_L1, or NORM_L2, respectively; or so that
+     * \(\min _I  \texttt{dst} (I)= \texttt{alpha} , \, \, \max _I  \texttt{dst} (I)= \texttt{beta}\)
+     *
+     * when normType=NORM_MINMAX (for dense arrays only). The optional mask specifies a sub-array to be
+     * normalized. This means that the norm or min-n-max are calculated over the sub-array, and then this
+     * sub-array is modified to be normalized. If you want to only use the mask to calculate the norm or
+     * min-max but modify the whole array, you can use norm and Mat::convertTo.
+     *
+     * In case of sparse matrices, only the non-zero values are analyzed and transformed. Because of this,
+     * the range transformation for sparse matrices is not allowed since it can shift the zero level.
+     *
+     * Possible usage with some positive example data:
+     * <code>
+     *     vector&lt;double&gt; positiveData = { 2.0, 8.0, 10.0 };
+     *     vector&lt;double&gt; normalizedData_l1, normalizedData_l2, normalizedData_inf, normalizedData_minmax;
+     *
+     *     // Norm to probability (total count)
+     *     // sum(numbers) = 20.0
+     *     // 2.0      0.1     (2.0/20.0)
+     *     // 8.0      0.4     (8.0/20.0)
+     *     // 10.0     0.5     (10.0/20.0)
+     *     normalize(positiveData, normalizedData_l1, 1.0, 0.0, NORM_L1);
+     *
+     *     // Norm to unit vector: ||positiveData|| = 1.0
+     *     // 2.0      0.15
+     *     // 8.0      0.62
+     *     // 10.0     0.77
+     *     normalize(positiveData, normalizedData_l2, 1.0, 0.0, NORM_L2);
+     *
+     *     // Norm to max element
+     *     // 2.0      0.2     (2.0/10.0)
+     *     // 8.0      0.8     (8.0/10.0)
+     *     // 10.0     1.0     (10.0/10.0)
+     *     normalize(positiveData, normalizedData_inf, 1.0, 0.0, NORM_INF);
+     *
+     *     // Norm to range [0.0;1.0]
+     *     // 2.0      0.0     (shift to left border)
+     *     // 8.0      0.75    (6.0/8.0)
+     *     // 10.0     1.0     (shift to right border)
+     *     normalize(positiveData, normalizedData_minmax, 1.0, 0.0, NORM_MINMAX);
+     * </code>
+     *
+     * @param src input array.
+     * @param dst output array of the same size as src .
+     * @param alpha norm value to normalize to or the lower range boundary in case of the range
+     * normalization.
+     * @param beta upper range boundary in case of the range normalization; it is not used for the norm
+     * normalization.
+     * number of channels as src and the depth =CV_MAT_DEPTH(dtype).
+     * SEE: norm, Mat::convertTo, SparseMat::convertTo
+     */
+    public static void normalize(Mat src, Mat dst, double alpha, double beta) {
+        normalize_3(src.nativeObj, dst.nativeObj, alpha, beta);
+    }
+
+    /**
+     * Normalizes the norm or value range of an array.
+     *
+     * The function cv::normalize normalizes scale and shift the input array elements so that
+     * \(\| \texttt{dst} \| _{L_p}= \texttt{alpha}\)
+     * (where p=Inf, 1 or 2) when normType=NORM_INF, NORM_L1, or NORM_L2, respectively; or so that
+     * \(\min _I  \texttt{dst} (I)= \texttt{alpha} , \, \, \max _I  \texttt{dst} (I)= \texttt{beta}\)
+     *
+     * when normType=NORM_MINMAX (for dense arrays only). The optional mask specifies a sub-array to be
+     * normalized. This means that the norm or min-n-max are calculated over the sub-array, and then this
+     * sub-array is modified to be normalized. If you want to only use the mask to calculate the norm or
+     * min-max but modify the whole array, you can use norm and Mat::convertTo.
+     *
+     * In case of sparse matrices, only the non-zero values are analyzed and transformed. Because of this,
+     * the range transformation for sparse matrices is not allowed since it can shift the zero level.
+     *
+     * Possible usage with some positive example data:
+     * <code>
+     *     vector&lt;double&gt; positiveData = { 2.0, 8.0, 10.0 };
+     *     vector&lt;double&gt; normalizedData_l1, normalizedData_l2, normalizedData_inf, normalizedData_minmax;
+     *
+     *     // Norm to probability (total count)
+     *     // sum(numbers) = 20.0
+     *     // 2.0      0.1     (2.0/20.0)
+     *     // 8.0      0.4     (8.0/20.0)
+     *     // 10.0     0.5     (10.0/20.0)
+     *     normalize(positiveData, normalizedData_l1, 1.0, 0.0, NORM_L1);
+     *
+     *     // Norm to unit vector: ||positiveData|| = 1.0
+     *     // 2.0      0.15
+     *     // 8.0      0.62
+     *     // 10.0     0.77
+     *     normalize(positiveData, normalizedData_l2, 1.0, 0.0, NORM_L2);
+     *
+     *     // Norm to max element
+     *     // 2.0      0.2     (2.0/10.0)
+     *     // 8.0      0.8     (8.0/10.0)
+     *     // 10.0     1.0     (10.0/10.0)
+     *     normalize(positiveData, normalizedData_inf, 1.0, 0.0, NORM_INF);
+     *
+     *     // Norm to range [0.0;1.0]
+     *     // 2.0      0.0     (shift to left border)
+     *     // 8.0      0.75    (6.0/8.0)
+     *     // 10.0     1.0     (shift to right border)
+     *     normalize(positiveData, normalizedData_minmax, 1.0, 0.0, NORM_MINMAX);
+     * </code>
+     *
+     * @param src input array.
+     * @param dst output array of the same size as src .
+     * @param alpha norm value to normalize to or the lower range boundary in case of the range
+     * normalization.
+     * normalization.
+     * number of channels as src and the depth =CV_MAT_DEPTH(dtype).
+     * SEE: norm, Mat::convertTo, SparseMat::convertTo
+     */
+    public static void normalize(Mat src, Mat dst, double alpha) {
+        normalize_4(src.nativeObj, dst.nativeObj, alpha);
+    }
+
+    /**
+     * Normalizes the norm or value range of an array.
+     *
+     * The function cv::normalize normalizes scale and shift the input array elements so that
+     * \(\| \texttt{dst} \| _{L_p}= \texttt{alpha}\)
+     * (where p=Inf, 1 or 2) when normType=NORM_INF, NORM_L1, or NORM_L2, respectively; or so that
+     * \(\min _I  \texttt{dst} (I)= \texttt{alpha} , \, \, \max _I  \texttt{dst} (I)= \texttt{beta}\)
+     *
+     * when normType=NORM_MINMAX (for dense arrays only). The optional mask specifies a sub-array to be
+     * normalized. This means that the norm or min-n-max are calculated over the sub-array, and then this
+     * sub-array is modified to be normalized. If you want to only use the mask to calculate the norm or
+     * min-max but modify the whole array, you can use norm and Mat::convertTo.
+     *
+     * In case of sparse matrices, only the non-zero values are analyzed and transformed. Because of this,
+     * the range transformation for sparse matrices is not allowed since it can shift the zero level.
+     *
+     * Possible usage with some positive example data:
+     * <code>
+     *     vector&lt;double&gt; positiveData = { 2.0, 8.0, 10.0 };
+     *     vector&lt;double&gt; normalizedData_l1, normalizedData_l2, normalizedData_inf, normalizedData_minmax;
+     *
+     *     // Norm to probability (total count)
+     *     // sum(numbers) = 20.0
+     *     // 2.0      0.1     (2.0/20.0)
+     *     // 8.0      0.4     (8.0/20.0)
+     *     // 10.0     0.5     (10.0/20.0)
+     *     normalize(positiveData, normalizedData_l1, 1.0, 0.0, NORM_L1);
+     *
+     *     // Norm to unit vector: ||positiveData|| = 1.0
+     *     // 2.0      0.15
+     *     // 8.0      0.62
+     *     // 10.0     0.77
+     *     normalize(positiveData, normalizedData_l2, 1.0, 0.0, NORM_L2);
+     *
+     *     // Norm to max element
+     *     // 2.0      0.2     (2.0/10.0)
+     *     // 8.0      0.8     (8.0/10.0)
+     *     // 10.0     1.0     (10.0/10.0)
+     *     normalize(positiveData, normalizedData_inf, 1.0, 0.0, NORM_INF);
+     *
+     *     // Norm to range [0.0;1.0]
+     *     // 2.0      0.0     (shift to left border)
+     *     // 8.0      0.75    (6.0/8.0)
+     *     // 10.0     1.0     (shift to right border)
+     *     normalize(positiveData, normalizedData_minmax, 1.0, 0.0, NORM_MINMAX);
+     * </code>
+     *
+     * @param src input array.
+     * @param dst output array of the same size as src .
+     * normalization.
+     * normalization.
+     * number of channels as src and the depth =CV_MAT_DEPTH(dtype).
+     * SEE: norm, Mat::convertTo, SparseMat::convertTo
+     */
+    public static void normalize(Mat src, Mat dst) {
+        normalize_5(src.nativeObj, dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::patchNaNs(Mat& a, double val = 0)
+    //
+
+    /**
+     * converts NaN's to the given number
+     * @param a automatically generated
+     * @param val automatically generated
+     */
+    public static void patchNaNs(Mat a, double val) {
+        patchNaNs_0(a.nativeObj, val);
+    }
+
+    /**
+     * converts NaN's to the given number
+     * @param a automatically generated
+     */
+    public static void patchNaNs(Mat a) {
+        patchNaNs_1(a.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::perspectiveTransform(Mat src, Mat& dst, Mat m)
+    //
+
+    /**
+     * Performs the perspective matrix transformation of vectors.
+     *
+     * The function cv::perspectiveTransform transforms every element of src by
+     * treating it as a 2D or 3D vector, in the following way:
+     * \((x, y, z)  \rightarrow (x'/w, y'/w, z'/w)\)
+     * where
+     * \((x', y', z', w') =  \texttt{mat} \cdot \begin{bmatrix} x &amp; y &amp; z &amp; 1  \end{bmatrix}\)
+     * and
+     * \(w =  \fork{w'}{if \(w' \ne 0\)}{\infty}{otherwise}\)
+     *
+     * Here a 3D vector transformation is shown. In case of a 2D vector
+     * transformation, the z component is omitted.
+     *
+     * <b>Note:</b> The function transforms a sparse set of 2D or 3D vectors. If you
+     * want to transform an image using perspective transformation, use
+     * warpPerspective . If you have an inverse problem, that is, you want to
+     * compute the most probable perspective transformation out of several
+     * pairs of corresponding points, you can use getPerspectiveTransform or
+     * findHomography .
+     * @param src input two-channel or three-channel floating-point array; each
+     * element is a 2D/3D vector to be transformed.
+     * @param dst output array of the same size and type as src.
+     * @param m 3x3 or 4x4 floating-point transformation matrix.
+     * SEE:  transform, warpPerspective, getPerspectiveTransform, findHomography
+     */
+    public static void perspectiveTransform(Mat src, Mat dst, Mat m) {
+        perspectiveTransform_0(src.nativeObj, dst.nativeObj, m.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::phase(Mat x, Mat y, Mat& angle, bool angleInDegrees = false)
+    //
+
+    /**
+     * Calculates the rotation angle of 2D vectors.
+     *
+     * The function cv::phase calculates the rotation angle of each 2D vector that
+     * is formed from the corresponding elements of x and y :
+     * \(\texttt{angle} (I) =  \texttt{atan2} ( \texttt{y} (I), \texttt{x} (I))\)
+     *
+     * The angle estimation accuracy is about 0.3 degrees. When x(I)=y(I)=0 ,
+     * the corresponding angle(I) is set to 0.
+     * @param x input floating-point array of x-coordinates of 2D vectors.
+     * @param y input array of y-coordinates of 2D vectors; it must have the
+     * same size and the same type as x.
+     * @param angle output array of vector angles; it has the same size and
+     * same type as x .
+     * @param angleInDegrees when true, the function calculates the angle in
+     * degrees, otherwise, they are measured in radians.
+     */
+    public static void phase(Mat x, Mat y, Mat angle, boolean angleInDegrees) {
+        phase_0(x.nativeObj, y.nativeObj, angle.nativeObj, angleInDegrees);
+    }
+
+    /**
+     * Calculates the rotation angle of 2D vectors.
+     *
+     * The function cv::phase calculates the rotation angle of each 2D vector that
+     * is formed from the corresponding elements of x and y :
+     * \(\texttt{angle} (I) =  \texttt{atan2} ( \texttt{y} (I), \texttt{x} (I))\)
+     *
+     * The angle estimation accuracy is about 0.3 degrees. When x(I)=y(I)=0 ,
+     * the corresponding angle(I) is set to 0.
+     * @param x input floating-point array of x-coordinates of 2D vectors.
+     * @param y input array of y-coordinates of 2D vectors; it must have the
+     * same size and the same type as x.
+     * @param angle output array of vector angles; it has the same size and
+     * same type as x .
+     * degrees, otherwise, they are measured in radians.
+     */
+    public static void phase(Mat x, Mat y, Mat angle) {
+        phase_1(x.nativeObj, y.nativeObj, angle.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::polarToCart(Mat magnitude, Mat angle, Mat& x, Mat& y, bool angleInDegrees = false)
+    //
+
+    /**
+     * Calculates x and y coordinates of 2D vectors from their magnitude and angle.
+     *
+     * The function cv::polarToCart calculates the Cartesian coordinates of each 2D
+     * vector represented by the corresponding elements of magnitude and angle:
+     * \(\begin{array}{l} \texttt{x} (I) =  \texttt{magnitude} (I) \cos ( \texttt{angle} (I)) \\ \texttt{y} (I) =  \texttt{magnitude} (I) \sin ( \texttt{angle} (I)) \\ \end{array}\)
+     *
+     * The relative accuracy of the estimated coordinates is about 1e-6.
+     * @param magnitude input floating-point array of magnitudes of 2D vectors;
+     * it can be an empty matrix (=Mat()), in this case, the function assumes
+     * that all the magnitudes are =1; if it is not empty, it must have the
+     * same size and type as angle.
+     * @param angle input floating-point array of angles of 2D vectors.
+     * @param x output array of x-coordinates of 2D vectors; it has the same
+     * size and type as angle.
+     * @param y output array of y-coordinates of 2D vectors; it has the same
+     * size and type as angle.
+     * @param angleInDegrees when true, the input angles are measured in
+     * degrees, otherwise, they are measured in radians.
+     * SEE: cartToPolar, magnitude, phase, exp, log, pow, sqrt
+     */
+    public static void polarToCart(Mat magnitude, Mat angle, Mat x, Mat y, boolean angleInDegrees) {
+        polarToCart_0(magnitude.nativeObj, angle.nativeObj, x.nativeObj, y.nativeObj, angleInDegrees);
+    }
+
+    /**
+     * Calculates x and y coordinates of 2D vectors from their magnitude and angle.
+     *
+     * The function cv::polarToCart calculates the Cartesian coordinates of each 2D
+     * vector represented by the corresponding elements of magnitude and angle:
+     * \(\begin{array}{l} \texttt{x} (I) =  \texttt{magnitude} (I) \cos ( \texttt{angle} (I)) \\ \texttt{y} (I) =  \texttt{magnitude} (I) \sin ( \texttt{angle} (I)) \\ \end{array}\)
+     *
+     * The relative accuracy of the estimated coordinates is about 1e-6.
+     * @param magnitude input floating-point array of magnitudes of 2D vectors;
+     * it can be an empty matrix (=Mat()), in this case, the function assumes
+     * that all the magnitudes are =1; if it is not empty, it must have the
+     * same size and type as angle.
+     * @param angle input floating-point array of angles of 2D vectors.
+     * @param x output array of x-coordinates of 2D vectors; it has the same
+     * size and type as angle.
+     * @param y output array of y-coordinates of 2D vectors; it has the same
+     * size and type as angle.
+     * degrees, otherwise, they are measured in radians.
+     * SEE: cartToPolar, magnitude, phase, exp, log, pow, sqrt
+     */
+    public static void polarToCart(Mat magnitude, Mat angle, Mat x, Mat y) {
+        polarToCart_1(magnitude.nativeObj, angle.nativeObj, x.nativeObj, y.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::pow(Mat src, double power, Mat& dst)
+    //
+
+    /**
+     * Raises every array element to a power.
+     *
+     * The function cv::pow raises every element of the input array to power :
+     * \(\texttt{dst} (I) =  \fork{\texttt{src}(I)^{power}}{if \(\texttt{power}\) is integer}{|\texttt{src}(I)|^{power}}{otherwise}\)
+     *
+     * So, for a non-integer power exponent, the absolute values of input array
+     * elements are used. However, it is possible to get true values for
+     * negative values using some extra operations. In the example below,
+     * computing the 5th root of array src shows:
+     * <code>
+     *     Mat mask = src &lt; 0;
+     *     pow(src, 1./5, dst);
+     *     subtract(Scalar::all(0), dst, dst, mask);
+     * </code>
+     * For some values of power, such as integer values, 0.5 and -0.5,
+     * specialized faster algorithms are used.
+     *
+     * Special values (NaN, Inf) are not handled.
+     * @param src input array.
+     * @param power exponent of power.
+     * @param dst output array of the same size and type as src.
+     * SEE: sqrt, exp, log, cartToPolar, polarToCart
+     */
+    public static void pow(Mat src, double power, Mat dst) {
+        pow_0(src.nativeObj, power, dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::randShuffle(Mat& dst, double iterFactor = 1., RNG* rng = 0)
+    //
+
+    /**
+     * Shuffles the array elements randomly.
+     *
+     * The function cv::randShuffle shuffles the specified 1D array by randomly choosing pairs of elements and
+     * swapping them. The number of such swap operations will be dst.rows\*dst.cols\*iterFactor .
+     * @param dst input/output numerical 1D array.
+     * @param iterFactor scale factor that determines the number of random swap operations (see the details
+     * below).
+     * instead.
+     * SEE: RNG, sort
+     */
+    public static void randShuffle(Mat dst, double iterFactor) {
+        randShuffle_0(dst.nativeObj, iterFactor);
+    }
+
+    /**
+     * Shuffles the array elements randomly.
+     *
+     * The function cv::randShuffle shuffles the specified 1D array by randomly choosing pairs of elements and
+     * swapping them. The number of such swap operations will be dst.rows\*dst.cols\*iterFactor .
+     * @param dst input/output numerical 1D array.
+     * below).
+     * instead.
+     * SEE: RNG, sort
+     */
+    public static void randShuffle(Mat dst) {
+        randShuffle_2(dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::randn(Mat& dst, double mean, double stddev)
+    //
+
+    /**
+     * Fills the array with normally distributed random numbers.
+     *
+     * The function cv::randn fills the matrix dst with normally distributed random numbers with the specified
+     * mean vector and the standard deviation matrix. The generated random numbers are clipped to fit the
+     * value range of the output array data type.
+     * @param dst output array of random numbers; the array must be pre-allocated and have 1 to 4 channels.
+     * @param mean mean value (expectation) of the generated random numbers.
+     * @param stddev standard deviation of the generated random numbers; it can be either a vector (in
+     * which case a diagonal standard deviation matrix is assumed) or a square matrix.
+     * SEE: RNG, randu
+     */
+    public static void randn(Mat dst, double mean, double stddev) {
+        randn_0(dst.nativeObj, mean, stddev);
+    }
+
+
+    //
+    // C++:  void cv::randu(Mat& dst, double low, double high)
+    //
+
+    /**
+     * Generates a single uniformly-distributed random number or an array of random numbers.
+     *
+     * Non-template variant of the function fills the matrix dst with uniformly-distributed
+     * random numbers from the specified range:
+     * \(\texttt{low} _c  \leq \texttt{dst} (I)_c &lt;  \texttt{high} _c\)
+     * @param dst output array of random numbers; the array must be pre-allocated.
+     * @param low inclusive lower boundary of the generated random numbers.
+     * @param high exclusive upper boundary of the generated random numbers.
+     * SEE: RNG, randn, theRNG
+     */
+    public static void randu(Mat dst, double low, double high) {
+        randu_0(dst.nativeObj, low, high);
+    }
+
+
+    //
+    // C++:  void cv::reduce(Mat src, Mat& dst, int dim, int rtype, int dtype = -1)
+    //
+
+    /**
+     * Reduces a matrix to a vector.
+     *
+     * The function #reduce reduces the matrix to a vector by treating the matrix rows/columns as a set of
+     * 1D vectors and performing the specified operation on the vectors until a single row/column is
+     * obtained. For example, the function can be used to compute horizontal and vertical projections of a
+     * raster image. In case of #REDUCE_MAX and #REDUCE_MIN , the output image should have the same type as the source one.
+     * In case of #REDUCE_SUM and #REDUCE_AVG , the output may have a larger element bit-depth to preserve accuracy.
+     * And multi-channel arrays are also supported in these two reduction modes.
+     *
+     * The following code demonstrates its usage for a single channel matrix.
+     * SNIPPET: snippets/core_reduce.cpp example
+     *
+     * And the following code demonstrates its usage for a two-channel matrix.
+     * SNIPPET: snippets/core_reduce.cpp example2
+     *
+     * @param src input 2D matrix.
+     * @param dst output vector. Its size and type is defined by dim and dtype parameters.
+     * @param dim dimension index along which the matrix is reduced. 0 means that the matrix is reduced to
+     * a single row. 1 means that the matrix is reduced to a single column.
+     * @param rtype reduction operation that could be one of #ReduceTypes
+     * @param dtype when negative, the output vector will have the same type as the input matrix,
+     * otherwise, its type will be CV_MAKE_TYPE(CV_MAT_DEPTH(dtype), src.channels()).
+     * SEE: repeat
+     */
+    public static void reduce(Mat src, Mat dst, int dim, int rtype, int dtype) {
+        reduce_0(src.nativeObj, dst.nativeObj, dim, rtype, dtype);
+    }
+
+    /**
+     * Reduces a matrix to a vector.
+     *
+     * The function #reduce reduces the matrix to a vector by treating the matrix rows/columns as a set of
+     * 1D vectors and performing the specified operation on the vectors until a single row/column is
+     * obtained. For example, the function can be used to compute horizontal and vertical projections of a
+     * raster image. In case of #REDUCE_MAX and #REDUCE_MIN , the output image should have the same type as the source one.
+     * In case of #REDUCE_SUM and #REDUCE_AVG , the output may have a larger element bit-depth to preserve accuracy.
+     * And multi-channel arrays are also supported in these two reduction modes.
+     *
+     * The following code demonstrates its usage for a single channel matrix.
+     * SNIPPET: snippets/core_reduce.cpp example
+     *
+     * And the following code demonstrates its usage for a two-channel matrix.
+     * SNIPPET: snippets/core_reduce.cpp example2
+     *
+     * @param src input 2D matrix.
+     * @param dst output vector. Its size and type is defined by dim and dtype parameters.
+     * @param dim dimension index along which the matrix is reduced. 0 means that the matrix is reduced to
+     * a single row. 1 means that the matrix is reduced to a single column.
+     * @param rtype reduction operation that could be one of #ReduceTypes
+     * otherwise, its type will be CV_MAKE_TYPE(CV_MAT_DEPTH(dtype), src.channels()).
+     * SEE: repeat
+     */
+    public static void reduce(Mat src, Mat dst, int dim, int rtype) {
+        reduce_1(src.nativeObj, dst.nativeObj, dim, rtype);
+    }
+
+
+    //
+    // C++:  void cv::repeat(Mat src, int ny, int nx, Mat& dst)
+    //
+
+    /**
+     * Fills the output array with repeated copies of the input array.
+     *
+     * The function cv::repeat duplicates the input array one or more times along each of the two axes:
+     * \(\texttt{dst} _{ij}= \texttt{src} _{i\mod src.rows, \; j\mod src.cols }\)
+     * The second variant of the function is more convenient to use with REF: MatrixExpressions.
+     * @param src input array to replicate.
+     * @param ny Flag to specify how many times the {@code src} is repeated along the
+     * vertical axis.
+     * @param nx Flag to specify how many times the {@code src} is repeated along the
+     * horizontal axis.
+     * @param dst output array of the same type as {@code src}.
+     * SEE: cv::reduce
+     */
+    public static void repeat(Mat src, int ny, int nx, Mat dst) {
+        repeat_0(src.nativeObj, ny, nx, dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::rotate(Mat src, Mat& dst, int rotateCode)
+    //
+
+    /**
+     * Rotates a 2D array in multiples of 90 degrees.
+     * The function cv::rotate rotates the array in one of three different ways:
+     * Rotate by 90 degrees clockwise (rotateCode = ROTATE_90_CLOCKWISE).
+     * Rotate by 180 degrees clockwise (rotateCode = ROTATE_180).
+     * Rotate by 270 degrees clockwise (rotateCode = ROTATE_90_COUNTERCLOCKWISE).
+     * @param src input array.
+     * @param dst output array of the same type as src.  The size is the same with ROTATE_180,
+     * and the rows and cols are switched for ROTATE_90_CLOCKWISE and ROTATE_90_COUNTERCLOCKWISE.
+     * @param rotateCode an enum to specify how to rotate the array; see the enum #RotateFlags
+     * SEE: transpose , repeat , completeSymm, flip, RotateFlags
+     */
+    public static void rotate(Mat src, Mat dst, int rotateCode) {
+        rotate_0(src.nativeObj, dst.nativeObj, rotateCode);
+    }
+
+
+    //
+    // C++:  void cv::scaleAdd(Mat src1, double alpha, Mat src2, Mat& dst)
+    //
+
+    /**
+     * Calculates the sum of a scaled array and another array.
+     *
+     * The function scaleAdd is one of the classical primitive linear algebra operations, known as DAXPY
+     * or SAXPY in [BLAS](http://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms). It calculates
+     * the sum of a scaled array and another array:
+     * \(\texttt{dst} (I)= \texttt{scale} \cdot \texttt{src1} (I) +  \texttt{src2} (I)\)
+     * The function can also be emulated with a matrix expression, for example:
+     * <code>
+     *     Mat A(3, 3, CV_64F);
+     *     ...
+     *     A.row(0) = A.row(1)*2 + A.row(2);
+     * </code>
+     * @param src1 first input array.
+     * @param alpha scale factor for the first array.
+     * @param src2 second input array of the same size and type as src1.
+     * @param dst output array of the same size and type as src1.
+     * SEE: add, addWeighted, subtract, Mat::dot, Mat::convertTo
+     */
+    public static void scaleAdd(Mat src1, double alpha, Mat src2, Mat dst) {
+        scaleAdd_0(src1.nativeObj, alpha, src2.nativeObj, dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::setErrorVerbosity(bool verbose)
+    //
+
+    public static void setErrorVerbosity(boolean verbose) {
+        setErrorVerbosity_0(verbose);
+    }
+
+
+    //
+    // C++:  void cv::setIdentity(Mat& mtx, Scalar s = Scalar(1))
+    //
+
+    /**
+     * Initializes a scaled identity matrix.
+     *
+     * The function cv::setIdentity initializes a scaled identity matrix:
+     * \(\texttt{mtx} (i,j)= \fork{\texttt{value}}{ if \(i=j\)}{0}{otherwise}\)
+     *
+     * The function can also be emulated using the matrix initializers and the
+     * matrix expressions:
+     * <code>
+     *     Mat A = Mat::eye(4, 3, CV_32F)*5;
+     *     // A will be set to [[5, 0, 0], [0, 5, 0], [0, 0, 5], [0, 0, 0]]
+     * </code>
+     * @param mtx matrix to initialize (not necessarily square).
+     * @param s value to assign to diagonal elements.
+     * SEE: Mat::zeros, Mat::ones, Mat::setTo, Mat::operator=
+     */
+    public static void setIdentity(Mat mtx, Scalar s) {
+        setIdentity_0(mtx.nativeObj, s.val[0], s.val[1], s.val[2], s.val[3]);
+    }
+
+    /**
+     * Initializes a scaled identity matrix.
+     *
+     * The function cv::setIdentity initializes a scaled identity matrix:
+     * \(\texttt{mtx} (i,j)= \fork{\texttt{value}}{ if \(i=j\)}{0}{otherwise}\)
+     *
+     * The function can also be emulated using the matrix initializers and the
+     * matrix expressions:
+     * <code>
+     *     Mat A = Mat::eye(4, 3, CV_32F)*5;
+     *     // A will be set to [[5, 0, 0], [0, 5, 0], [0, 0, 5], [0, 0, 0]]
+     * </code>
+     * @param mtx matrix to initialize (not necessarily square).
+     * SEE: Mat::zeros, Mat::ones, Mat::setTo, Mat::operator=
+     */
+    public static void setIdentity(Mat mtx) {
+        setIdentity_1(mtx.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::setNumThreads(int nthreads)
+    //
+
+    /**
+     * OpenCV will try to set the number of threads for the next parallel region.
+     *
+     * If threads == 0, OpenCV will disable threading optimizations and run all it's functions
+     * sequentially. Passing threads &lt; 0 will reset threads number to system default. This function must
+     * be called outside of parallel region.
+     *
+     * OpenCV will try to run its functions with specified threads number, but some behaviour differs from
+     * framework:
+     * <ul>
+     *   <li>
+     *    {@code TBB} - User-defined parallel constructions will run with the same threads number, if
+     *     another is not specified. If later on user creates his own scheduler, OpenCV will use it.
+     *   </li>
+     *   <li>
+     *    {@code OpenMP} - No special defined behaviour.
+     *   </li>
+     *   <li>
+     *    {@code Concurrency} - If threads == 1, OpenCV will disable threading optimizations and run its
+     *     functions sequentially.
+     *   </li>
+     *   <li>
+     *    {@code GCD} - Supports only values &lt;= 0.
+     *   </li>
+     *   <li>
+     *    {@code C=} - No special defined behaviour.
+     * @param nthreads Number of threads used by OpenCV.
+     * SEE: getNumThreads, getThreadNum
+     *   </li>
+     * </ul>
+     */
+    public static void setNumThreads(int nthreads) {
+        setNumThreads_0(nthreads);
+    }
+
+
+    //
+    // C++:  void cv::setRNGSeed(int seed)
+    //
+
+    /**
+     * Sets state of default random number generator.
+     *
+     * The function cv::setRNGSeed sets state of default random number generator to custom value.
+     * @param seed new state for default random number generator
+     * SEE: RNG, randu, randn
+     */
+    public static void setRNGSeed(int seed) {
+        setRNGSeed_0(seed);
+    }
+
+
+    //
+    // C++:  void cv::sort(Mat src, Mat& dst, int flags)
+    //
+
+    /**
+     * Sorts each row or each column of a matrix.
+     *
+     * The function cv::sort sorts each matrix row or each matrix column in
+     * ascending or descending order. So you should pass two operation flags to
+     * get desired behaviour. If you want to sort matrix rows or columns
+     * lexicographically, you can use STL std::sort generic function with the
+     * proper comparison predicate.
+     *
+     * @param src input single-channel array.
+     * @param dst output array of the same size and type as src.
+     * @param flags operation flags, a combination of #SortFlags
+     * SEE: sortIdx, randShuffle
+     */
+    public static void sort(Mat src, Mat dst, int flags) {
+        sort_0(src.nativeObj, dst.nativeObj, flags);
+    }
+
+
+    //
+    // C++:  void cv::sortIdx(Mat src, Mat& dst, int flags)
+    //
+
+    /**
+     * Sorts each row or each column of a matrix.
+     *
+     * The function cv::sortIdx sorts each matrix row or each matrix column in the
+     * ascending or descending order. So you should pass two operation flags to
+     * get desired behaviour. Instead of reordering the elements themselves, it
+     * stores the indices of sorted elements in the output array. For example:
+     * <code>
+     *     Mat A = Mat::eye(3,3,CV_32F), B;
+     *     sortIdx(A, B, SORT_EVERY_ROW + SORT_ASCENDING);
+     *     // B will probably contain
+     *     // (because of equal elements in A some permutations are possible):
+     *     // [[1, 2, 0], [0, 2, 1], [0, 1, 2]]
+     * </code>
+     * @param src input single-channel array.
+     * @param dst output integer array of the same size as src.
+     * @param flags operation flags that could be a combination of cv::SortFlags
+     * SEE: sort, randShuffle
+     */
+    public static void sortIdx(Mat src, Mat dst, int flags) {
+        sortIdx_0(src.nativeObj, dst.nativeObj, flags);
+    }
+
+
+    //
+    // C++:  void cv::split(Mat m, vector_Mat& mv)
+    //
+
+    /**
+     *
+     * @param m input multi-channel array.
+     * @param mv output vector of arrays; the arrays themselves are reallocated, if needed.
+     */
+    public static void split(Mat m, List<Mat> mv) {
+        Mat mv_mat = new Mat();
+        split_0(m.nativeObj, mv_mat.nativeObj);
+        Converters.Mat_to_vector_Mat(mv_mat, mv);
+        mv_mat.release();
+    }
+
+
+    //
+    // C++:  void cv::sqrt(Mat src, Mat& dst)
+    //
+
+    /**
+     * Calculates a square root of array elements.
+     *
+     * The function cv::sqrt calculates a square root of each input array element.
+     * In case of multi-channel arrays, each channel is processed
+     * independently. The accuracy is approximately the same as of the built-in
+     * std::sqrt .
+     * @param src input floating-point array.
+     * @param dst output array of the same size and type as src.
+     */
+    public static void sqrt(Mat src, Mat dst) {
+        sqrt_0(src.nativeObj, dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::subtract(Mat src1, Mat src2, Mat& dst, Mat mask = Mat(), int dtype = -1)
+    //
+
+    /**
+     * Calculates the per-element difference between two arrays or array and a scalar.
+     *
+     * The function subtract calculates:
+     * <ul>
+     *   <li>
+     *  Difference between two arrays, when both input arrays have the same size and the same number of
+     * channels:
+     *     \(\texttt{dst}(I) =  \texttt{saturate} ( \texttt{src1}(I) -  \texttt{src2}(I)) \quad \texttt{if mask}(I) \ne0\)
+     *   </li>
+     *   <li>
+     *  Difference between an array and a scalar, when src2 is constructed from Scalar or has the same
+     * number of elements as {@code src1.channels()}:
+     *     \(\texttt{dst}(I) =  \texttt{saturate} ( \texttt{src1}(I) -  \texttt{src2} ) \quad \texttt{if mask}(I) \ne0\)
+     *   </li>
+     *   <li>
+     *  Difference between a scalar and an array, when src1 is constructed from Scalar or has the same
+     * number of elements as {@code src2.channels()}:
+     *     \(\texttt{dst}(I) =  \texttt{saturate} ( \texttt{src1} -  \texttt{src2}(I) ) \quad \texttt{if mask}(I) \ne0\)
+     *   </li>
+     *   <li>
+     *  The reverse difference between a scalar and an array in the case of {@code SubRS}:
+     *     \(\texttt{dst}(I) =  \texttt{saturate} ( \texttt{src2} -  \texttt{src1}(I) ) \quad \texttt{if mask}(I) \ne0\)
+     * where I is a multi-dimensional index of array elements. In case of multi-channel arrays, each
+     * channel is processed independently.
+     *   </li>
+     * </ul>
+     *
+     * The first function in the list above can be replaced with matrix expressions:
+     * <code>
+     *     dst = src1 - src2;
+     *     dst -= src1; // equivalent to subtract(dst, src1, dst);
+     * </code>
+     * The input arrays and the output array can all have the same or different depths. For example, you
+     * can subtract to 8-bit unsigned arrays and store the difference in a 16-bit signed array. Depth of
+     * the output array is determined by dtype parameter. In the second and third cases above, as well as
+     * in the first case, when src1.depth() == src2.depth(), dtype can be set to the default -1. In this
+     * case the output array will have the same depth as the input array, be it src1, src2 or both.
+     * <b>Note:</b> Saturation is not applied when the output array has the depth CV_32S. You may even get
+     * result of an incorrect sign in the case of overflow.
+     * @param src1 first input array or a scalar.
+     * @param src2 second input array or a scalar.
+     * @param dst output array of the same size and the same number of channels as the input array.
+     * @param mask optional operation mask; this is an 8-bit single channel array that specifies elements
+     * of the output array to be changed.
+     * @param dtype optional depth of the output array
+     * SEE:  add, addWeighted, scaleAdd, Mat::convertTo
+     */
+    public static void subtract(Mat src1, Mat src2, Mat dst, Mat mask, int dtype) {
+        subtract_0(src1.nativeObj, src2.nativeObj, dst.nativeObj, mask.nativeObj, dtype);
+    }
+
+    /**
+     * Calculates the per-element difference between two arrays or array and a scalar.
+     *
+     * The function subtract calculates:
+     * <ul>
+     *   <li>
+     *  Difference between two arrays, when both input arrays have the same size and the same number of
+     * channels:
+     *     \(\texttt{dst}(I) =  \texttt{saturate} ( \texttt{src1}(I) -  \texttt{src2}(I)) \quad \texttt{if mask}(I) \ne0\)
+     *   </li>
+     *   <li>
+     *  Difference between an array and a scalar, when src2 is constructed from Scalar or has the same
+     * number of elements as {@code src1.channels()}:
+     *     \(\texttt{dst}(I) =  \texttt{saturate} ( \texttt{src1}(I) -  \texttt{src2} ) \quad \texttt{if mask}(I) \ne0\)
+     *   </li>
+     *   <li>
+     *  Difference between a scalar and an array, when src1 is constructed from Scalar or has the same
+     * number of elements as {@code src2.channels()}:
+     *     \(\texttt{dst}(I) =  \texttt{saturate} ( \texttt{src1} -  \texttt{src2}(I) ) \quad \texttt{if mask}(I) \ne0\)
+     *   </li>
+     *   <li>
+     *  The reverse difference between a scalar and an array in the case of {@code SubRS}:
+     *     \(\texttt{dst}(I) =  \texttt{saturate} ( \texttt{src2} -  \texttt{src1}(I) ) \quad \texttt{if mask}(I) \ne0\)
+     * where I is a multi-dimensional index of array elements. In case of multi-channel arrays, each
+     * channel is processed independently.
+     *   </li>
+     * </ul>
+     *
+     * The first function in the list above can be replaced with matrix expressions:
+     * <code>
+     *     dst = src1 - src2;
+     *     dst -= src1; // equivalent to subtract(dst, src1, dst);
+     * </code>
+     * The input arrays and the output array can all have the same or different depths. For example, you
+     * can subtract to 8-bit unsigned arrays and store the difference in a 16-bit signed array. Depth of
+     * the output array is determined by dtype parameter. In the second and third cases above, as well as
+     * in the first case, when src1.depth() == src2.depth(), dtype can be set to the default -1. In this
+     * case the output array will have the same depth as the input array, be it src1, src2 or both.
+     * <b>Note:</b> Saturation is not applied when the output array has the depth CV_32S. You may even get
+     * result of an incorrect sign in the case of overflow.
+     * @param src1 first input array or a scalar.
+     * @param src2 second input array or a scalar.
+     * @param dst output array of the same size and the same number of channels as the input array.
+     * @param mask optional operation mask; this is an 8-bit single channel array that specifies elements
+     * of the output array to be changed.
+     * SEE:  add, addWeighted, scaleAdd, Mat::convertTo
+     */
+    public static void subtract(Mat src1, Mat src2, Mat dst, Mat mask) {
+        subtract_1(src1.nativeObj, src2.nativeObj, dst.nativeObj, mask.nativeObj);
+    }
+
+    /**
+     * Calculates the per-element difference between two arrays or array and a scalar.
+     *
+     * The function subtract calculates:
+     * <ul>
+     *   <li>
+     *  Difference between two arrays, when both input arrays have the same size and the same number of
+     * channels:
+     *     \(\texttt{dst}(I) =  \texttt{saturate} ( \texttt{src1}(I) -  \texttt{src2}(I)) \quad \texttt{if mask}(I) \ne0\)
+     *   </li>
+     *   <li>
+     *  Difference between an array and a scalar, when src2 is constructed from Scalar or has the same
+     * number of elements as {@code src1.channels()}:
+     *     \(\texttt{dst}(I) =  \texttt{saturate} ( \texttt{src1}(I) -  \texttt{src2} ) \quad \texttt{if mask}(I) \ne0\)
+     *   </li>
+     *   <li>
+     *  Difference between a scalar and an array, when src1 is constructed from Scalar or has the same
+     * number of elements as {@code src2.channels()}:
+     *     \(\texttt{dst}(I) =  \texttt{saturate} ( \texttt{src1} -  \texttt{src2}(I) ) \quad \texttt{if mask}(I) \ne0\)
+     *   </li>
+     *   <li>
+     *  The reverse difference between a scalar and an array in the case of {@code SubRS}:
+     *     \(\texttt{dst}(I) =  \texttt{saturate} ( \texttt{src2} -  \texttt{src1}(I) ) \quad \texttt{if mask}(I) \ne0\)
+     * where I is a multi-dimensional index of array elements. In case of multi-channel arrays, each
+     * channel is processed independently.
+     *   </li>
+     * </ul>
+     *
+     * The first function in the list above can be replaced with matrix expressions:
+     * <code>
+     *     dst = src1 - src2;
+     *     dst -= src1; // equivalent to subtract(dst, src1, dst);
+     * </code>
+     * The input arrays and the output array can all have the same or different depths. For example, you
+     * can subtract to 8-bit unsigned arrays and store the difference in a 16-bit signed array. Depth of
+     * the output array is determined by dtype parameter. In the second and third cases above, as well as
+     * in the first case, when src1.depth() == src2.depth(), dtype can be set to the default -1. In this
+     * case the output array will have the same depth as the input array, be it src1, src2 or both.
+     * <b>Note:</b> Saturation is not applied when the output array has the depth CV_32S. You may even get
+     * result of an incorrect sign in the case of overflow.
+     * @param src1 first input array or a scalar.
+     * @param src2 second input array or a scalar.
+     * @param dst output array of the same size and the same number of channels as the input array.
+     * of the output array to be changed.
+     * SEE:  add, addWeighted, scaleAdd, Mat::convertTo
+     */
+    public static void subtract(Mat src1, Mat src2, Mat dst) {
+        subtract_2(src1.nativeObj, src2.nativeObj, dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::subtract(Mat src1, Scalar src2, Mat& dst, Mat mask = Mat(), int dtype = -1)
+    //
+
+    public static void subtract(Mat src1, Scalar src2, Mat dst, Mat mask, int dtype) {
+        subtract_3(src1.nativeObj, src2.val[0], src2.val[1], src2.val[2], src2.val[3], dst.nativeObj, mask.nativeObj, dtype);
+    }
+
+    public static void subtract(Mat src1, Scalar src2, Mat dst, Mat mask) {
+        subtract_4(src1.nativeObj, src2.val[0], src2.val[1], src2.val[2], src2.val[3], dst.nativeObj, mask.nativeObj);
+    }
+
+    public static void subtract(Mat src1, Scalar src2, Mat dst) {
+        subtract_5(src1.nativeObj, src2.val[0], src2.val[1], src2.val[2], src2.val[3], dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::transform(Mat src, Mat& dst, Mat m)
+    //
+
+    /**
+     * Performs the matrix transformation of every array element.
+     *
+     * The function cv::transform performs the matrix transformation of every
+     * element of the array src and stores the results in dst :
+     * \(\texttt{dst} (I) =  \texttt{m} \cdot \texttt{src} (I)\)
+     * (when m.cols=src.channels() ), or
+     * \(\texttt{dst} (I) =  \texttt{m} \cdot [ \texttt{src} (I); 1]\)
+     * (when m.cols=src.channels()+1 )
+     *
+     * Every element of the N -channel array src is interpreted as N -element
+     * vector that is transformed using the M x N or M x (N+1) matrix m to
+     * M-element vector - the corresponding element of the output array dst .
+     *
+     * The function may be used for geometrical transformation of
+     * N -dimensional points, arbitrary linear color space transformation (such
+     * as various kinds of RGB to YUV transforms), shuffling the image
+     * channels, and so forth.
+     * @param src input array that must have as many channels (1 to 4) as
+     * m.cols or m.cols-1.
+     * @param dst output array of the same size and depth as src; it has as
+     * many channels as m.rows.
+     * @param m transformation 2x2 or 2x3 floating-point matrix.
+     * SEE: perspectiveTransform, getAffineTransform, estimateAffine2D, warpAffine, warpPerspective
+     */
+    public static void transform(Mat src, Mat dst, Mat m) {
+        transform_0(src.nativeObj, dst.nativeObj, m.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::transpose(Mat src, Mat& dst)
+    //
+
+    /**
+     * Transposes a matrix.
+     *
+     * The function cv::transpose transposes the matrix src :
+     * \(\texttt{dst} (i,j) =  \texttt{src} (j,i)\)
+     * <b>Note:</b> No complex conjugation is done in case of a complex matrix. It
+     * should be done separately if needed.
+     * @param src input array.
+     * @param dst output array of the same type as src.
+     */
+    public static void transpose(Mat src, Mat dst) {
+        transpose_0(src.nativeObj, dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::vconcat(vector_Mat src, Mat& dst)
+    //
+
+    /**
+     *
+     *  <code>
+     *     std::vector&lt;cv::Mat&gt; matrices = { cv::Mat(1, 4, CV_8UC1, cv::Scalar(1)),
+     *                                       cv::Mat(1, 4, CV_8UC1, cv::Scalar(2)),
+     *                                       cv::Mat(1, 4, CV_8UC1, cv::Scalar(3)),};
+     *
+     *     cv::Mat out;
+     *     cv::vconcat( matrices, out );
+     *     //out:
+     *     //[1,   1,   1,   1;
+     *     // 2,   2,   2,   2;
+     *     // 3,   3,   3,   3]
+     *  </code>
+     *  @param src input array or vector of matrices. all of the matrices must have the same number of cols and the same depth
+     *  @param dst output array. It has the same number of cols and depth as the src, and the sum of rows of the src.
+     * same depth.
+     */
+    public static void vconcat(List<Mat> src, Mat dst) {
+        Mat src_mat = Converters.vector_Mat_to_Mat(src);
+        vconcat_0(src_mat.nativeObj, dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::ipp::setUseIPP(bool flag)
+    //
+
+    public static void setUseIPP(boolean flag) {
+        setUseIPP_0(flag);
+    }
+
+
+    //
+    // C++:  void cv::ipp::setUseIPP_NE(bool flag)
+    //
+
+    public static void setUseIPP_NE(boolean flag) {
+        setUseIPP_NE_0(flag);
+    }
+
+
+    //
+    // C++:  void cv::ipp::setUseIPP_NotExact(bool flag)
+    //
+
+    public static void setUseIPP_NotExact(boolean flag) {
+        setUseIPP_NotExact_0(flag);
+    }
+
+
+    //
+    // C++:  void cv::samples::addSamplesDataSearchPath(String path)
+    //
+
+    /**
+     * Override search data path by adding new search location
+     *
+     * Use this only to override default behavior
+     * Passed paths are used in LIFO order.
+     *
+     * @param path Path to used samples data
+     */
+    public static void addSamplesDataSearchPath(String path) {
+        addSamplesDataSearchPath_0(path);
+    }
+
+
+    //
+    // C++:  void cv::samples::addSamplesDataSearchSubDirectory(String subdir)
+    //
+
+    /**
+     * Append samples search data sub directory
+     *
+     * General usage is to add OpenCV modules name ({@code &lt;opencv_contrib&gt;/modules/&lt;name&gt;/samples/data} -&gt; {@code &lt;name&gt;/samples/data} + {@code modules/&lt;name&gt;/samples/data}).
+     * Passed subdirectories are used in LIFO order.
+     *
+     * @param subdir samples data sub directory
+     */
+    public static void addSamplesDataSearchSubDirectory(String subdir) {
+        addSamplesDataSearchSubDirectory_0(subdir);
+    }
+
+// manual port
+public static class MinMaxLocResult {
+    public double minVal;
+    public double maxVal;
+    public Point minLoc;
+    public Point maxLoc;
+
+
+    public MinMaxLocResult() {
+        minVal=0; maxVal=0;
+        minLoc=new Point();
+        maxLoc=new Point();
+    }
+}
+
+
+// C++: minMaxLoc(Mat src, double* minVal, double* maxVal=0, Point* minLoc=0, Point* maxLoc=0, InputArray mask=noArray())
+
+
+//javadoc: minMaxLoc(src, mask)
+public static MinMaxLocResult minMaxLoc(Mat src, Mat mask) {
+    MinMaxLocResult res = new MinMaxLocResult();
+    long maskNativeObj=0;
+    if (mask != null) {
+        maskNativeObj=mask.nativeObj;
+    }
+    double resarr[] = n_minMaxLocManual(src.nativeObj, maskNativeObj);
+    res.minVal=resarr[0];
+    res.maxVal=resarr[1];
+    res.minLoc.x=resarr[2];
+    res.minLoc.y=resarr[3];
+    res.maxLoc.x=resarr[4];
+    res.maxLoc.y=resarr[5];
+    return res;
+}
+
+
+//javadoc: minMaxLoc(src)
+public static MinMaxLocResult minMaxLoc(Mat src) {
+    return minMaxLoc(src, null);
+}
+
+
+    // C++:  Scalar cv::mean(Mat src, Mat mask = Mat())
+    private static native double[] mean_0(long src_nativeObj, long mask_nativeObj);
+    private static native double[] mean_1(long src_nativeObj);
+
+    // C++:  Scalar cv::sum(Mat src)
+    private static native double[] sumElems_0(long src_nativeObj);
+
+    // C++:  Scalar cv::trace(Mat mtx)
+    private static native double[] trace_0(long mtx_nativeObj);
+
+    // C++:  String cv::getBuildInformation()
+    private static native String getBuildInformation_0();
+
+    // C++:  String cv::getHardwareFeatureName(int feature)
+    private static native String getHardwareFeatureName_0(int feature);
+
+    // C++:  String cv::getVersionString()
+    private static native String getVersionString_0();
+
+    // C++:  String cv::ipp::getIppVersion()
+    private static native String getIppVersion_0();
+
+    // C++:  String cv::samples::findFile(String relative_path, bool required = true, bool silentMode = false)
+    private static native String findFile_0(String relative_path, boolean required, boolean silentMode);
+    private static native String findFile_1(String relative_path, boolean required);
+    private static native String findFile_2(String relative_path);
+
+    // C++:  String cv::samples::findFileOrKeep(String relative_path, bool silentMode = false)
+    private static native String findFileOrKeep_0(String relative_path, boolean silentMode);
+    private static native String findFileOrKeep_1(String relative_path);
+
+    // C++:  bool cv::checkRange(Mat a, bool quiet = true,  _hidden_ * pos = 0, double minVal = -DBL_MAX, double maxVal = DBL_MAX)
+    private static native boolean checkRange_0(long a_nativeObj, boolean quiet, double minVal, double maxVal);
+    private static native boolean checkRange_1(long a_nativeObj, boolean quiet, double minVal);
+    private static native boolean checkRange_2(long a_nativeObj, boolean quiet);
+    private static native boolean checkRange_4(long a_nativeObj);
+
+    // C++:  bool cv::eigen(Mat src, Mat& eigenvalues, Mat& eigenvectors = Mat())
+    private static native boolean eigen_0(long src_nativeObj, long eigenvalues_nativeObj, long eigenvectors_nativeObj);
+    private static native boolean eigen_1(long src_nativeObj, long eigenvalues_nativeObj);
+
+    // C++:  bool cv::solve(Mat src1, Mat src2, Mat& dst, int flags = DECOMP_LU)
+    private static native boolean solve_0(long src1_nativeObj, long src2_nativeObj, long dst_nativeObj, int flags);
+    private static native boolean solve_1(long src1_nativeObj, long src2_nativeObj, long dst_nativeObj);
+
+    // C++:  bool cv::ipp::useIPP()
+    private static native boolean useIPP_0();
+
+    // C++:  bool cv::ipp::useIPP_NE()
+    private static native boolean useIPP_NE_0();
+
+    // C++:  bool cv::ipp::useIPP_NotExact()
+    private static native boolean useIPP_NotExact_0();
+
+    // C++:  double cv::Mahalanobis(Mat v1, Mat v2, Mat icovar)
+    private static native double Mahalanobis_0(long v1_nativeObj, long v2_nativeObj, long icovar_nativeObj);
+
+    // C++:  double cv::PSNR(Mat src1, Mat src2)
+    private static native double PSNR_0(long src1_nativeObj, long src2_nativeObj);
+
+    // C++:  double cv::determinant(Mat mtx)
+    private static native double determinant_0(long mtx_nativeObj);
+
+    // C++:  double cv::getTickFrequency()
+    private static native double getTickFrequency_0();
+
+    // C++:  double cv::invert(Mat src, Mat& dst, int flags = DECOMP_LU)
+    private static native double invert_0(long src_nativeObj, long dst_nativeObj, int flags);
+    private static native double invert_1(long src_nativeObj, long dst_nativeObj);
+
+    // C++:  double cv::kmeans(Mat data, int K, Mat& bestLabels, TermCriteria criteria, int attempts, int flags, Mat& centers = Mat())
+    private static native double kmeans_0(long data_nativeObj, int K, long bestLabels_nativeObj, int criteria_type, int criteria_maxCount, double criteria_epsilon, int attempts, int flags, long centers_nativeObj);
+    private static native double kmeans_1(long data_nativeObj, int K, long bestLabels_nativeObj, int criteria_type, int criteria_maxCount, double criteria_epsilon, int attempts, int flags);
+
+    // C++:  double cv::norm(Mat src1, Mat src2, int normType = NORM_L2, Mat mask = Mat())
+    private static native double norm_0(long src1_nativeObj, long src2_nativeObj, int normType, long mask_nativeObj);
+    private static native double norm_1(long src1_nativeObj, long src2_nativeObj, int normType);
+    private static native double norm_2(long src1_nativeObj, long src2_nativeObj);
+
+    // C++:  double cv::norm(Mat src1, int normType = NORM_L2, Mat mask = Mat())
+    private static native double norm_3(long src1_nativeObj, int normType, long mask_nativeObj);
+    private static native double norm_4(long src1_nativeObj, int normType);
+    private static native double norm_5(long src1_nativeObj);
+
+    // C++:  double cv::solvePoly(Mat coeffs, Mat& roots, int maxIters = 300)
+    private static native double solvePoly_0(long coeffs_nativeObj, long roots_nativeObj, int maxIters);
+    private static native double solvePoly_1(long coeffs_nativeObj, long roots_nativeObj);
+
+    // C++:  float cv::cubeRoot(float val)
+    private static native float cubeRoot_0(float val);
+
+    // C++:  float cv::fastAtan2(float y, float x)
+    private static native float fastAtan2_0(float y, float x);
+
+    // C++:  int cv::borderInterpolate(int p, int len, int borderType)
+    private static native int borderInterpolate_0(int p, int len, int borderType);
+
+    // C++:  int cv::countNonZero(Mat src)
+    private static native int countNonZero_0(long src_nativeObj);
+
+    // C++:  int cv::getNumThreads()
+    private static native int getNumThreads_0();
+
+    // C++:  int cv::getNumberOfCPUs()
+    private static native int getNumberOfCPUs_0();
+
+    // C++:  int cv::getOptimalDFTSize(int vecsize)
+    private static native int getOptimalDFTSize_0(int vecsize);
+
+    // C++:  int cv::getThreadNum()
+    private static native int getThreadNum_0();
+
+    // C++:  int cv::getVersionMajor()
+    private static native int getVersionMajor_0();
+
+    // C++:  int cv::getVersionMinor()
+    private static native int getVersionMinor_0();
+
+    // C++:  int cv::getVersionRevision()
+    private static native int getVersionRevision_0();
+
+    // C++:  int cv::solveCubic(Mat coeffs, Mat& roots)
+    private static native int solveCubic_0(long coeffs_nativeObj, long roots_nativeObj);
+
+    // C++:  int64 cv::getCPUTickCount()
+    private static native long getCPUTickCount_0();
+
+    // C++:  int64 cv::getTickCount()
+    private static native long getTickCount_0();
+
+    // C++:  void cv::LUT(Mat src, Mat lut, Mat& dst)
+    private static native void LUT_0(long src_nativeObj, long lut_nativeObj, long dst_nativeObj);
+
+    // C++:  void cv::PCABackProject(Mat data, Mat mean, Mat eigenvectors, Mat& result)
+    private static native void PCABackProject_0(long data_nativeObj, long mean_nativeObj, long eigenvectors_nativeObj, long result_nativeObj);
+
+    // C++:  void cv::PCACompute(Mat data, Mat& mean, Mat& eigenvectors, Mat& eigenvalues, double retainedVariance)
+    private static native void PCACompute2_0(long data_nativeObj, long mean_nativeObj, long eigenvectors_nativeObj, long eigenvalues_nativeObj, double retainedVariance);
+
+    // C++:  void cv::PCACompute(Mat data, Mat& mean, Mat& eigenvectors, Mat& eigenvalues, int maxComponents = 0)
+    private static native void PCACompute2_1(long data_nativeObj, long mean_nativeObj, long eigenvectors_nativeObj, long eigenvalues_nativeObj, int maxComponents);
+    private static native void PCACompute2_2(long data_nativeObj, long mean_nativeObj, long eigenvectors_nativeObj, long eigenvalues_nativeObj);
+
+    // C++:  void cv::PCACompute(Mat data, Mat& mean, Mat& eigenvectors, double retainedVariance)
+    private static native void PCACompute_0(long data_nativeObj, long mean_nativeObj, long eigenvectors_nativeObj, double retainedVariance);
+
+    // C++:  void cv::PCACompute(Mat data, Mat& mean, Mat& eigenvectors, int maxComponents = 0)
+    private static native void PCACompute_1(long data_nativeObj, long mean_nativeObj, long eigenvectors_nativeObj, int maxComponents);
+    private static native void PCACompute_2(long data_nativeObj, long mean_nativeObj, long eigenvectors_nativeObj);
+
+    // C++:  void cv::PCAProject(Mat data, Mat mean, Mat eigenvectors, Mat& result)
+    private static native void PCAProject_0(long data_nativeObj, long mean_nativeObj, long eigenvectors_nativeObj, long result_nativeObj);
+
+    // C++:  void cv::SVBackSubst(Mat w, Mat u, Mat vt, Mat rhs, Mat& dst)
+    private static native void SVBackSubst_0(long w_nativeObj, long u_nativeObj, long vt_nativeObj, long rhs_nativeObj, long dst_nativeObj);
+
+    // C++:  void cv::SVDecomp(Mat src, Mat& w, Mat& u, Mat& vt, int flags = 0)
+    private static native void SVDecomp_0(long src_nativeObj, long w_nativeObj, long u_nativeObj, long vt_nativeObj, int flags);
+    private static native void SVDecomp_1(long src_nativeObj, long w_nativeObj, long u_nativeObj, long vt_nativeObj);
+
+    // C++:  void cv::absdiff(Mat src1, Mat src2, Mat& dst)
+    private static native void absdiff_0(long src1_nativeObj, long src2_nativeObj, long dst_nativeObj);
+
+    // C++:  void cv::absdiff(Mat src1, Scalar src2, Mat& dst)
+    private static native void absdiff_1(long src1_nativeObj, double src2_val0, double src2_val1, double src2_val2, double src2_val3, long dst_nativeObj);
+
+    // C++:  void cv::add(Mat src1, Mat src2, Mat& dst, Mat mask = Mat(), int dtype = -1)
+    private static native void add_0(long src1_nativeObj, long src2_nativeObj, long dst_nativeObj, long mask_nativeObj, int dtype);
+    private static native void add_1(long src1_nativeObj, long src2_nativeObj, long dst_nativeObj, long mask_nativeObj);
+    private static native void add_2(long src1_nativeObj, long src2_nativeObj, long dst_nativeObj);
+
+    // C++:  void cv::add(Mat src1, Scalar src2, Mat& dst, Mat mask = Mat(), int dtype = -1)
+    private static native void add_3(long src1_nativeObj, double src2_val0, double src2_val1, double src2_val2, double src2_val3, long dst_nativeObj, long mask_nativeObj, int dtype);
+    private static native void add_4(long src1_nativeObj, double src2_val0, double src2_val1, double src2_val2, double src2_val3, long dst_nativeObj, long mask_nativeObj);
+    private static native void add_5(long src1_nativeObj, double src2_val0, double src2_val1, double src2_val2, double src2_val3, long dst_nativeObj);
+
+    // C++:  void cv::addWeighted(Mat src1, double alpha, Mat src2, double beta, double gamma, Mat& dst, int dtype = -1)
+    private static native void addWeighted_0(long src1_nativeObj, double alpha, long src2_nativeObj, double beta, double gamma, long dst_nativeObj, int dtype);
+    private static native void addWeighted_1(long src1_nativeObj, double alpha, long src2_nativeObj, double beta, double gamma, long dst_nativeObj);
+
+    // C++:  void cv::batchDistance(Mat src1, Mat src2, Mat& dist, int dtype, Mat& nidx, int normType = NORM_L2, int K = 0, Mat mask = Mat(), int update = 0, bool crosscheck = false)
+    private static native void batchDistance_0(long src1_nativeObj, long src2_nativeObj, long dist_nativeObj, int dtype, long nidx_nativeObj, int normType, int K, long mask_nativeObj, int update, boolean crosscheck);
+    private static native void batchDistance_1(long src1_nativeObj, long src2_nativeObj, long dist_nativeObj, int dtype, long nidx_nativeObj, int normType, int K, long mask_nativeObj, int update);
+    private static native void batchDistance_2(long src1_nativeObj, long src2_nativeObj, long dist_nativeObj, int dtype, long nidx_nativeObj, int normType, int K, long mask_nativeObj);
+    private static native void batchDistance_3(long src1_nativeObj, long src2_nativeObj, long dist_nativeObj, int dtype, long nidx_nativeObj, int normType, int K);
+    private static native void batchDistance_4(long src1_nativeObj, long src2_nativeObj, long dist_nativeObj, int dtype, long nidx_nativeObj, int normType);
+    private static native void batchDistance_5(long src1_nativeObj, long src2_nativeObj, long dist_nativeObj, int dtype, long nidx_nativeObj);
+
+    // C++:  void cv::bitwise_and(Mat src1, Mat src2, Mat& dst, Mat mask = Mat())
+    private static native void bitwise_and_0(long src1_nativeObj, long src2_nativeObj, long dst_nativeObj, long mask_nativeObj);
+    private static native void bitwise_and_1(long src1_nativeObj, long src2_nativeObj, long dst_nativeObj);
+
+    // C++:  void cv::bitwise_not(Mat src, Mat& dst, Mat mask = Mat())
+    private static native void bitwise_not_0(long src_nativeObj, long dst_nativeObj, long mask_nativeObj);
+    private static native void bitwise_not_1(long src_nativeObj, long dst_nativeObj);
+
+    // C++:  void cv::bitwise_or(Mat src1, Mat src2, Mat& dst, Mat mask = Mat())
+    private static native void bitwise_or_0(long src1_nativeObj, long src2_nativeObj, long dst_nativeObj, long mask_nativeObj);
+    private static native void bitwise_or_1(long src1_nativeObj, long src2_nativeObj, long dst_nativeObj);
+
+    // C++:  void cv::bitwise_xor(Mat src1, Mat src2, Mat& dst, Mat mask = Mat())
+    private static native void bitwise_xor_0(long src1_nativeObj, long src2_nativeObj, long dst_nativeObj, long mask_nativeObj);
+    private static native void bitwise_xor_1(long src1_nativeObj, long src2_nativeObj, long dst_nativeObj);
+
+    // C++:  void cv::calcCovarMatrix(Mat samples, Mat& covar, Mat& mean, int flags, int ctype = CV_64F)
+    private static native void calcCovarMatrix_0(long samples_nativeObj, long covar_nativeObj, long mean_nativeObj, int flags, int ctype);
+    private static native void calcCovarMatrix_1(long samples_nativeObj, long covar_nativeObj, long mean_nativeObj, int flags);
+
+    // C++:  void cv::cartToPolar(Mat x, Mat y, Mat& magnitude, Mat& angle, bool angleInDegrees = false)
+    private static native void cartToPolar_0(long x_nativeObj, long y_nativeObj, long magnitude_nativeObj, long angle_nativeObj, boolean angleInDegrees);
+    private static native void cartToPolar_1(long x_nativeObj, long y_nativeObj, long magnitude_nativeObj, long angle_nativeObj);
+
+    // C++:  void cv::compare(Mat src1, Mat src2, Mat& dst, int cmpop)
+    private static native void compare_0(long src1_nativeObj, long src2_nativeObj, long dst_nativeObj, int cmpop);
+
+    // C++:  void cv::compare(Mat src1, Scalar src2, Mat& dst, int cmpop)
+    private static native void compare_1(long src1_nativeObj, double src2_val0, double src2_val1, double src2_val2, double src2_val3, long dst_nativeObj, int cmpop);
+
+    // C++:  void cv::completeSymm(Mat& m, bool lowerToUpper = false)
+    private static native void completeSymm_0(long m_nativeObj, boolean lowerToUpper);
+    private static native void completeSymm_1(long m_nativeObj);
+
+    // C++:  void cv::convertFp16(Mat src, Mat& dst)
+    private static native void convertFp16_0(long src_nativeObj, long dst_nativeObj);
+
+    // C++:  void cv::convertScaleAbs(Mat src, Mat& dst, double alpha = 1, double beta = 0)
+    private static native void convertScaleAbs_0(long src_nativeObj, long dst_nativeObj, double alpha, double beta);
+    private static native void convertScaleAbs_1(long src_nativeObj, long dst_nativeObj, double alpha);
+    private static native void convertScaleAbs_2(long src_nativeObj, long dst_nativeObj);
+
+    // C++:  void cv::copyMakeBorder(Mat src, Mat& dst, int top, int bottom, int left, int right, int borderType, Scalar value = Scalar())
+    private static native void copyMakeBorder_0(long src_nativeObj, long dst_nativeObj, int top, int bottom, int left, int right, int borderType, double value_val0, double value_val1, double value_val2, double value_val3);
+    private static native void copyMakeBorder_1(long src_nativeObj, long dst_nativeObj, int top, int bottom, int left, int right, int borderType);
+
+    // C++:  void cv::dct(Mat src, Mat& dst, int flags = 0)
+    private static native void dct_0(long src_nativeObj, long dst_nativeObj, int flags);
+    private static native void dct_1(long src_nativeObj, long dst_nativeObj);
+
+    // C++:  void cv::dft(Mat src, Mat& dst, int flags = 0, int nonzeroRows = 0)
+    private static native void dft_0(long src_nativeObj, long dst_nativeObj, int flags, int nonzeroRows);
+    private static native void dft_1(long src_nativeObj, long dst_nativeObj, int flags);
+    private static native void dft_2(long src_nativeObj, long dst_nativeObj);
+
+    // C++:  void cv::divide(Mat src1, Mat src2, Mat& dst, double scale = 1, int dtype = -1)
+    private static native void divide_0(long src1_nativeObj, long src2_nativeObj, long dst_nativeObj, double scale, int dtype);
+    private static native void divide_1(long src1_nativeObj, long src2_nativeObj, long dst_nativeObj, double scale);
+    private static native void divide_2(long src1_nativeObj, long src2_nativeObj, long dst_nativeObj);
+
+    // C++:  void cv::divide(Mat src1, Scalar src2, Mat& dst, double scale = 1, int dtype = -1)
+    private static native void divide_3(long src1_nativeObj, double src2_val0, double src2_val1, double src2_val2, double src2_val3, long dst_nativeObj, double scale, int dtype);
+    private static native void divide_4(long src1_nativeObj, double src2_val0, double src2_val1, double src2_val2, double src2_val3, long dst_nativeObj, double scale);
+    private static native void divide_5(long src1_nativeObj, double src2_val0, double src2_val1, double src2_val2, double src2_val3, long dst_nativeObj);
+
+    // C++:  void cv::divide(double scale, Mat src2, Mat& dst, int dtype = -1)
+    private static native void divide_6(double scale, long src2_nativeObj, long dst_nativeObj, int dtype);
+    private static native void divide_7(double scale, long src2_nativeObj, long dst_nativeObj);
+
+    // C++:  void cv::eigenNonSymmetric(Mat src, Mat& eigenvalues, Mat& eigenvectors)
+    private static native void eigenNonSymmetric_0(long src_nativeObj, long eigenvalues_nativeObj, long eigenvectors_nativeObj);
+
+    // C++:  void cv::exp(Mat src, Mat& dst)
+    private static native void exp_0(long src_nativeObj, long dst_nativeObj);
+
+    // C++:  void cv::extractChannel(Mat src, Mat& dst, int coi)
+    private static native void extractChannel_0(long src_nativeObj, long dst_nativeObj, int coi);
+
+    // C++:  void cv::findNonZero(Mat src, Mat& idx)
+    private static native void findNonZero_0(long src_nativeObj, long idx_nativeObj);
+
+    // C++:  void cv::flip(Mat src, Mat& dst, int flipCode)
+    private static native void flip_0(long src_nativeObj, long dst_nativeObj, int flipCode);
+
+    // C++:  void cv::gemm(Mat src1, Mat src2, double alpha, Mat src3, double beta, Mat& dst, int flags = 0)
+    private static native void gemm_0(long src1_nativeObj, long src2_nativeObj, double alpha, long src3_nativeObj, double beta, long dst_nativeObj, int flags);
+    private static native void gemm_1(long src1_nativeObj, long src2_nativeObj, double alpha, long src3_nativeObj, double beta, long dst_nativeObj);
+
+    // C++:  void cv::hconcat(vector_Mat src, Mat& dst)
+    private static native void hconcat_0(long src_mat_nativeObj, long dst_nativeObj);
+
+    // C++:  void cv::idct(Mat src, Mat& dst, int flags = 0)
+    private static native void idct_0(long src_nativeObj, long dst_nativeObj, int flags);
+    private static native void idct_1(long src_nativeObj, long dst_nativeObj);
+
+    // C++:  void cv::idft(Mat src, Mat& dst, int flags = 0, int nonzeroRows = 0)
+    private static native void idft_0(long src_nativeObj, long dst_nativeObj, int flags, int nonzeroRows);
+    private static native void idft_1(long src_nativeObj, long dst_nativeObj, int flags);
+    private static native void idft_2(long src_nativeObj, long dst_nativeObj);
+
+    // C++:  void cv::inRange(Mat src, Scalar lowerb, Scalar upperb, Mat& dst)
+    private static native void inRange_0(long src_nativeObj, double lowerb_val0, double lowerb_val1, double lowerb_val2, double lowerb_val3, double upperb_val0, double upperb_val1, double upperb_val2, double upperb_val3, long dst_nativeObj);
+
+    // C++:  void cv::insertChannel(Mat src, Mat& dst, int coi)
+    private static native void insertChannel_0(long src_nativeObj, long dst_nativeObj, int coi);
+
+    // C++:  void cv::log(Mat src, Mat& dst)
+    private static native void log_0(long src_nativeObj, long dst_nativeObj);
+
+    // C++:  void cv::magnitude(Mat x, Mat y, Mat& magnitude)
+    private static native void magnitude_0(long x_nativeObj, long y_nativeObj, long magnitude_nativeObj);
+
+    // C++:  void cv::max(Mat src1, Mat src2, Mat& dst)
+    private static native void max_0(long src1_nativeObj, long src2_nativeObj, long dst_nativeObj);
+
+    // C++:  void cv::max(Mat src1, Scalar src2, Mat& dst)
+    private static native void max_1(long src1_nativeObj, double src2_val0, double src2_val1, double src2_val2, double src2_val3, long dst_nativeObj);
+
+    // C++:  void cv::meanStdDev(Mat src, vector_double& mean, vector_double& stddev, Mat mask = Mat())
+    private static native void meanStdDev_0(long src_nativeObj, long mean_mat_nativeObj, long stddev_mat_nativeObj, long mask_nativeObj);
+    private static native void meanStdDev_1(long src_nativeObj, long mean_mat_nativeObj, long stddev_mat_nativeObj);
+
+    // C++:  void cv::merge(vector_Mat mv, Mat& dst)
+    private static native void merge_0(long mv_mat_nativeObj, long dst_nativeObj);
+
+    // C++:  void cv::min(Mat src1, Mat src2, Mat& dst)
+    private static native void min_0(long src1_nativeObj, long src2_nativeObj, long dst_nativeObj);
+
+    // C++:  void cv::min(Mat src1, Scalar src2, Mat& dst)
+    private static native void min_1(long src1_nativeObj, double src2_val0, double src2_val1, double src2_val2, double src2_val3, long dst_nativeObj);
+
+    // C++:  void cv::mixChannels(vector_Mat src, vector_Mat dst, vector_int fromTo)
+    private static native void mixChannels_0(long src_mat_nativeObj, long dst_mat_nativeObj, long fromTo_mat_nativeObj);
+
+    // C++:  void cv::mulSpectrums(Mat a, Mat b, Mat& c, int flags, bool conjB = false)
+    private static native void mulSpectrums_0(long a_nativeObj, long b_nativeObj, long c_nativeObj, int flags, boolean conjB);
+    private static native void mulSpectrums_1(long a_nativeObj, long b_nativeObj, long c_nativeObj, int flags);
+
+    // C++:  void cv::mulTransposed(Mat src, Mat& dst, bool aTa, Mat delta = Mat(), double scale = 1, int dtype = -1)
+    private static native void mulTransposed_0(long src_nativeObj, long dst_nativeObj, boolean aTa, long delta_nativeObj, double scale, int dtype);
+    private static native void mulTransposed_1(long src_nativeObj, long dst_nativeObj, boolean aTa, long delta_nativeObj, double scale);
+    private static native void mulTransposed_2(long src_nativeObj, long dst_nativeObj, boolean aTa, long delta_nativeObj);
+    private static native void mulTransposed_3(long src_nativeObj, long dst_nativeObj, boolean aTa);
+
+    // C++:  void cv::multiply(Mat src1, Mat src2, Mat& dst, double scale = 1, int dtype = -1)
+    private static native void multiply_0(long src1_nativeObj, long src2_nativeObj, long dst_nativeObj, double scale, int dtype);
+    private static native void multiply_1(long src1_nativeObj, long src2_nativeObj, long dst_nativeObj, double scale);
+    private static native void multiply_2(long src1_nativeObj, long src2_nativeObj, long dst_nativeObj);
+
+    // C++:  void cv::multiply(Mat src1, Scalar src2, Mat& dst, double scale = 1, int dtype = -1)
+    private static native void multiply_3(long src1_nativeObj, double src2_val0, double src2_val1, double src2_val2, double src2_val3, long dst_nativeObj, double scale, int dtype);
+    private static native void multiply_4(long src1_nativeObj, double src2_val0, double src2_val1, double src2_val2, double src2_val3, long dst_nativeObj, double scale);
+    private static native void multiply_5(long src1_nativeObj, double src2_val0, double src2_val1, double src2_val2, double src2_val3, long dst_nativeObj);
+
+    // C++:  void cv::normalize(Mat src, Mat& dst, double alpha = 1, double beta = 0, int norm_type = NORM_L2, int dtype = -1, Mat mask = Mat())
+    private static native void normalize_0(long src_nativeObj, long dst_nativeObj, double alpha, double beta, int norm_type, int dtype, long mask_nativeObj);
+    private static native void normalize_1(long src_nativeObj, long dst_nativeObj, double alpha, double beta, int norm_type, int dtype);
+    private static native void normalize_2(long src_nativeObj, long dst_nativeObj, double alpha, double beta, int norm_type);
+    private static native void normalize_3(long src_nativeObj, long dst_nativeObj, double alpha, double beta);
+    private static native void normalize_4(long src_nativeObj, long dst_nativeObj, double alpha);
+    private static native void normalize_5(long src_nativeObj, long dst_nativeObj);
+
+    // C++:  void cv::patchNaNs(Mat& a, double val = 0)
+    private static native void patchNaNs_0(long a_nativeObj, double val);
+    private static native void patchNaNs_1(long a_nativeObj);
+
+    // C++:  void cv::perspectiveTransform(Mat src, Mat& dst, Mat m)
+    private static native void perspectiveTransform_0(long src_nativeObj, long dst_nativeObj, long m_nativeObj);
+
+    // C++:  void cv::phase(Mat x, Mat y, Mat& angle, bool angleInDegrees = false)
+    private static native void phase_0(long x_nativeObj, long y_nativeObj, long angle_nativeObj, boolean angleInDegrees);
+    private static native void phase_1(long x_nativeObj, long y_nativeObj, long angle_nativeObj);
+
+    // C++:  void cv::polarToCart(Mat magnitude, Mat angle, Mat& x, Mat& y, bool angleInDegrees = false)
+    private static native void polarToCart_0(long magnitude_nativeObj, long angle_nativeObj, long x_nativeObj, long y_nativeObj, boolean angleInDegrees);
+    private static native void polarToCart_1(long magnitude_nativeObj, long angle_nativeObj, long x_nativeObj, long y_nativeObj);
+
+    // C++:  void cv::pow(Mat src, double power, Mat& dst)
+    private static native void pow_0(long src_nativeObj, double power, long dst_nativeObj);
+
+    // C++:  void cv::randShuffle(Mat& dst, double iterFactor = 1., RNG* rng = 0)
+    private static native void randShuffle_0(long dst_nativeObj, double iterFactor);
+    private static native void randShuffle_2(long dst_nativeObj);
+
+    // C++:  void cv::randn(Mat& dst, double mean, double stddev)
+    private static native void randn_0(long dst_nativeObj, double mean, double stddev);
+
+    // C++:  void cv::randu(Mat& dst, double low, double high)
+    private static native void randu_0(long dst_nativeObj, double low, double high);
+
+    // C++:  void cv::reduce(Mat src, Mat& dst, int dim, int rtype, int dtype = -1)
+    private static native void reduce_0(long src_nativeObj, long dst_nativeObj, int dim, int rtype, int dtype);
+    private static native void reduce_1(long src_nativeObj, long dst_nativeObj, int dim, int rtype);
+
+    // C++:  void cv::repeat(Mat src, int ny, int nx, Mat& dst)
+    private static native void repeat_0(long src_nativeObj, int ny, int nx, long dst_nativeObj);
+
+    // C++:  void cv::rotate(Mat src, Mat& dst, int rotateCode)
+    private static native void rotate_0(long src_nativeObj, long dst_nativeObj, int rotateCode);
+
+    // C++:  void cv::scaleAdd(Mat src1, double alpha, Mat src2, Mat& dst)
+    private static native void scaleAdd_0(long src1_nativeObj, double alpha, long src2_nativeObj, long dst_nativeObj);
+
+    // C++:  void cv::setErrorVerbosity(bool verbose)
+    private static native void setErrorVerbosity_0(boolean verbose);
+
+    // C++:  void cv::setIdentity(Mat& mtx, Scalar s = Scalar(1))
+    private static native void setIdentity_0(long mtx_nativeObj, double s_val0, double s_val1, double s_val2, double s_val3);
+    private static native void setIdentity_1(long mtx_nativeObj);
+
+    // C++:  void cv::setNumThreads(int nthreads)
+    private static native void setNumThreads_0(int nthreads);
+
+    // C++:  void cv::setRNGSeed(int seed)
+    private static native void setRNGSeed_0(int seed);
+
+    // C++:  void cv::sort(Mat src, Mat& dst, int flags)
+    private static native void sort_0(long src_nativeObj, long dst_nativeObj, int flags);
+
+    // C++:  void cv::sortIdx(Mat src, Mat& dst, int flags)
+    private static native void sortIdx_0(long src_nativeObj, long dst_nativeObj, int flags);
+
+    // C++:  void cv::split(Mat m, vector_Mat& mv)
+    private static native void split_0(long m_nativeObj, long mv_mat_nativeObj);
+
+    // C++:  void cv::sqrt(Mat src, Mat& dst)
+    private static native void sqrt_0(long src_nativeObj, long dst_nativeObj);
+
+    // C++:  void cv::subtract(Mat src1, Mat src2, Mat& dst, Mat mask = Mat(), int dtype = -1)
+    private static native void subtract_0(long src1_nativeObj, long src2_nativeObj, long dst_nativeObj, long mask_nativeObj, int dtype);
+    private static native void subtract_1(long src1_nativeObj, long src2_nativeObj, long dst_nativeObj, long mask_nativeObj);
+    private static native void subtract_2(long src1_nativeObj, long src2_nativeObj, long dst_nativeObj);
+
+    // C++:  void cv::subtract(Mat src1, Scalar src2, Mat& dst, Mat mask = Mat(), int dtype = -1)
+    private static native void subtract_3(long src1_nativeObj, double src2_val0, double src2_val1, double src2_val2, double src2_val3, long dst_nativeObj, long mask_nativeObj, int dtype);
+    private static native void subtract_4(long src1_nativeObj, double src2_val0, double src2_val1, double src2_val2, double src2_val3, long dst_nativeObj, long mask_nativeObj);
+    private static native void subtract_5(long src1_nativeObj, double src2_val0, double src2_val1, double src2_val2, double src2_val3, long dst_nativeObj);
+
+    // C++:  void cv::transform(Mat src, Mat& dst, Mat m)
+    private static native void transform_0(long src_nativeObj, long dst_nativeObj, long m_nativeObj);
+
+    // C++:  void cv::transpose(Mat src, Mat& dst)
+    private static native void transpose_0(long src_nativeObj, long dst_nativeObj);
+
+    // C++:  void cv::vconcat(vector_Mat src, Mat& dst)
+    private static native void vconcat_0(long src_mat_nativeObj, long dst_nativeObj);
+
+    // C++:  void cv::ipp::setUseIPP(bool flag)
+    private static native void setUseIPP_0(boolean flag);
+
+    // C++:  void cv::ipp::setUseIPP_NE(bool flag)
+    private static native void setUseIPP_NE_0(boolean flag);
+
+    // C++:  void cv::ipp::setUseIPP_NotExact(bool flag)
+    private static native void setUseIPP_NotExact_0(boolean flag);
+
+    // C++:  void cv::samples::addSamplesDataSearchPath(String path)
+    private static native void addSamplesDataSearchPath_0(String path);
+
+    // C++:  void cv::samples::addSamplesDataSearchSubDirectory(String subdir)
+    private static native void addSamplesDataSearchSubDirectory_0(String subdir);
+private static native double[] n_minMaxLocManual(long src_nativeObj, long mask_nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/core/CvException.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/core/CvException.java	(date 1605830247281)
+++ openCVLibrary3411/src/main/java/org/opencv/core/CvException.java	(date 1605830247281)
@@ -0,0 +1,15 @@
+package org.opencv.core;
+
+public class CvException extends RuntimeException {
+
+    private static final long serialVersionUID = 1L;
+
+    public CvException(String msg) {
+        super(msg);
+    }
+
+    @Override
+    public String toString() {
+        return "CvException [" + super.toString() + "]";
+    }
+}
Index: openCVLibrary3411/src/main/java/org/opencv/core/CvType.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/core/CvType.java	(date 1605830247288)
+++ openCVLibrary3411/src/main/java/org/opencv/core/CvType.java	(date 1605830247288)
@@ -0,0 +1,136 @@
+package org.opencv.core;
+
+public final class CvType {
+
+    // type depth constants
+    public static final int
+            CV_8U = 0, CV_8S = 1,
+            CV_16U = 2, CV_16S = 3,
+            CV_32S = 4,
+            CV_32F = 5,
+            CV_64F = 6,
+            CV_USRTYPE1 = 7;
+
+    // predefined type constants
+    public static final int
+            CV_8UC1 = CV_8UC(1), CV_8UC2 = CV_8UC(2), CV_8UC3 = CV_8UC(3), CV_8UC4 = CV_8UC(4),
+            CV_8SC1 = CV_8SC(1), CV_8SC2 = CV_8SC(2), CV_8SC3 = CV_8SC(3), CV_8SC4 = CV_8SC(4),
+            CV_16UC1 = CV_16UC(1), CV_16UC2 = CV_16UC(2), CV_16UC3 = CV_16UC(3), CV_16UC4 = CV_16UC(4),
+            CV_16SC1 = CV_16SC(1), CV_16SC2 = CV_16SC(2), CV_16SC3 = CV_16SC(3), CV_16SC4 = CV_16SC(4),
+            CV_32SC1 = CV_32SC(1), CV_32SC2 = CV_32SC(2), CV_32SC3 = CV_32SC(3), CV_32SC4 = CV_32SC(4),
+            CV_32FC1 = CV_32FC(1), CV_32FC2 = CV_32FC(2), CV_32FC3 = CV_32FC(3), CV_32FC4 = CV_32FC(4),
+            CV_64FC1 = CV_64FC(1), CV_64FC2 = CV_64FC(2), CV_64FC3 = CV_64FC(3), CV_64FC4 = CV_64FC(4);
+
+    private static final int CV_CN_MAX = 512, CV_CN_SHIFT = 3, CV_DEPTH_MAX = (1 << CV_CN_SHIFT);
+
+    public static final int makeType(int depth, int channels) {
+        if (channels <= 0 || channels >= CV_CN_MAX) {
+            throw new UnsupportedOperationException(
+                    "Channels count should be 1.." + (CV_CN_MAX - 1));
+        }
+        if (depth < 0 || depth >= CV_DEPTH_MAX) {
+            throw new UnsupportedOperationException(
+                    "Data type depth should be 0.." + (CV_DEPTH_MAX - 1));
+        }
+        return (depth & (CV_DEPTH_MAX - 1)) + ((channels - 1) << CV_CN_SHIFT);
+    }
+
+    public static final int CV_8UC(int ch) {
+        return makeType(CV_8U, ch);
+    }
+
+    public static final int CV_8SC(int ch) {
+        return makeType(CV_8S, ch);
+    }
+
+    public static final int CV_16UC(int ch) {
+        return makeType(CV_16U, ch);
+    }
+
+    public static final int CV_16SC(int ch) {
+        return makeType(CV_16S, ch);
+    }
+
+    public static final int CV_32SC(int ch) {
+        return makeType(CV_32S, ch);
+    }
+
+    public static final int CV_32FC(int ch) {
+        return makeType(CV_32F, ch);
+    }
+
+    public static final int CV_64FC(int ch) {
+        return makeType(CV_64F, ch);
+    }
+
+    public static final int channels(int type) {
+        return (type >> CV_CN_SHIFT) + 1;
+    }
+
+    public static final int depth(int type) {
+        return type & (CV_DEPTH_MAX - 1);
+    }
+
+    public static final boolean isInteger(int type) {
+        return depth(type) < CV_32F;
+    }
+
+    public static final int ELEM_SIZE(int type) {
+        switch (depth(type)) {
+        case CV_8U:
+        case CV_8S:
+            return channels(type);
+        case CV_16U:
+        case CV_16S:
+            return 2 * channels(type);
+        case CV_32S:
+        case CV_32F:
+            return 4 * channels(type);
+        case CV_64F:
+            return 8 * channels(type);
+        default:
+            throw new UnsupportedOperationException(
+                    "Unsupported CvType value: " + type);
+        }
+    }
+
+    public static final String typeToString(int type) {
+        String s;
+        switch (depth(type)) {
+        case CV_8U:
+            s = "CV_8U";
+            break;
+        case CV_8S:
+            s = "CV_8S";
+            break;
+        case CV_16U:
+            s = "CV_16U";
+            break;
+        case CV_16S:
+            s = "CV_16S";
+            break;
+        case CV_32S:
+            s = "CV_32S";
+            break;
+        case CV_32F:
+            s = "CV_32F";
+            break;
+        case CV_64F:
+            s = "CV_64F";
+            break;
+        case CV_USRTYPE1:
+            s = "CV_USRTYPE1";
+            break;
+        default:
+            throw new UnsupportedOperationException(
+                    "Unsupported CvType value: " + type);
+        }
+
+        int ch = channels(type);
+        if (ch <= 4)
+            return s + "C" + ch;
+        else
+            return s + "C(" + ch + ")";
+    }
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/dnn/DictValue.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/dnn/DictValue.java	(date 1605830247472)
+++ openCVLibrary3411/src/main/java/org/opencv/dnn/DictValue.java	(date 1605830247472)
@@ -0,0 +1,156 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.dnn;
+
+
+
+// C++: class DictValue
+/**
+ * This struct stores the scalar value (or array) of one of the following type: double, cv::String or int64.
+ * TODO: Maybe int64 is useless because double type exactly stores at least 2^52 integers.
+ */
+public class DictValue {
+
+    protected final long nativeObj;
+    protected DictValue(long addr) { nativeObj = addr; }
+
+    public long getNativeObjAddr() { return nativeObj; }
+
+    // internal usage only
+    public static DictValue __fromPtr__(long addr) { return new DictValue(addr); }
+
+    //
+    // C++:   cv::dnn::DictValue::DictValue(String s)
+    //
+
+    public DictValue(String s) {
+        nativeObj = DictValue_0(s);
+    }
+
+
+    //
+    // C++:   cv::dnn::DictValue::DictValue(double p)
+    //
+
+    public DictValue(double p) {
+        nativeObj = DictValue_1(p);
+    }
+
+
+    //
+    // C++:   cv::dnn::DictValue::DictValue(int i)
+    //
+
+    public DictValue(int i) {
+        nativeObj = DictValue_2(i);
+    }
+
+
+    //
+    // C++:  String cv::dnn::DictValue::getStringValue(int idx = -1)
+    //
+
+    public String getStringValue(int idx) {
+        return getStringValue_0(nativeObj, idx);
+    }
+
+    public String getStringValue() {
+        return getStringValue_1(nativeObj);
+    }
+
+
+    //
+    // C++:  bool cv::dnn::DictValue::isInt()
+    //
+
+    public boolean isInt() {
+        return isInt_0(nativeObj);
+    }
+
+
+    //
+    // C++:  bool cv::dnn::DictValue::isReal()
+    //
+
+    public boolean isReal() {
+        return isReal_0(nativeObj);
+    }
+
+
+    //
+    // C++:  bool cv::dnn::DictValue::isString()
+    //
+
+    public boolean isString() {
+        return isString_0(nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::dnn::DictValue::getRealValue(int idx = -1)
+    //
+
+    public double getRealValue(int idx) {
+        return getRealValue_0(nativeObj, idx);
+    }
+
+    public double getRealValue() {
+        return getRealValue_1(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::dnn::DictValue::getIntValue(int idx = -1)
+    //
+
+    public int getIntValue(int idx) {
+        return getIntValue_0(nativeObj, idx);
+    }
+
+    public int getIntValue() {
+        return getIntValue_1(nativeObj);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++:   cv::dnn::DictValue::DictValue(String s)
+    private static native long DictValue_0(String s);
+
+    // C++:   cv::dnn::DictValue::DictValue(double p)
+    private static native long DictValue_1(double p);
+
+    // C++:   cv::dnn::DictValue::DictValue(int i)
+    private static native long DictValue_2(int i);
+
+    // C++:  String cv::dnn::DictValue::getStringValue(int idx = -1)
+    private static native String getStringValue_0(long nativeObj, int idx);
+    private static native String getStringValue_1(long nativeObj);
+
+    // C++:  bool cv::dnn::DictValue::isInt()
+    private static native boolean isInt_0(long nativeObj);
+
+    // C++:  bool cv::dnn::DictValue::isReal()
+    private static native boolean isReal_0(long nativeObj);
+
+    // C++:  bool cv::dnn::DictValue::isString()
+    private static native boolean isString_0(long nativeObj);
+
+    // C++:  double cv::dnn::DictValue::getRealValue(int idx = -1)
+    private static native double getRealValue_0(long nativeObj, int idx);
+    private static native double getRealValue_1(long nativeObj);
+
+    // C++:  int cv::dnn::DictValue::getIntValue(int idx = -1)
+    private static native int getIntValue_0(long nativeObj, int idx);
+    private static native int getIntValue_1(long nativeObj);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/dnn/Dnn.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/dnn/Dnn.java	(date 1605830247487)
+++ openCVLibrary3411/src/main/java/org/opencv/dnn/Dnn.java	(date 1605830247487)
@@ -0,0 +1,1137 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.dnn;
+
+import java.util.ArrayList;
+import java.util.List;
+import org.opencv.core.Mat;
+import org.opencv.core.MatOfByte;
+import org.opencv.core.MatOfFloat;
+import org.opencv.core.MatOfInt;
+import org.opencv.core.MatOfRect2d;
+import org.opencv.core.MatOfRotatedRect;
+import org.opencv.core.Scalar;
+import org.opencv.core.Size;
+import org.opencv.dnn.Net;
+import org.opencv.utils.Converters;
+
+// C++: class Dnn
+
+public class Dnn {
+
+    // C++: enum Backend
+    public static final int
+            DNN_BACKEND_DEFAULT = 0,
+            DNN_BACKEND_HALIDE = 0+1,
+            DNN_BACKEND_INFERENCE_ENGINE = 0+2,
+            DNN_BACKEND_OPENCV = 0+3;
+
+
+    // C++: enum Target
+    public static final int
+            DNN_TARGET_CPU = 0,
+            DNN_TARGET_OPENCL = 0+1,
+            DNN_TARGET_OPENCL_FP16 = 0+2,
+            DNN_TARGET_MYRIAD = 0+3,
+            DNN_TARGET_FPGA = 0+4;
+
+
+    //
+    // C++:  Mat cv::dnn::blobFromImage(Mat image, double scalefactor = 1.0, Size size = Size(), Scalar mean = Scalar(), bool swapRB = false, bool crop = false, int ddepth = CV_32F)
+    //
+
+    /**
+     * Creates 4-dimensional blob from image. Optionally resizes and crops {@code image} from center,
+     * subtract {@code mean} values, scales values by {@code scalefactor}, swap Blue and Red channels.
+     * @param image input image (with 1-, 3- or 4-channels).
+     * @param size spatial size for output image
+     * @param mean scalar with mean values which are subtracted from channels. Values are intended
+     * to be in (mean-R, mean-G, mean-B) order if {@code image} has BGR ordering and {@code swapRB} is true.
+     * @param scalefactor multiplier for {@code image} values.
+     * @param swapRB flag which indicates that swap first and last channels
+     * in 3-channel image is necessary.
+     * @param crop flag which indicates whether image will be cropped after resize or not
+     * @param ddepth Depth of output blob. Choose CV_32F or CV_8U.
+     * if {@code crop} is true, input image is resized so one side after resize is equal to corresponding
+     * dimension in {@code size} and another one is equal or larger. Then, crop from the center is performed.
+     * If {@code crop} is false, direct resize without cropping and preserving aspect ratio is performed.
+     * @return 4-dimensional Mat with NCHW dimensions order.
+     */
+    public static Mat blobFromImage(Mat image, double scalefactor, Size size, Scalar mean, boolean swapRB, boolean crop, int ddepth) {
+        return new Mat(blobFromImage_0(image.nativeObj, scalefactor, size.width, size.height, mean.val[0], mean.val[1], mean.val[2], mean.val[3], swapRB, crop, ddepth));
+    }
+
+    /**
+     * Creates 4-dimensional blob from image. Optionally resizes and crops {@code image} from center,
+     * subtract {@code mean} values, scales values by {@code scalefactor}, swap Blue and Red channels.
+     * @param image input image (with 1-, 3- or 4-channels).
+     * @param size spatial size for output image
+     * @param mean scalar with mean values which are subtracted from channels. Values are intended
+     * to be in (mean-R, mean-G, mean-B) order if {@code image} has BGR ordering and {@code swapRB} is true.
+     * @param scalefactor multiplier for {@code image} values.
+     * @param swapRB flag which indicates that swap first and last channels
+     * in 3-channel image is necessary.
+     * @param crop flag which indicates whether image will be cropped after resize or not
+     * if {@code crop} is true, input image is resized so one side after resize is equal to corresponding
+     * dimension in {@code size} and another one is equal or larger. Then, crop from the center is performed.
+     * If {@code crop} is false, direct resize without cropping and preserving aspect ratio is performed.
+     * @return 4-dimensional Mat with NCHW dimensions order.
+     */
+    public static Mat blobFromImage(Mat image, double scalefactor, Size size, Scalar mean, boolean swapRB, boolean crop) {
+        return new Mat(blobFromImage_1(image.nativeObj, scalefactor, size.width, size.height, mean.val[0], mean.val[1], mean.val[2], mean.val[3], swapRB, crop));
+    }
+
+    /**
+     * Creates 4-dimensional blob from image. Optionally resizes and crops {@code image} from center,
+     * subtract {@code mean} values, scales values by {@code scalefactor}, swap Blue and Red channels.
+     * @param image input image (with 1-, 3- or 4-channels).
+     * @param size spatial size for output image
+     * @param mean scalar with mean values which are subtracted from channels. Values are intended
+     * to be in (mean-R, mean-G, mean-B) order if {@code image} has BGR ordering and {@code swapRB} is true.
+     * @param scalefactor multiplier for {@code image} values.
+     * @param swapRB flag which indicates that swap first and last channels
+     * in 3-channel image is necessary.
+     * if {@code crop} is true, input image is resized so one side after resize is equal to corresponding
+     * dimension in {@code size} and another one is equal or larger. Then, crop from the center is performed.
+     * If {@code crop} is false, direct resize without cropping and preserving aspect ratio is performed.
+     * @return 4-dimensional Mat with NCHW dimensions order.
+     */
+    public static Mat blobFromImage(Mat image, double scalefactor, Size size, Scalar mean, boolean swapRB) {
+        return new Mat(blobFromImage_2(image.nativeObj, scalefactor, size.width, size.height, mean.val[0], mean.val[1], mean.val[2], mean.val[3], swapRB));
+    }
+
+    /**
+     * Creates 4-dimensional blob from image. Optionally resizes and crops {@code image} from center,
+     * subtract {@code mean} values, scales values by {@code scalefactor}, swap Blue and Red channels.
+     * @param image input image (with 1-, 3- or 4-channels).
+     * @param size spatial size for output image
+     * @param mean scalar with mean values which are subtracted from channels. Values are intended
+     * to be in (mean-R, mean-G, mean-B) order if {@code image} has BGR ordering and {@code swapRB} is true.
+     * @param scalefactor multiplier for {@code image} values.
+     * in 3-channel image is necessary.
+     * if {@code crop} is true, input image is resized so one side after resize is equal to corresponding
+     * dimension in {@code size} and another one is equal or larger. Then, crop from the center is performed.
+     * If {@code crop} is false, direct resize without cropping and preserving aspect ratio is performed.
+     * @return 4-dimensional Mat with NCHW dimensions order.
+     */
+    public static Mat blobFromImage(Mat image, double scalefactor, Size size, Scalar mean) {
+        return new Mat(blobFromImage_3(image.nativeObj, scalefactor, size.width, size.height, mean.val[0], mean.val[1], mean.val[2], mean.val[3]));
+    }
+
+    /**
+     * Creates 4-dimensional blob from image. Optionally resizes and crops {@code image} from center,
+     * subtract {@code mean} values, scales values by {@code scalefactor}, swap Blue and Red channels.
+     * @param image input image (with 1-, 3- or 4-channels).
+     * @param size spatial size for output image
+     * to be in (mean-R, mean-G, mean-B) order if {@code image} has BGR ordering and {@code swapRB} is true.
+     * @param scalefactor multiplier for {@code image} values.
+     * in 3-channel image is necessary.
+     * if {@code crop} is true, input image is resized so one side after resize is equal to corresponding
+     * dimension in {@code size} and another one is equal or larger. Then, crop from the center is performed.
+     * If {@code crop} is false, direct resize without cropping and preserving aspect ratio is performed.
+     * @return 4-dimensional Mat with NCHW dimensions order.
+     */
+    public static Mat blobFromImage(Mat image, double scalefactor, Size size) {
+        return new Mat(blobFromImage_4(image.nativeObj, scalefactor, size.width, size.height));
+    }
+
+    /**
+     * Creates 4-dimensional blob from image. Optionally resizes and crops {@code image} from center,
+     * subtract {@code mean} values, scales values by {@code scalefactor}, swap Blue and Red channels.
+     * @param image input image (with 1-, 3- or 4-channels).
+     * to be in (mean-R, mean-G, mean-B) order if {@code image} has BGR ordering and {@code swapRB} is true.
+     * @param scalefactor multiplier for {@code image} values.
+     * in 3-channel image is necessary.
+     * if {@code crop} is true, input image is resized so one side after resize is equal to corresponding
+     * dimension in {@code size} and another one is equal or larger. Then, crop from the center is performed.
+     * If {@code crop} is false, direct resize without cropping and preserving aspect ratio is performed.
+     * @return 4-dimensional Mat with NCHW dimensions order.
+     */
+    public static Mat blobFromImage(Mat image, double scalefactor) {
+        return new Mat(blobFromImage_5(image.nativeObj, scalefactor));
+    }
+
+    /**
+     * Creates 4-dimensional blob from image. Optionally resizes and crops {@code image} from center,
+     * subtract {@code mean} values, scales values by {@code scalefactor}, swap Blue and Red channels.
+     * @param image input image (with 1-, 3- or 4-channels).
+     * to be in (mean-R, mean-G, mean-B) order if {@code image} has BGR ordering and {@code swapRB} is true.
+     * in 3-channel image is necessary.
+     * if {@code crop} is true, input image is resized so one side after resize is equal to corresponding
+     * dimension in {@code size} and another one is equal or larger. Then, crop from the center is performed.
+     * If {@code crop} is false, direct resize without cropping and preserving aspect ratio is performed.
+     * @return 4-dimensional Mat with NCHW dimensions order.
+     */
+    public static Mat blobFromImage(Mat image) {
+        return new Mat(blobFromImage_6(image.nativeObj));
+    }
+
+
+    //
+    // C++:  Mat cv::dnn::blobFromImages(vector_Mat images, double scalefactor = 1.0, Size size = Size(), Scalar mean = Scalar(), bool swapRB = false, bool crop = false, int ddepth = CV_32F)
+    //
+
+    /**
+     * Creates 4-dimensional blob from series of images. Optionally resizes and
+     * crops {@code images} from center, subtract {@code mean} values, scales values by {@code scalefactor},
+     * swap Blue and Red channels.
+     * @param images input images (all with 1-, 3- or 4-channels).
+     * @param size spatial size for output image
+     * @param mean scalar with mean values which are subtracted from channels. Values are intended
+     * to be in (mean-R, mean-G, mean-B) order if {@code image} has BGR ordering and {@code swapRB} is true.
+     * @param scalefactor multiplier for {@code images} values.
+     * @param swapRB flag which indicates that swap first and last channels
+     * in 3-channel image is necessary.
+     * @param crop flag which indicates whether image will be cropped after resize or not
+     * @param ddepth Depth of output blob. Choose CV_32F or CV_8U.
+     * if {@code crop} is true, input image is resized so one side after resize is equal to corresponding
+     * dimension in {@code size} and another one is equal or larger. Then, crop from the center is performed.
+     * If {@code crop} is false, direct resize without cropping and preserving aspect ratio is performed.
+     * @return 4-dimensional Mat with NCHW dimensions order.
+     */
+    public static Mat blobFromImages(List<Mat> images, double scalefactor, Size size, Scalar mean, boolean swapRB, boolean crop, int ddepth) {
+        Mat images_mat = Converters.vector_Mat_to_Mat(images);
+        return new Mat(blobFromImages_0(images_mat.nativeObj, scalefactor, size.width, size.height, mean.val[0], mean.val[1], mean.val[2], mean.val[3], swapRB, crop, ddepth));
+    }
+
+    /**
+     * Creates 4-dimensional blob from series of images. Optionally resizes and
+     * crops {@code images} from center, subtract {@code mean} values, scales values by {@code scalefactor},
+     * swap Blue and Red channels.
+     * @param images input images (all with 1-, 3- or 4-channels).
+     * @param size spatial size for output image
+     * @param mean scalar with mean values which are subtracted from channels. Values are intended
+     * to be in (mean-R, mean-G, mean-B) order if {@code image} has BGR ordering and {@code swapRB} is true.
+     * @param scalefactor multiplier for {@code images} values.
+     * @param swapRB flag which indicates that swap first and last channels
+     * in 3-channel image is necessary.
+     * @param crop flag which indicates whether image will be cropped after resize or not
+     * if {@code crop} is true, input image is resized so one side after resize is equal to corresponding
+     * dimension in {@code size} and another one is equal or larger. Then, crop from the center is performed.
+     * If {@code crop} is false, direct resize without cropping and preserving aspect ratio is performed.
+     * @return 4-dimensional Mat with NCHW dimensions order.
+     */
+    public static Mat blobFromImages(List<Mat> images, double scalefactor, Size size, Scalar mean, boolean swapRB, boolean crop) {
+        Mat images_mat = Converters.vector_Mat_to_Mat(images);
+        return new Mat(blobFromImages_1(images_mat.nativeObj, scalefactor, size.width, size.height, mean.val[0], mean.val[1], mean.val[2], mean.val[3], swapRB, crop));
+    }
+
+    /**
+     * Creates 4-dimensional blob from series of images. Optionally resizes and
+     * crops {@code images} from center, subtract {@code mean} values, scales values by {@code scalefactor},
+     * swap Blue and Red channels.
+     * @param images input images (all with 1-, 3- or 4-channels).
+     * @param size spatial size for output image
+     * @param mean scalar with mean values which are subtracted from channels. Values are intended
+     * to be in (mean-R, mean-G, mean-B) order if {@code image} has BGR ordering and {@code swapRB} is true.
+     * @param scalefactor multiplier for {@code images} values.
+     * @param swapRB flag which indicates that swap first and last channels
+     * in 3-channel image is necessary.
+     * if {@code crop} is true, input image is resized so one side after resize is equal to corresponding
+     * dimension in {@code size} and another one is equal or larger. Then, crop from the center is performed.
+     * If {@code crop} is false, direct resize without cropping and preserving aspect ratio is performed.
+     * @return 4-dimensional Mat with NCHW dimensions order.
+     */
+    public static Mat blobFromImages(List<Mat> images, double scalefactor, Size size, Scalar mean, boolean swapRB) {
+        Mat images_mat = Converters.vector_Mat_to_Mat(images);
+        return new Mat(blobFromImages_2(images_mat.nativeObj, scalefactor, size.width, size.height, mean.val[0], mean.val[1], mean.val[2], mean.val[3], swapRB));
+    }
+
+    /**
+     * Creates 4-dimensional blob from series of images. Optionally resizes and
+     * crops {@code images} from center, subtract {@code mean} values, scales values by {@code scalefactor},
+     * swap Blue and Red channels.
+     * @param images input images (all with 1-, 3- or 4-channels).
+     * @param size spatial size for output image
+     * @param mean scalar with mean values which are subtracted from channels. Values are intended
+     * to be in (mean-R, mean-G, mean-B) order if {@code image} has BGR ordering and {@code swapRB} is true.
+     * @param scalefactor multiplier for {@code images} values.
+     * in 3-channel image is necessary.
+     * if {@code crop} is true, input image is resized so one side after resize is equal to corresponding
+     * dimension in {@code size} and another one is equal or larger. Then, crop from the center is performed.
+     * If {@code crop} is false, direct resize without cropping and preserving aspect ratio is performed.
+     * @return 4-dimensional Mat with NCHW dimensions order.
+     */
+    public static Mat blobFromImages(List<Mat> images, double scalefactor, Size size, Scalar mean) {
+        Mat images_mat = Converters.vector_Mat_to_Mat(images);
+        return new Mat(blobFromImages_3(images_mat.nativeObj, scalefactor, size.width, size.height, mean.val[0], mean.val[1], mean.val[2], mean.val[3]));
+    }
+
+    /**
+     * Creates 4-dimensional blob from series of images. Optionally resizes and
+     * crops {@code images} from center, subtract {@code mean} values, scales values by {@code scalefactor},
+     * swap Blue and Red channels.
+     * @param images input images (all with 1-, 3- or 4-channels).
+     * @param size spatial size for output image
+     * to be in (mean-R, mean-G, mean-B) order if {@code image} has BGR ordering and {@code swapRB} is true.
+     * @param scalefactor multiplier for {@code images} values.
+     * in 3-channel image is necessary.
+     * if {@code crop} is true, input image is resized so one side after resize is equal to corresponding
+     * dimension in {@code size} and another one is equal or larger. Then, crop from the center is performed.
+     * If {@code crop} is false, direct resize without cropping and preserving aspect ratio is performed.
+     * @return 4-dimensional Mat with NCHW dimensions order.
+     */
+    public static Mat blobFromImages(List<Mat> images, double scalefactor, Size size) {
+        Mat images_mat = Converters.vector_Mat_to_Mat(images);
+        return new Mat(blobFromImages_4(images_mat.nativeObj, scalefactor, size.width, size.height));
+    }
+
+    /**
+     * Creates 4-dimensional blob from series of images. Optionally resizes and
+     * crops {@code images} from center, subtract {@code mean} values, scales values by {@code scalefactor},
+     * swap Blue and Red channels.
+     * @param images input images (all with 1-, 3- or 4-channels).
+     * to be in (mean-R, mean-G, mean-B) order if {@code image} has BGR ordering and {@code swapRB} is true.
+     * @param scalefactor multiplier for {@code images} values.
+     * in 3-channel image is necessary.
+     * if {@code crop} is true, input image is resized so one side after resize is equal to corresponding
+     * dimension in {@code size} and another one is equal or larger. Then, crop from the center is performed.
+     * If {@code crop} is false, direct resize without cropping and preserving aspect ratio is performed.
+     * @return 4-dimensional Mat with NCHW dimensions order.
+     */
+    public static Mat blobFromImages(List<Mat> images, double scalefactor) {
+        Mat images_mat = Converters.vector_Mat_to_Mat(images);
+        return new Mat(blobFromImages_5(images_mat.nativeObj, scalefactor));
+    }
+
+    /**
+     * Creates 4-dimensional blob from series of images. Optionally resizes and
+     * crops {@code images} from center, subtract {@code mean} values, scales values by {@code scalefactor},
+     * swap Blue and Red channels.
+     * @param images input images (all with 1-, 3- or 4-channels).
+     * to be in (mean-R, mean-G, mean-B) order if {@code image} has BGR ordering and {@code swapRB} is true.
+     * in 3-channel image is necessary.
+     * if {@code crop} is true, input image is resized so one side after resize is equal to corresponding
+     * dimension in {@code size} and another one is equal or larger. Then, crop from the center is performed.
+     * If {@code crop} is false, direct resize without cropping and preserving aspect ratio is performed.
+     * @return 4-dimensional Mat with NCHW dimensions order.
+     */
+    public static Mat blobFromImages(List<Mat> images) {
+        Mat images_mat = Converters.vector_Mat_to_Mat(images);
+        return new Mat(blobFromImages_6(images_mat.nativeObj));
+    }
+
+
+    //
+    // C++:  Mat cv::dnn::readTensorFromONNX(String path)
+    //
+
+    /**
+     * Creates blob from .pb file.
+     * @param path to the .pb file with input tensor.
+     * @return Mat.
+     */
+    public static Mat readTensorFromONNX(String path) {
+        return new Mat(readTensorFromONNX_0(path));
+    }
+
+
+    //
+    // C++:  Mat cv::dnn::readTorchBlob(String filename, bool isBinary = true)
+    //
+
+    /**
+     * Loads blob which was serialized as torch.Tensor object of Torch7 framework.
+     * WARNING: This function has the same limitations as readNetFromTorch().
+     * @param filename automatically generated
+     * @param isBinary automatically generated
+     * @return automatically generated
+     */
+    public static Mat readTorchBlob(String filename, boolean isBinary) {
+        return new Mat(readTorchBlob_0(filename, isBinary));
+    }
+
+    /**
+     * Loads blob which was serialized as torch.Tensor object of Torch7 framework.
+     * WARNING: This function has the same limitations as readNetFromTorch().
+     * @param filename automatically generated
+     * @return automatically generated
+     */
+    public static Mat readTorchBlob(String filename) {
+        return new Mat(readTorchBlob_1(filename));
+    }
+
+
+    //
+    // C++:  Net cv::dnn::readNet(String framework, vector_uchar bufferModel, vector_uchar bufferConfig = std::vector<uchar>())
+    //
+
+    /**
+     * Read deep learning network represented in one of the supported formats.
+     * This is an overloaded member function, provided for convenience.
+     * It differs from the above function only in what argument(s) it accepts.
+     * @param framework    Name of origin framework.
+     * @param bufferModel  A buffer with a content of binary file with weights
+     * @param bufferConfig A buffer with a content of text file contains network configuration.
+     * @return Net object.
+     */
+    public static Net readNet(String framework, MatOfByte bufferModel, MatOfByte bufferConfig) {
+        Mat bufferModel_mat = bufferModel;
+        Mat bufferConfig_mat = bufferConfig;
+        return new Net(readNet_0(framework, bufferModel_mat.nativeObj, bufferConfig_mat.nativeObj));
+    }
+
+    /**
+     * Read deep learning network represented in one of the supported formats.
+     * This is an overloaded member function, provided for convenience.
+     * It differs from the above function only in what argument(s) it accepts.
+     * @param framework    Name of origin framework.
+     * @param bufferModel  A buffer with a content of binary file with weights
+     * @return Net object.
+     */
+    public static Net readNet(String framework, MatOfByte bufferModel) {
+        Mat bufferModel_mat = bufferModel;
+        return new Net(readNet_1(framework, bufferModel_mat.nativeObj));
+    }
+
+
+    //
+    // C++:  Net cv::dnn::readNet(String model, String config = "", String framework = "")
+    //
+
+    /**
+     * Read deep learning network represented in one of the supported formats.
+     * @param model Binary file contains trained weights. The following file
+     * extensions are expected for models from different frameworks:
+     * * {@code *.caffemodel} (Caffe, http://caffe.berkeleyvision.org/)
+     * * {@code *.pb} (TensorFlow, https://www.tensorflow.org/)
+     * * {@code *.t7} | {@code *.net} (Torch, http://torch.ch/)
+     * * {@code *.weights} (Darknet, https://pjreddie.com/darknet/)
+     * * {@code *.bin} (DLDT, https://software.intel.com/openvino-toolkit)
+     * * {@code *.onnx} (ONNX, https://onnx.ai/)
+     * @param config Text file contains network configuration. It could be a
+     * file with the following extensions:
+     * * {@code *.prototxt} (Caffe, http://caffe.berkeleyvision.org/)
+     * * {@code *.pbtxt} (TensorFlow, https://www.tensorflow.org/)
+     * * {@code *.cfg} (Darknet, https://pjreddie.com/darknet/)
+     * * {@code *.xml} (DLDT, https://software.intel.com/openvino-toolkit)
+     * @param framework Explicit framework name tag to determine a format.
+     * @return Net object.
+     *
+     * This function automatically detects an origin framework of trained model
+     * and calls an appropriate function such REF: readNetFromCaffe, REF: readNetFromTensorflow,
+     * REF: readNetFromTorch or REF: readNetFromDarknet. An order of {@code model} and {@code config}
+     * arguments does not matter.
+     */
+    public static Net readNet(String model, String config, String framework) {
+        return new Net(readNet_2(model, config, framework));
+    }
+
+    /**
+     * Read deep learning network represented in one of the supported formats.
+     * @param model Binary file contains trained weights. The following file
+     * extensions are expected for models from different frameworks:
+     * * {@code *.caffemodel} (Caffe, http://caffe.berkeleyvision.org/)
+     * * {@code *.pb} (TensorFlow, https://www.tensorflow.org/)
+     * * {@code *.t7} | {@code *.net} (Torch, http://torch.ch/)
+     * * {@code *.weights} (Darknet, https://pjreddie.com/darknet/)
+     * * {@code *.bin} (DLDT, https://software.intel.com/openvino-toolkit)
+     * * {@code *.onnx} (ONNX, https://onnx.ai/)
+     * @param config Text file contains network configuration. It could be a
+     * file with the following extensions:
+     * * {@code *.prototxt} (Caffe, http://caffe.berkeleyvision.org/)
+     * * {@code *.pbtxt} (TensorFlow, https://www.tensorflow.org/)
+     * * {@code *.cfg} (Darknet, https://pjreddie.com/darknet/)
+     * * {@code *.xml} (DLDT, https://software.intel.com/openvino-toolkit)
+     * @return Net object.
+     *
+     * This function automatically detects an origin framework of trained model
+     * and calls an appropriate function such REF: readNetFromCaffe, REF: readNetFromTensorflow,
+     * REF: readNetFromTorch or REF: readNetFromDarknet. An order of {@code model} and {@code config}
+     * arguments does not matter.
+     */
+    public static Net readNet(String model, String config) {
+        return new Net(readNet_3(model, config));
+    }
+
+    /**
+     * Read deep learning network represented in one of the supported formats.
+     * @param model Binary file contains trained weights. The following file
+     * extensions are expected for models from different frameworks:
+     * * {@code *.caffemodel} (Caffe, http://caffe.berkeleyvision.org/)
+     * * {@code *.pb} (TensorFlow, https://www.tensorflow.org/)
+     * * {@code *.t7} | {@code *.net} (Torch, http://torch.ch/)
+     * * {@code *.weights} (Darknet, https://pjreddie.com/darknet/)
+     * * {@code *.bin} (DLDT, https://software.intel.com/openvino-toolkit)
+     * * {@code *.onnx} (ONNX, https://onnx.ai/)
+     * file with the following extensions:
+     * * {@code *.prototxt} (Caffe, http://caffe.berkeleyvision.org/)
+     * * {@code *.pbtxt} (TensorFlow, https://www.tensorflow.org/)
+     * * {@code *.cfg} (Darknet, https://pjreddie.com/darknet/)
+     * * {@code *.xml} (DLDT, https://software.intel.com/openvino-toolkit)
+     * @return Net object.
+     *
+     * This function automatically detects an origin framework of trained model
+     * and calls an appropriate function such REF: readNetFromCaffe, REF: readNetFromTensorflow,
+     * REF: readNetFromTorch or REF: readNetFromDarknet. An order of {@code model} and {@code config}
+     * arguments does not matter.
+     */
+    public static Net readNet(String model) {
+        return new Net(readNet_4(model));
+    }
+
+
+    //
+    // C++:  Net cv::dnn::readNetFromCaffe(String prototxt, String caffeModel = String())
+    //
+
+    /**
+     * Reads a network model stored in &lt;a href="http://caffe.berkeleyvision.org"&gt;Caffe&lt;/a&gt; framework's format.
+     * @param prototxt   path to the .prototxt file with text description of the network architecture.
+     * @param caffeModel path to the .caffemodel file with learned network.
+     * @return Net object.
+     */
+    public static Net readNetFromCaffe(String prototxt, String caffeModel) {
+        return new Net(readNetFromCaffe_0(prototxt, caffeModel));
+    }
+
+    /**
+     * Reads a network model stored in &lt;a href="http://caffe.berkeleyvision.org"&gt;Caffe&lt;/a&gt; framework's format.
+     * @param prototxt   path to the .prototxt file with text description of the network architecture.
+     * @return Net object.
+     */
+    public static Net readNetFromCaffe(String prototxt) {
+        return new Net(readNetFromCaffe_1(prototxt));
+    }
+
+
+    //
+    // C++:  Net cv::dnn::readNetFromCaffe(vector_uchar bufferProto, vector_uchar bufferModel = std::vector<uchar>())
+    //
+
+    /**
+     * Reads a network model stored in Caffe model in memory.
+     * @param bufferProto buffer containing the content of the .prototxt file
+     * @param bufferModel buffer containing the content of the .caffemodel file
+     * @return Net object.
+     */
+    public static Net readNetFromCaffe(MatOfByte bufferProto, MatOfByte bufferModel) {
+        Mat bufferProto_mat = bufferProto;
+        Mat bufferModel_mat = bufferModel;
+        return new Net(readNetFromCaffe_2(bufferProto_mat.nativeObj, bufferModel_mat.nativeObj));
+    }
+
+    /**
+     * Reads a network model stored in Caffe model in memory.
+     * @param bufferProto buffer containing the content of the .prototxt file
+     * @return Net object.
+     */
+    public static Net readNetFromCaffe(MatOfByte bufferProto) {
+        Mat bufferProto_mat = bufferProto;
+        return new Net(readNetFromCaffe_3(bufferProto_mat.nativeObj));
+    }
+
+
+    //
+    // C++:  Net cv::dnn::readNetFromDarknet(String cfgFile, String darknetModel = String())
+    //
+
+    /**
+     * Reads a network model stored in &lt;a href="https://pjreddie.com/darknet/"&gt;Darknet&lt;/a&gt; model files.
+     * @param cfgFile      path to the .cfg file with text description of the network architecture.
+     * @param darknetModel path to the .weights file with learned network.
+     * @return Network object that ready to do forward, throw an exception in failure cases.
+     * @return Net object.
+     */
+    public static Net readNetFromDarknet(String cfgFile, String darknetModel) {
+        return new Net(readNetFromDarknet_0(cfgFile, darknetModel));
+    }
+
+    /**
+     * Reads a network model stored in &lt;a href="https://pjreddie.com/darknet/"&gt;Darknet&lt;/a&gt; model files.
+     * @param cfgFile      path to the .cfg file with text description of the network architecture.
+     * @return Network object that ready to do forward, throw an exception in failure cases.
+     * @return Net object.
+     */
+    public static Net readNetFromDarknet(String cfgFile) {
+        return new Net(readNetFromDarknet_1(cfgFile));
+    }
+
+
+    //
+    // C++:  Net cv::dnn::readNetFromDarknet(vector_uchar bufferCfg, vector_uchar bufferModel = std::vector<uchar>())
+    //
+
+    /**
+     * Reads a network model stored in &lt;a href="https://pjreddie.com/darknet/"&gt;Darknet&lt;/a&gt; model files.
+     * @param bufferCfg   A buffer contains a content of .cfg file with text description of the network architecture.
+     * @param bufferModel A buffer contains a content of .weights file with learned network.
+     * @return Net object.
+     */
+    public static Net readNetFromDarknet(MatOfByte bufferCfg, MatOfByte bufferModel) {
+        Mat bufferCfg_mat = bufferCfg;
+        Mat bufferModel_mat = bufferModel;
+        return new Net(readNetFromDarknet_2(bufferCfg_mat.nativeObj, bufferModel_mat.nativeObj));
+    }
+
+    /**
+     * Reads a network model stored in &lt;a href="https://pjreddie.com/darknet/"&gt;Darknet&lt;/a&gt; model files.
+     * @param bufferCfg   A buffer contains a content of .cfg file with text description of the network architecture.
+     * @return Net object.
+     */
+    public static Net readNetFromDarknet(MatOfByte bufferCfg) {
+        Mat bufferCfg_mat = bufferCfg;
+        return new Net(readNetFromDarknet_3(bufferCfg_mat.nativeObj));
+    }
+
+
+    //
+    // C++:  Net cv::dnn::readNetFromModelOptimizer(String xml, String bin)
+    //
+
+    /**
+     * Load a network from Intel's Model Optimizer intermediate representation.
+     * @param xml XML configuration file with network's topology.
+     * @param bin Binary file with trained weights.
+     * @return Net object.
+     * Networks imported from Intel's Model Optimizer are launched in Intel's Inference Engine
+     * backend.
+     */
+    public static Net readNetFromModelOptimizer(String xml, String bin) {
+        return new Net(readNetFromModelOptimizer_0(xml, bin));
+    }
+
+
+    //
+    // C++:  Net cv::dnn::readNetFromModelOptimizer(vector_uchar bufferModelConfig, vector_uchar bufferWeights)
+    //
+
+    /**
+     * Load a network from Intel's Model Optimizer intermediate representation.
+     * @param bufferModelConfig Buffer contains XML configuration with network's topology.
+     * @param bufferWeights Buffer contains binary data with trained weights.
+     * @return Net object.
+     * Networks imported from Intel's Model Optimizer are launched in Intel's Inference Engine
+     * backend.
+     */
+    public static Net readNetFromModelOptimizer(MatOfByte bufferModelConfig, MatOfByte bufferWeights) {
+        Mat bufferModelConfig_mat = bufferModelConfig;
+        Mat bufferWeights_mat = bufferWeights;
+        return new Net(readNetFromModelOptimizer_1(bufferModelConfig_mat.nativeObj, bufferWeights_mat.nativeObj));
+    }
+
+
+    //
+    // C++:  Net cv::dnn::readNetFromONNX(String onnxFile)
+    //
+
+    /**
+     * Reads a network model &lt;a href="https://onnx.ai/"&gt;ONNX&lt;/a&gt;.
+     * @param onnxFile path to the .onnx file with text description of the network architecture.
+     * @return Network object that ready to do forward, throw an exception in failure cases.
+     */
+    public static Net readNetFromONNX(String onnxFile) {
+        return new Net(readNetFromONNX_0(onnxFile));
+    }
+
+
+    //
+    // C++:  Net cv::dnn::readNetFromONNX(vector_uchar buffer)
+    //
+
+    /**
+     * Reads a network model from &lt;a href="https://onnx.ai/"&gt;ONNX&lt;/a&gt;
+     * in-memory buffer.
+     * @param buffer in-memory buffer that stores the ONNX model bytes.
+     * @return Network object that ready to do forward, throw an exception
+     * in failure cases.
+     */
+    public static Net readNetFromONNX(MatOfByte buffer) {
+        Mat buffer_mat = buffer;
+        return new Net(readNetFromONNX_1(buffer_mat.nativeObj));
+    }
+
+
+    //
+    // C++:  Net cv::dnn::readNetFromTensorflow(String model, String config = String())
+    //
+
+    /**
+     * Reads a network model stored in &lt;a href="https://www.tensorflow.org/"&gt;TensorFlow&lt;/a&gt; framework's format.
+     * @param model  path to the .pb file with binary protobuf description of the network architecture
+     * @param config path to the .pbtxt file that contains text graph definition in protobuf format.
+     * Resulting Net object is built by text graph using weights from a binary one that
+     * let us make it more flexible.
+     * @return Net object.
+     */
+    public static Net readNetFromTensorflow(String model, String config) {
+        return new Net(readNetFromTensorflow_0(model, config));
+    }
+
+    /**
+     * Reads a network model stored in &lt;a href="https://www.tensorflow.org/"&gt;TensorFlow&lt;/a&gt; framework's format.
+     * @param model  path to the .pb file with binary protobuf description of the network architecture
+     * Resulting Net object is built by text graph using weights from a binary one that
+     * let us make it more flexible.
+     * @return Net object.
+     */
+    public static Net readNetFromTensorflow(String model) {
+        return new Net(readNetFromTensorflow_1(model));
+    }
+
+
+    //
+    // C++:  Net cv::dnn::readNetFromTensorflow(vector_uchar bufferModel, vector_uchar bufferConfig = std::vector<uchar>())
+    //
+
+    /**
+     * Reads a network model stored in &lt;a href="https://www.tensorflow.org/"&gt;TensorFlow&lt;/a&gt; framework's format.
+     * @param bufferModel buffer containing the content of the pb file
+     * @param bufferConfig buffer containing the content of the pbtxt file
+     * @return Net object.
+     */
+    public static Net readNetFromTensorflow(MatOfByte bufferModel, MatOfByte bufferConfig) {
+        Mat bufferModel_mat = bufferModel;
+        Mat bufferConfig_mat = bufferConfig;
+        return new Net(readNetFromTensorflow_2(bufferModel_mat.nativeObj, bufferConfig_mat.nativeObj));
+    }
+
+    /**
+     * Reads a network model stored in &lt;a href="https://www.tensorflow.org/"&gt;TensorFlow&lt;/a&gt; framework's format.
+     * @param bufferModel buffer containing the content of the pb file
+     * @return Net object.
+     */
+    public static Net readNetFromTensorflow(MatOfByte bufferModel) {
+        Mat bufferModel_mat = bufferModel;
+        return new Net(readNetFromTensorflow_3(bufferModel_mat.nativeObj));
+    }
+
+
+    //
+    // C++:  Net cv::dnn::readNetFromTorch(String model, bool isBinary = true, bool evaluate = true)
+    //
+
+    /**
+     * Reads a network model stored in &lt;a href="http://torch.ch"&gt;Torch7&lt;/a&gt; framework's format.
+     * @param model    path to the file, dumped from Torch by using torch.save() function.
+     * @param isBinary specifies whether the network was serialized in ascii mode or binary.
+     * @param evaluate specifies testing phase of network. If true, it's similar to evaluate() method in Torch.
+     * @return Net object.
+     *
+     * <b>Note:</b> Ascii mode of Torch serializer is more preferable, because binary mode extensively use {@code long} type of C language,
+     * which has various bit-length on different systems.
+     *
+     * The loading file must contain serialized &lt;a href="https://github.com/torch/nn/blob/master/doc/module.md"&gt;nn.Module&lt;/a&gt; object
+     * with importing network. Try to eliminate a custom objects from serialazing data to avoid importing errors.
+     *
+     * List of supported layers (i.e. object instances derived from Torch nn.Module class):
+     * - nn.Sequential
+     * - nn.Parallel
+     * - nn.Concat
+     * - nn.Linear
+     * - nn.SpatialConvolution
+     * - nn.SpatialMaxPooling, nn.SpatialAveragePooling
+     * - nn.ReLU, nn.TanH, nn.Sigmoid
+     * - nn.Reshape
+     * - nn.SoftMax, nn.LogSoftMax
+     *
+     * Also some equivalents of these classes from cunn, cudnn, and fbcunn may be successfully imported.
+     */
+    public static Net readNetFromTorch(String model, boolean isBinary, boolean evaluate) {
+        return new Net(readNetFromTorch_0(model, isBinary, evaluate));
+    }
+
+    /**
+     * Reads a network model stored in &lt;a href="http://torch.ch"&gt;Torch7&lt;/a&gt; framework's format.
+     * @param model    path to the file, dumped from Torch by using torch.save() function.
+     * @param isBinary specifies whether the network was serialized in ascii mode or binary.
+     * @return Net object.
+     *
+     * <b>Note:</b> Ascii mode of Torch serializer is more preferable, because binary mode extensively use {@code long} type of C language,
+     * which has various bit-length on different systems.
+     *
+     * The loading file must contain serialized &lt;a href="https://github.com/torch/nn/blob/master/doc/module.md"&gt;nn.Module&lt;/a&gt; object
+     * with importing network. Try to eliminate a custom objects from serialazing data to avoid importing errors.
+     *
+     * List of supported layers (i.e. object instances derived from Torch nn.Module class):
+     * - nn.Sequential
+     * - nn.Parallel
+     * - nn.Concat
+     * - nn.Linear
+     * - nn.SpatialConvolution
+     * - nn.SpatialMaxPooling, nn.SpatialAveragePooling
+     * - nn.ReLU, nn.TanH, nn.Sigmoid
+     * - nn.Reshape
+     * - nn.SoftMax, nn.LogSoftMax
+     *
+     * Also some equivalents of these classes from cunn, cudnn, and fbcunn may be successfully imported.
+     */
+    public static Net readNetFromTorch(String model, boolean isBinary) {
+        return new Net(readNetFromTorch_1(model, isBinary));
+    }
+
+    /**
+     * Reads a network model stored in &lt;a href="http://torch.ch"&gt;Torch7&lt;/a&gt; framework's format.
+     * @param model    path to the file, dumped from Torch by using torch.save() function.
+     * @return Net object.
+     *
+     * <b>Note:</b> Ascii mode of Torch serializer is more preferable, because binary mode extensively use {@code long} type of C language,
+     * which has various bit-length on different systems.
+     *
+     * The loading file must contain serialized &lt;a href="https://github.com/torch/nn/blob/master/doc/module.md"&gt;nn.Module&lt;/a&gt; object
+     * with importing network. Try to eliminate a custom objects from serialazing data to avoid importing errors.
+     *
+     * List of supported layers (i.e. object instances derived from Torch nn.Module class):
+     * - nn.Sequential
+     * - nn.Parallel
+     * - nn.Concat
+     * - nn.Linear
+     * - nn.SpatialConvolution
+     * - nn.SpatialMaxPooling, nn.SpatialAveragePooling
+     * - nn.ReLU, nn.TanH, nn.Sigmoid
+     * - nn.Reshape
+     * - nn.SoftMax, nn.LogSoftMax
+     *
+     * Also some equivalents of these classes from cunn, cudnn, and fbcunn may be successfully imported.
+     */
+    public static Net readNetFromTorch(String model) {
+        return new Net(readNetFromTorch_2(model));
+    }
+
+
+    //
+    // C++:  String cv::dnn::getInferenceEngineBackendType()
+    //
+
+    /**
+     * Returns Inference Engine internal backend API.
+     *
+     * See values of {@code CV_DNN_BACKEND_INFERENCE_ENGINE_*} macros.
+     *
+     * Default value is controlled through {@code OPENCV_DNN_BACKEND_INFERENCE_ENGINE_TYPE} runtime parameter (environment variable).
+     * @return automatically generated
+     */
+    public static String getInferenceEngineBackendType() {
+        return getInferenceEngineBackendType_0();
+    }
+
+
+    //
+    // C++:  String cv::dnn::getInferenceEngineVPUType()
+    //
+
+    /**
+     * Returns Inference Engine VPU type.
+     *
+     * See values of {@code CV_DNN_INFERENCE_ENGINE_VPU_TYPE_*} macros.
+     * @return automatically generated
+     */
+    public static String getInferenceEngineVPUType() {
+        return getInferenceEngineVPUType_0();
+    }
+
+
+    //
+    // C++:  String cv::dnn::setInferenceEngineBackendType(String newBackendType)
+    //
+
+    /**
+     * Specify Inference Engine internal backend API.
+     *
+     * See values of {@code CV_DNN_BACKEND_INFERENCE_ENGINE_*} macros.
+     *
+     * @return previous value of internal backend API
+     * @param newBackendType automatically generated
+     */
+    public static String setInferenceEngineBackendType(String newBackendType) {
+        return setInferenceEngineBackendType_0(newBackendType);
+    }
+
+
+    //
+    // C++:  vector_Target cv::dnn::getAvailableTargets(dnn_Backend be)
+    //
+
+    public static List<Integer> getAvailableTargets(int be) {
+        return getAvailableTargets_0(be);
+    }
+
+
+    //
+    // C++:  void cv::dnn::NMSBoxes(vector_Rect2d bboxes, vector_float scores, float score_threshold, float nms_threshold, vector_int& indices, float eta = 1.f, int top_k = 0)
+    //
+
+    /**
+     * Performs non maximum suppression given boxes and corresponding scores.
+     *
+     * @param bboxes a set of bounding boxes to apply NMS.
+     * @param scores a set of corresponding confidences.
+     * @param score_threshold a threshold used to filter boxes by score.
+     * @param nms_threshold a threshold used in non maximum suppression.
+     * @param indices the kept indices of bboxes after NMS.
+     * @param eta a coefficient in adaptive threshold formula: \(nms\_threshold_{i+1}=eta\cdot nms\_threshold_i\).
+     * @param top_k if {@code &gt;0}, keep at most {@code top_k} picked indices.
+     */
+    public static void NMSBoxes(MatOfRect2d bboxes, MatOfFloat scores, float score_threshold, float nms_threshold, MatOfInt indices, float eta, int top_k) {
+        Mat bboxes_mat = bboxes;
+        Mat scores_mat = scores;
+        Mat indices_mat = indices;
+        NMSBoxes_0(bboxes_mat.nativeObj, scores_mat.nativeObj, score_threshold, nms_threshold, indices_mat.nativeObj, eta, top_k);
+    }
+
+    /**
+     * Performs non maximum suppression given boxes and corresponding scores.
+     *
+     * @param bboxes a set of bounding boxes to apply NMS.
+     * @param scores a set of corresponding confidences.
+     * @param score_threshold a threshold used to filter boxes by score.
+     * @param nms_threshold a threshold used in non maximum suppression.
+     * @param indices the kept indices of bboxes after NMS.
+     * @param eta a coefficient in adaptive threshold formula: \(nms\_threshold_{i+1}=eta\cdot nms\_threshold_i\).
+     */
+    public static void NMSBoxes(MatOfRect2d bboxes, MatOfFloat scores, float score_threshold, float nms_threshold, MatOfInt indices, float eta) {
+        Mat bboxes_mat = bboxes;
+        Mat scores_mat = scores;
+        Mat indices_mat = indices;
+        NMSBoxes_1(bboxes_mat.nativeObj, scores_mat.nativeObj, score_threshold, nms_threshold, indices_mat.nativeObj, eta);
+    }
+
+    /**
+     * Performs non maximum suppression given boxes and corresponding scores.
+     *
+     * @param bboxes a set of bounding boxes to apply NMS.
+     * @param scores a set of corresponding confidences.
+     * @param score_threshold a threshold used to filter boxes by score.
+     * @param nms_threshold a threshold used in non maximum suppression.
+     * @param indices the kept indices of bboxes after NMS.
+     */
+    public static void NMSBoxes(MatOfRect2d bboxes, MatOfFloat scores, float score_threshold, float nms_threshold, MatOfInt indices) {
+        Mat bboxes_mat = bboxes;
+        Mat scores_mat = scores;
+        Mat indices_mat = indices;
+        NMSBoxes_2(bboxes_mat.nativeObj, scores_mat.nativeObj, score_threshold, nms_threshold, indices_mat.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::dnn::NMSBoxes(vector_RotatedRect bboxes, vector_float scores, float score_threshold, float nms_threshold, vector_int& indices, float eta = 1.f, int top_k = 0)
+    //
+
+    public static void NMSBoxesRotated(MatOfRotatedRect bboxes, MatOfFloat scores, float score_threshold, float nms_threshold, MatOfInt indices, float eta, int top_k) {
+        Mat bboxes_mat = bboxes;
+        Mat scores_mat = scores;
+        Mat indices_mat = indices;
+        NMSBoxesRotated_0(bboxes_mat.nativeObj, scores_mat.nativeObj, score_threshold, nms_threshold, indices_mat.nativeObj, eta, top_k);
+    }
+
+    public static void NMSBoxesRotated(MatOfRotatedRect bboxes, MatOfFloat scores, float score_threshold, float nms_threshold, MatOfInt indices, float eta) {
+        Mat bboxes_mat = bboxes;
+        Mat scores_mat = scores;
+        Mat indices_mat = indices;
+        NMSBoxesRotated_1(bboxes_mat.nativeObj, scores_mat.nativeObj, score_threshold, nms_threshold, indices_mat.nativeObj, eta);
+    }
+
+    public static void NMSBoxesRotated(MatOfRotatedRect bboxes, MatOfFloat scores, float score_threshold, float nms_threshold, MatOfInt indices) {
+        Mat bboxes_mat = bboxes;
+        Mat scores_mat = scores;
+        Mat indices_mat = indices;
+        NMSBoxesRotated_2(bboxes_mat.nativeObj, scores_mat.nativeObj, score_threshold, nms_threshold, indices_mat.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::dnn::imagesFromBlob(Mat blob_, vector_Mat& images_)
+    //
+
+    /**
+     * Parse a 4D blob and output the images it contains as 2D arrays through a simpler data structure
+     * (std::vector&lt;cv::Mat&gt;).
+     * @param blob_ 4 dimensional array (images, channels, height, width) in floating point precision (CV_32F) from
+     * which you would like to extract the images.
+     * @param images_ array of 2D Mat containing the images extracted from the blob in floating point precision
+     * (CV_32F). They are non normalized neither mean added. The number of returned images equals the first dimension
+     * of the blob (batch size). Every image has a number of channels equals to the second dimension of the blob (depth).
+     */
+    public static void imagesFromBlob(Mat blob_, List<Mat> images_) {
+        Mat images__mat = new Mat();
+        imagesFromBlob_0(blob_.nativeObj, images__mat.nativeObj);
+        Converters.Mat_to_vector_Mat(images__mat, images_);
+        images__mat.release();
+    }
+
+
+    //
+    // C++:  void cv::dnn::resetMyriadDevice()
+    //
+
+    /**
+     * Release a Myriad device (binded by OpenCV).
+     *
+     * Single Myriad device cannot be shared across multiple processes which uses
+     * Inference Engine's Myriad plugin.
+     */
+    public static void resetMyriadDevice() {
+        resetMyriadDevice_0();
+    }
+
+
+    //
+    // C++:  void cv::dnn::shrinkCaffeModel(String src, String dst, vector_String layersTypes = std::vector<String>())
+    //
+
+    /**
+     * Convert all weights of Caffe network to half precision floating point.
+     * @param src Path to origin model from Caffe framework contains single
+     * precision floating point weights (usually has {@code .caffemodel} extension).
+     * @param dst Path to destination model with updated weights.
+     * @param layersTypes Set of layers types which parameters will be converted.
+     * By default, converts only Convolutional and Fully-Connected layers'
+     * weights.
+     *
+     * <b>Note:</b> Shrinked model has no origin float32 weights so it can't be used
+     * in origin Caffe framework anymore. However the structure of data
+     * is taken from NVidia's Caffe fork: https://github.com/NVIDIA/caffe.
+     * So the resulting model may be used there.
+     */
+    public static void shrinkCaffeModel(String src, String dst, List<String> layersTypes) {
+        shrinkCaffeModel_0(src, dst, layersTypes);
+    }
+
+    /**
+     * Convert all weights of Caffe network to half precision floating point.
+     * @param src Path to origin model from Caffe framework contains single
+     * precision floating point weights (usually has {@code .caffemodel} extension).
+     * @param dst Path to destination model with updated weights.
+     * By default, converts only Convolutional and Fully-Connected layers'
+     * weights.
+     *
+     * <b>Note:</b> Shrinked model has no origin float32 weights so it can't be used
+     * in origin Caffe framework anymore. However the structure of data
+     * is taken from NVidia's Caffe fork: https://github.com/NVIDIA/caffe.
+     * So the resulting model may be used there.
+     */
+    public static void shrinkCaffeModel(String src, String dst) {
+        shrinkCaffeModel_1(src, dst);
+    }
+
+
+    //
+    // C++:  void cv::dnn::writeTextGraph(String model, String output)
+    //
+
+    /**
+     * Create a text representation for a binary network stored in protocol buffer format.
+     * @param model  A path to binary network.
+     * @param output A path to output text file to be created.
+     *
+     * <b>Note:</b> To reduce output file size, trained weights are not included.
+     */
+    public static void writeTextGraph(String model, String output) {
+        writeTextGraph_0(model, output);
+    }
+
+
+
+
+    // C++:  Mat cv::dnn::blobFromImage(Mat image, double scalefactor = 1.0, Size size = Size(), Scalar mean = Scalar(), bool swapRB = false, bool crop = false, int ddepth = CV_32F)
+    private static native long blobFromImage_0(long image_nativeObj, double scalefactor, double size_width, double size_height, double mean_val0, double mean_val1, double mean_val2, double mean_val3, boolean swapRB, boolean crop, int ddepth);
+    private static native long blobFromImage_1(long image_nativeObj, double scalefactor, double size_width, double size_height, double mean_val0, double mean_val1, double mean_val2, double mean_val3, boolean swapRB, boolean crop);
+    private static native long blobFromImage_2(long image_nativeObj, double scalefactor, double size_width, double size_height, double mean_val0, double mean_val1, double mean_val2, double mean_val3, boolean swapRB);
+    private static native long blobFromImage_3(long image_nativeObj, double scalefactor, double size_width, double size_height, double mean_val0, double mean_val1, double mean_val2, double mean_val3);
+    private static native long blobFromImage_4(long image_nativeObj, double scalefactor, double size_width, double size_height);
+    private static native long blobFromImage_5(long image_nativeObj, double scalefactor);
+    private static native long blobFromImage_6(long image_nativeObj);
+
+    // C++:  Mat cv::dnn::blobFromImages(vector_Mat images, double scalefactor = 1.0, Size size = Size(), Scalar mean = Scalar(), bool swapRB = false, bool crop = false, int ddepth = CV_32F)
+    private static native long blobFromImages_0(long images_mat_nativeObj, double scalefactor, double size_width, double size_height, double mean_val0, double mean_val1, double mean_val2, double mean_val3, boolean swapRB, boolean crop, int ddepth);
+    private static native long blobFromImages_1(long images_mat_nativeObj, double scalefactor, double size_width, double size_height, double mean_val0, double mean_val1, double mean_val2, double mean_val3, boolean swapRB, boolean crop);
+    private static native long blobFromImages_2(long images_mat_nativeObj, double scalefactor, double size_width, double size_height, double mean_val0, double mean_val1, double mean_val2, double mean_val3, boolean swapRB);
+    private static native long blobFromImages_3(long images_mat_nativeObj, double scalefactor, double size_width, double size_height, double mean_val0, double mean_val1, double mean_val2, double mean_val3);
+    private static native long blobFromImages_4(long images_mat_nativeObj, double scalefactor, double size_width, double size_height);
+    private static native long blobFromImages_5(long images_mat_nativeObj, double scalefactor);
+    private static native long blobFromImages_6(long images_mat_nativeObj);
+
+    // C++:  Mat cv::dnn::readTensorFromONNX(String path)
+    private static native long readTensorFromONNX_0(String path);
+
+    // C++:  Mat cv::dnn::readTorchBlob(String filename, bool isBinary = true)
+    private static native long readTorchBlob_0(String filename, boolean isBinary);
+    private static native long readTorchBlob_1(String filename);
+
+    // C++:  Net cv::dnn::readNet(String framework, vector_uchar bufferModel, vector_uchar bufferConfig = std::vector<uchar>())
+    private static native long readNet_0(String framework, long bufferModel_mat_nativeObj, long bufferConfig_mat_nativeObj);
+    private static native long readNet_1(String framework, long bufferModel_mat_nativeObj);
+
+    // C++:  Net cv::dnn::readNet(String model, String config = "", String framework = "")
+    private static native long readNet_2(String model, String config, String framework);
+    private static native long readNet_3(String model, String config);
+    private static native long readNet_4(String model);
+
+    // C++:  Net cv::dnn::readNetFromCaffe(String prototxt, String caffeModel = String())
+    private static native long readNetFromCaffe_0(String prototxt, String caffeModel);
+    private static native long readNetFromCaffe_1(String prototxt);
+
+    // C++:  Net cv::dnn::readNetFromCaffe(vector_uchar bufferProto, vector_uchar bufferModel = std::vector<uchar>())
+    private static native long readNetFromCaffe_2(long bufferProto_mat_nativeObj, long bufferModel_mat_nativeObj);
+    private static native long readNetFromCaffe_3(long bufferProto_mat_nativeObj);
+
+    // C++:  Net cv::dnn::readNetFromDarknet(String cfgFile, String darknetModel = String())
+    private static native long readNetFromDarknet_0(String cfgFile, String darknetModel);
+    private static native long readNetFromDarknet_1(String cfgFile);
+
+    // C++:  Net cv::dnn::readNetFromDarknet(vector_uchar bufferCfg, vector_uchar bufferModel = std::vector<uchar>())
+    private static native long readNetFromDarknet_2(long bufferCfg_mat_nativeObj, long bufferModel_mat_nativeObj);
+    private static native long readNetFromDarknet_3(long bufferCfg_mat_nativeObj);
+
+    // C++:  Net cv::dnn::readNetFromModelOptimizer(String xml, String bin)
+    private static native long readNetFromModelOptimizer_0(String xml, String bin);
+
+    // C++:  Net cv::dnn::readNetFromModelOptimizer(vector_uchar bufferModelConfig, vector_uchar bufferWeights)
+    private static native long readNetFromModelOptimizer_1(long bufferModelConfig_mat_nativeObj, long bufferWeights_mat_nativeObj);
+
+    // C++:  Net cv::dnn::readNetFromONNX(String onnxFile)
+    private static native long readNetFromONNX_0(String onnxFile);
+
+    // C++:  Net cv::dnn::readNetFromONNX(vector_uchar buffer)
+    private static native long readNetFromONNX_1(long buffer_mat_nativeObj);
+
+    // C++:  Net cv::dnn::readNetFromTensorflow(String model, String config = String())
+    private static native long readNetFromTensorflow_0(String model, String config);
+    private static native long readNetFromTensorflow_1(String model);
+
+    // C++:  Net cv::dnn::readNetFromTensorflow(vector_uchar bufferModel, vector_uchar bufferConfig = std::vector<uchar>())
+    private static native long readNetFromTensorflow_2(long bufferModel_mat_nativeObj, long bufferConfig_mat_nativeObj);
+    private static native long readNetFromTensorflow_3(long bufferModel_mat_nativeObj);
+
+    // C++:  Net cv::dnn::readNetFromTorch(String model, bool isBinary = true, bool evaluate = true)
+    private static native long readNetFromTorch_0(String model, boolean isBinary, boolean evaluate);
+    private static native long readNetFromTorch_1(String model, boolean isBinary);
+    private static native long readNetFromTorch_2(String model);
+
+    // C++:  String cv::dnn::getInferenceEngineBackendType()
+    private static native String getInferenceEngineBackendType_0();
+
+    // C++:  String cv::dnn::getInferenceEngineVPUType()
+    private static native String getInferenceEngineVPUType_0();
+
+    // C++:  String cv::dnn::setInferenceEngineBackendType(String newBackendType)
+    private static native String setInferenceEngineBackendType_0(String newBackendType);
+
+    // C++:  vector_Target cv::dnn::getAvailableTargets(dnn_Backend be)
+    private static native List<Integer> getAvailableTargets_0(int be);
+
+    // C++:  void cv::dnn::NMSBoxes(vector_Rect2d bboxes, vector_float scores, float score_threshold, float nms_threshold, vector_int& indices, float eta = 1.f, int top_k = 0)
+    private static native void NMSBoxes_0(long bboxes_mat_nativeObj, long scores_mat_nativeObj, float score_threshold, float nms_threshold, long indices_mat_nativeObj, float eta, int top_k);
+    private static native void NMSBoxes_1(long bboxes_mat_nativeObj, long scores_mat_nativeObj, float score_threshold, float nms_threshold, long indices_mat_nativeObj, float eta);
+    private static native void NMSBoxes_2(long bboxes_mat_nativeObj, long scores_mat_nativeObj, float score_threshold, float nms_threshold, long indices_mat_nativeObj);
+
+    // C++:  void cv::dnn::NMSBoxes(vector_RotatedRect bboxes, vector_float scores, float score_threshold, float nms_threshold, vector_int& indices, float eta = 1.f, int top_k = 0)
+    private static native void NMSBoxesRotated_0(long bboxes_mat_nativeObj, long scores_mat_nativeObj, float score_threshold, float nms_threshold, long indices_mat_nativeObj, float eta, int top_k);
+    private static native void NMSBoxesRotated_1(long bboxes_mat_nativeObj, long scores_mat_nativeObj, float score_threshold, float nms_threshold, long indices_mat_nativeObj, float eta);
+    private static native void NMSBoxesRotated_2(long bboxes_mat_nativeObj, long scores_mat_nativeObj, float score_threshold, float nms_threshold, long indices_mat_nativeObj);
+
+    // C++:  void cv::dnn::imagesFromBlob(Mat blob_, vector_Mat& images_)
+    private static native void imagesFromBlob_0(long blob__nativeObj, long images__mat_nativeObj);
+
+    // C++:  void cv::dnn::resetMyriadDevice()
+    private static native void resetMyriadDevice_0();
+
+    // C++:  void cv::dnn::shrinkCaffeModel(String src, String dst, vector_String layersTypes = std::vector<String>())
+    private static native void shrinkCaffeModel_0(String src, String dst, List<String> layersTypes);
+    private static native void shrinkCaffeModel_1(String src, String dst);
+
+    // C++:  void cv::dnn::writeTextGraph(String model, String output)
+    private static native void writeTextGraph_0(String model, String output);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/dnn/Layer.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/dnn/Layer.java	(date 1605830247489)
+++ openCVLibrary3411/src/main/java/org/opencv/dnn/Layer.java	(date 1605830247489)
@@ -0,0 +1,169 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.dnn;
+
+import java.util.ArrayList;
+import java.util.List;
+import org.opencv.core.Algorithm;
+import org.opencv.core.Mat;
+import org.opencv.utils.Converters;
+
+// C++: class Layer
+/**
+ * This interface class allows to build new Layers - are building blocks of networks.
+ *
+ * Each class, derived from Layer, must implement allocate() methods to declare own outputs and forward() to compute outputs.
+ * Also before using the new layer into networks you must register your layer by using one of REF: dnnLayerFactory "LayerFactory" macros.
+ */
+public class Layer extends Algorithm {
+
+    protected Layer(long addr) { super(addr); }
+
+    // internal usage only
+    public static Layer __fromPtr__(long addr) { return new Layer(addr); }
+
+    //
+    // C++:  int cv::dnn::Layer::outputNameToIndex(String outputName)
+    //
+
+    /**
+     * Returns index of output blob in output array.
+     * SEE: inputNameToIndex()
+     * @param outputName automatically generated
+     * @return automatically generated
+     */
+    public int outputNameToIndex(String outputName) {
+        return outputNameToIndex_0(nativeObj, outputName);
+    }
+
+
+    //
+    // C++:  void cv::dnn::Layer::finalize(vector_Mat inputs, vector_Mat& outputs)
+    //
+
+    /**
+     * Computes and sets internal parameters according to inputs, outputs and blobs.
+     * @param outputs vector of already allocated output blobs
+     *
+     * If this method is called after network has allocated all memory for input and output blobs
+     * and before inferencing.
+     * @param inputs automatically generated
+     */
+    public void finalize(List<Mat> inputs, List<Mat> outputs) {
+        Mat inputs_mat = Converters.vector_Mat_to_Mat(inputs);
+        Mat outputs_mat = new Mat();
+        finalize_0(nativeObj, inputs_mat.nativeObj, outputs_mat.nativeObj);
+        Converters.Mat_to_vector_Mat(outputs_mat, outputs);
+        outputs_mat.release();
+    }
+
+
+    //
+    // C++:  void cv::dnn::Layer::run(vector_Mat inputs, vector_Mat& outputs, vector_Mat& internals)
+    //
+
+    /**
+     * Allocates layer and computes output.
+     * @deprecated This method will be removed in the future release.
+     * @param inputs automatically generated
+     * @param outputs automatically generated
+     * @param internals automatically generated
+     */
+    @Deprecated
+    public void run(List<Mat> inputs, List<Mat> outputs, List<Mat> internals) {
+        Mat inputs_mat = Converters.vector_Mat_to_Mat(inputs);
+        Mat outputs_mat = new Mat();
+        Mat internals_mat = Converters.vector_Mat_to_Mat(internals);
+        run_0(nativeObj, inputs_mat.nativeObj, outputs_mat.nativeObj, internals_mat.nativeObj);
+        Converters.Mat_to_vector_Mat(outputs_mat, outputs);
+        outputs_mat.release();
+        Converters.Mat_to_vector_Mat(internals_mat, internals);
+        internals_mat.release();
+    }
+
+
+    //
+    // C++: vector_Mat Layer::blobs
+    //
+
+    public List<Mat> get_blobs() {
+        List<Mat> retVal = new ArrayList<Mat>();
+        Mat retValMat = new Mat(get_blobs_0(nativeObj));
+        Converters.Mat_to_vector_Mat(retValMat, retVal);
+        return retVal;
+    }
+
+
+    //
+    // C++: void Layer::blobs
+    //
+
+    public void set_blobs(List<Mat> blobs) {
+        Mat blobs_mat = Converters.vector_Mat_to_Mat(blobs);
+        set_blobs_0(nativeObj, blobs_mat.nativeObj);
+    }
+
+
+    //
+    // C++: String Layer::name
+    //
+
+    public String get_name() {
+        return get_name_0(nativeObj);
+    }
+
+
+    //
+    // C++: String Layer::type
+    //
+
+    public String get_type() {
+        return get_type_0(nativeObj);
+    }
+
+
+    //
+    // C++: int Layer::preferableTarget
+    //
+
+    public int get_preferableTarget() {
+        return get_preferableTarget_0(nativeObj);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++:  int cv::dnn::Layer::outputNameToIndex(String outputName)
+    private static native int outputNameToIndex_0(long nativeObj, String outputName);
+
+    // C++:  void cv::dnn::Layer::finalize(vector_Mat inputs, vector_Mat& outputs)
+    private static native void finalize_0(long nativeObj, long inputs_mat_nativeObj, long outputs_mat_nativeObj);
+
+    // C++:  void cv::dnn::Layer::run(vector_Mat inputs, vector_Mat& outputs, vector_Mat& internals)
+    private static native void run_0(long nativeObj, long inputs_mat_nativeObj, long outputs_mat_nativeObj, long internals_mat_nativeObj);
+
+    // C++: vector_Mat Layer::blobs
+    private static native long get_blobs_0(long nativeObj);
+
+    // C++: void Layer::blobs
+    private static native void set_blobs_0(long nativeObj, long blobs_mat_nativeObj);
+
+    // C++: String Layer::name
+    private static native String get_name_0(long nativeObj);
+
+    // C++: String Layer::type
+    private static native String get_type_0(long nativeObj);
+
+    // C++: int Layer::preferableTarget
+    private static native int get_preferableTarget_0(long nativeObj);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/dnn/Net.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/dnn/Net.java	(date 1605830247491)
+++ openCVLibrary3411/src/main/java/org/opencv/dnn/Net.java	(date 1605830247491)
@@ -0,0 +1,766 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.dnn;
+
+import java.util.ArrayList;
+import java.util.List;
+import org.opencv.core.Mat;
+import org.opencv.core.MatOfByte;
+import org.opencv.core.MatOfDouble;
+import org.opencv.core.MatOfInt;
+import org.opencv.core.Scalar;
+import org.opencv.dnn.DictValue;
+import org.opencv.dnn.Layer;
+import org.opencv.dnn.Net;
+import org.opencv.utils.Converters;
+
+// C++: class Net
+/**
+ * This class allows to create and manipulate comprehensive artificial neural networks.
+ *
+ * Neural network is presented as directed acyclic graph (DAG), where vertices are Layer instances,
+ * and edges specify relationships between layers inputs and outputs.
+ *
+ * Each network layer has unique integer id and unique string name inside its network.
+ * LayerId can store either layer name or layer id.
+ *
+ * This class supports reference counting of its instances, i. e. copies point to the same instance.
+ */
+public class Net {
+
+    protected final long nativeObj;
+    protected Net(long addr) { nativeObj = addr; }
+
+    public long getNativeObjAddr() { return nativeObj; }
+
+    // internal usage only
+    public static Net __fromPtr__(long addr) { return new Net(addr); }
+
+    //
+    // C++:   cv::dnn::Net::Net()
+    //
+
+    public Net() {
+        nativeObj = Net_0();
+    }
+
+
+    //
+    // C++:  AsyncArray cv::dnn::Net::forwardAsync(String outputName = String())
+    //
+
+    // Return type 'AsyncArray' is not supported, skipping the function
+
+
+    //
+    // C++:  Mat cv::dnn::Net::forward(String outputName = String())
+    //
+
+    /**
+     * Runs forward pass to compute output of layer with name {@code outputName}.
+     * @param outputName name for layer which output is needed to get
+     * @return blob for first output of specified layer.
+     * By default runs forward pass for the whole network.
+     */
+    public Mat forward(String outputName) {
+        return new Mat(forward_0(nativeObj, outputName));
+    }
+
+    /**
+     * Runs forward pass to compute output of layer with name {@code outputName}.
+     * @return blob for first output of specified layer.
+     * By default runs forward pass for the whole network.
+     */
+    public Mat forward() {
+        return new Mat(forward_1(nativeObj));
+    }
+
+
+    //
+    // C++:  Mat cv::dnn::Net::getParam(LayerId layer, int numParam = 0)
+    //
+
+    /**
+     * Returns parameter blob of the layer.
+     * @param layer name or id of the layer.
+     * @param numParam index of the layer parameter in the Layer::blobs array.
+     * SEE: Layer::blobs
+     * @return automatically generated
+     */
+    public Mat getParam(DictValue layer, int numParam) {
+        return new Mat(getParam_0(nativeObj, layer.getNativeObjAddr(), numParam));
+    }
+
+    /**
+     * Returns parameter blob of the layer.
+     * @param layer name or id of the layer.
+     * SEE: Layer::blobs
+     * @return automatically generated
+     */
+    public Mat getParam(DictValue layer) {
+        return new Mat(getParam_1(nativeObj, layer.getNativeObjAddr()));
+    }
+
+
+    //
+    // C++: static Net cv::dnn::Net::readFromModelOptimizer(String xml, String bin)
+    //
+
+    /**
+     * Create a network from Intel's Model Optimizer intermediate representation (IR).
+     * @param xml XML configuration file with network's topology.
+     * @param bin Binary file with trained weights.
+     * Networks imported from Intel's Model Optimizer are launched in Intel's Inference Engine
+     * backend.
+     * @return automatically generated
+     */
+    public static Net readFromModelOptimizer(String xml, String bin) {
+        return new Net(readFromModelOptimizer_0(xml, bin));
+    }
+
+
+    //
+    // C++: static Net cv::dnn::Net::readFromModelOptimizer(vector_uchar bufferModelConfig, vector_uchar bufferWeights)
+    //
+
+    /**
+     * Create a network from Intel's Model Optimizer in-memory buffers with intermediate representation (IR).
+     * @param bufferModelConfig buffer with model's configuration.
+     * @param bufferWeights buffer with model's trained weights.
+     * @return Net object.
+     */
+    public static Net readFromModelOptimizer(MatOfByte bufferModelConfig, MatOfByte bufferWeights) {
+        Mat bufferModelConfig_mat = bufferModelConfig;
+        Mat bufferWeights_mat = bufferWeights;
+        return new Net(readFromModelOptimizer_1(bufferModelConfig_mat.nativeObj, bufferWeights_mat.nativeObj));
+    }
+
+
+    //
+    // C++:  Ptr_Layer cv::dnn::Net::getLayer(LayerId layerId)
+    //
+
+    /**
+     * Returns pointer to layer with specified id or name which the network use.
+     * @param layerId automatically generated
+     * @return automatically generated
+     */
+    public Layer getLayer(DictValue layerId) {
+        return Layer.__fromPtr__(getLayer_0(nativeObj, layerId.getNativeObjAddr()));
+    }
+
+
+    //
+    // C++:  String cv::dnn::Net::dump()
+    //
+
+    /**
+     * Dump net to String
+     * @return String with structure, hyperparameters, backend, target and fusion
+     * Call method after setInput(). To see correct backend, target and fusion run after forward().
+     */
+    public String dump() {
+        return dump_0(nativeObj);
+    }
+
+
+    //
+    // C++:  bool cv::dnn::Net::empty()
+    //
+
+    /**
+     * Returns true if there are no layers in the network.
+     * @return automatically generated
+     */
+    public boolean empty() {
+        return empty_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::dnn::Net::getLayerId(String layer)
+    //
+
+    /**
+     * Converts string name of the layer to the integer identifier.
+     * @return id of the layer, or -1 if the layer wasn't found.
+     * @param layer automatically generated
+     */
+    public int getLayerId(String layer) {
+        return getLayerId_0(nativeObj, layer);
+    }
+
+
+    //
+    // C++:  int cv::dnn::Net::getLayersCount(String layerType)
+    //
+
+    /**
+     * Returns count of layers of specified type.
+     * @param layerType type.
+     * @return count of layers
+     */
+    public int getLayersCount(String layerType) {
+        return getLayersCount_0(nativeObj, layerType);
+    }
+
+
+    //
+    // C++:  int64 cv::dnn::Net::getFLOPS(MatShape netInputShape)
+    //
+
+    public long getFLOPS(MatOfInt netInputShape) {
+        Mat netInputShape_mat = netInputShape;
+        return getFLOPS_0(nativeObj, netInputShape_mat.nativeObj);
+    }
+
+
+    //
+    // C++:  int64 cv::dnn::Net::getFLOPS(int layerId, MatShape netInputShape)
+    //
+
+    public long getFLOPS(int layerId, MatOfInt netInputShape) {
+        Mat netInputShape_mat = netInputShape;
+        return getFLOPS_1(nativeObj, layerId, netInputShape_mat.nativeObj);
+    }
+
+
+    //
+    // C++:  int64 cv::dnn::Net::getFLOPS(int layerId, vector_MatShape netInputShapes)
+    //
+
+    public long getFLOPS(int layerId, List<MatOfInt> netInputShapes) {
+        return getFLOPS_2(nativeObj, layerId, netInputShapes);
+    }
+
+
+    //
+    // C++:  int64 cv::dnn::Net::getFLOPS(vector_MatShape netInputShapes)
+    //
+
+    /**
+     * Computes FLOP for whole loaded model with specified input shapes.
+     * @param netInputShapes vector of shapes for all net inputs.
+     * @return computed FLOP.
+     */
+    public long getFLOPS(List<MatOfInt> netInputShapes) {
+        return getFLOPS_3(nativeObj, netInputShapes);
+    }
+
+
+    //
+    // C++:  int64 cv::dnn::Net::getPerfProfile(vector_double& timings)
+    //
+
+    /**
+     * Returns overall time for inference and timings (in ticks) for layers.
+     * Indexes in returned vector correspond to layers ids. Some layers can be fused with others,
+     * in this case zero ticks count will be return for that skipped layers.
+     * @param timings vector for tick timings for all layers.
+     * @return overall ticks for model inference.
+     */
+    public long getPerfProfile(MatOfDouble timings) {
+        Mat timings_mat = timings;
+        return getPerfProfile_0(nativeObj, timings_mat.nativeObj);
+    }
+
+
+    //
+    // C++:  vector_String cv::dnn::Net::getLayerNames()
+    //
+
+    public List<String> getLayerNames() {
+        return getLayerNames_0(nativeObj);
+    }
+
+
+    //
+    // C++:  vector_String cv::dnn::Net::getUnconnectedOutLayersNames()
+    //
+
+    /**
+     * Returns names of layers with unconnected outputs.
+     * @return automatically generated
+     */
+    public List<String> getUnconnectedOutLayersNames() {
+        return getUnconnectedOutLayersNames_0(nativeObj);
+    }
+
+
+    //
+    // C++:  vector_int cv::dnn::Net::getUnconnectedOutLayers()
+    //
+
+    /**
+     * Returns indexes of layers with unconnected outputs.
+     * @return automatically generated
+     */
+    public MatOfInt getUnconnectedOutLayers() {
+        return MatOfInt.fromNativeAddr(getUnconnectedOutLayers_0(nativeObj));
+    }
+
+
+    //
+    // C++:  void cv::dnn::Net::connect(String outPin, String inpPin)
+    //
+
+    /**
+     * Connects output of the first layer to input of the second layer.
+     * @param outPin descriptor of the first layer output.
+     * @param inpPin descriptor of the second layer input.
+     *
+     * Descriptors have the following template &lt;DFN&gt;&amp;lt;layer_name&amp;gt;[.input_number]&lt;/DFN&gt;:
+     * - the first part of the template &lt;DFN&gt;layer_name&lt;/DFN&gt; is string name of the added layer.
+     * If this part is empty then the network input pseudo layer will be used;
+     * - the second optional part of the template &lt;DFN&gt;input_number&lt;/DFN&gt;
+     * is either number of the layer input, either label one.
+     * If this part is omitted then the first layer input will be used.
+     *
+     * SEE: setNetInputs(), Layer::inputNameToIndex(), Layer::outputNameToIndex()
+     */
+    public void connect(String outPin, String inpPin) {
+        connect_0(nativeObj, outPin, inpPin);
+    }
+
+
+    //
+    // C++:  void cv::dnn::Net::dumpToFile(String path)
+    //
+
+    /**
+     * Dump net structure, hyperparameters, backend, target and fusion to dot file
+     * @param path   path to output file with .dot extension
+     * SEE: dump()
+     */
+    public void dumpToFile(String path) {
+        dumpToFile_0(nativeObj, path);
+    }
+
+
+    //
+    // C++:  void cv::dnn::Net::enableFusion(bool fusion)
+    //
+
+    /**
+     * Enables or disables layer fusion in the network.
+     * @param fusion true to enable the fusion, false to disable. The fusion is enabled by default.
+     */
+    public void enableFusion(boolean fusion) {
+        enableFusion_0(nativeObj, fusion);
+    }
+
+
+    //
+    // C++:  void cv::dnn::Net::forward(vector_Mat& outputBlobs, String outputName = String())
+    //
+
+    /**
+     * Runs forward pass to compute output of layer with name {@code outputName}.
+     * @param outputBlobs contains all output blobs for specified layer.
+     * @param outputName name for layer which output is needed to get
+     * If {@code outputName} is empty, runs forward pass for the whole network.
+     */
+    public void forward(List<Mat> outputBlobs, String outputName) {
+        Mat outputBlobs_mat = new Mat();
+        forward_2(nativeObj, outputBlobs_mat.nativeObj, outputName);
+        Converters.Mat_to_vector_Mat(outputBlobs_mat, outputBlobs);
+        outputBlobs_mat.release();
+    }
+
+    /**
+     * Runs forward pass to compute output of layer with name {@code outputName}.
+     * @param outputBlobs contains all output blobs for specified layer.
+     * If {@code outputName} is empty, runs forward pass for the whole network.
+     */
+    public void forward(List<Mat> outputBlobs) {
+        Mat outputBlobs_mat = new Mat();
+        forward_3(nativeObj, outputBlobs_mat.nativeObj);
+        Converters.Mat_to_vector_Mat(outputBlobs_mat, outputBlobs);
+        outputBlobs_mat.release();
+    }
+
+
+    //
+    // C++:  void cv::dnn::Net::forward(vector_Mat& outputBlobs, vector_String outBlobNames)
+    //
+
+    /**
+     * Runs forward pass to compute outputs of layers listed in {@code outBlobNames}.
+     * @param outputBlobs contains blobs for first outputs of specified layers.
+     * @param outBlobNames names for layers which outputs are needed to get
+     */
+    public void forward(List<Mat> outputBlobs, List<String> outBlobNames) {
+        Mat outputBlobs_mat = new Mat();
+        forward_4(nativeObj, outputBlobs_mat.nativeObj, outBlobNames);
+        Converters.Mat_to_vector_Mat(outputBlobs_mat, outputBlobs);
+        outputBlobs_mat.release();
+    }
+
+
+    //
+    // C++:  void cv::dnn::Net::forward(vector_vector_Mat& outputBlobs, vector_String outBlobNames)
+    //
+
+    // Unknown type 'vector_vector_Mat' (O), skipping the function
+
+
+    //
+    // C++:  void cv::dnn::Net::getLayerTypes(vector_String& layersTypes)
+    //
+
+    /**
+     * Returns list of types for layer used in model.
+     * @param layersTypes output parameter for returning types.
+     */
+    public void getLayerTypes(List<String> layersTypes) {
+        getLayerTypes_0(nativeObj, layersTypes);
+    }
+
+
+    //
+    // C++:  void cv::dnn::Net::getLayersShapes(MatShape netInputShape, vector_int& layersIds, vector_vector_MatShape& inLayersShapes, vector_vector_MatShape& outLayersShapes)
+    //
+
+    // Unknown type 'vector_vector_MatShape' (O), skipping the function
+
+
+    //
+    // C++:  void cv::dnn::Net::getLayersShapes(vector_MatShape netInputShapes, vector_int& layersIds, vector_vector_MatShape& inLayersShapes, vector_vector_MatShape& outLayersShapes)
+    //
+
+    // Unknown type 'vector_vector_MatShape' (O), skipping the function
+
+
+    //
+    // C++:  void cv::dnn::Net::getMemoryConsumption(MatShape netInputShape, size_t& weights, size_t& blobs)
+    //
+
+    public void getMemoryConsumption(MatOfInt netInputShape, long[] weights, long[] blobs) {
+        Mat netInputShape_mat = netInputShape;
+        double[] weights_out = new double[1];
+        double[] blobs_out = new double[1];
+        getMemoryConsumption_0(nativeObj, netInputShape_mat.nativeObj, weights_out, blobs_out);
+        if(weights!=null) weights[0] = (long)weights_out[0];
+        if(blobs!=null) blobs[0] = (long)blobs_out[0];
+    }
+
+
+    //
+    // C++:  void cv::dnn::Net::getMemoryConsumption(int layerId, MatShape netInputShape, size_t& weights, size_t& blobs)
+    //
+
+    public void getMemoryConsumption(int layerId, MatOfInt netInputShape, long[] weights, long[] blobs) {
+        Mat netInputShape_mat = netInputShape;
+        double[] weights_out = new double[1];
+        double[] blobs_out = new double[1];
+        getMemoryConsumption_1(nativeObj, layerId, netInputShape_mat.nativeObj, weights_out, blobs_out);
+        if(weights!=null) weights[0] = (long)weights_out[0];
+        if(blobs!=null) blobs[0] = (long)blobs_out[0];
+    }
+
+
+    //
+    // C++:  void cv::dnn::Net::getMemoryConsumption(int layerId, vector_MatShape netInputShapes, size_t& weights, size_t& blobs)
+    //
+
+    public void getMemoryConsumption(int layerId, List<MatOfInt> netInputShapes, long[] weights, long[] blobs) {
+        double[] weights_out = new double[1];
+        double[] blobs_out = new double[1];
+        getMemoryConsumption_2(nativeObj, layerId, netInputShapes, weights_out, blobs_out);
+        if(weights!=null) weights[0] = (long)weights_out[0];
+        if(blobs!=null) blobs[0] = (long)blobs_out[0];
+    }
+
+
+    //
+    // C++:  void cv::dnn::Net::setHalideScheduler(String scheduler)
+    //
+
+    /**
+     * Compile Halide layers.
+     * @param scheduler Path to YAML file with scheduling directives.
+     * SEE: setPreferableBackend
+     *
+     * Schedule layers that support Halide backend. Then compile them for
+     * specific target. For layers that not represented in scheduling file
+     * or if no manual scheduling used at all, automatic scheduling will be applied.
+     */
+    public void setHalideScheduler(String scheduler) {
+        setHalideScheduler_0(nativeObj, scheduler);
+    }
+
+
+    //
+    // C++:  void cv::dnn::Net::setInput(Mat blob, String name = "", double scalefactor = 1.0, Scalar mean = Scalar())
+    //
+
+    /**
+     * Sets the new input value for the network
+     * @param blob        A new blob. Should have CV_32F or CV_8U depth.
+     * @param name        A name of input layer.
+     * @param scalefactor An optional normalization scale.
+     * @param mean        An optional mean subtraction values.
+     * SEE: connect(String, String) to know format of the descriptor.
+     *
+     * If scale or mean values are specified, a final input blob is computed
+     * as:
+     * \(input(n,c,h,w) = scalefactor \times (blob(n,c,h,w) - mean_c)\)
+     */
+    public void setInput(Mat blob, String name, double scalefactor, Scalar mean) {
+        setInput_0(nativeObj, blob.nativeObj, name, scalefactor, mean.val[0], mean.val[1], mean.val[2], mean.val[3]);
+    }
+
+    /**
+     * Sets the new input value for the network
+     * @param blob        A new blob. Should have CV_32F or CV_8U depth.
+     * @param name        A name of input layer.
+     * @param scalefactor An optional normalization scale.
+     * SEE: connect(String, String) to know format of the descriptor.
+     *
+     * If scale or mean values are specified, a final input blob is computed
+     * as:
+     * \(input(n,c,h,w) = scalefactor \times (blob(n,c,h,w) - mean_c)\)
+     */
+    public void setInput(Mat blob, String name, double scalefactor) {
+        setInput_1(nativeObj, blob.nativeObj, name, scalefactor);
+    }
+
+    /**
+     * Sets the new input value for the network
+     * @param blob        A new blob. Should have CV_32F or CV_8U depth.
+     * @param name        A name of input layer.
+     * SEE: connect(String, String) to know format of the descriptor.
+     *
+     * If scale or mean values are specified, a final input blob is computed
+     * as:
+     * \(input(n,c,h,w) = scalefactor \times (blob(n,c,h,w) - mean_c)\)
+     */
+    public void setInput(Mat blob, String name) {
+        setInput_2(nativeObj, blob.nativeObj, name);
+    }
+
+    /**
+     * Sets the new input value for the network
+     * @param blob        A new blob. Should have CV_32F or CV_8U depth.
+     * SEE: connect(String, String) to know format of the descriptor.
+     *
+     * If scale or mean values are specified, a final input blob is computed
+     * as:
+     * \(input(n,c,h,w) = scalefactor \times (blob(n,c,h,w) - mean_c)\)
+     */
+    public void setInput(Mat blob) {
+        setInput_3(nativeObj, blob.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::dnn::Net::setInputShape(String inputName, MatShape shape)
+    //
+
+    /**
+     * Specify shape of network input.
+     * @param inputName automatically generated
+     * @param shape automatically generated
+     */
+    public void setInputShape(String inputName, MatOfInt shape) {
+        Mat shape_mat = shape;
+        setInputShape_0(nativeObj, inputName, shape_mat.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::dnn::Net::setInputsNames(vector_String inputBlobNames)
+    //
+
+    /**
+     * Sets outputs names of the network input pseudo layer.
+     *
+     * Each net always has special own the network input pseudo layer with id=0.
+     * This layer stores the user blobs only and don't make any computations.
+     * In fact, this layer provides the only way to pass user data into the network.
+     * As any other layer, this layer can label its outputs and this function provides an easy way to do this.
+     * @param inputBlobNames automatically generated
+     */
+    public void setInputsNames(List<String> inputBlobNames) {
+        setInputsNames_0(nativeObj, inputBlobNames);
+    }
+
+
+    //
+    // C++:  void cv::dnn::Net::setParam(LayerId layer, int numParam, Mat blob)
+    //
+
+    /**
+     * Sets the new value for the learned param of the layer.
+     * @param layer name or id of the layer.
+     * @param numParam index of the layer parameter in the Layer::blobs array.
+     * @param blob the new value.
+     * SEE: Layer::blobs
+     * <b>Note:</b> If shape of the new blob differs from the previous shape,
+     * then the following forward pass may fail.
+     */
+    public void setParam(DictValue layer, int numParam, Mat blob) {
+        setParam_0(nativeObj, layer.getNativeObjAddr(), numParam, blob.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::dnn::Net::setPreferableBackend(int backendId)
+    //
+
+    /**
+     * Ask network to use specific computation backend where it supported.
+     * @param backendId backend identifier.
+     * SEE: Backend
+     *
+     * If OpenCV is compiled with Intel's Inference Engine library, DNN_BACKEND_DEFAULT
+     * means DNN_BACKEND_INFERENCE_ENGINE. Otherwise it equals to DNN_BACKEND_OPENCV.
+     */
+    public void setPreferableBackend(int backendId) {
+        setPreferableBackend_0(nativeObj, backendId);
+    }
+
+
+    //
+    // C++:  void cv::dnn::Net::setPreferableTarget(int targetId)
+    //
+
+    /**
+     * Ask network to make computations on specific target device.
+     * @param targetId target identifier.
+     * SEE: Target
+     *
+     * List of supported combinations backend / target:
+     * |                        | DNN_BACKEND_OPENCV | DNN_BACKEND_INFERENCE_ENGINE | DNN_BACKEND_HALIDE |
+     * |------------------------|--------------------|------------------------------|--------------------|
+     * | DNN_TARGET_CPU         |                  + |                            + |                  + |
+     * | DNN_TARGET_OPENCL      |                  + |                            + |                  + |
+     * | DNN_TARGET_OPENCL_FP16 |                  + |                            + |                    |
+     * | DNN_TARGET_MYRIAD      |                    |                            + |                    |
+     * | DNN_TARGET_FPGA        |                    |                            + |                    |
+     */
+    public void setPreferableTarget(int targetId) {
+        setPreferableTarget_0(nativeObj, targetId);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++:   cv::dnn::Net::Net()
+    private static native long Net_0();
+
+    // C++:  Mat cv::dnn::Net::forward(String outputName = String())
+    private static native long forward_0(long nativeObj, String outputName);
+    private static native long forward_1(long nativeObj);
+
+    // C++:  Mat cv::dnn::Net::getParam(LayerId layer, int numParam = 0)
+    private static native long getParam_0(long nativeObj, long layer_nativeObj, int numParam);
+    private static native long getParam_1(long nativeObj, long layer_nativeObj);
+
+    // C++: static Net cv::dnn::Net::readFromModelOptimizer(String xml, String bin)
+    private static native long readFromModelOptimizer_0(String xml, String bin);
+
+    // C++: static Net cv::dnn::Net::readFromModelOptimizer(vector_uchar bufferModelConfig, vector_uchar bufferWeights)
+    private static native long readFromModelOptimizer_1(long bufferModelConfig_mat_nativeObj, long bufferWeights_mat_nativeObj);
+
+    // C++:  Ptr_Layer cv::dnn::Net::getLayer(LayerId layerId)
+    private static native long getLayer_0(long nativeObj, long layerId_nativeObj);
+
+    // C++:  String cv::dnn::Net::dump()
+    private static native String dump_0(long nativeObj);
+
+    // C++:  bool cv::dnn::Net::empty()
+    private static native boolean empty_0(long nativeObj);
+
+    // C++:  int cv::dnn::Net::getLayerId(String layer)
+    private static native int getLayerId_0(long nativeObj, String layer);
+
+    // C++:  int cv::dnn::Net::getLayersCount(String layerType)
+    private static native int getLayersCount_0(long nativeObj, String layerType);
+
+    // C++:  int64 cv::dnn::Net::getFLOPS(MatShape netInputShape)
+    private static native long getFLOPS_0(long nativeObj, long netInputShape_mat_nativeObj);
+
+    // C++:  int64 cv::dnn::Net::getFLOPS(int layerId, MatShape netInputShape)
+    private static native long getFLOPS_1(long nativeObj, int layerId, long netInputShape_mat_nativeObj);
+
+    // C++:  int64 cv::dnn::Net::getFLOPS(int layerId, vector_MatShape netInputShapes)
+    private static native long getFLOPS_2(long nativeObj, int layerId, List<MatOfInt> netInputShapes);
+
+    // C++:  int64 cv::dnn::Net::getFLOPS(vector_MatShape netInputShapes)
+    private static native long getFLOPS_3(long nativeObj, List<MatOfInt> netInputShapes);
+
+    // C++:  int64 cv::dnn::Net::getPerfProfile(vector_double& timings)
+    private static native long getPerfProfile_0(long nativeObj, long timings_mat_nativeObj);
+
+    // C++:  vector_String cv::dnn::Net::getLayerNames()
+    private static native List<String> getLayerNames_0(long nativeObj);
+
+    // C++:  vector_String cv::dnn::Net::getUnconnectedOutLayersNames()
+    private static native List<String> getUnconnectedOutLayersNames_0(long nativeObj);
+
+    // C++:  vector_int cv::dnn::Net::getUnconnectedOutLayers()
+    private static native long getUnconnectedOutLayers_0(long nativeObj);
+
+    // C++:  void cv::dnn::Net::connect(String outPin, String inpPin)
+    private static native void connect_0(long nativeObj, String outPin, String inpPin);
+
+    // C++:  void cv::dnn::Net::dumpToFile(String path)
+    private static native void dumpToFile_0(long nativeObj, String path);
+
+    // C++:  void cv::dnn::Net::enableFusion(bool fusion)
+    private static native void enableFusion_0(long nativeObj, boolean fusion);
+
+    // C++:  void cv::dnn::Net::forward(vector_Mat& outputBlobs, String outputName = String())
+    private static native void forward_2(long nativeObj, long outputBlobs_mat_nativeObj, String outputName);
+    private static native void forward_3(long nativeObj, long outputBlobs_mat_nativeObj);
+
+    // C++:  void cv::dnn::Net::forward(vector_Mat& outputBlobs, vector_String outBlobNames)
+    private static native void forward_4(long nativeObj, long outputBlobs_mat_nativeObj, List<String> outBlobNames);
+
+    // C++:  void cv::dnn::Net::getLayerTypes(vector_String& layersTypes)
+    private static native void getLayerTypes_0(long nativeObj, List<String> layersTypes);
+
+    // C++:  void cv::dnn::Net::getMemoryConsumption(MatShape netInputShape, size_t& weights, size_t& blobs)
+    private static native void getMemoryConsumption_0(long nativeObj, long netInputShape_mat_nativeObj, double[] weights_out, double[] blobs_out);
+
+    // C++:  void cv::dnn::Net::getMemoryConsumption(int layerId, MatShape netInputShape, size_t& weights, size_t& blobs)
+    private static native void getMemoryConsumption_1(long nativeObj, int layerId, long netInputShape_mat_nativeObj, double[] weights_out, double[] blobs_out);
+
+    // C++:  void cv::dnn::Net::getMemoryConsumption(int layerId, vector_MatShape netInputShapes, size_t& weights, size_t& blobs)
+    private static native void getMemoryConsumption_2(long nativeObj, int layerId, List<MatOfInt> netInputShapes, double[] weights_out, double[] blobs_out);
+
+    // C++:  void cv::dnn::Net::setHalideScheduler(String scheduler)
+    private static native void setHalideScheduler_0(long nativeObj, String scheduler);
+
+    // C++:  void cv::dnn::Net::setInput(Mat blob, String name = "", double scalefactor = 1.0, Scalar mean = Scalar())
+    private static native void setInput_0(long nativeObj, long blob_nativeObj, String name, double scalefactor, double mean_val0, double mean_val1, double mean_val2, double mean_val3);
+    private static native void setInput_1(long nativeObj, long blob_nativeObj, String name, double scalefactor);
+    private static native void setInput_2(long nativeObj, long blob_nativeObj, String name);
+    private static native void setInput_3(long nativeObj, long blob_nativeObj);
+
+    // C++:  void cv::dnn::Net::setInputShape(String inputName, MatShape shape)
+    private static native void setInputShape_0(long nativeObj, String inputName, long shape_mat_nativeObj);
+
+    // C++:  void cv::dnn::Net::setInputsNames(vector_String inputBlobNames)
+    private static native void setInputsNames_0(long nativeObj, List<String> inputBlobNames);
+
+    // C++:  void cv::dnn::Net::setParam(LayerId layer, int numParam, Mat blob)
+    private static native void setParam_0(long nativeObj, long layer_nativeObj, int numParam, long blob_nativeObj);
+
+    // C++:  void cv::dnn::Net::setPreferableBackend(int backendId)
+    private static native void setPreferableBackend_0(long nativeObj, int backendId);
+
+    // C++:  void cv::dnn::Net::setPreferableTarget(int targetId)
+    private static native void setPreferableTarget_0(long nativeObj, int targetId);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/ml/StatModel.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/ml/StatModel.java	(date 1605830247917)
+++ openCVLibrary3411/src/main/java/org/opencv/ml/StatModel.java	(date 1605830247917)
@@ -0,0 +1,223 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.ml;
+
+import org.opencv.core.Algorithm;
+import org.opencv.core.Mat;
+import org.opencv.ml.TrainData;
+
+// C++: class StatModel
+/**
+ * Base class for statistical models in OpenCV ML.
+ */
+public class StatModel extends Algorithm {
+
+    protected StatModel(long addr) { super(addr); }
+
+    // internal usage only
+    public static StatModel __fromPtr__(long addr) { return new StatModel(addr); }
+
+    // C++: enum Flags
+    public static final int
+            UPDATE_MODEL = 1,
+            RAW_OUTPUT = 1,
+            COMPRESSED_INPUT = 2,
+            PREPROCESSED_INPUT = 4;
+
+
+    //
+    // C++:  bool cv::ml::StatModel::empty()
+    //
+
+    public boolean empty() {
+        return empty_0(nativeObj);
+    }
+
+
+    //
+    // C++:  bool cv::ml::StatModel::isClassifier()
+    //
+
+    /**
+     * Returns true if the model is classifier
+     * @return automatically generated
+     */
+    public boolean isClassifier() {
+        return isClassifier_0(nativeObj);
+    }
+
+
+    //
+    // C++:  bool cv::ml::StatModel::isTrained()
+    //
+
+    /**
+     * Returns true if the model is trained
+     * @return automatically generated
+     */
+    public boolean isTrained() {
+        return isTrained_0(nativeObj);
+    }
+
+
+    //
+    // C++:  bool cv::ml::StatModel::train(Mat samples, int layout, Mat responses)
+    //
+
+    /**
+     * Trains the statistical model
+     *
+     *     @param samples training samples
+     *     @param layout See ml::SampleTypes.
+     *     @param responses vector of responses associated with the training samples.
+     * @return automatically generated
+     */
+    public boolean train(Mat samples, int layout, Mat responses) {
+        return train_0(nativeObj, samples.nativeObj, layout, responses.nativeObj);
+    }
+
+
+    //
+    // C++:  bool cv::ml::StatModel::train(Ptr_TrainData trainData, int flags = 0)
+    //
+
+    /**
+     * Trains the statistical model
+     *
+     *     @param trainData training data that can be loaded from file using TrainData::loadFromCSV or
+     *         created with TrainData::create.
+     *     @param flags optional flags, depending on the model. Some of the models can be updated with the
+     *         new training samples, not completely overwritten (such as NormalBayesClassifier or ANN_MLP).
+     * @return automatically generated
+     */
+    public boolean train(TrainData trainData, int flags) {
+        return train_1(nativeObj, trainData.getNativeObjAddr(), flags);
+    }
+
+    /**
+     * Trains the statistical model
+     *
+     *     @param trainData training data that can be loaded from file using TrainData::loadFromCSV or
+     *         created with TrainData::create.
+     *         new training samples, not completely overwritten (such as NormalBayesClassifier or ANN_MLP).
+     * @return automatically generated
+     */
+    public boolean train(TrainData trainData) {
+        return train_2(nativeObj, trainData.getNativeObjAddr());
+    }
+
+
+    //
+    // C++:  float cv::ml::StatModel::calcError(Ptr_TrainData data, bool test, Mat& resp)
+    //
+
+    /**
+     * Computes error on the training or test dataset
+     *
+     *     @param data the training data
+     *     @param test if true, the error is computed over the test subset of the data, otherwise it's
+     *         computed over the training subset of the data. Please note that if you loaded a completely
+     *         different dataset to evaluate already trained classifier, you will probably want not to set
+     *         the test subset at all with TrainData::setTrainTestSplitRatio and specify test=false, so
+     *         that the error is computed for the whole new set. Yes, this sounds a bit confusing.
+     *     @param resp the optional output responses.
+     *
+     *     The method uses StatModel::predict to compute the error. For regression models the error is
+     *     computed as RMS, for classifiers - as a percent of missclassified samples (0%-100%).
+     * @return automatically generated
+     */
+    public float calcError(TrainData data, boolean test, Mat resp) {
+        return calcError_0(nativeObj, data.getNativeObjAddr(), test, resp.nativeObj);
+    }
+
+
+    //
+    // C++:  float cv::ml::StatModel::predict(Mat samples, Mat& results = Mat(), int flags = 0)
+    //
+
+    /**
+     * Predicts response(s) for the provided sample(s)
+     *
+     *     @param samples The input samples, floating-point matrix
+     *     @param results The optional output matrix of results.
+     *     @param flags The optional flags, model-dependent. See cv::ml::StatModel::Flags.
+     * @return automatically generated
+     */
+    public float predict(Mat samples, Mat results, int flags) {
+        return predict_0(nativeObj, samples.nativeObj, results.nativeObj, flags);
+    }
+
+    /**
+     * Predicts response(s) for the provided sample(s)
+     *
+     *     @param samples The input samples, floating-point matrix
+     *     @param results The optional output matrix of results.
+     * @return automatically generated
+     */
+    public float predict(Mat samples, Mat results) {
+        return predict_1(nativeObj, samples.nativeObj, results.nativeObj);
+    }
+
+    /**
+     * Predicts response(s) for the provided sample(s)
+     *
+     *     @param samples The input samples, floating-point matrix
+     * @return automatically generated
+     */
+    public float predict(Mat samples) {
+        return predict_2(nativeObj, samples.nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::ml::StatModel::getVarCount()
+    //
+
+    /**
+     * Returns the number of variables in training samples
+     * @return automatically generated
+     */
+    public int getVarCount() {
+        return getVarCount_0(nativeObj);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++:  bool cv::ml::StatModel::empty()
+    private static native boolean empty_0(long nativeObj);
+
+    // C++:  bool cv::ml::StatModel::isClassifier()
+    private static native boolean isClassifier_0(long nativeObj);
+
+    // C++:  bool cv::ml::StatModel::isTrained()
+    private static native boolean isTrained_0(long nativeObj);
+
+    // C++:  bool cv::ml::StatModel::train(Mat samples, int layout, Mat responses)
+    private static native boolean train_0(long nativeObj, long samples_nativeObj, int layout, long responses_nativeObj);
+
+    // C++:  bool cv::ml::StatModel::train(Ptr_TrainData trainData, int flags = 0)
+    private static native boolean train_1(long nativeObj, long trainData_nativeObj, int flags);
+    private static native boolean train_2(long nativeObj, long trainData_nativeObj);
+
+    // C++:  float cv::ml::StatModel::calcError(Ptr_TrainData data, bool test, Mat& resp)
+    private static native float calcError_0(long nativeObj, long data_nativeObj, boolean test, long resp_nativeObj);
+
+    // C++:  float cv::ml::StatModel::predict(Mat samples, Mat& results = Mat(), int flags = 0)
+    private static native float predict_0(long nativeObj, long samples_nativeObj, long results_nativeObj, int flags);
+    private static native float predict_1(long nativeObj, long samples_nativeObj, long results_nativeObj);
+    private static native float predict_2(long nativeObj, long samples_nativeObj);
+
+    // C++:  int cv::ml::StatModel::getVarCount()
+    private static native int getVarCount_0(long nativeObj);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/ml/SVM.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/ml/SVM.java	(date 1605830247935)
+++ openCVLibrary3411/src/main/java/org/opencv/ml/SVM.java	(date 1605830247935)
@@ -0,0 +1,802 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.ml;
+
+import org.opencv.core.Mat;
+import org.opencv.core.TermCriteria;
+import org.opencv.ml.ParamGrid;
+import org.opencv.ml.SVM;
+import org.opencv.ml.StatModel;
+
+// C++: class SVM
+/**
+ * Support Vector Machines.
+ *
+ * SEE: REF: ml_intro_svm
+ */
+public class SVM extends StatModel {
+
+    protected SVM(long addr) { super(addr); }
+
+    // internal usage only
+    public static SVM __fromPtr__(long addr) { return new SVM(addr); }
+
+    // C++: enum KernelTypes
+    public static final int
+            CUSTOM = -1,
+            LINEAR = 0,
+            POLY = 1,
+            RBF = 2,
+            SIGMOID = 3,
+            CHI2 = 4,
+            INTER = 5;
+
+
+    // C++: enum Types
+    public static final int
+            C_SVC = 100,
+            NU_SVC = 101,
+            ONE_CLASS = 102,
+            EPS_SVR = 103,
+            NU_SVR = 104;
+
+
+    // C++: enum ParamTypes
+    public static final int
+            C = 0,
+            GAMMA = 1,
+            P = 2,
+            NU = 3,
+            COEF = 4,
+            DEGREE = 5;
+
+
+    //
+    // C++:  Mat cv::ml::SVM::getClassWeights()
+    //
+
+    /**
+     * SEE: setClassWeights
+     * @return automatically generated
+     */
+    public Mat getClassWeights() {
+        return new Mat(getClassWeights_0(nativeObj));
+    }
+
+
+    //
+    // C++:  Mat cv::ml::SVM::getSupportVectors()
+    //
+
+    /**
+     * Retrieves all the support vectors
+     *
+     *     The method returns all the support vectors as a floating-point matrix, where support vectors are
+     *     stored as matrix rows.
+     * @return automatically generated
+     */
+    public Mat getSupportVectors() {
+        return new Mat(getSupportVectors_0(nativeObj));
+    }
+
+
+    //
+    // C++:  Mat cv::ml::SVM::getUncompressedSupportVectors()
+    //
+
+    /**
+     * Retrieves all the uncompressed support vectors of a linear %SVM
+     *
+     *     The method returns all the uncompressed support vectors of a linear %SVM that the compressed
+     *     support vector, used for prediction, was derived from. They are returned in a floating-point
+     *     matrix, where the support vectors are stored as matrix rows.
+     * @return automatically generated
+     */
+    public Mat getUncompressedSupportVectors() {
+        return new Mat(getUncompressedSupportVectors_0(nativeObj));
+    }
+
+
+    //
+    // C++: static Ptr_ParamGrid cv::ml::SVM::getDefaultGridPtr(int param_id)
+    //
+
+    /**
+     * Generates a grid for %SVM parameters.
+     *
+     *     @param param_id %SVM parameters IDs that must be one of the SVM::ParamTypes. The grid is
+     *     generated for the parameter with this ID.
+     *
+     *     The function generates a grid pointer for the specified parameter of the %SVM algorithm.
+     *     The grid may be passed to the function SVM::trainAuto.
+     * @return automatically generated
+     */
+    public static ParamGrid getDefaultGridPtr(int param_id) {
+        return ParamGrid.__fromPtr__(getDefaultGridPtr_0(param_id));
+    }
+
+
+    //
+    // C++: static Ptr_SVM cv::ml::SVM::create()
+    //
+
+    /**
+     * Creates empty model.
+     *     Use StatModel::train to train the model. Since %SVM has several parameters, you may want to
+     * find the best parameters for your problem, it can be done with SVM::trainAuto.
+     * @return automatically generated
+     */
+    public static SVM create() {
+        return SVM.__fromPtr__(create_0());
+    }
+
+
+    //
+    // C++: static Ptr_SVM cv::ml::SVM::load(String filepath)
+    //
+
+    /**
+     * Loads and creates a serialized svm from a file
+     *
+     * Use SVM::save to serialize and store an SVM to disk.
+     * Load the SVM from this file again, by calling this function with the path to the file.
+     *
+     * @param filepath path to serialized svm
+     * @return automatically generated
+     */
+    public static SVM load(String filepath) {
+        return SVM.__fromPtr__(load_0(filepath));
+    }
+
+
+    //
+    // C++:  TermCriteria cv::ml::SVM::getTermCriteria()
+    //
+
+    /**
+     * SEE: setTermCriteria
+     * @return automatically generated
+     */
+    public TermCriteria getTermCriteria() {
+        return new TermCriteria(getTermCriteria_0(nativeObj));
+    }
+
+
+    //
+    // C++:  bool cv::ml::SVM::trainAuto(Mat samples, int layout, Mat responses, int kFold = 10, Ptr_ParamGrid Cgrid = SVM::getDefaultGridPtr(SVM::C), Ptr_ParamGrid gammaGrid = SVM::getDefaultGridPtr(SVM::GAMMA), Ptr_ParamGrid pGrid = SVM::getDefaultGridPtr(SVM::P), Ptr_ParamGrid nuGrid = SVM::getDefaultGridPtr(SVM::NU), Ptr_ParamGrid coeffGrid = SVM::getDefaultGridPtr(SVM::COEF), Ptr_ParamGrid degreeGrid = SVM::getDefaultGridPtr(SVM::DEGREE), bool balanced = false)
+    //
+
+    /**
+     * Trains an %SVM with optimal parameters
+     *
+     *     @param samples training samples
+     *     @param layout See ml::SampleTypes.
+     *     @param responses vector of responses associated with the training samples.
+     *     @param kFold Cross-validation parameter. The training set is divided into kFold subsets. One
+     *         subset is used to test the model, the others form the train set. So, the %SVM algorithm is
+     *     @param Cgrid grid for C
+     *     @param gammaGrid grid for gamma
+     *     @param pGrid grid for p
+     *     @param nuGrid grid for nu
+     *     @param coeffGrid grid for coeff
+     *     @param degreeGrid grid for degree
+     *     @param balanced If true and the problem is 2-class classification then the method creates more
+     *         balanced cross-validation subsets that is proportions between classes in subsets are close
+     *         to such proportion in the whole train dataset.
+     *
+     *     The method trains the %SVM model automatically by choosing the optimal parameters C, gamma, p,
+     *     nu, coef0, degree. Parameters are considered optimal when the cross-validation
+     *     estimate of the test set error is minimal.
+     *
+     *     This function only makes use of SVM::getDefaultGrid for parameter optimization and thus only
+     *     offers rudimentary parameter options.
+     *
+     *     This function works for the classification (SVM::C_SVC or SVM::NU_SVC) as well as for the
+     *     regression (SVM::EPS_SVR or SVM::NU_SVR). If it is SVM::ONE_CLASS, no optimization is made and
+     *     the usual %SVM with parameters specified in params is executed.
+     * @return automatically generated
+     */
+    public boolean trainAuto(Mat samples, int layout, Mat responses, int kFold, ParamGrid Cgrid, ParamGrid gammaGrid, ParamGrid pGrid, ParamGrid nuGrid, ParamGrid coeffGrid, ParamGrid degreeGrid, boolean balanced) {
+        return trainAuto_0(nativeObj, samples.nativeObj, layout, responses.nativeObj, kFold, Cgrid.getNativeObjAddr(), gammaGrid.getNativeObjAddr(), pGrid.getNativeObjAddr(), nuGrid.getNativeObjAddr(), coeffGrid.getNativeObjAddr(), degreeGrid.getNativeObjAddr(), balanced);
+    }
+
+    /**
+     * Trains an %SVM with optimal parameters
+     *
+     *     @param samples training samples
+     *     @param layout See ml::SampleTypes.
+     *     @param responses vector of responses associated with the training samples.
+     *     @param kFold Cross-validation parameter. The training set is divided into kFold subsets. One
+     *         subset is used to test the model, the others form the train set. So, the %SVM algorithm is
+     *     @param Cgrid grid for C
+     *     @param gammaGrid grid for gamma
+     *     @param pGrid grid for p
+     *     @param nuGrid grid for nu
+     *     @param coeffGrid grid for coeff
+     *     @param degreeGrid grid for degree
+     *         balanced cross-validation subsets that is proportions between classes in subsets are close
+     *         to such proportion in the whole train dataset.
+     *
+     *     The method trains the %SVM model automatically by choosing the optimal parameters C, gamma, p,
+     *     nu, coef0, degree. Parameters are considered optimal when the cross-validation
+     *     estimate of the test set error is minimal.
+     *
+     *     This function only makes use of SVM::getDefaultGrid for parameter optimization and thus only
+     *     offers rudimentary parameter options.
+     *
+     *     This function works for the classification (SVM::C_SVC or SVM::NU_SVC) as well as for the
+     *     regression (SVM::EPS_SVR or SVM::NU_SVR). If it is SVM::ONE_CLASS, no optimization is made and
+     *     the usual %SVM with parameters specified in params is executed.
+     * @return automatically generated
+     */
+    public boolean trainAuto(Mat samples, int layout, Mat responses, int kFold, ParamGrid Cgrid, ParamGrid gammaGrid, ParamGrid pGrid, ParamGrid nuGrid, ParamGrid coeffGrid, ParamGrid degreeGrid) {
+        return trainAuto_1(nativeObj, samples.nativeObj, layout, responses.nativeObj, kFold, Cgrid.getNativeObjAddr(), gammaGrid.getNativeObjAddr(), pGrid.getNativeObjAddr(), nuGrid.getNativeObjAddr(), coeffGrid.getNativeObjAddr(), degreeGrid.getNativeObjAddr());
+    }
+
+    /**
+     * Trains an %SVM with optimal parameters
+     *
+     *     @param samples training samples
+     *     @param layout See ml::SampleTypes.
+     *     @param responses vector of responses associated with the training samples.
+     *     @param kFold Cross-validation parameter. The training set is divided into kFold subsets. One
+     *         subset is used to test the model, the others form the train set. So, the %SVM algorithm is
+     *     @param Cgrid grid for C
+     *     @param gammaGrid grid for gamma
+     *     @param pGrid grid for p
+     *     @param nuGrid grid for nu
+     *     @param coeffGrid grid for coeff
+     *         balanced cross-validation subsets that is proportions between classes in subsets are close
+     *         to such proportion in the whole train dataset.
+     *
+     *     The method trains the %SVM model automatically by choosing the optimal parameters C, gamma, p,
+     *     nu, coef0, degree. Parameters are considered optimal when the cross-validation
+     *     estimate of the test set error is minimal.
+     *
+     *     This function only makes use of SVM::getDefaultGrid for parameter optimization and thus only
+     *     offers rudimentary parameter options.
+     *
+     *     This function works for the classification (SVM::C_SVC or SVM::NU_SVC) as well as for the
+     *     regression (SVM::EPS_SVR or SVM::NU_SVR). If it is SVM::ONE_CLASS, no optimization is made and
+     *     the usual %SVM with parameters specified in params is executed.
+     * @return automatically generated
+     */
+    public boolean trainAuto(Mat samples, int layout, Mat responses, int kFold, ParamGrid Cgrid, ParamGrid gammaGrid, ParamGrid pGrid, ParamGrid nuGrid, ParamGrid coeffGrid) {
+        return trainAuto_2(nativeObj, samples.nativeObj, layout, responses.nativeObj, kFold, Cgrid.getNativeObjAddr(), gammaGrid.getNativeObjAddr(), pGrid.getNativeObjAddr(), nuGrid.getNativeObjAddr(), coeffGrid.getNativeObjAddr());
+    }
+
+    /**
+     * Trains an %SVM with optimal parameters
+     *
+     *     @param samples training samples
+     *     @param layout See ml::SampleTypes.
+     *     @param responses vector of responses associated with the training samples.
+     *     @param kFold Cross-validation parameter. The training set is divided into kFold subsets. One
+     *         subset is used to test the model, the others form the train set. So, the %SVM algorithm is
+     *     @param Cgrid grid for C
+     *     @param gammaGrid grid for gamma
+     *     @param pGrid grid for p
+     *     @param nuGrid grid for nu
+     *         balanced cross-validation subsets that is proportions between classes in subsets are close
+     *         to such proportion in the whole train dataset.
+     *
+     *     The method trains the %SVM model automatically by choosing the optimal parameters C, gamma, p,
+     *     nu, coef0, degree. Parameters are considered optimal when the cross-validation
+     *     estimate of the test set error is minimal.
+     *
+     *     This function only makes use of SVM::getDefaultGrid for parameter optimization and thus only
+     *     offers rudimentary parameter options.
+     *
+     *     This function works for the classification (SVM::C_SVC or SVM::NU_SVC) as well as for the
+     *     regression (SVM::EPS_SVR or SVM::NU_SVR). If it is SVM::ONE_CLASS, no optimization is made and
+     *     the usual %SVM with parameters specified in params is executed.
+     * @return automatically generated
+     */
+    public boolean trainAuto(Mat samples, int layout, Mat responses, int kFold, ParamGrid Cgrid, ParamGrid gammaGrid, ParamGrid pGrid, ParamGrid nuGrid) {
+        return trainAuto_3(nativeObj, samples.nativeObj, layout, responses.nativeObj, kFold, Cgrid.getNativeObjAddr(), gammaGrid.getNativeObjAddr(), pGrid.getNativeObjAddr(), nuGrid.getNativeObjAddr());
+    }
+
+    /**
+     * Trains an %SVM with optimal parameters
+     *
+     *     @param samples training samples
+     *     @param layout See ml::SampleTypes.
+     *     @param responses vector of responses associated with the training samples.
+     *     @param kFold Cross-validation parameter. The training set is divided into kFold subsets. One
+     *         subset is used to test the model, the others form the train set. So, the %SVM algorithm is
+     *     @param Cgrid grid for C
+     *     @param gammaGrid grid for gamma
+     *     @param pGrid grid for p
+     *         balanced cross-validation subsets that is proportions between classes in subsets are close
+     *         to such proportion in the whole train dataset.
+     *
+     *     The method trains the %SVM model automatically by choosing the optimal parameters C, gamma, p,
+     *     nu, coef0, degree. Parameters are considered optimal when the cross-validation
+     *     estimate of the test set error is minimal.
+     *
+     *     This function only makes use of SVM::getDefaultGrid for parameter optimization and thus only
+     *     offers rudimentary parameter options.
+     *
+     *     This function works for the classification (SVM::C_SVC or SVM::NU_SVC) as well as for the
+     *     regression (SVM::EPS_SVR or SVM::NU_SVR). If it is SVM::ONE_CLASS, no optimization is made and
+     *     the usual %SVM with parameters specified in params is executed.
+     * @return automatically generated
+     */
+    public boolean trainAuto(Mat samples, int layout, Mat responses, int kFold, ParamGrid Cgrid, ParamGrid gammaGrid, ParamGrid pGrid) {
+        return trainAuto_4(nativeObj, samples.nativeObj, layout, responses.nativeObj, kFold, Cgrid.getNativeObjAddr(), gammaGrid.getNativeObjAddr(), pGrid.getNativeObjAddr());
+    }
+
+    /**
+     * Trains an %SVM with optimal parameters
+     *
+     *     @param samples training samples
+     *     @param layout See ml::SampleTypes.
+     *     @param responses vector of responses associated with the training samples.
+     *     @param kFold Cross-validation parameter. The training set is divided into kFold subsets. One
+     *         subset is used to test the model, the others form the train set. So, the %SVM algorithm is
+     *     @param Cgrid grid for C
+     *     @param gammaGrid grid for gamma
+     *         balanced cross-validation subsets that is proportions between classes in subsets are close
+     *         to such proportion in the whole train dataset.
+     *
+     *     The method trains the %SVM model automatically by choosing the optimal parameters C, gamma, p,
+     *     nu, coef0, degree. Parameters are considered optimal when the cross-validation
+     *     estimate of the test set error is minimal.
+     *
+     *     This function only makes use of SVM::getDefaultGrid for parameter optimization and thus only
+     *     offers rudimentary parameter options.
+     *
+     *     This function works for the classification (SVM::C_SVC or SVM::NU_SVC) as well as for the
+     *     regression (SVM::EPS_SVR or SVM::NU_SVR). If it is SVM::ONE_CLASS, no optimization is made and
+     *     the usual %SVM with parameters specified in params is executed.
+     * @return automatically generated
+     */
+    public boolean trainAuto(Mat samples, int layout, Mat responses, int kFold, ParamGrid Cgrid, ParamGrid gammaGrid) {
+        return trainAuto_5(nativeObj, samples.nativeObj, layout, responses.nativeObj, kFold, Cgrid.getNativeObjAddr(), gammaGrid.getNativeObjAddr());
+    }
+
+    /**
+     * Trains an %SVM with optimal parameters
+     *
+     *     @param samples training samples
+     *     @param layout See ml::SampleTypes.
+     *     @param responses vector of responses associated with the training samples.
+     *     @param kFold Cross-validation parameter. The training set is divided into kFold subsets. One
+     *         subset is used to test the model, the others form the train set. So, the %SVM algorithm is
+     *     @param Cgrid grid for C
+     *         balanced cross-validation subsets that is proportions between classes in subsets are close
+     *         to such proportion in the whole train dataset.
+     *
+     *     The method trains the %SVM model automatically by choosing the optimal parameters C, gamma, p,
+     *     nu, coef0, degree. Parameters are considered optimal when the cross-validation
+     *     estimate of the test set error is minimal.
+     *
+     *     This function only makes use of SVM::getDefaultGrid for parameter optimization and thus only
+     *     offers rudimentary parameter options.
+     *
+     *     This function works for the classification (SVM::C_SVC or SVM::NU_SVC) as well as for the
+     *     regression (SVM::EPS_SVR or SVM::NU_SVR). If it is SVM::ONE_CLASS, no optimization is made and
+     *     the usual %SVM with parameters specified in params is executed.
+     * @return automatically generated
+     */
+    public boolean trainAuto(Mat samples, int layout, Mat responses, int kFold, ParamGrid Cgrid) {
+        return trainAuto_6(nativeObj, samples.nativeObj, layout, responses.nativeObj, kFold, Cgrid.getNativeObjAddr());
+    }
+
+    /**
+     * Trains an %SVM with optimal parameters
+     *
+     *     @param samples training samples
+     *     @param layout See ml::SampleTypes.
+     *     @param responses vector of responses associated with the training samples.
+     *     @param kFold Cross-validation parameter. The training set is divided into kFold subsets. One
+     *         subset is used to test the model, the others form the train set. So, the %SVM algorithm is
+     *         balanced cross-validation subsets that is proportions between classes in subsets are close
+     *         to such proportion in the whole train dataset.
+     *
+     *     The method trains the %SVM model automatically by choosing the optimal parameters C, gamma, p,
+     *     nu, coef0, degree. Parameters are considered optimal when the cross-validation
+     *     estimate of the test set error is minimal.
+     *
+     *     This function only makes use of SVM::getDefaultGrid for parameter optimization and thus only
+     *     offers rudimentary parameter options.
+     *
+     *     This function works for the classification (SVM::C_SVC or SVM::NU_SVC) as well as for the
+     *     regression (SVM::EPS_SVR or SVM::NU_SVR). If it is SVM::ONE_CLASS, no optimization is made and
+     *     the usual %SVM with parameters specified in params is executed.
+     * @return automatically generated
+     */
+    public boolean trainAuto(Mat samples, int layout, Mat responses, int kFold) {
+        return trainAuto_7(nativeObj, samples.nativeObj, layout, responses.nativeObj, kFold);
+    }
+
+    /**
+     * Trains an %SVM with optimal parameters
+     *
+     *     @param samples training samples
+     *     @param layout See ml::SampleTypes.
+     *     @param responses vector of responses associated with the training samples.
+     *         subset is used to test the model, the others form the train set. So, the %SVM algorithm is
+     *         balanced cross-validation subsets that is proportions between classes in subsets are close
+     *         to such proportion in the whole train dataset.
+     *
+     *     The method trains the %SVM model automatically by choosing the optimal parameters C, gamma, p,
+     *     nu, coef0, degree. Parameters are considered optimal when the cross-validation
+     *     estimate of the test set error is minimal.
+     *
+     *     This function only makes use of SVM::getDefaultGrid for parameter optimization and thus only
+     *     offers rudimentary parameter options.
+     *
+     *     This function works for the classification (SVM::C_SVC or SVM::NU_SVC) as well as for the
+     *     regression (SVM::EPS_SVR or SVM::NU_SVR). If it is SVM::ONE_CLASS, no optimization is made and
+     *     the usual %SVM with parameters specified in params is executed.
+     * @return automatically generated
+     */
+    public boolean trainAuto(Mat samples, int layout, Mat responses) {
+        return trainAuto_8(nativeObj, samples.nativeObj, layout, responses.nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::ml::SVM::getC()
+    //
+
+    /**
+     * SEE: setC
+     * @return automatically generated
+     */
+    public double getC() {
+        return getC_0(nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::ml::SVM::getCoef0()
+    //
+
+    /**
+     * SEE: setCoef0
+     * @return automatically generated
+     */
+    public double getCoef0() {
+        return getCoef0_0(nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::ml::SVM::getDecisionFunction(int i, Mat& alpha, Mat& svidx)
+    //
+
+    /**
+     * Retrieves the decision function
+     *
+     *     @param i the index of the decision function. If the problem solved is regression, 1-class or
+     *         2-class classification, then there will be just one decision function and the index should
+     *         always be 0. Otherwise, in the case of N-class classification, there will be \(N(N-1)/2\)
+     *         decision functions.
+     *     @param alpha the optional output vector for weights, corresponding to different support vectors.
+     *         In the case of linear %SVM all the alpha's will be 1's.
+     *     @param svidx the optional output vector of indices of support vectors within the matrix of
+     *         support vectors (which can be retrieved by SVM::getSupportVectors). In the case of linear
+     *         %SVM each decision function consists of a single "compressed" support vector.
+     *
+     *     The method returns rho parameter of the decision function, a scalar subtracted from the weighted
+     *     sum of kernel responses.
+     * @return automatically generated
+     */
+    public double getDecisionFunction(int i, Mat alpha, Mat svidx) {
+        return getDecisionFunction_0(nativeObj, i, alpha.nativeObj, svidx.nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::ml::SVM::getDegree()
+    //
+
+    /**
+     * SEE: setDegree
+     * @return automatically generated
+     */
+    public double getDegree() {
+        return getDegree_0(nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::ml::SVM::getGamma()
+    //
+
+    /**
+     * SEE: setGamma
+     * @return automatically generated
+     */
+    public double getGamma() {
+        return getGamma_0(nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::ml::SVM::getNu()
+    //
+
+    /**
+     * SEE: setNu
+     * @return automatically generated
+     */
+    public double getNu() {
+        return getNu_0(nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::ml::SVM::getP()
+    //
+
+    /**
+     * SEE: setP
+     * @return automatically generated
+     */
+    public double getP() {
+        return getP_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::ml::SVM::getKernelType()
+    //
+
+    /**
+     * Type of a %SVM kernel.
+     * See SVM::KernelTypes. Default value is SVM::RBF.
+     * @return automatically generated
+     */
+    public int getKernelType() {
+        return getKernelType_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::ml::SVM::getType()
+    //
+
+    /**
+     * SEE: setType
+     * @return automatically generated
+     */
+    public int getType() {
+        return getType_0(nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::ml::SVM::setC(double val)
+    //
+
+    /**
+     *  getC SEE: getC
+     * @param val automatically generated
+     */
+    public void setC(double val) {
+        setC_0(nativeObj, val);
+    }
+
+
+    //
+    // C++:  void cv::ml::SVM::setClassWeights(Mat val)
+    //
+
+    /**
+     *  getClassWeights SEE: getClassWeights
+     * @param val automatically generated
+     */
+    public void setClassWeights(Mat val) {
+        setClassWeights_0(nativeObj, val.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::ml::SVM::setCoef0(double val)
+    //
+
+    /**
+     *  getCoef0 SEE: getCoef0
+     * @param val automatically generated
+     */
+    public void setCoef0(double val) {
+        setCoef0_0(nativeObj, val);
+    }
+
+
+    //
+    // C++:  void cv::ml::SVM::setDegree(double val)
+    //
+
+    /**
+     *  getDegree SEE: getDegree
+     * @param val automatically generated
+     */
+    public void setDegree(double val) {
+        setDegree_0(nativeObj, val);
+    }
+
+
+    //
+    // C++:  void cv::ml::SVM::setGamma(double val)
+    //
+
+    /**
+     *  getGamma SEE: getGamma
+     * @param val automatically generated
+     */
+    public void setGamma(double val) {
+        setGamma_0(nativeObj, val);
+    }
+
+
+    //
+    // C++:  void cv::ml::SVM::setKernel(int kernelType)
+    //
+
+    /**
+     * Initialize with one of predefined kernels.
+     * See SVM::KernelTypes.
+     * @param kernelType automatically generated
+     */
+    public void setKernel(int kernelType) {
+        setKernel_0(nativeObj, kernelType);
+    }
+
+
+    //
+    // C++:  void cv::ml::SVM::setNu(double val)
+    //
+
+    /**
+     *  getNu SEE: getNu
+     * @param val automatically generated
+     */
+    public void setNu(double val) {
+        setNu_0(nativeObj, val);
+    }
+
+
+    //
+    // C++:  void cv::ml::SVM::setP(double val)
+    //
+
+    /**
+     *  getP SEE: getP
+     * @param val automatically generated
+     */
+    public void setP(double val) {
+        setP_0(nativeObj, val);
+    }
+
+
+    //
+    // C++:  void cv::ml::SVM::setTermCriteria(TermCriteria val)
+    //
+
+    /**
+     *  getTermCriteria SEE: getTermCriteria
+     * @param val automatically generated
+     */
+    public void setTermCriteria(TermCriteria val) {
+        setTermCriteria_0(nativeObj, val.type, val.maxCount, val.epsilon);
+    }
+
+
+    //
+    // C++:  void cv::ml::SVM::setType(int val)
+    //
+
+    /**
+     *  getType SEE: getType
+     * @param val automatically generated
+     */
+    public void setType(int val) {
+        setType_0(nativeObj, val);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++:  Mat cv::ml::SVM::getClassWeights()
+    private static native long getClassWeights_0(long nativeObj);
+
+    // C++:  Mat cv::ml::SVM::getSupportVectors()
+    private static native long getSupportVectors_0(long nativeObj);
+
+    // C++:  Mat cv::ml::SVM::getUncompressedSupportVectors()
+    private static native long getUncompressedSupportVectors_0(long nativeObj);
+
+    // C++: static Ptr_ParamGrid cv::ml::SVM::getDefaultGridPtr(int param_id)
+    private static native long getDefaultGridPtr_0(int param_id);
+
+    // C++: static Ptr_SVM cv::ml::SVM::create()
+    private static native long create_0();
+
+    // C++: static Ptr_SVM cv::ml::SVM::load(String filepath)
+    private static native long load_0(String filepath);
+
+    // C++:  TermCriteria cv::ml::SVM::getTermCriteria()
+    private static native double[] getTermCriteria_0(long nativeObj);
+
+    // C++:  bool cv::ml::SVM::trainAuto(Mat samples, int layout, Mat responses, int kFold = 10, Ptr_ParamGrid Cgrid = SVM::getDefaultGridPtr(SVM::C), Ptr_ParamGrid gammaGrid = SVM::getDefaultGridPtr(SVM::GAMMA), Ptr_ParamGrid pGrid = SVM::getDefaultGridPtr(SVM::P), Ptr_ParamGrid nuGrid = SVM::getDefaultGridPtr(SVM::NU), Ptr_ParamGrid coeffGrid = SVM::getDefaultGridPtr(SVM::COEF), Ptr_ParamGrid degreeGrid = SVM::getDefaultGridPtr(SVM::DEGREE), bool balanced = false)
+    private static native boolean trainAuto_0(long nativeObj, long samples_nativeObj, int layout, long responses_nativeObj, int kFold, long Cgrid_nativeObj, long gammaGrid_nativeObj, long pGrid_nativeObj, long nuGrid_nativeObj, long coeffGrid_nativeObj, long degreeGrid_nativeObj, boolean balanced);
+    private static native boolean trainAuto_1(long nativeObj, long samples_nativeObj, int layout, long responses_nativeObj, int kFold, long Cgrid_nativeObj, long gammaGrid_nativeObj, long pGrid_nativeObj, long nuGrid_nativeObj, long coeffGrid_nativeObj, long degreeGrid_nativeObj);
+    private static native boolean trainAuto_2(long nativeObj, long samples_nativeObj, int layout, long responses_nativeObj, int kFold, long Cgrid_nativeObj, long gammaGrid_nativeObj, long pGrid_nativeObj, long nuGrid_nativeObj, long coeffGrid_nativeObj);
+    private static native boolean trainAuto_3(long nativeObj, long samples_nativeObj, int layout, long responses_nativeObj, int kFold, long Cgrid_nativeObj, long gammaGrid_nativeObj, long pGrid_nativeObj, long nuGrid_nativeObj);
+    private static native boolean trainAuto_4(long nativeObj, long samples_nativeObj, int layout, long responses_nativeObj, int kFold, long Cgrid_nativeObj, long gammaGrid_nativeObj, long pGrid_nativeObj);
+    private static native boolean trainAuto_5(long nativeObj, long samples_nativeObj, int layout, long responses_nativeObj, int kFold, long Cgrid_nativeObj, long gammaGrid_nativeObj);
+    private static native boolean trainAuto_6(long nativeObj, long samples_nativeObj, int layout, long responses_nativeObj, int kFold, long Cgrid_nativeObj);
+    private static native boolean trainAuto_7(long nativeObj, long samples_nativeObj, int layout, long responses_nativeObj, int kFold);
+    private static native boolean trainAuto_8(long nativeObj, long samples_nativeObj, int layout, long responses_nativeObj);
+
+    // C++:  double cv::ml::SVM::getC()
+    private static native double getC_0(long nativeObj);
+
+    // C++:  double cv::ml::SVM::getCoef0()
+    private static native double getCoef0_0(long nativeObj);
+
+    // C++:  double cv::ml::SVM::getDecisionFunction(int i, Mat& alpha, Mat& svidx)
+    private static native double getDecisionFunction_0(long nativeObj, int i, long alpha_nativeObj, long svidx_nativeObj);
+
+    // C++:  double cv::ml::SVM::getDegree()
+    private static native double getDegree_0(long nativeObj);
+
+    // C++:  double cv::ml::SVM::getGamma()
+    private static native double getGamma_0(long nativeObj);
+
+    // C++:  double cv::ml::SVM::getNu()
+    private static native double getNu_0(long nativeObj);
+
+    // C++:  double cv::ml::SVM::getP()
+    private static native double getP_0(long nativeObj);
+
+    // C++:  int cv::ml::SVM::getKernelType()
+    private static native int getKernelType_0(long nativeObj);
+
+    // C++:  int cv::ml::SVM::getType()
+    private static native int getType_0(long nativeObj);
+
+    // C++:  void cv::ml::SVM::setC(double val)
+    private static native void setC_0(long nativeObj, double val);
+
+    // C++:  void cv::ml::SVM::setClassWeights(Mat val)
+    private static native void setClassWeights_0(long nativeObj, long val_nativeObj);
+
+    // C++:  void cv::ml::SVM::setCoef0(double val)
+    private static native void setCoef0_0(long nativeObj, double val);
+
+    // C++:  void cv::ml::SVM::setDegree(double val)
+    private static native void setDegree_0(long nativeObj, double val);
+
+    // C++:  void cv::ml::SVM::setGamma(double val)
+    private static native void setGamma_0(long nativeObj, double val);
+
+    // C++:  void cv::ml::SVM::setKernel(int kernelType)
+    private static native void setKernel_0(long nativeObj, int kernelType);
+
+    // C++:  void cv::ml::SVM::setNu(double val)
+    private static native void setNu_0(long nativeObj, double val);
+
+    // C++:  void cv::ml::SVM::setP(double val)
+    private static native void setP_0(long nativeObj, double val);
+
+    // C++:  void cv::ml::SVM::setTermCriteria(TermCriteria val)
+    private static native void setTermCriteria_0(long nativeObj, int val_type, int val_maxCount, double val_epsilon);
+
+    // C++:  void cv::ml::SVM::setType(int val)
+    private static native void setType_0(long nativeObj, int val);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/ml/SVMSGD.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/ml/SVMSGD.java	(date 1605830247948)
+++ openCVLibrary3411/src/main/java/org/opencv/ml/SVMSGD.java	(date 1605830247948)
@@ -0,0 +1,358 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.ml;
+
+import org.opencv.core.Mat;
+import org.opencv.core.TermCriteria;
+import org.opencv.ml.SVMSGD;
+import org.opencv.ml.StatModel;
+
+// C++: class SVMSGD
+/**
+ * *************************************************************************************\
+ * Stochastic Gradient Descent SVM Classifier                      *
+ * \***************************************************************************************
+ */
+public class SVMSGD extends StatModel {
+
+    protected SVMSGD(long addr) { super(addr); }
+
+    // internal usage only
+    public static SVMSGD __fromPtr__(long addr) { return new SVMSGD(addr); }
+
+    // C++: enum SvmsgdType
+    public static final int
+            SGD = 0,
+            ASGD = 1;
+
+
+    // C++: enum MarginType
+    public static final int
+            SOFT_MARGIN = 0,
+            HARD_MARGIN = 1;
+
+
+    //
+    // C++:  Mat cv::ml::SVMSGD::getWeights()
+    //
+
+    /**
+     * @return the weights of the trained model (decision function f(x) = weights * x + shift).
+     */
+    public Mat getWeights() {
+        return new Mat(getWeights_0(nativeObj));
+    }
+
+
+    //
+    // C++: static Ptr_SVMSGD cv::ml::SVMSGD::create()
+    //
+
+    /**
+     * Creates empty model.
+     * Use StatModel::train to train the model. Since %SVMSGD has several parameters, you may want to
+     * find the best parameters for your problem or use setOptimalParameters() to set some default parameters.
+     * @return automatically generated
+     */
+    public static SVMSGD create() {
+        return SVMSGD.__fromPtr__(create_0());
+    }
+
+
+    //
+    // C++: static Ptr_SVMSGD cv::ml::SVMSGD::load(String filepath, String nodeName = String())
+    //
+
+    /**
+     * Loads and creates a serialized SVMSGD from a file
+     *
+     * Use SVMSGD::save to serialize and store an SVMSGD to disk.
+     * Load the SVMSGD from this file again, by calling this function with the path to the file.
+     * Optionally specify the node for the file containing the classifier
+     *
+     * @param filepath path to serialized SVMSGD
+     * @param nodeName name of node containing the classifier
+     * @return automatically generated
+     */
+    public static SVMSGD load(String filepath, String nodeName) {
+        return SVMSGD.__fromPtr__(load_0(filepath, nodeName));
+    }
+
+    /**
+     * Loads and creates a serialized SVMSGD from a file
+     *
+     * Use SVMSGD::save to serialize and store an SVMSGD to disk.
+     * Load the SVMSGD from this file again, by calling this function with the path to the file.
+     * Optionally specify the node for the file containing the classifier
+     *
+     * @param filepath path to serialized SVMSGD
+     * @return automatically generated
+     */
+    public static SVMSGD load(String filepath) {
+        return SVMSGD.__fromPtr__(load_1(filepath));
+    }
+
+
+    //
+    // C++:  TermCriteria cv::ml::SVMSGD::getTermCriteria()
+    //
+
+    /**
+     * SEE: setTermCriteria
+     * @return automatically generated
+     */
+    public TermCriteria getTermCriteria() {
+        return new TermCriteria(getTermCriteria_0(nativeObj));
+    }
+
+
+    //
+    // C++:  float cv::ml::SVMSGD::getInitialStepSize()
+    //
+
+    /**
+     * SEE: setInitialStepSize
+     * @return automatically generated
+     */
+    public float getInitialStepSize() {
+        return getInitialStepSize_0(nativeObj);
+    }
+
+
+    //
+    // C++:  float cv::ml::SVMSGD::getMarginRegularization()
+    //
+
+    /**
+     * SEE: setMarginRegularization
+     * @return automatically generated
+     */
+    public float getMarginRegularization() {
+        return getMarginRegularization_0(nativeObj);
+    }
+
+
+    //
+    // C++:  float cv::ml::SVMSGD::getShift()
+    //
+
+    /**
+     * @return the shift of the trained model (decision function f(x) = weights * x + shift).
+     */
+    public float getShift() {
+        return getShift_0(nativeObj);
+    }
+
+
+    //
+    // C++:  float cv::ml::SVMSGD::getStepDecreasingPower()
+    //
+
+    /**
+     * SEE: setStepDecreasingPower
+     * @return automatically generated
+     */
+    public float getStepDecreasingPower() {
+        return getStepDecreasingPower_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::ml::SVMSGD::getMarginType()
+    //
+
+    /**
+     * SEE: setMarginType
+     * @return automatically generated
+     */
+    public int getMarginType() {
+        return getMarginType_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::ml::SVMSGD::getSvmsgdType()
+    //
+
+    /**
+     * SEE: setSvmsgdType
+     * @return automatically generated
+     */
+    public int getSvmsgdType() {
+        return getSvmsgdType_0(nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::ml::SVMSGD::setInitialStepSize(float InitialStepSize)
+    //
+
+    /**
+     *  getInitialStepSize SEE: getInitialStepSize
+     * @param InitialStepSize automatically generated
+     */
+    public void setInitialStepSize(float InitialStepSize) {
+        setInitialStepSize_0(nativeObj, InitialStepSize);
+    }
+
+
+    //
+    // C++:  void cv::ml::SVMSGD::setMarginRegularization(float marginRegularization)
+    //
+
+    /**
+     *  getMarginRegularization SEE: getMarginRegularization
+     * @param marginRegularization automatically generated
+     */
+    public void setMarginRegularization(float marginRegularization) {
+        setMarginRegularization_0(nativeObj, marginRegularization);
+    }
+
+
+    //
+    // C++:  void cv::ml::SVMSGD::setMarginType(int marginType)
+    //
+
+    /**
+     *  getMarginType SEE: getMarginType
+     * @param marginType automatically generated
+     */
+    public void setMarginType(int marginType) {
+        setMarginType_0(nativeObj, marginType);
+    }
+
+
+    //
+    // C++:  void cv::ml::SVMSGD::setOptimalParameters(int svmsgdType = SVMSGD::ASGD, int marginType = SVMSGD::SOFT_MARGIN)
+    //
+
+    /**
+     * Function sets optimal parameters values for chosen SVM SGD model.
+     * @param svmsgdType is the type of SVMSGD classifier.
+     * @param marginType is the type of margin constraint.
+     */
+    public void setOptimalParameters(int svmsgdType, int marginType) {
+        setOptimalParameters_0(nativeObj, svmsgdType, marginType);
+    }
+
+    /**
+     * Function sets optimal parameters values for chosen SVM SGD model.
+     * @param svmsgdType is the type of SVMSGD classifier.
+     */
+    public void setOptimalParameters(int svmsgdType) {
+        setOptimalParameters_1(nativeObj, svmsgdType);
+    }
+
+    /**
+     * Function sets optimal parameters values for chosen SVM SGD model.
+     */
+    public void setOptimalParameters() {
+        setOptimalParameters_2(nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::ml::SVMSGD::setStepDecreasingPower(float stepDecreasingPower)
+    //
+
+    /**
+     *  getStepDecreasingPower SEE: getStepDecreasingPower
+     * @param stepDecreasingPower automatically generated
+     */
+    public void setStepDecreasingPower(float stepDecreasingPower) {
+        setStepDecreasingPower_0(nativeObj, stepDecreasingPower);
+    }
+
+
+    //
+    // C++:  void cv::ml::SVMSGD::setSvmsgdType(int svmsgdType)
+    //
+
+    /**
+     *  getSvmsgdType SEE: getSvmsgdType
+     * @param svmsgdType automatically generated
+     */
+    public void setSvmsgdType(int svmsgdType) {
+        setSvmsgdType_0(nativeObj, svmsgdType);
+    }
+
+
+    //
+    // C++:  void cv::ml::SVMSGD::setTermCriteria(TermCriteria val)
+    //
+
+    /**
+     *  getTermCriteria SEE: getTermCriteria
+     * @param val automatically generated
+     */
+    public void setTermCriteria(TermCriteria val) {
+        setTermCriteria_0(nativeObj, val.type, val.maxCount, val.epsilon);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++:  Mat cv::ml::SVMSGD::getWeights()
+    private static native long getWeights_0(long nativeObj);
+
+    // C++: static Ptr_SVMSGD cv::ml::SVMSGD::create()
+    private static native long create_0();
+
+    // C++: static Ptr_SVMSGD cv::ml::SVMSGD::load(String filepath, String nodeName = String())
+    private static native long load_0(String filepath, String nodeName);
+    private static native long load_1(String filepath);
+
+    // C++:  TermCriteria cv::ml::SVMSGD::getTermCriteria()
+    private static native double[] getTermCriteria_0(long nativeObj);
+
+    // C++:  float cv::ml::SVMSGD::getInitialStepSize()
+    private static native float getInitialStepSize_0(long nativeObj);
+
+    // C++:  float cv::ml::SVMSGD::getMarginRegularization()
+    private static native float getMarginRegularization_0(long nativeObj);
+
+    // C++:  float cv::ml::SVMSGD::getShift()
+    private static native float getShift_0(long nativeObj);
+
+    // C++:  float cv::ml::SVMSGD::getStepDecreasingPower()
+    private static native float getStepDecreasingPower_0(long nativeObj);
+
+    // C++:  int cv::ml::SVMSGD::getMarginType()
+    private static native int getMarginType_0(long nativeObj);
+
+    // C++:  int cv::ml::SVMSGD::getSvmsgdType()
+    private static native int getSvmsgdType_0(long nativeObj);
+
+    // C++:  void cv::ml::SVMSGD::setInitialStepSize(float InitialStepSize)
+    private static native void setInitialStepSize_0(long nativeObj, float InitialStepSize);
+
+    // C++:  void cv::ml::SVMSGD::setMarginRegularization(float marginRegularization)
+    private static native void setMarginRegularization_0(long nativeObj, float marginRegularization);
+
+    // C++:  void cv::ml::SVMSGD::setMarginType(int marginType)
+    private static native void setMarginType_0(long nativeObj, int marginType);
+
+    // C++:  void cv::ml::SVMSGD::setOptimalParameters(int svmsgdType = SVMSGD::ASGD, int marginType = SVMSGD::SOFT_MARGIN)
+    private static native void setOptimalParameters_0(long nativeObj, int svmsgdType, int marginType);
+    private static native void setOptimalParameters_1(long nativeObj, int svmsgdType);
+    private static native void setOptimalParameters_2(long nativeObj);
+
+    // C++:  void cv::ml::SVMSGD::setStepDecreasingPower(float stepDecreasingPower)
+    private static native void setStepDecreasingPower_0(long nativeObj, float stepDecreasingPower);
+
+    // C++:  void cv::ml::SVMSGD::setSvmsgdType(int svmsgdType)
+    private static native void setSvmsgdType_0(long nativeObj, int svmsgdType);
+
+    // C++:  void cv::ml::SVMSGD::setTermCriteria(TermCriteria val)
+    private static native void setTermCriteria_0(long nativeObj, int val_type, int val_maxCount, double val_epsilon);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/ml/TrainData.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/ml/TrainData.java	(date 1605830247963)
+++ openCVLibrary3411/src/main/java/org/opencv/ml/TrainData.java	(date 1605830247963)
@@ -0,0 +1,775 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.ml;
+
+import java.util.ArrayList;
+import java.util.List;
+import org.opencv.core.Mat;
+import org.opencv.ml.TrainData;
+import org.opencv.utils.Converters;
+
+// C++: class TrainData
+/**
+ * Class encapsulating training data.
+ *
+ * Please note that the class only specifies the interface of training data, but not implementation.
+ * All the statistical model classes in _ml_ module accepts Ptr&lt;TrainData&gt; as parameter. In other
+ * words, you can create your own class derived from TrainData and pass smart pointer to the instance
+ * of this class into StatModel::train.
+ *
+ * SEE: REF: ml_intro_data
+ */
+public class TrainData {
+
+    protected final long nativeObj;
+    protected TrainData(long addr) { nativeObj = addr; }
+
+    public long getNativeObjAddr() { return nativeObj; }
+
+    // internal usage only
+    public static TrainData __fromPtr__(long addr) { return new TrainData(addr); }
+
+    //
+    // C++:  Mat cv::ml::TrainData::getCatMap()
+    //
+
+    public Mat getCatMap() {
+        return new Mat(getCatMap_0(nativeObj));
+    }
+
+
+    //
+    // C++:  Mat cv::ml::TrainData::getCatOfs()
+    //
+
+    public Mat getCatOfs() {
+        return new Mat(getCatOfs_0(nativeObj));
+    }
+
+
+    //
+    // C++:  Mat cv::ml::TrainData::getClassLabels()
+    //
+
+    /**
+     * Returns the vector of class labels
+     *
+     *     The function returns vector of unique labels occurred in the responses.
+     * @return automatically generated
+     */
+    public Mat getClassLabels() {
+        return new Mat(getClassLabels_0(nativeObj));
+    }
+
+
+    //
+    // C++:  Mat cv::ml::TrainData::getDefaultSubstValues()
+    //
+
+    public Mat getDefaultSubstValues() {
+        return new Mat(getDefaultSubstValues_0(nativeObj));
+    }
+
+
+    //
+    // C++:  Mat cv::ml::TrainData::getMissing()
+    //
+
+    public Mat getMissing() {
+        return new Mat(getMissing_0(nativeObj));
+    }
+
+
+    //
+    // C++:  Mat cv::ml::TrainData::getNormCatResponses()
+    //
+
+    public Mat getNormCatResponses() {
+        return new Mat(getNormCatResponses_0(nativeObj));
+    }
+
+
+    //
+    // C++:  Mat cv::ml::TrainData::getResponses()
+    //
+
+    public Mat getResponses() {
+        return new Mat(getResponses_0(nativeObj));
+    }
+
+
+    //
+    // C++:  Mat cv::ml::TrainData::getSampleWeights()
+    //
+
+    public Mat getSampleWeights() {
+        return new Mat(getSampleWeights_0(nativeObj));
+    }
+
+
+    //
+    // C++:  Mat cv::ml::TrainData::getSamples()
+    //
+
+    public Mat getSamples() {
+        return new Mat(getSamples_0(nativeObj));
+    }
+
+
+    //
+    // C++: static Mat cv::ml::TrainData::getSubMatrix(Mat matrix, Mat idx, int layout)
+    //
+
+    /**
+     * Extract from matrix rows/cols specified by passed indexes.
+     *     @param matrix input matrix (supported types: CV_32S, CV_32F, CV_64F)
+     *     @param idx 1D index vector
+     *     @param layout specifies to extract rows (cv::ml::ROW_SAMPLES) or to extract columns (cv::ml::COL_SAMPLES)
+     * @return automatically generated
+     */
+    public static Mat getSubMatrix(Mat matrix, Mat idx, int layout) {
+        return new Mat(getSubMatrix_0(matrix.nativeObj, idx.nativeObj, layout));
+    }
+
+
+    //
+    // C++: static Mat cv::ml::TrainData::getSubVector(Mat vec, Mat idx)
+    //
+
+    /**
+     * Extract from 1D vector elements specified by passed indexes.
+     *     @param vec input vector (supported types: CV_32S, CV_32F, CV_64F)
+     *     @param idx 1D index vector
+     * @return automatically generated
+     */
+    public static Mat getSubVector(Mat vec, Mat idx) {
+        return new Mat(getSubVector_0(vec.nativeObj, idx.nativeObj));
+    }
+
+
+    //
+    // C++:  Mat cv::ml::TrainData::getTestNormCatResponses()
+    //
+
+    public Mat getTestNormCatResponses() {
+        return new Mat(getTestNormCatResponses_0(nativeObj));
+    }
+
+
+    //
+    // C++:  Mat cv::ml::TrainData::getTestResponses()
+    //
+
+    public Mat getTestResponses() {
+        return new Mat(getTestResponses_0(nativeObj));
+    }
+
+
+    //
+    // C++:  Mat cv::ml::TrainData::getTestSampleIdx()
+    //
+
+    public Mat getTestSampleIdx() {
+        return new Mat(getTestSampleIdx_0(nativeObj));
+    }
+
+
+    //
+    // C++:  Mat cv::ml::TrainData::getTestSampleWeights()
+    //
+
+    public Mat getTestSampleWeights() {
+        return new Mat(getTestSampleWeights_0(nativeObj));
+    }
+
+
+    //
+    // C++:  Mat cv::ml::TrainData::getTestSamples()
+    //
+
+    /**
+     * Returns matrix of test samples
+     * @return automatically generated
+     */
+    public Mat getTestSamples() {
+        return new Mat(getTestSamples_0(nativeObj));
+    }
+
+
+    //
+    // C++:  Mat cv::ml::TrainData::getTrainNormCatResponses()
+    //
+
+    /**
+     * Returns the vector of normalized categorical responses
+     *
+     *     The function returns vector of responses. Each response is integer from {@code 0} to `&lt;number of
+     *     classes&gt;-1`. The actual label value can be retrieved then from the class label vector, see
+     *     TrainData::getClassLabels.
+     * @return automatically generated
+     */
+    public Mat getTrainNormCatResponses() {
+        return new Mat(getTrainNormCatResponses_0(nativeObj));
+    }
+
+
+    //
+    // C++:  Mat cv::ml::TrainData::getTrainResponses()
+    //
+
+    /**
+     * Returns the vector of responses
+     *
+     *     The function returns ordered or the original categorical responses. Usually it's used in
+     *     regression algorithms.
+     * @return automatically generated
+     */
+    public Mat getTrainResponses() {
+        return new Mat(getTrainResponses_0(nativeObj));
+    }
+
+
+    //
+    // C++:  Mat cv::ml::TrainData::getTrainSampleIdx()
+    //
+
+    public Mat getTrainSampleIdx() {
+        return new Mat(getTrainSampleIdx_0(nativeObj));
+    }
+
+
+    //
+    // C++:  Mat cv::ml::TrainData::getTrainSampleWeights()
+    //
+
+    public Mat getTrainSampleWeights() {
+        return new Mat(getTrainSampleWeights_0(nativeObj));
+    }
+
+
+    //
+    // C++:  Mat cv::ml::TrainData::getTrainSamples(int layout = ROW_SAMPLE, bool compressSamples = true, bool compressVars = true)
+    //
+
+    /**
+     * Returns matrix of train samples
+     *
+     *     @param layout The requested layout. If it's different from the initial one, the matrix is
+     *         transposed. See ml::SampleTypes.
+     *     @param compressSamples if true, the function returns only the training samples (specified by
+     *         sampleIdx)
+     *     @param compressVars if true, the function returns the shorter training samples, containing only
+     *         the active variables.
+     *
+     *     In current implementation the function tries to avoid physical data copying and returns the
+     *     matrix stored inside TrainData (unless the transposition or compression is needed).
+     * @return automatically generated
+     */
+    public Mat getTrainSamples(int layout, boolean compressSamples, boolean compressVars) {
+        return new Mat(getTrainSamples_0(nativeObj, layout, compressSamples, compressVars));
+    }
+
+    /**
+     * Returns matrix of train samples
+     *
+     *     @param layout The requested layout. If it's different from the initial one, the matrix is
+     *         transposed. See ml::SampleTypes.
+     *     @param compressSamples if true, the function returns only the training samples (specified by
+     *         sampleIdx)
+     *         the active variables.
+     *
+     *     In current implementation the function tries to avoid physical data copying and returns the
+     *     matrix stored inside TrainData (unless the transposition or compression is needed).
+     * @return automatically generated
+     */
+    public Mat getTrainSamples(int layout, boolean compressSamples) {
+        return new Mat(getTrainSamples_1(nativeObj, layout, compressSamples));
+    }
+
+    /**
+     * Returns matrix of train samples
+     *
+     *     @param layout The requested layout. If it's different from the initial one, the matrix is
+     *         transposed. See ml::SampleTypes.
+     *         sampleIdx)
+     *         the active variables.
+     *
+     *     In current implementation the function tries to avoid physical data copying and returns the
+     *     matrix stored inside TrainData (unless the transposition or compression is needed).
+     * @return automatically generated
+     */
+    public Mat getTrainSamples(int layout) {
+        return new Mat(getTrainSamples_2(nativeObj, layout));
+    }
+
+    /**
+     * Returns matrix of train samples
+     *
+     *         transposed. See ml::SampleTypes.
+     *         sampleIdx)
+     *         the active variables.
+     *
+     *     In current implementation the function tries to avoid physical data copying and returns the
+     *     matrix stored inside TrainData (unless the transposition or compression is needed).
+     * @return automatically generated
+     */
+    public Mat getTrainSamples() {
+        return new Mat(getTrainSamples_3(nativeObj));
+    }
+
+
+    //
+    // C++:  Mat cv::ml::TrainData::getVarIdx()
+    //
+
+    public Mat getVarIdx() {
+        return new Mat(getVarIdx_0(nativeObj));
+    }
+
+
+    //
+    // C++:  Mat cv::ml::TrainData::getVarSymbolFlags()
+    //
+
+    public Mat getVarSymbolFlags() {
+        return new Mat(getVarSymbolFlags_0(nativeObj));
+    }
+
+
+    //
+    // C++:  Mat cv::ml::TrainData::getVarType()
+    //
+
+    public Mat getVarType() {
+        return new Mat(getVarType_0(nativeObj));
+    }
+
+
+    //
+    // C++: static Ptr_TrainData cv::ml::TrainData::create(Mat samples, int layout, Mat responses, Mat varIdx = Mat(), Mat sampleIdx = Mat(), Mat sampleWeights = Mat(), Mat varType = Mat())
+    //
+
+    /**
+     * Creates training data from in-memory arrays.
+     *
+     *     @param samples matrix of samples. It should have CV_32F type.
+     *     @param layout see ml::SampleTypes.
+     *     @param responses matrix of responses. If the responses are scalar, they should be stored as a
+     *         single row or as a single column. The matrix should have type CV_32F or CV_32S (in the
+     *         former case the responses are considered as ordered by default; in the latter case - as
+     *         categorical)
+     *     @param varIdx vector specifying which variables to use for training. It can be an integer vector
+     *         (CV_32S) containing 0-based variable indices or byte vector (CV_8U) containing a mask of
+     *         active variables.
+     *     @param sampleIdx vector specifying which samples to use for training. It can be an integer
+     *         vector (CV_32S) containing 0-based sample indices or byte vector (CV_8U) containing a mask
+     *         of training samples.
+     *     @param sampleWeights optional vector with weights for each sample. It should have CV_32F type.
+     *     @param varType optional vector of type CV_8U and size `&lt;number_of_variables_in_samples&gt; +
+     *         &lt;number_of_variables_in_responses&gt;`, containing types of each input and output variable. See
+     *         ml::VariableTypes.
+     * @return automatically generated
+     */
+    public static TrainData create(Mat samples, int layout, Mat responses, Mat varIdx, Mat sampleIdx, Mat sampleWeights, Mat varType) {
+        return TrainData.__fromPtr__(create_0(samples.nativeObj, layout, responses.nativeObj, varIdx.nativeObj, sampleIdx.nativeObj, sampleWeights.nativeObj, varType.nativeObj));
+    }
+
+    /**
+     * Creates training data from in-memory arrays.
+     *
+     *     @param samples matrix of samples. It should have CV_32F type.
+     *     @param layout see ml::SampleTypes.
+     *     @param responses matrix of responses. If the responses are scalar, they should be stored as a
+     *         single row or as a single column. The matrix should have type CV_32F or CV_32S (in the
+     *         former case the responses are considered as ordered by default; in the latter case - as
+     *         categorical)
+     *     @param varIdx vector specifying which variables to use for training. It can be an integer vector
+     *         (CV_32S) containing 0-based variable indices or byte vector (CV_8U) containing a mask of
+     *         active variables.
+     *     @param sampleIdx vector specifying which samples to use for training. It can be an integer
+     *         vector (CV_32S) containing 0-based sample indices or byte vector (CV_8U) containing a mask
+     *         of training samples.
+     *     @param sampleWeights optional vector with weights for each sample. It should have CV_32F type.
+     *         &lt;number_of_variables_in_responses&gt;`, containing types of each input and output variable. See
+     *         ml::VariableTypes.
+     * @return automatically generated
+     */
+    public static TrainData create(Mat samples, int layout, Mat responses, Mat varIdx, Mat sampleIdx, Mat sampleWeights) {
+        return TrainData.__fromPtr__(create_1(samples.nativeObj, layout, responses.nativeObj, varIdx.nativeObj, sampleIdx.nativeObj, sampleWeights.nativeObj));
+    }
+
+    /**
+     * Creates training data from in-memory arrays.
+     *
+     *     @param samples matrix of samples. It should have CV_32F type.
+     *     @param layout see ml::SampleTypes.
+     *     @param responses matrix of responses. If the responses are scalar, they should be stored as a
+     *         single row or as a single column. The matrix should have type CV_32F or CV_32S (in the
+     *         former case the responses are considered as ordered by default; in the latter case - as
+     *         categorical)
+     *     @param varIdx vector specifying which variables to use for training. It can be an integer vector
+     *         (CV_32S) containing 0-based variable indices or byte vector (CV_8U) containing a mask of
+     *         active variables.
+     *     @param sampleIdx vector specifying which samples to use for training. It can be an integer
+     *         vector (CV_32S) containing 0-based sample indices or byte vector (CV_8U) containing a mask
+     *         of training samples.
+     *         &lt;number_of_variables_in_responses&gt;`, containing types of each input and output variable. See
+     *         ml::VariableTypes.
+     * @return automatically generated
+     */
+    public static TrainData create(Mat samples, int layout, Mat responses, Mat varIdx, Mat sampleIdx) {
+        return TrainData.__fromPtr__(create_2(samples.nativeObj, layout, responses.nativeObj, varIdx.nativeObj, sampleIdx.nativeObj));
+    }
+
+    /**
+     * Creates training data from in-memory arrays.
+     *
+     *     @param samples matrix of samples. It should have CV_32F type.
+     *     @param layout see ml::SampleTypes.
+     *     @param responses matrix of responses. If the responses are scalar, they should be stored as a
+     *         single row or as a single column. The matrix should have type CV_32F or CV_32S (in the
+     *         former case the responses are considered as ordered by default; in the latter case - as
+     *         categorical)
+     *     @param varIdx vector specifying which variables to use for training. It can be an integer vector
+     *         (CV_32S) containing 0-based variable indices or byte vector (CV_8U) containing a mask of
+     *         active variables.
+     *         vector (CV_32S) containing 0-based sample indices or byte vector (CV_8U) containing a mask
+     *         of training samples.
+     *         &lt;number_of_variables_in_responses&gt;`, containing types of each input and output variable. See
+     *         ml::VariableTypes.
+     * @return automatically generated
+     */
+    public static TrainData create(Mat samples, int layout, Mat responses, Mat varIdx) {
+        return TrainData.__fromPtr__(create_3(samples.nativeObj, layout, responses.nativeObj, varIdx.nativeObj));
+    }
+
+    /**
+     * Creates training data from in-memory arrays.
+     *
+     *     @param samples matrix of samples. It should have CV_32F type.
+     *     @param layout see ml::SampleTypes.
+     *     @param responses matrix of responses. If the responses are scalar, they should be stored as a
+     *         single row or as a single column. The matrix should have type CV_32F or CV_32S (in the
+     *         former case the responses are considered as ordered by default; in the latter case - as
+     *         categorical)
+     *         (CV_32S) containing 0-based variable indices or byte vector (CV_8U) containing a mask of
+     *         active variables.
+     *         vector (CV_32S) containing 0-based sample indices or byte vector (CV_8U) containing a mask
+     *         of training samples.
+     *         &lt;number_of_variables_in_responses&gt;`, containing types of each input and output variable. See
+     *         ml::VariableTypes.
+     * @return automatically generated
+     */
+    public static TrainData create(Mat samples, int layout, Mat responses) {
+        return TrainData.__fromPtr__(create_4(samples.nativeObj, layout, responses.nativeObj));
+    }
+
+
+    //
+    // C++:  int cv::ml::TrainData::getCatCount(int vi)
+    //
+
+    public int getCatCount(int vi) {
+        return getCatCount_0(nativeObj, vi);
+    }
+
+
+    //
+    // C++:  int cv::ml::TrainData::getLayout()
+    //
+
+    public int getLayout() {
+        return getLayout_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::ml::TrainData::getNAllVars()
+    //
+
+    public int getNAllVars() {
+        return getNAllVars_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::ml::TrainData::getNSamples()
+    //
+
+    public int getNSamples() {
+        return getNSamples_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::ml::TrainData::getNTestSamples()
+    //
+
+    public int getNTestSamples() {
+        return getNTestSamples_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::ml::TrainData::getNTrainSamples()
+    //
+
+    public int getNTrainSamples() {
+        return getNTrainSamples_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::ml::TrainData::getNVars()
+    //
+
+    public int getNVars() {
+        return getNVars_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::ml::TrainData::getResponseType()
+    //
+
+    public int getResponseType() {
+        return getResponseType_0(nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::ml::TrainData::getNames(vector_String names)
+    //
+
+    /**
+     * Returns vector of symbolic names captured in loadFromCSV()
+     * @param names automatically generated
+     */
+    public void getNames(List<String> names) {
+        getNames_0(nativeObj, names);
+    }
+
+
+    //
+    // C++:  void cv::ml::TrainData::getSample(Mat varIdx, int sidx, float* buf)
+    //
+
+    public void getSample(Mat varIdx, int sidx, float buf) {
+        getSample_0(nativeObj, varIdx.nativeObj, sidx, buf);
+    }
+
+
+    //
+    // C++:  void cv::ml::TrainData::getValues(int vi, Mat sidx, float* values)
+    //
+
+    public void getValues(int vi, Mat sidx, float values) {
+        getValues_0(nativeObj, vi, sidx.nativeObj, values);
+    }
+
+
+    //
+    // C++:  void cv::ml::TrainData::setTrainTestSplit(int count, bool shuffle = true)
+    //
+
+    /**
+     * Splits the training data into the training and test parts
+     *     SEE: TrainData::setTrainTestSplitRatio
+     * @param count automatically generated
+     * @param shuffle automatically generated
+     */
+    public void setTrainTestSplit(int count, boolean shuffle) {
+        setTrainTestSplit_0(nativeObj, count, shuffle);
+    }
+
+    /**
+     * Splits the training data into the training and test parts
+     *     SEE: TrainData::setTrainTestSplitRatio
+     * @param count automatically generated
+     */
+    public void setTrainTestSplit(int count) {
+        setTrainTestSplit_1(nativeObj, count);
+    }
+
+
+    //
+    // C++:  void cv::ml::TrainData::setTrainTestSplitRatio(double ratio, bool shuffle = true)
+    //
+
+    /**
+     * Splits the training data into the training and test parts
+     *
+     *     The function selects a subset of specified relative size and then returns it as the training
+     *     set. If the function is not called, all the data is used for training. Please, note that for
+     *     each of TrainData::getTrain\* there is corresponding TrainData::getTest\*, so that the test
+     *     subset can be retrieved and processed as well.
+     *     SEE: TrainData::setTrainTestSplit
+     * @param ratio automatically generated
+     * @param shuffle automatically generated
+     */
+    public void setTrainTestSplitRatio(double ratio, boolean shuffle) {
+        setTrainTestSplitRatio_0(nativeObj, ratio, shuffle);
+    }
+
+    /**
+     * Splits the training data into the training and test parts
+     *
+     *     The function selects a subset of specified relative size and then returns it as the training
+     *     set. If the function is not called, all the data is used for training. Please, note that for
+     *     each of TrainData::getTrain\* there is corresponding TrainData::getTest\*, so that the test
+     *     subset can be retrieved and processed as well.
+     *     SEE: TrainData::setTrainTestSplit
+     * @param ratio automatically generated
+     */
+    public void setTrainTestSplitRatio(double ratio) {
+        setTrainTestSplitRatio_1(nativeObj, ratio);
+    }
+
+
+    //
+    // C++:  void cv::ml::TrainData::shuffleTrainTest()
+    //
+
+    public void shuffleTrainTest() {
+        shuffleTrainTest_0(nativeObj);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++:  Mat cv::ml::TrainData::getCatMap()
+    private static native long getCatMap_0(long nativeObj);
+
+    // C++:  Mat cv::ml::TrainData::getCatOfs()
+    private static native long getCatOfs_0(long nativeObj);
+
+    // C++:  Mat cv::ml::TrainData::getClassLabels()
+    private static native long getClassLabels_0(long nativeObj);
+
+    // C++:  Mat cv::ml::TrainData::getDefaultSubstValues()
+    private static native long getDefaultSubstValues_0(long nativeObj);
+
+    // C++:  Mat cv::ml::TrainData::getMissing()
+    private static native long getMissing_0(long nativeObj);
+
+    // C++:  Mat cv::ml::TrainData::getNormCatResponses()
+    private static native long getNormCatResponses_0(long nativeObj);
+
+    // C++:  Mat cv::ml::TrainData::getResponses()
+    private static native long getResponses_0(long nativeObj);
+
+    // C++:  Mat cv::ml::TrainData::getSampleWeights()
+    private static native long getSampleWeights_0(long nativeObj);
+
+    // C++:  Mat cv::ml::TrainData::getSamples()
+    private static native long getSamples_0(long nativeObj);
+
+    // C++: static Mat cv::ml::TrainData::getSubMatrix(Mat matrix, Mat idx, int layout)
+    private static native long getSubMatrix_0(long matrix_nativeObj, long idx_nativeObj, int layout);
+
+    // C++: static Mat cv::ml::TrainData::getSubVector(Mat vec, Mat idx)
+    private static native long getSubVector_0(long vec_nativeObj, long idx_nativeObj);
+
+    // C++:  Mat cv::ml::TrainData::getTestNormCatResponses()
+    private static native long getTestNormCatResponses_0(long nativeObj);
+
+    // C++:  Mat cv::ml::TrainData::getTestResponses()
+    private static native long getTestResponses_0(long nativeObj);
+
+    // C++:  Mat cv::ml::TrainData::getTestSampleIdx()
+    private static native long getTestSampleIdx_0(long nativeObj);
+
+    // C++:  Mat cv::ml::TrainData::getTestSampleWeights()
+    private static native long getTestSampleWeights_0(long nativeObj);
+
+    // C++:  Mat cv::ml::TrainData::getTestSamples()
+    private static native long getTestSamples_0(long nativeObj);
+
+    // C++:  Mat cv::ml::TrainData::getTrainNormCatResponses()
+    private static native long getTrainNormCatResponses_0(long nativeObj);
+
+    // C++:  Mat cv::ml::TrainData::getTrainResponses()
+    private static native long getTrainResponses_0(long nativeObj);
+
+    // C++:  Mat cv::ml::TrainData::getTrainSampleIdx()
+    private static native long getTrainSampleIdx_0(long nativeObj);
+
+    // C++:  Mat cv::ml::TrainData::getTrainSampleWeights()
+    private static native long getTrainSampleWeights_0(long nativeObj);
+
+    // C++:  Mat cv::ml::TrainData::getTrainSamples(int layout = ROW_SAMPLE, bool compressSamples = true, bool compressVars = true)
+    private static native long getTrainSamples_0(long nativeObj, int layout, boolean compressSamples, boolean compressVars);
+    private static native long getTrainSamples_1(long nativeObj, int layout, boolean compressSamples);
+    private static native long getTrainSamples_2(long nativeObj, int layout);
+    private static native long getTrainSamples_3(long nativeObj);
+
+    // C++:  Mat cv::ml::TrainData::getVarIdx()
+    private static native long getVarIdx_0(long nativeObj);
+
+    // C++:  Mat cv::ml::TrainData::getVarSymbolFlags()
+    private static native long getVarSymbolFlags_0(long nativeObj);
+
+    // C++:  Mat cv::ml::TrainData::getVarType()
+    private static native long getVarType_0(long nativeObj);
+
+    // C++: static Ptr_TrainData cv::ml::TrainData::create(Mat samples, int layout, Mat responses, Mat varIdx = Mat(), Mat sampleIdx = Mat(), Mat sampleWeights = Mat(), Mat varType = Mat())
+    private static native long create_0(long samples_nativeObj, int layout, long responses_nativeObj, long varIdx_nativeObj, long sampleIdx_nativeObj, long sampleWeights_nativeObj, long varType_nativeObj);
+    private static native long create_1(long samples_nativeObj, int layout, long responses_nativeObj, long varIdx_nativeObj, long sampleIdx_nativeObj, long sampleWeights_nativeObj);
+    private static native long create_2(long samples_nativeObj, int layout, long responses_nativeObj, long varIdx_nativeObj, long sampleIdx_nativeObj);
+    private static native long create_3(long samples_nativeObj, int layout, long responses_nativeObj, long varIdx_nativeObj);
+    private static native long create_4(long samples_nativeObj, int layout, long responses_nativeObj);
+
+    // C++:  int cv::ml::TrainData::getCatCount(int vi)
+    private static native int getCatCount_0(long nativeObj, int vi);
+
+    // C++:  int cv::ml::TrainData::getLayout()
+    private static native int getLayout_0(long nativeObj);
+
+    // C++:  int cv::ml::TrainData::getNAllVars()
+    private static native int getNAllVars_0(long nativeObj);
+
+    // C++:  int cv::ml::TrainData::getNSamples()
+    private static native int getNSamples_0(long nativeObj);
+
+    // C++:  int cv::ml::TrainData::getNTestSamples()
+    private static native int getNTestSamples_0(long nativeObj);
+
+    // C++:  int cv::ml::TrainData::getNTrainSamples()
+    private static native int getNTrainSamples_0(long nativeObj);
+
+    // C++:  int cv::ml::TrainData::getNVars()
+    private static native int getNVars_0(long nativeObj);
+
+    // C++:  int cv::ml::TrainData::getResponseType()
+    private static native int getResponseType_0(long nativeObj);
+
+    // C++:  void cv::ml::TrainData::getNames(vector_String names)
+    private static native void getNames_0(long nativeObj, List<String> names);
+
+    // C++:  void cv::ml::TrainData::getSample(Mat varIdx, int sidx, float* buf)
+    private static native void getSample_0(long nativeObj, long varIdx_nativeObj, int sidx, float buf);
+
+    // C++:  void cv::ml::TrainData::getValues(int vi, Mat sidx, float* values)
+    private static native void getValues_0(long nativeObj, int vi, long sidx_nativeObj, float values);
+
+    // C++:  void cv::ml::TrainData::setTrainTestSplit(int count, bool shuffle = true)
+    private static native void setTrainTestSplit_0(long nativeObj, int count, boolean shuffle);
+    private static native void setTrainTestSplit_1(long nativeObj, int count);
+
+    // C++:  void cv::ml::TrainData::setTrainTestSplitRatio(double ratio, bool shuffle = true)
+    private static native void setTrainTestSplitRatio_0(long nativeObj, double ratio, boolean shuffle);
+    private static native void setTrainTestSplitRatio_1(long nativeObj, double ratio);
+
+    // C++:  void cv::ml::TrainData::shuffleTrainTest()
+    private static native void shuffleTrainTest_0(long nativeObj);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/ml/Ml.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/ml/Ml.java	(date 1605830247874)
+++ openCVLibrary3411/src/main/java/org/opencv/ml/Ml.java	(date 1605830247874)
@@ -0,0 +1,33 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.ml;
+
+
+
+// C++: class Ml
+
+public class Ml {
+
+    // C++: enum SampleTypes
+    public static final int
+            ROW_SAMPLE = 0,
+            COL_SAMPLE = 1;
+
+
+    // C++: enum VariableTypes
+    public static final int
+            VAR_NUMERICAL = 0,
+            VAR_ORDERED = 0,
+            VAR_CATEGORICAL = 1;
+
+
+    // C++: enum ErrorTypes
+    public static final int
+            TEST_ERROR = 0,
+            TRAIN_ERROR = 1;
+
+
+
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/ml/NormalBayesClassifier.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/ml/NormalBayesClassifier.java	(date 1605830247885)
+++ openCVLibrary3411/src/main/java/org/opencv/ml/NormalBayesClassifier.java	(date 1605830247885)
@@ -0,0 +1,132 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.ml;
+
+import org.opencv.core.Mat;
+import org.opencv.ml.NormalBayesClassifier;
+import org.opencv.ml.StatModel;
+
+// C++: class NormalBayesClassifier
+/**
+ * Bayes classifier for normally distributed data.
+ *
+ * SEE: REF: ml_intro_bayes
+ */
+public class NormalBayesClassifier extends StatModel {
+
+    protected NormalBayesClassifier(long addr) { super(addr); }
+
+    // internal usage only
+    public static NormalBayesClassifier __fromPtr__(long addr) { return new NormalBayesClassifier(addr); }
+
+    //
+    // C++: static Ptr_NormalBayesClassifier cv::ml::NormalBayesClassifier::create()
+    //
+
+    /**
+     * Creates empty model
+     * Use StatModel::train to train the model after creation.
+     * @return automatically generated
+     */
+    public static NormalBayesClassifier create() {
+        return NormalBayesClassifier.__fromPtr__(create_0());
+    }
+
+
+    //
+    // C++: static Ptr_NormalBayesClassifier cv::ml::NormalBayesClassifier::load(String filepath, String nodeName = String())
+    //
+
+    /**
+     * Loads and creates a serialized NormalBayesClassifier from a file
+     *
+     * Use NormalBayesClassifier::save to serialize and store an NormalBayesClassifier to disk.
+     * Load the NormalBayesClassifier from this file again, by calling this function with the path to the file.
+     * Optionally specify the node for the file containing the classifier
+     *
+     * @param filepath path to serialized NormalBayesClassifier
+     * @param nodeName name of node containing the classifier
+     * @return automatically generated
+     */
+    public static NormalBayesClassifier load(String filepath, String nodeName) {
+        return NormalBayesClassifier.__fromPtr__(load_0(filepath, nodeName));
+    }
+
+    /**
+     * Loads and creates a serialized NormalBayesClassifier from a file
+     *
+     * Use NormalBayesClassifier::save to serialize and store an NormalBayesClassifier to disk.
+     * Load the NormalBayesClassifier from this file again, by calling this function with the path to the file.
+     * Optionally specify the node for the file containing the classifier
+     *
+     * @param filepath path to serialized NormalBayesClassifier
+     * @return automatically generated
+     */
+    public static NormalBayesClassifier load(String filepath) {
+        return NormalBayesClassifier.__fromPtr__(load_1(filepath));
+    }
+
+
+    //
+    // C++:  float cv::ml::NormalBayesClassifier::predictProb(Mat inputs, Mat& outputs, Mat& outputProbs, int flags = 0)
+    //
+
+    /**
+     * Predicts the response for sample(s).
+     *
+     *     The method estimates the most probable classes for input vectors. Input vectors (one or more)
+     *     are stored as rows of the matrix inputs. In case of multiple input vectors, there should be one
+     *     output vector outputs. The predicted class for a single input vector is returned by the method.
+     *     The vector outputProbs contains the output probabilities corresponding to each element of
+     *     result.
+     * @param inputs automatically generated
+     * @param outputs automatically generated
+     * @param outputProbs automatically generated
+     * @param flags automatically generated
+     * @return automatically generated
+     */
+    public float predictProb(Mat inputs, Mat outputs, Mat outputProbs, int flags) {
+        return predictProb_0(nativeObj, inputs.nativeObj, outputs.nativeObj, outputProbs.nativeObj, flags);
+    }
+
+    /**
+     * Predicts the response for sample(s).
+     *
+     *     The method estimates the most probable classes for input vectors. Input vectors (one or more)
+     *     are stored as rows of the matrix inputs. In case of multiple input vectors, there should be one
+     *     output vector outputs. The predicted class for a single input vector is returned by the method.
+     *     The vector outputProbs contains the output probabilities corresponding to each element of
+     *     result.
+     * @param inputs automatically generated
+     * @param outputs automatically generated
+     * @param outputProbs automatically generated
+     * @return automatically generated
+     */
+    public float predictProb(Mat inputs, Mat outputs, Mat outputProbs) {
+        return predictProb_1(nativeObj, inputs.nativeObj, outputs.nativeObj, outputProbs.nativeObj);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++: static Ptr_NormalBayesClassifier cv::ml::NormalBayesClassifier::create()
+    private static native long create_0();
+
+    // C++: static Ptr_NormalBayesClassifier cv::ml::NormalBayesClassifier::load(String filepath, String nodeName = String())
+    private static native long load_0(String filepath, String nodeName);
+    private static native long load_1(String filepath);
+
+    // C++:  float cv::ml::NormalBayesClassifier::predictProb(Mat inputs, Mat& outputs, Mat& outputProbs, int flags = 0)
+    private static native float predictProb_0(long nativeObj, long inputs_nativeObj, long outputs_nativeObj, long outputProbs_nativeObj, int flags);
+    private static native float predictProb_1(long nativeObj, long inputs_nativeObj, long outputs_nativeObj, long outputProbs_nativeObj);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/ml/ParamGrid.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/ml/ParamGrid.java	(date 1605830247902)
+++ openCVLibrary3411/src/main/java/org/opencv/ml/ParamGrid.java	(date 1605830247902)
@@ -0,0 +1,160 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.ml;
+
+import org.opencv.ml.ParamGrid;
+
+// C++: class ParamGrid
+/**
+ * The structure represents the logarithmic grid range of statmodel parameters.
+ *
+ * It is used for optimizing statmodel accuracy by varying model parameters, the accuracy estimate
+ * being computed by cross-validation.
+ */
+public class ParamGrid {
+
+    protected final long nativeObj;
+    protected ParamGrid(long addr) { nativeObj = addr; }
+
+    public long getNativeObjAddr() { return nativeObj; }
+
+    // internal usage only
+    public static ParamGrid __fromPtr__(long addr) { return new ParamGrid(addr); }
+
+    //
+    // C++: static Ptr_ParamGrid cv::ml::ParamGrid::create(double minVal = 0., double maxVal = 0., double logstep = 1.)
+    //
+
+    /**
+     * Creates a ParamGrid Ptr that can be given to the %SVM::trainAuto method
+     *
+     *     @param minVal minimum value of the parameter grid
+     *     @param maxVal maximum value of the parameter grid
+     *     @param logstep Logarithmic step for iterating the statmodel parameter
+     * @return automatically generated
+     */
+    public static ParamGrid create(double minVal, double maxVal, double logstep) {
+        return ParamGrid.__fromPtr__(create_0(minVal, maxVal, logstep));
+    }
+
+    /**
+     * Creates a ParamGrid Ptr that can be given to the %SVM::trainAuto method
+     *
+     *     @param minVal minimum value of the parameter grid
+     *     @param maxVal maximum value of the parameter grid
+     * @return automatically generated
+     */
+    public static ParamGrid create(double minVal, double maxVal) {
+        return ParamGrid.__fromPtr__(create_1(minVal, maxVal));
+    }
+
+    /**
+     * Creates a ParamGrid Ptr that can be given to the %SVM::trainAuto method
+     *
+     *     @param minVal minimum value of the parameter grid
+     * @return automatically generated
+     */
+    public static ParamGrid create(double minVal) {
+        return ParamGrid.__fromPtr__(create_2(minVal));
+    }
+
+    /**
+     * Creates a ParamGrid Ptr that can be given to the %SVM::trainAuto method
+     *
+     * @return automatically generated
+     */
+    public static ParamGrid create() {
+        return ParamGrid.__fromPtr__(create_3());
+    }
+
+
+    //
+    // C++: double ParamGrid::minVal
+    //
+
+    public double get_minVal() {
+        return get_minVal_0(nativeObj);
+    }
+
+
+    //
+    // C++: void ParamGrid::minVal
+    //
+
+    public void set_minVal(double minVal) {
+        set_minVal_0(nativeObj, minVal);
+    }
+
+
+    //
+    // C++: double ParamGrid::maxVal
+    //
+
+    public double get_maxVal() {
+        return get_maxVal_0(nativeObj);
+    }
+
+
+    //
+    // C++: void ParamGrid::maxVal
+    //
+
+    public void set_maxVal(double maxVal) {
+        set_maxVal_0(nativeObj, maxVal);
+    }
+
+
+    //
+    // C++: double ParamGrid::logStep
+    //
+
+    public double get_logStep() {
+        return get_logStep_0(nativeObj);
+    }
+
+
+    //
+    // C++: void ParamGrid::logStep
+    //
+
+    public void set_logStep(double logStep) {
+        set_logStep_0(nativeObj, logStep);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++: static Ptr_ParamGrid cv::ml::ParamGrid::create(double minVal = 0., double maxVal = 0., double logstep = 1.)
+    private static native long create_0(double minVal, double maxVal, double logstep);
+    private static native long create_1(double minVal, double maxVal);
+    private static native long create_2(double minVal);
+    private static native long create_3();
+
+    // C++: double ParamGrid::minVal
+    private static native double get_minVal_0(long nativeObj);
+
+    // C++: void ParamGrid::minVal
+    private static native void set_minVal_0(long nativeObj, double minVal);
+
+    // C++: double ParamGrid::maxVal
+    private static native double get_maxVal_0(long nativeObj);
+
+    // C++: void ParamGrid::maxVal
+    private static native void set_maxVal_0(long nativeObj, double maxVal);
+
+    // C++: double ParamGrid::logStep
+    private static native double get_logStep_0(long nativeObj);
+
+    // C++: void ParamGrid::logStep
+    private static native void set_logStep_0(long nativeObj, double logStep);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/ml/RTrees.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/ml/RTrees.java	(date 1605830247903)
+++ openCVLibrary3411/src/main/java/org/opencv/ml/RTrees.java	(date 1605830247903)
@@ -0,0 +1,227 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.ml;
+
+import org.opencv.core.Mat;
+import org.opencv.core.TermCriteria;
+import org.opencv.ml.DTrees;
+import org.opencv.ml.RTrees;
+
+// C++: class RTrees
+/**
+ * The class implements the random forest predictor.
+ *
+ * SEE: REF: ml_intro_rtrees
+ */
+public class RTrees extends DTrees {
+
+    protected RTrees(long addr) { super(addr); }
+
+    // internal usage only
+    public static RTrees __fromPtr__(long addr) { return new RTrees(addr); }
+
+    //
+    // C++:  Mat cv::ml::RTrees::getVarImportance()
+    //
+
+    /**
+     * Returns the variable importance array.
+     *     The method returns the variable importance vector, computed at the training stage when
+     *     CalculateVarImportance is set to true. If this flag was set to false, the empty matrix is
+     *     returned.
+     * @return automatically generated
+     */
+    public Mat getVarImportance() {
+        return new Mat(getVarImportance_0(nativeObj));
+    }
+
+
+    //
+    // C++: static Ptr_RTrees cv::ml::RTrees::create()
+    //
+
+    /**
+     * Creates the empty model.
+     *     Use StatModel::train to train the model, StatModel::train to create and train the model,
+     *     Algorithm::load to load the pre-trained model.
+     * @return automatically generated
+     */
+    public static RTrees create() {
+        return RTrees.__fromPtr__(create_0());
+    }
+
+
+    //
+    // C++: static Ptr_RTrees cv::ml::RTrees::load(String filepath, String nodeName = String())
+    //
+
+    /**
+     * Loads and creates a serialized RTree from a file
+     *
+     * Use RTree::save to serialize and store an RTree to disk.
+     * Load the RTree from this file again, by calling this function with the path to the file.
+     * Optionally specify the node for the file containing the classifier
+     *
+     * @param filepath path to serialized RTree
+     * @param nodeName name of node containing the classifier
+     * @return automatically generated
+     */
+    public static RTrees load(String filepath, String nodeName) {
+        return RTrees.__fromPtr__(load_0(filepath, nodeName));
+    }
+
+    /**
+     * Loads and creates a serialized RTree from a file
+     *
+     * Use RTree::save to serialize and store an RTree to disk.
+     * Load the RTree from this file again, by calling this function with the path to the file.
+     * Optionally specify the node for the file containing the classifier
+     *
+     * @param filepath path to serialized RTree
+     * @return automatically generated
+     */
+    public static RTrees load(String filepath) {
+        return RTrees.__fromPtr__(load_1(filepath));
+    }
+
+
+    //
+    // C++:  TermCriteria cv::ml::RTrees::getTermCriteria()
+    //
+
+    /**
+     * SEE: setTermCriteria
+     * @return automatically generated
+     */
+    public TermCriteria getTermCriteria() {
+        return new TermCriteria(getTermCriteria_0(nativeObj));
+    }
+
+
+    //
+    // C++:  bool cv::ml::RTrees::getCalculateVarImportance()
+    //
+
+    /**
+     * SEE: setCalculateVarImportance
+     * @return automatically generated
+     */
+    public boolean getCalculateVarImportance() {
+        return getCalculateVarImportance_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::ml::RTrees::getActiveVarCount()
+    //
+
+    /**
+     * SEE: setActiveVarCount
+     * @return automatically generated
+     */
+    public int getActiveVarCount() {
+        return getActiveVarCount_0(nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::ml::RTrees::getVotes(Mat samples, Mat& results, int flags)
+    //
+
+    /**
+     * Returns the result of each individual tree in the forest.
+     *     In case the model is a regression problem, the method will return each of the trees'
+     *     results for each of the sample cases. If the model is a classifier, it will return
+     *     a Mat with samples + 1 rows, where the first row gives the class number and the
+     *     following rows return the votes each class had for each sample.
+     *         @param samples Array containing the samples for which votes will be calculated.
+     *         @param results Array where the result of the calculation will be written.
+     *         @param flags Flags for defining the type of RTrees.
+     */
+    public void getVotes(Mat samples, Mat results, int flags) {
+        getVotes_0(nativeObj, samples.nativeObj, results.nativeObj, flags);
+    }
+
+
+    //
+    // C++:  void cv::ml::RTrees::setActiveVarCount(int val)
+    //
+
+    /**
+     *  getActiveVarCount SEE: getActiveVarCount
+     * @param val automatically generated
+     */
+    public void setActiveVarCount(int val) {
+        setActiveVarCount_0(nativeObj, val);
+    }
+
+
+    //
+    // C++:  void cv::ml::RTrees::setCalculateVarImportance(bool val)
+    //
+
+    /**
+     *  getCalculateVarImportance SEE: getCalculateVarImportance
+     * @param val automatically generated
+     */
+    public void setCalculateVarImportance(boolean val) {
+        setCalculateVarImportance_0(nativeObj, val);
+    }
+
+
+    //
+    // C++:  void cv::ml::RTrees::setTermCriteria(TermCriteria val)
+    //
+
+    /**
+     *  getTermCriteria SEE: getTermCriteria
+     * @param val automatically generated
+     */
+    public void setTermCriteria(TermCriteria val) {
+        setTermCriteria_0(nativeObj, val.type, val.maxCount, val.epsilon);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++:  Mat cv::ml::RTrees::getVarImportance()
+    private static native long getVarImportance_0(long nativeObj);
+
+    // C++: static Ptr_RTrees cv::ml::RTrees::create()
+    private static native long create_0();
+
+    // C++: static Ptr_RTrees cv::ml::RTrees::load(String filepath, String nodeName = String())
+    private static native long load_0(String filepath, String nodeName);
+    private static native long load_1(String filepath);
+
+    // C++:  TermCriteria cv::ml::RTrees::getTermCriteria()
+    private static native double[] getTermCriteria_0(long nativeObj);
+
+    // C++:  bool cv::ml::RTrees::getCalculateVarImportance()
+    private static native boolean getCalculateVarImportance_0(long nativeObj);
+
+    // C++:  int cv::ml::RTrees::getActiveVarCount()
+    private static native int getActiveVarCount_0(long nativeObj);
+
+    // C++:  void cv::ml::RTrees::getVotes(Mat samples, Mat& results, int flags)
+    private static native void getVotes_0(long nativeObj, long samples_nativeObj, long results_nativeObj, int flags);
+
+    // C++:  void cv::ml::RTrees::setActiveVarCount(int val)
+    private static native void setActiveVarCount_0(long nativeObj, int val);
+
+    // C++:  void cv::ml::RTrees::setCalculateVarImportance(bool val)
+    private static native void setCalculateVarImportance_0(long nativeObj, boolean val);
+
+    // C++:  void cv::ml::RTrees::setTermCriteria(TermCriteria val)
+    private static native void setTermCriteria_0(long nativeObj, int val_type, int val_maxCount, double val_epsilon);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/ml/DTrees.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/ml/DTrees.java	(date 1605830247849)
+++ openCVLibrary3411/src/main/java/org/opencv/ml/DTrees.java	(date 1605830247849)
@@ -0,0 +1,392 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.ml;
+
+import org.opencv.core.Mat;
+import org.opencv.ml.DTrees;
+import org.opencv.ml.StatModel;
+
+// C++: class DTrees
+/**
+ * The class represents a single decision tree or a collection of decision trees.
+ *
+ * The current public interface of the class allows user to train only a single decision tree, however
+ * the class is capable of storing multiple decision trees and using them for prediction (by summing
+ * responses or using a voting schemes), and the derived from DTrees classes (such as RTrees and Boost)
+ * use this capability to implement decision tree ensembles.
+ *
+ * SEE: REF: ml_intro_trees
+ */
+public class DTrees extends StatModel {
+
+    protected DTrees(long addr) { super(addr); }
+
+    // internal usage only
+    public static DTrees __fromPtr__(long addr) { return new DTrees(addr); }
+
+    // C++: enum Flags
+    public static final int
+            PREDICT_AUTO = 0,
+            PREDICT_SUM = (1<<8),
+            PREDICT_MAX_VOTE = (2<<8),
+            PREDICT_MASK = (3<<8);
+
+
+    //
+    // C++:  Mat cv::ml::DTrees::getPriors()
+    //
+
+    /**
+     * SEE: setPriors
+     * @return automatically generated
+     */
+    public Mat getPriors() {
+        return new Mat(getPriors_0(nativeObj));
+    }
+
+
+    //
+    // C++: static Ptr_DTrees cv::ml::DTrees::create()
+    //
+
+    /**
+     * Creates the empty model
+     *
+     *     The static method creates empty decision tree with the specified parameters. It should be then
+     *     trained using train method (see StatModel::train). Alternatively, you can load the model from
+     *     file using Algorithm::load&lt;DTrees&gt;(filename).
+     * @return automatically generated
+     */
+    public static DTrees create() {
+        return DTrees.__fromPtr__(create_0());
+    }
+
+
+    //
+    // C++: static Ptr_DTrees cv::ml::DTrees::load(String filepath, String nodeName = String())
+    //
+
+    /**
+     * Loads and creates a serialized DTrees from a file
+     *
+     * Use DTree::save to serialize and store an DTree to disk.
+     * Load the DTree from this file again, by calling this function with the path to the file.
+     * Optionally specify the node for the file containing the classifier
+     *
+     * @param filepath path to serialized DTree
+     * @param nodeName name of node containing the classifier
+     * @return automatically generated
+     */
+    public static DTrees load(String filepath, String nodeName) {
+        return DTrees.__fromPtr__(load_0(filepath, nodeName));
+    }
+
+    /**
+     * Loads and creates a serialized DTrees from a file
+     *
+     * Use DTree::save to serialize and store an DTree to disk.
+     * Load the DTree from this file again, by calling this function with the path to the file.
+     * Optionally specify the node for the file containing the classifier
+     *
+     * @param filepath path to serialized DTree
+     * @return automatically generated
+     */
+    public static DTrees load(String filepath) {
+        return DTrees.__fromPtr__(load_1(filepath));
+    }
+
+
+    //
+    // C++:  bool cv::ml::DTrees::getTruncatePrunedTree()
+    //
+
+    /**
+     * SEE: setTruncatePrunedTree
+     * @return automatically generated
+     */
+    public boolean getTruncatePrunedTree() {
+        return getTruncatePrunedTree_0(nativeObj);
+    }
+
+
+    //
+    // C++:  bool cv::ml::DTrees::getUse1SERule()
+    //
+
+    /**
+     * SEE: setUse1SERule
+     * @return automatically generated
+     */
+    public boolean getUse1SERule() {
+        return getUse1SERule_0(nativeObj);
+    }
+
+
+    //
+    // C++:  bool cv::ml::DTrees::getUseSurrogates()
+    //
+
+    /**
+     * SEE: setUseSurrogates
+     * @return automatically generated
+     */
+    public boolean getUseSurrogates() {
+        return getUseSurrogates_0(nativeObj);
+    }
+
+
+    //
+    // C++:  float cv::ml::DTrees::getRegressionAccuracy()
+    //
+
+    /**
+     * SEE: setRegressionAccuracy
+     * @return automatically generated
+     */
+    public float getRegressionAccuracy() {
+        return getRegressionAccuracy_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::ml::DTrees::getCVFolds()
+    //
+
+    /**
+     * SEE: setCVFolds
+     * @return automatically generated
+     */
+    public int getCVFolds() {
+        return getCVFolds_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::ml::DTrees::getMaxCategories()
+    //
+
+    /**
+     * SEE: setMaxCategories
+     * @return automatically generated
+     */
+    public int getMaxCategories() {
+        return getMaxCategories_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::ml::DTrees::getMaxDepth()
+    //
+
+    /**
+     * SEE: setMaxDepth
+     * @return automatically generated
+     */
+    public int getMaxDepth() {
+        return getMaxDepth_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::ml::DTrees::getMinSampleCount()
+    //
+
+    /**
+     * SEE: setMinSampleCount
+     * @return automatically generated
+     */
+    public int getMinSampleCount() {
+        return getMinSampleCount_0(nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::ml::DTrees::setCVFolds(int val)
+    //
+
+    /**
+     *  getCVFolds SEE: getCVFolds
+     * @param val automatically generated
+     */
+    public void setCVFolds(int val) {
+        setCVFolds_0(nativeObj, val);
+    }
+
+
+    //
+    // C++:  void cv::ml::DTrees::setMaxCategories(int val)
+    //
+
+    /**
+     *  getMaxCategories SEE: getMaxCategories
+     * @param val automatically generated
+     */
+    public void setMaxCategories(int val) {
+        setMaxCategories_0(nativeObj, val);
+    }
+
+
+    //
+    // C++:  void cv::ml::DTrees::setMaxDepth(int val)
+    //
+
+    /**
+     *  getMaxDepth SEE: getMaxDepth
+     * @param val automatically generated
+     */
+    public void setMaxDepth(int val) {
+        setMaxDepth_0(nativeObj, val);
+    }
+
+
+    //
+    // C++:  void cv::ml::DTrees::setMinSampleCount(int val)
+    //
+
+    /**
+     *  getMinSampleCount SEE: getMinSampleCount
+     * @param val automatically generated
+     */
+    public void setMinSampleCount(int val) {
+        setMinSampleCount_0(nativeObj, val);
+    }
+
+
+    //
+    // C++:  void cv::ml::DTrees::setPriors(Mat val)
+    //
+
+    /**
+     *  getPriors SEE: getPriors
+     * @param val automatically generated
+     */
+    public void setPriors(Mat val) {
+        setPriors_0(nativeObj, val.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::ml::DTrees::setRegressionAccuracy(float val)
+    //
+
+    /**
+     *  getRegressionAccuracy SEE: getRegressionAccuracy
+     * @param val automatically generated
+     */
+    public void setRegressionAccuracy(float val) {
+        setRegressionAccuracy_0(nativeObj, val);
+    }
+
+
+    //
+    // C++:  void cv::ml::DTrees::setTruncatePrunedTree(bool val)
+    //
+
+    /**
+     *  getTruncatePrunedTree SEE: getTruncatePrunedTree
+     * @param val automatically generated
+     */
+    public void setTruncatePrunedTree(boolean val) {
+        setTruncatePrunedTree_0(nativeObj, val);
+    }
+
+
+    //
+    // C++:  void cv::ml::DTrees::setUse1SERule(bool val)
+    //
+
+    /**
+     *  getUse1SERule SEE: getUse1SERule
+     * @param val automatically generated
+     */
+    public void setUse1SERule(boolean val) {
+        setUse1SERule_0(nativeObj, val);
+    }
+
+
+    //
+    // C++:  void cv::ml::DTrees::setUseSurrogates(bool val)
+    //
+
+    /**
+     *  getUseSurrogates SEE: getUseSurrogates
+     * @param val automatically generated
+     */
+    public void setUseSurrogates(boolean val) {
+        setUseSurrogates_0(nativeObj, val);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++:  Mat cv::ml::DTrees::getPriors()
+    private static native long getPriors_0(long nativeObj);
+
+    // C++: static Ptr_DTrees cv::ml::DTrees::create()
+    private static native long create_0();
+
+    // C++: static Ptr_DTrees cv::ml::DTrees::load(String filepath, String nodeName = String())
+    private static native long load_0(String filepath, String nodeName);
+    private static native long load_1(String filepath);
+
+    // C++:  bool cv::ml::DTrees::getTruncatePrunedTree()
+    private static native boolean getTruncatePrunedTree_0(long nativeObj);
+
+    // C++:  bool cv::ml::DTrees::getUse1SERule()
+    private static native boolean getUse1SERule_0(long nativeObj);
+
+    // C++:  bool cv::ml::DTrees::getUseSurrogates()
+    private static native boolean getUseSurrogates_0(long nativeObj);
+
+    // C++:  float cv::ml::DTrees::getRegressionAccuracy()
+    private static native float getRegressionAccuracy_0(long nativeObj);
+
+    // C++:  int cv::ml::DTrees::getCVFolds()
+    private static native int getCVFolds_0(long nativeObj);
+
+    // C++:  int cv::ml::DTrees::getMaxCategories()
+    private static native int getMaxCategories_0(long nativeObj);
+
+    // C++:  int cv::ml::DTrees::getMaxDepth()
+    private static native int getMaxDepth_0(long nativeObj);
+
+    // C++:  int cv::ml::DTrees::getMinSampleCount()
+    private static native int getMinSampleCount_0(long nativeObj);
+
+    // C++:  void cv::ml::DTrees::setCVFolds(int val)
+    private static native void setCVFolds_0(long nativeObj, int val);
+
+    // C++:  void cv::ml::DTrees::setMaxCategories(int val)
+    private static native void setMaxCategories_0(long nativeObj, int val);
+
+    // C++:  void cv::ml::DTrees::setMaxDepth(int val)
+    private static native void setMaxDepth_0(long nativeObj, int val);
+
+    // C++:  void cv::ml::DTrees::setMinSampleCount(int val)
+    private static native void setMinSampleCount_0(long nativeObj, int val);
+
+    // C++:  void cv::ml::DTrees::setPriors(Mat val)
+    private static native void setPriors_0(long nativeObj, long val_nativeObj);
+
+    // C++:  void cv::ml::DTrees::setRegressionAccuracy(float val)
+    private static native void setRegressionAccuracy_0(long nativeObj, float val);
+
+    // C++:  void cv::ml::DTrees::setTruncatePrunedTree(bool val)
+    private static native void setTruncatePrunedTree_0(long nativeObj, boolean val);
+
+    // C++:  void cv::ml::DTrees::setUse1SERule(bool val)
+    private static native void setUse1SERule_0(long nativeObj, boolean val);
+
+    // C++:  void cv::ml::DTrees::setUseSurrogates(bool val)
+    private static native void setUseSurrogates_0(long nativeObj, boolean val);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/ml/EM.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/ml/EM.java	(date 1605830247864)
+++ openCVLibrary3411/src/main/java/org/opencv/ml/EM.java	(date 1605830247864)
@@ -0,0 +1,775 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.ml;
+
+import java.util.ArrayList;
+import java.util.List;
+import org.opencv.core.Mat;
+import org.opencv.core.TermCriteria;
+import org.opencv.ml.EM;
+import org.opencv.ml.StatModel;
+import org.opencv.utils.Converters;
+
+// C++: class EM
+/**
+ * The class implements the Expectation Maximization algorithm.
+ *
+ * SEE: REF: ml_intro_em
+ */
+public class EM extends StatModel {
+
+    protected EM(long addr) { super(addr); }
+
+    // internal usage only
+    public static EM __fromPtr__(long addr) { return new EM(addr); }
+
+    // C++: enum <unnamed>
+    public static final int
+            DEFAULT_NCLUSTERS = 5,
+            DEFAULT_MAX_ITERS = 100,
+            START_E_STEP = 1,
+            START_M_STEP = 2,
+            START_AUTO_STEP = 0;
+
+
+    // C++: enum Types
+    public static final int
+            COV_MAT_SPHERICAL = 0,
+            COV_MAT_DIAGONAL = 1,
+            COV_MAT_GENERIC = 2,
+            COV_MAT_DEFAULT = 1;
+
+
+    //
+    // C++:  Mat cv::ml::EM::getMeans()
+    //
+
+    /**
+     * Returns the cluster centers (means of the Gaussian mixture)
+     *
+     *     Returns matrix with the number of rows equal to the number of mixtures and number of columns
+     *     equal to the space dimensionality.
+     * @return automatically generated
+     */
+    public Mat getMeans() {
+        return new Mat(getMeans_0(nativeObj));
+    }
+
+
+    //
+    // C++:  Mat cv::ml::EM::getWeights()
+    //
+
+    /**
+     * Returns weights of the mixtures
+     *
+     *     Returns vector with the number of elements equal to the number of mixtures.
+     * @return automatically generated
+     */
+    public Mat getWeights() {
+        return new Mat(getWeights_0(nativeObj));
+    }
+
+
+    //
+    // C++: static Ptr_EM cv::ml::EM::create()
+    //
+
+    /**
+     * Creates empty %EM model.
+     *     The model should be trained then using StatModel::train(traindata, flags) method. Alternatively, you
+     *     can use one of the EM::train\* methods or load it from file using Algorithm::load&lt;EM&gt;(filename).
+     * @return automatically generated
+     */
+    public static EM create() {
+        return EM.__fromPtr__(create_0());
+    }
+
+
+    //
+    // C++: static Ptr_EM cv::ml::EM::load(String filepath, String nodeName = String())
+    //
+
+    /**
+     * Loads and creates a serialized EM from a file
+     *
+     * Use EM::save to serialize and store an EM to disk.
+     * Load the EM from this file again, by calling this function with the path to the file.
+     * Optionally specify the node for the file containing the classifier
+     *
+     * @param filepath path to serialized EM
+     * @param nodeName name of node containing the classifier
+     * @return automatically generated
+     */
+    public static EM load(String filepath, String nodeName) {
+        return EM.__fromPtr__(load_0(filepath, nodeName));
+    }
+
+    /**
+     * Loads and creates a serialized EM from a file
+     *
+     * Use EM::save to serialize and store an EM to disk.
+     * Load the EM from this file again, by calling this function with the path to the file.
+     * Optionally specify the node for the file containing the classifier
+     *
+     * @param filepath path to serialized EM
+     * @return automatically generated
+     */
+    public static EM load(String filepath) {
+        return EM.__fromPtr__(load_1(filepath));
+    }
+
+
+    //
+    // C++:  TermCriteria cv::ml::EM::getTermCriteria()
+    //
+
+    /**
+     * SEE: setTermCriteria
+     * @return automatically generated
+     */
+    public TermCriteria getTermCriteria() {
+        return new TermCriteria(getTermCriteria_0(nativeObj));
+    }
+
+
+    //
+    // C++:  Vec2d cv::ml::EM::predict2(Mat sample, Mat& probs)
+    //
+
+    /**
+     * Returns a likelihood logarithm value and an index of the most probable mixture component
+     *     for the given sample.
+     *
+     *     @param sample A sample for classification. It should be a one-channel matrix of
+     *         \(1 \times dims\) or \(dims \times 1\) size.
+     *     @param probs Optional output matrix that contains posterior probabilities of each component
+     *         given the sample. It has \(1 \times nclusters\) size and CV_64FC1 type.
+     *
+     *     The method returns a two-element double vector. Zero element is a likelihood logarithm value for
+     *     the sample. First element is an index of the most probable mixture component for the given
+     *     sample.
+     * @return automatically generated
+     */
+    public double[] predict2(Mat sample, Mat probs) {
+        return predict2_0(nativeObj, sample.nativeObj, probs.nativeObj);
+    }
+
+
+    //
+    // C++:  bool cv::ml::EM::trainE(Mat samples, Mat means0, Mat covs0 = Mat(), Mat weights0 = Mat(), Mat& logLikelihoods = Mat(), Mat& labels = Mat(), Mat& probs = Mat())
+    //
+
+    /**
+     * Estimate the Gaussian mixture parameters from a samples set.
+     *
+     *     This variation starts with Expectation step. You need to provide initial means \(a_k\) of
+     *     mixture components. Optionally you can pass initial weights \(\pi_k\) and covariance matrices
+     *     \(S_k\) of mixture components.
+     *
+     *     @param samples Samples from which the Gaussian mixture model will be estimated. It should be a
+     *         one-channel matrix, each row of which is a sample. If the matrix does not have CV_64F type
+     *         it will be converted to the inner matrix of such type for the further computing.
+     *     @param means0 Initial means \(a_k\) of mixture components. It is a one-channel matrix of
+     *         \(nclusters \times dims\) size. If the matrix does not have CV_64F type it will be
+     *         converted to the inner matrix of such type for the further computing.
+     *     @param covs0 The vector of initial covariance matrices \(S_k\) of mixture components. Each of
+     *         covariance matrices is a one-channel matrix of \(dims \times dims\) size. If the matrices
+     *         do not have CV_64F type they will be converted to the inner matrices of such type for the
+     *         further computing.
+     *     @param weights0 Initial weights \(\pi_k\) of mixture components. It should be a one-channel
+     *         floating-point matrix with \(1 \times nclusters\) or \(nclusters \times 1\) size.
+     *     @param logLikelihoods The optional output matrix that contains a likelihood logarithm value for
+     *         each sample. It has \(nsamples \times 1\) size and CV_64FC1 type.
+     *     @param labels The optional output "class label" for each sample:
+     *         \(\texttt{labels}_i=\texttt{arg max}_k(p_{i,k}), i=1..N\) (indices of the most probable
+     *         mixture component for each sample). It has \(nsamples \times 1\) size and CV_32SC1 type.
+     *     @param probs The optional output matrix that contains posterior probabilities of each Gaussian
+     *         mixture component given the each sample. It has \(nsamples \times nclusters\) size and
+     *         CV_64FC1 type.
+     * @return automatically generated
+     */
+    public boolean trainE(Mat samples, Mat means0, Mat covs0, Mat weights0, Mat logLikelihoods, Mat labels, Mat probs) {
+        return trainE_0(nativeObj, samples.nativeObj, means0.nativeObj, covs0.nativeObj, weights0.nativeObj, logLikelihoods.nativeObj, labels.nativeObj, probs.nativeObj);
+    }
+
+    /**
+     * Estimate the Gaussian mixture parameters from a samples set.
+     *
+     *     This variation starts with Expectation step. You need to provide initial means \(a_k\) of
+     *     mixture components. Optionally you can pass initial weights \(\pi_k\) and covariance matrices
+     *     \(S_k\) of mixture components.
+     *
+     *     @param samples Samples from which the Gaussian mixture model will be estimated. It should be a
+     *         one-channel matrix, each row of which is a sample. If the matrix does not have CV_64F type
+     *         it will be converted to the inner matrix of such type for the further computing.
+     *     @param means0 Initial means \(a_k\) of mixture components. It is a one-channel matrix of
+     *         \(nclusters \times dims\) size. If the matrix does not have CV_64F type it will be
+     *         converted to the inner matrix of such type for the further computing.
+     *     @param covs0 The vector of initial covariance matrices \(S_k\) of mixture components. Each of
+     *         covariance matrices is a one-channel matrix of \(dims \times dims\) size. If the matrices
+     *         do not have CV_64F type they will be converted to the inner matrices of such type for the
+     *         further computing.
+     *     @param weights0 Initial weights \(\pi_k\) of mixture components. It should be a one-channel
+     *         floating-point matrix with \(1 \times nclusters\) or \(nclusters \times 1\) size.
+     *     @param logLikelihoods The optional output matrix that contains a likelihood logarithm value for
+     *         each sample. It has \(nsamples \times 1\) size and CV_64FC1 type.
+     *     @param labels The optional output "class label" for each sample:
+     *         \(\texttt{labels}_i=\texttt{arg max}_k(p_{i,k}), i=1..N\) (indices of the most probable
+     *         mixture component for each sample). It has \(nsamples \times 1\) size and CV_32SC1 type.
+     *         mixture component given the each sample. It has \(nsamples \times nclusters\) size and
+     *         CV_64FC1 type.
+     * @return automatically generated
+     */
+    public boolean trainE(Mat samples, Mat means0, Mat covs0, Mat weights0, Mat logLikelihoods, Mat labels) {
+        return trainE_1(nativeObj, samples.nativeObj, means0.nativeObj, covs0.nativeObj, weights0.nativeObj, logLikelihoods.nativeObj, labels.nativeObj);
+    }
+
+    /**
+     * Estimate the Gaussian mixture parameters from a samples set.
+     *
+     *     This variation starts with Expectation step. You need to provide initial means \(a_k\) of
+     *     mixture components. Optionally you can pass initial weights \(\pi_k\) and covariance matrices
+     *     \(S_k\) of mixture components.
+     *
+     *     @param samples Samples from which the Gaussian mixture model will be estimated. It should be a
+     *         one-channel matrix, each row of which is a sample. If the matrix does not have CV_64F type
+     *         it will be converted to the inner matrix of such type for the further computing.
+     *     @param means0 Initial means \(a_k\) of mixture components. It is a one-channel matrix of
+     *         \(nclusters \times dims\) size. If the matrix does not have CV_64F type it will be
+     *         converted to the inner matrix of such type for the further computing.
+     *     @param covs0 The vector of initial covariance matrices \(S_k\) of mixture components. Each of
+     *         covariance matrices is a one-channel matrix of \(dims \times dims\) size. If the matrices
+     *         do not have CV_64F type they will be converted to the inner matrices of such type for the
+     *         further computing.
+     *     @param weights0 Initial weights \(\pi_k\) of mixture components. It should be a one-channel
+     *         floating-point matrix with \(1 \times nclusters\) or \(nclusters \times 1\) size.
+     *     @param logLikelihoods The optional output matrix that contains a likelihood logarithm value for
+     *         each sample. It has \(nsamples \times 1\) size and CV_64FC1 type.
+     *         \(\texttt{labels}_i=\texttt{arg max}_k(p_{i,k}), i=1..N\) (indices of the most probable
+     *         mixture component for each sample). It has \(nsamples \times 1\) size and CV_32SC1 type.
+     *         mixture component given the each sample. It has \(nsamples \times nclusters\) size and
+     *         CV_64FC1 type.
+     * @return automatically generated
+     */
+    public boolean trainE(Mat samples, Mat means0, Mat covs0, Mat weights0, Mat logLikelihoods) {
+        return trainE_2(nativeObj, samples.nativeObj, means0.nativeObj, covs0.nativeObj, weights0.nativeObj, logLikelihoods.nativeObj);
+    }
+
+    /**
+     * Estimate the Gaussian mixture parameters from a samples set.
+     *
+     *     This variation starts with Expectation step. You need to provide initial means \(a_k\) of
+     *     mixture components. Optionally you can pass initial weights \(\pi_k\) and covariance matrices
+     *     \(S_k\) of mixture components.
+     *
+     *     @param samples Samples from which the Gaussian mixture model will be estimated. It should be a
+     *         one-channel matrix, each row of which is a sample. If the matrix does not have CV_64F type
+     *         it will be converted to the inner matrix of such type for the further computing.
+     *     @param means0 Initial means \(a_k\) of mixture components. It is a one-channel matrix of
+     *         \(nclusters \times dims\) size. If the matrix does not have CV_64F type it will be
+     *         converted to the inner matrix of such type for the further computing.
+     *     @param covs0 The vector of initial covariance matrices \(S_k\) of mixture components. Each of
+     *         covariance matrices is a one-channel matrix of \(dims \times dims\) size. If the matrices
+     *         do not have CV_64F type they will be converted to the inner matrices of such type for the
+     *         further computing.
+     *     @param weights0 Initial weights \(\pi_k\) of mixture components. It should be a one-channel
+     *         floating-point matrix with \(1 \times nclusters\) or \(nclusters \times 1\) size.
+     *         each sample. It has \(nsamples \times 1\) size and CV_64FC1 type.
+     *         \(\texttt{labels}_i=\texttt{arg max}_k(p_{i,k}), i=1..N\) (indices of the most probable
+     *         mixture component for each sample). It has \(nsamples \times 1\) size and CV_32SC1 type.
+     *         mixture component given the each sample. It has \(nsamples \times nclusters\) size and
+     *         CV_64FC1 type.
+     * @return automatically generated
+     */
+    public boolean trainE(Mat samples, Mat means0, Mat covs0, Mat weights0) {
+        return trainE_3(nativeObj, samples.nativeObj, means0.nativeObj, covs0.nativeObj, weights0.nativeObj);
+    }
+
+    /**
+     * Estimate the Gaussian mixture parameters from a samples set.
+     *
+     *     This variation starts with Expectation step. You need to provide initial means \(a_k\) of
+     *     mixture components. Optionally you can pass initial weights \(\pi_k\) and covariance matrices
+     *     \(S_k\) of mixture components.
+     *
+     *     @param samples Samples from which the Gaussian mixture model will be estimated. It should be a
+     *         one-channel matrix, each row of which is a sample. If the matrix does not have CV_64F type
+     *         it will be converted to the inner matrix of such type for the further computing.
+     *     @param means0 Initial means \(a_k\) of mixture components. It is a one-channel matrix of
+     *         \(nclusters \times dims\) size. If the matrix does not have CV_64F type it will be
+     *         converted to the inner matrix of such type for the further computing.
+     *     @param covs0 The vector of initial covariance matrices \(S_k\) of mixture components. Each of
+     *         covariance matrices is a one-channel matrix of \(dims \times dims\) size. If the matrices
+     *         do not have CV_64F type they will be converted to the inner matrices of such type for the
+     *         further computing.
+     *         floating-point matrix with \(1 \times nclusters\) or \(nclusters \times 1\) size.
+     *         each sample. It has \(nsamples \times 1\) size and CV_64FC1 type.
+     *         \(\texttt{labels}_i=\texttt{arg max}_k(p_{i,k}), i=1..N\) (indices of the most probable
+     *         mixture component for each sample). It has \(nsamples \times 1\) size and CV_32SC1 type.
+     *         mixture component given the each sample. It has \(nsamples \times nclusters\) size and
+     *         CV_64FC1 type.
+     * @return automatically generated
+     */
+    public boolean trainE(Mat samples, Mat means0, Mat covs0) {
+        return trainE_4(nativeObj, samples.nativeObj, means0.nativeObj, covs0.nativeObj);
+    }
+
+    /**
+     * Estimate the Gaussian mixture parameters from a samples set.
+     *
+     *     This variation starts with Expectation step. You need to provide initial means \(a_k\) of
+     *     mixture components. Optionally you can pass initial weights \(\pi_k\) and covariance matrices
+     *     \(S_k\) of mixture components.
+     *
+     *     @param samples Samples from which the Gaussian mixture model will be estimated. It should be a
+     *         one-channel matrix, each row of which is a sample. If the matrix does not have CV_64F type
+     *         it will be converted to the inner matrix of such type for the further computing.
+     *     @param means0 Initial means \(a_k\) of mixture components. It is a one-channel matrix of
+     *         \(nclusters \times dims\) size. If the matrix does not have CV_64F type it will be
+     *         converted to the inner matrix of such type for the further computing.
+     *         covariance matrices is a one-channel matrix of \(dims \times dims\) size. If the matrices
+     *         do not have CV_64F type they will be converted to the inner matrices of such type for the
+     *         further computing.
+     *         floating-point matrix with \(1 \times nclusters\) or \(nclusters \times 1\) size.
+     *         each sample. It has \(nsamples \times 1\) size and CV_64FC1 type.
+     *         \(\texttt{labels}_i=\texttt{arg max}_k(p_{i,k}), i=1..N\) (indices of the most probable
+     *         mixture component for each sample). It has \(nsamples \times 1\) size and CV_32SC1 type.
+     *         mixture component given the each sample. It has \(nsamples \times nclusters\) size and
+     *         CV_64FC1 type.
+     * @return automatically generated
+     */
+    public boolean trainE(Mat samples, Mat means0) {
+        return trainE_5(nativeObj, samples.nativeObj, means0.nativeObj);
+    }
+
+
+    //
+    // C++:  bool cv::ml::EM::trainEM(Mat samples, Mat& logLikelihoods = Mat(), Mat& labels = Mat(), Mat& probs = Mat())
+    //
+
+    /**
+     * Estimate the Gaussian mixture parameters from a samples set.
+     *
+     *     This variation starts with Expectation step. Initial values of the model parameters will be
+     *     estimated by the k-means algorithm.
+     *
+     *     Unlike many of the ML models, %EM is an unsupervised learning algorithm and it does not take
+     *     responses (class labels or function values) as input. Instead, it computes the *Maximum
+     *     Likelihood Estimate* of the Gaussian mixture parameters from an input sample set, stores all the
+     *     parameters inside the structure: \(p_{i,k}\) in probs, \(a_k\) in means , \(S_k\) in
+     *     covs[k], \(\pi_k\) in weights , and optionally computes the output "class label" for each
+     *     sample: \(\texttt{labels}_i=\texttt{arg max}_k(p_{i,k}), i=1..N\) (indices of the most
+     *     probable mixture component for each sample).
+     *
+     *     The trained model can be used further for prediction, just like any other classifier. The
+     *     trained model is similar to the NormalBayesClassifier.
+     *
+     *     @param samples Samples from which the Gaussian mixture model will be estimated. It should be a
+     *         one-channel matrix, each row of which is a sample. If the matrix does not have CV_64F type
+     *         it will be converted to the inner matrix of such type for the further computing.
+     *     @param logLikelihoods The optional output matrix that contains a likelihood logarithm value for
+     *         each sample. It has \(nsamples \times 1\) size and CV_64FC1 type.
+     *     @param labels The optional output "class label" for each sample:
+     *         \(\texttt{labels}_i=\texttt{arg max}_k(p_{i,k}), i=1..N\) (indices of the most probable
+     *         mixture component for each sample). It has \(nsamples \times 1\) size and CV_32SC1 type.
+     *     @param probs The optional output matrix that contains posterior probabilities of each Gaussian
+     *         mixture component given the each sample. It has \(nsamples \times nclusters\) size and
+     *         CV_64FC1 type.
+     * @return automatically generated
+     */
+    public boolean trainEM(Mat samples, Mat logLikelihoods, Mat labels, Mat probs) {
+        return trainEM_0(nativeObj, samples.nativeObj, logLikelihoods.nativeObj, labels.nativeObj, probs.nativeObj);
+    }
+
+    /**
+     * Estimate the Gaussian mixture parameters from a samples set.
+     *
+     *     This variation starts with Expectation step. Initial values of the model parameters will be
+     *     estimated by the k-means algorithm.
+     *
+     *     Unlike many of the ML models, %EM is an unsupervised learning algorithm and it does not take
+     *     responses (class labels or function values) as input. Instead, it computes the *Maximum
+     *     Likelihood Estimate* of the Gaussian mixture parameters from an input sample set, stores all the
+     *     parameters inside the structure: \(p_{i,k}\) in probs, \(a_k\) in means , \(S_k\) in
+     *     covs[k], \(\pi_k\) in weights , and optionally computes the output "class label" for each
+     *     sample: \(\texttt{labels}_i=\texttt{arg max}_k(p_{i,k}), i=1..N\) (indices of the most
+     *     probable mixture component for each sample).
+     *
+     *     The trained model can be used further for prediction, just like any other classifier. The
+     *     trained model is similar to the NormalBayesClassifier.
+     *
+     *     @param samples Samples from which the Gaussian mixture model will be estimated. It should be a
+     *         one-channel matrix, each row of which is a sample. If the matrix does not have CV_64F type
+     *         it will be converted to the inner matrix of such type for the further computing.
+     *     @param logLikelihoods The optional output matrix that contains a likelihood logarithm value for
+     *         each sample. It has \(nsamples \times 1\) size and CV_64FC1 type.
+     *     @param labels The optional output "class label" for each sample:
+     *         \(\texttt{labels}_i=\texttt{arg max}_k(p_{i,k}), i=1..N\) (indices of the most probable
+     *         mixture component for each sample). It has \(nsamples \times 1\) size and CV_32SC1 type.
+     *         mixture component given the each sample. It has \(nsamples \times nclusters\) size and
+     *         CV_64FC1 type.
+     * @return automatically generated
+     */
+    public boolean trainEM(Mat samples, Mat logLikelihoods, Mat labels) {
+        return trainEM_1(nativeObj, samples.nativeObj, logLikelihoods.nativeObj, labels.nativeObj);
+    }
+
+    /**
+     * Estimate the Gaussian mixture parameters from a samples set.
+     *
+     *     This variation starts with Expectation step. Initial values of the model parameters will be
+     *     estimated by the k-means algorithm.
+     *
+     *     Unlike many of the ML models, %EM is an unsupervised learning algorithm and it does not take
+     *     responses (class labels or function values) as input. Instead, it computes the *Maximum
+     *     Likelihood Estimate* of the Gaussian mixture parameters from an input sample set, stores all the
+     *     parameters inside the structure: \(p_{i,k}\) in probs, \(a_k\) in means , \(S_k\) in
+     *     covs[k], \(\pi_k\) in weights , and optionally computes the output "class label" for each
+     *     sample: \(\texttt{labels}_i=\texttt{arg max}_k(p_{i,k}), i=1..N\) (indices of the most
+     *     probable mixture component for each sample).
+     *
+     *     The trained model can be used further for prediction, just like any other classifier. The
+     *     trained model is similar to the NormalBayesClassifier.
+     *
+     *     @param samples Samples from which the Gaussian mixture model will be estimated. It should be a
+     *         one-channel matrix, each row of which is a sample. If the matrix does not have CV_64F type
+     *         it will be converted to the inner matrix of such type for the further computing.
+     *     @param logLikelihoods The optional output matrix that contains a likelihood logarithm value for
+     *         each sample. It has \(nsamples \times 1\) size and CV_64FC1 type.
+     *         \(\texttt{labels}_i=\texttt{arg max}_k(p_{i,k}), i=1..N\) (indices of the most probable
+     *         mixture component for each sample). It has \(nsamples \times 1\) size and CV_32SC1 type.
+     *         mixture component given the each sample. It has \(nsamples \times nclusters\) size and
+     *         CV_64FC1 type.
+     * @return automatically generated
+     */
+    public boolean trainEM(Mat samples, Mat logLikelihoods) {
+        return trainEM_2(nativeObj, samples.nativeObj, logLikelihoods.nativeObj);
+    }
+
+    /**
+     * Estimate the Gaussian mixture parameters from a samples set.
+     *
+     *     This variation starts with Expectation step. Initial values of the model parameters will be
+     *     estimated by the k-means algorithm.
+     *
+     *     Unlike many of the ML models, %EM is an unsupervised learning algorithm and it does not take
+     *     responses (class labels or function values) as input. Instead, it computes the *Maximum
+     *     Likelihood Estimate* of the Gaussian mixture parameters from an input sample set, stores all the
+     *     parameters inside the structure: \(p_{i,k}\) in probs, \(a_k\) in means , \(S_k\) in
+     *     covs[k], \(\pi_k\) in weights , and optionally computes the output "class label" for each
+     *     sample: \(\texttt{labels}_i=\texttt{arg max}_k(p_{i,k}), i=1..N\) (indices of the most
+     *     probable mixture component for each sample).
+     *
+     *     The trained model can be used further for prediction, just like any other classifier. The
+     *     trained model is similar to the NormalBayesClassifier.
+     *
+     *     @param samples Samples from which the Gaussian mixture model will be estimated. It should be a
+     *         one-channel matrix, each row of which is a sample. If the matrix does not have CV_64F type
+     *         it will be converted to the inner matrix of such type for the further computing.
+     *         each sample. It has \(nsamples \times 1\) size and CV_64FC1 type.
+     *         \(\texttt{labels}_i=\texttt{arg max}_k(p_{i,k}), i=1..N\) (indices of the most probable
+     *         mixture component for each sample). It has \(nsamples \times 1\) size and CV_32SC1 type.
+     *         mixture component given the each sample. It has \(nsamples \times nclusters\) size and
+     *         CV_64FC1 type.
+     * @return automatically generated
+     */
+    public boolean trainEM(Mat samples) {
+        return trainEM_3(nativeObj, samples.nativeObj);
+    }
+
+
+    //
+    // C++:  bool cv::ml::EM::trainM(Mat samples, Mat probs0, Mat& logLikelihoods = Mat(), Mat& labels = Mat(), Mat& probs = Mat())
+    //
+
+    /**
+     * Estimate the Gaussian mixture parameters from a samples set.
+     *
+     *     This variation starts with Maximization step. You need to provide initial probabilities
+     *     \(p_{i,k}\) to use this option.
+     *
+     *     @param samples Samples from which the Gaussian mixture model will be estimated. It should be a
+     *         one-channel matrix, each row of which is a sample. If the matrix does not have CV_64F type
+     *         it will be converted to the inner matrix of such type for the further computing.
+     *     @param probs0 the probabilities
+     *     @param logLikelihoods The optional output matrix that contains a likelihood logarithm value for
+     *         each sample. It has \(nsamples \times 1\) size and CV_64FC1 type.
+     *     @param labels The optional output "class label" for each sample:
+     *         \(\texttt{labels}_i=\texttt{arg max}_k(p_{i,k}), i=1..N\) (indices of the most probable
+     *         mixture component for each sample). It has \(nsamples \times 1\) size and CV_32SC1 type.
+     *     @param probs The optional output matrix that contains posterior probabilities of each Gaussian
+     *         mixture component given the each sample. It has \(nsamples \times nclusters\) size and
+     *         CV_64FC1 type.
+     * @return automatically generated
+     */
+    public boolean trainM(Mat samples, Mat probs0, Mat logLikelihoods, Mat labels, Mat probs) {
+        return trainM_0(nativeObj, samples.nativeObj, probs0.nativeObj, logLikelihoods.nativeObj, labels.nativeObj, probs.nativeObj);
+    }
+
+    /**
+     * Estimate the Gaussian mixture parameters from a samples set.
+     *
+     *     This variation starts with Maximization step. You need to provide initial probabilities
+     *     \(p_{i,k}\) to use this option.
+     *
+     *     @param samples Samples from which the Gaussian mixture model will be estimated. It should be a
+     *         one-channel matrix, each row of which is a sample. If the matrix does not have CV_64F type
+     *         it will be converted to the inner matrix of such type for the further computing.
+     *     @param probs0 the probabilities
+     *     @param logLikelihoods The optional output matrix that contains a likelihood logarithm value for
+     *         each sample. It has \(nsamples \times 1\) size and CV_64FC1 type.
+     *     @param labels The optional output "class label" for each sample:
+     *         \(\texttt{labels}_i=\texttt{arg max}_k(p_{i,k}), i=1..N\) (indices of the most probable
+     *         mixture component for each sample). It has \(nsamples \times 1\) size and CV_32SC1 type.
+     *         mixture component given the each sample. It has \(nsamples \times nclusters\) size and
+     *         CV_64FC1 type.
+     * @return automatically generated
+     */
+    public boolean trainM(Mat samples, Mat probs0, Mat logLikelihoods, Mat labels) {
+        return trainM_1(nativeObj, samples.nativeObj, probs0.nativeObj, logLikelihoods.nativeObj, labels.nativeObj);
+    }
+
+    /**
+     * Estimate the Gaussian mixture parameters from a samples set.
+     *
+     *     This variation starts with Maximization step. You need to provide initial probabilities
+     *     \(p_{i,k}\) to use this option.
+     *
+     *     @param samples Samples from which the Gaussian mixture model will be estimated. It should be a
+     *         one-channel matrix, each row of which is a sample. If the matrix does not have CV_64F type
+     *         it will be converted to the inner matrix of such type for the further computing.
+     *     @param probs0 the probabilities
+     *     @param logLikelihoods The optional output matrix that contains a likelihood logarithm value for
+     *         each sample. It has \(nsamples \times 1\) size and CV_64FC1 type.
+     *         \(\texttt{labels}_i=\texttt{arg max}_k(p_{i,k}), i=1..N\) (indices of the most probable
+     *         mixture component for each sample). It has \(nsamples \times 1\) size and CV_32SC1 type.
+     *         mixture component given the each sample. It has \(nsamples \times nclusters\) size and
+     *         CV_64FC1 type.
+     * @return automatically generated
+     */
+    public boolean trainM(Mat samples, Mat probs0, Mat logLikelihoods) {
+        return trainM_2(nativeObj, samples.nativeObj, probs0.nativeObj, logLikelihoods.nativeObj);
+    }
+
+    /**
+     * Estimate the Gaussian mixture parameters from a samples set.
+     *
+     *     This variation starts with Maximization step. You need to provide initial probabilities
+     *     \(p_{i,k}\) to use this option.
+     *
+     *     @param samples Samples from which the Gaussian mixture model will be estimated. It should be a
+     *         one-channel matrix, each row of which is a sample. If the matrix does not have CV_64F type
+     *         it will be converted to the inner matrix of such type for the further computing.
+     *     @param probs0 the probabilities
+     *         each sample. It has \(nsamples \times 1\) size and CV_64FC1 type.
+     *         \(\texttt{labels}_i=\texttt{arg max}_k(p_{i,k}), i=1..N\) (indices of the most probable
+     *         mixture component for each sample). It has \(nsamples \times 1\) size and CV_32SC1 type.
+     *         mixture component given the each sample. It has \(nsamples \times nclusters\) size and
+     *         CV_64FC1 type.
+     * @return automatically generated
+     */
+    public boolean trainM(Mat samples, Mat probs0) {
+        return trainM_3(nativeObj, samples.nativeObj, probs0.nativeObj);
+    }
+
+
+    //
+    // C++:  float cv::ml::EM::predict(Mat samples, Mat& results = Mat(), int flags = 0)
+    //
+
+    /**
+     * Returns posterior probabilities for the provided samples
+     *
+     *     @param samples The input samples, floating-point matrix
+     *     @param results The optional output \( nSamples \times nClusters\) matrix of results. It contains
+     *     posterior probabilities for each sample from the input
+     *     @param flags This parameter will be ignored
+     * @return automatically generated
+     */
+    public float predict(Mat samples, Mat results, int flags) {
+        return predict_0(nativeObj, samples.nativeObj, results.nativeObj, flags);
+    }
+
+    /**
+     * Returns posterior probabilities for the provided samples
+     *
+     *     @param samples The input samples, floating-point matrix
+     *     @param results The optional output \( nSamples \times nClusters\) matrix of results. It contains
+     *     posterior probabilities for each sample from the input
+     * @return automatically generated
+     */
+    public float predict(Mat samples, Mat results) {
+        return predict_1(nativeObj, samples.nativeObj, results.nativeObj);
+    }
+
+    /**
+     * Returns posterior probabilities for the provided samples
+     *
+     *     @param samples The input samples, floating-point matrix
+     *     posterior probabilities for each sample from the input
+     * @return automatically generated
+     */
+    public float predict(Mat samples) {
+        return predict_2(nativeObj, samples.nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::ml::EM::getClustersNumber()
+    //
+
+    /**
+     * SEE: setClustersNumber
+     * @return automatically generated
+     */
+    public int getClustersNumber() {
+        return getClustersNumber_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::ml::EM::getCovarianceMatrixType()
+    //
+
+    /**
+     * SEE: setCovarianceMatrixType
+     * @return automatically generated
+     */
+    public int getCovarianceMatrixType() {
+        return getCovarianceMatrixType_0(nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::ml::EM::getCovs(vector_Mat& covs)
+    //
+
+    /**
+     * Returns covariation matrices
+     *
+     *     Returns vector of covariation matrices. Number of matrices is the number of gaussian mixtures,
+     *     each matrix is a square floating-point matrix NxN, where N is the space dimensionality.
+     * @param covs automatically generated
+     */
+    public void getCovs(List<Mat> covs) {
+        Mat covs_mat = new Mat();
+        getCovs_0(nativeObj, covs_mat.nativeObj);
+        Converters.Mat_to_vector_Mat(covs_mat, covs);
+        covs_mat.release();
+    }
+
+
+    //
+    // C++:  void cv::ml::EM::setClustersNumber(int val)
+    //
+
+    /**
+     *  getClustersNumber SEE: getClustersNumber
+     * @param val automatically generated
+     */
+    public void setClustersNumber(int val) {
+        setClustersNumber_0(nativeObj, val);
+    }
+
+
+    //
+    // C++:  void cv::ml::EM::setCovarianceMatrixType(int val)
+    //
+
+    /**
+     *  getCovarianceMatrixType SEE: getCovarianceMatrixType
+     * @param val automatically generated
+     */
+    public void setCovarianceMatrixType(int val) {
+        setCovarianceMatrixType_0(nativeObj, val);
+    }
+
+
+    //
+    // C++:  void cv::ml::EM::setTermCriteria(TermCriteria val)
+    //
+
+    /**
+     *  getTermCriteria SEE: getTermCriteria
+     * @param val automatically generated
+     */
+    public void setTermCriteria(TermCriteria val) {
+        setTermCriteria_0(nativeObj, val.type, val.maxCount, val.epsilon);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++:  Mat cv::ml::EM::getMeans()
+    private static native long getMeans_0(long nativeObj);
+
+    // C++:  Mat cv::ml::EM::getWeights()
+    private static native long getWeights_0(long nativeObj);
+
+    // C++: static Ptr_EM cv::ml::EM::create()
+    private static native long create_0();
+
+    // C++: static Ptr_EM cv::ml::EM::load(String filepath, String nodeName = String())
+    private static native long load_0(String filepath, String nodeName);
+    private static native long load_1(String filepath);
+
+    // C++:  TermCriteria cv::ml::EM::getTermCriteria()
+    private static native double[] getTermCriteria_0(long nativeObj);
+
+    // C++:  Vec2d cv::ml::EM::predict2(Mat sample, Mat& probs)
+    private static native double[] predict2_0(long nativeObj, long sample_nativeObj, long probs_nativeObj);
+
+    // C++:  bool cv::ml::EM::trainE(Mat samples, Mat means0, Mat covs0 = Mat(), Mat weights0 = Mat(), Mat& logLikelihoods = Mat(), Mat& labels = Mat(), Mat& probs = Mat())
+    private static native boolean trainE_0(long nativeObj, long samples_nativeObj, long means0_nativeObj, long covs0_nativeObj, long weights0_nativeObj, long logLikelihoods_nativeObj, long labels_nativeObj, long probs_nativeObj);
+    private static native boolean trainE_1(long nativeObj, long samples_nativeObj, long means0_nativeObj, long covs0_nativeObj, long weights0_nativeObj, long logLikelihoods_nativeObj, long labels_nativeObj);
+    private static native boolean trainE_2(long nativeObj, long samples_nativeObj, long means0_nativeObj, long covs0_nativeObj, long weights0_nativeObj, long logLikelihoods_nativeObj);
+    private static native boolean trainE_3(long nativeObj, long samples_nativeObj, long means0_nativeObj, long covs0_nativeObj, long weights0_nativeObj);
+    private static native boolean trainE_4(long nativeObj, long samples_nativeObj, long means0_nativeObj, long covs0_nativeObj);
+    private static native boolean trainE_5(long nativeObj, long samples_nativeObj, long means0_nativeObj);
+
+    // C++:  bool cv::ml::EM::trainEM(Mat samples, Mat& logLikelihoods = Mat(), Mat& labels = Mat(), Mat& probs = Mat())
+    private static native boolean trainEM_0(long nativeObj, long samples_nativeObj, long logLikelihoods_nativeObj, long labels_nativeObj, long probs_nativeObj);
+    private static native boolean trainEM_1(long nativeObj, long samples_nativeObj, long logLikelihoods_nativeObj, long labels_nativeObj);
+    private static native boolean trainEM_2(long nativeObj, long samples_nativeObj, long logLikelihoods_nativeObj);
+    private static native boolean trainEM_3(long nativeObj, long samples_nativeObj);
+
+    // C++:  bool cv::ml::EM::trainM(Mat samples, Mat probs0, Mat& logLikelihoods = Mat(), Mat& labels = Mat(), Mat& probs = Mat())
+    private static native boolean trainM_0(long nativeObj, long samples_nativeObj, long probs0_nativeObj, long logLikelihoods_nativeObj, long labels_nativeObj, long probs_nativeObj);
+    private static native boolean trainM_1(long nativeObj, long samples_nativeObj, long probs0_nativeObj, long logLikelihoods_nativeObj, long labels_nativeObj);
+    private static native boolean trainM_2(long nativeObj, long samples_nativeObj, long probs0_nativeObj, long logLikelihoods_nativeObj);
+    private static native boolean trainM_3(long nativeObj, long samples_nativeObj, long probs0_nativeObj);
+
+    // C++:  float cv::ml::EM::predict(Mat samples, Mat& results = Mat(), int flags = 0)
+    private static native float predict_0(long nativeObj, long samples_nativeObj, long results_nativeObj, int flags);
+    private static native float predict_1(long nativeObj, long samples_nativeObj, long results_nativeObj);
+    private static native float predict_2(long nativeObj, long samples_nativeObj);
+
+    // C++:  int cv::ml::EM::getClustersNumber()
+    private static native int getClustersNumber_0(long nativeObj);
+
+    // C++:  int cv::ml::EM::getCovarianceMatrixType()
+    private static native int getCovarianceMatrixType_0(long nativeObj);
+
+    // C++:  void cv::ml::EM::getCovs(vector_Mat& covs)
+    private static native void getCovs_0(long nativeObj, long covs_mat_nativeObj);
+
+    // C++:  void cv::ml::EM::setClustersNumber(int val)
+    private static native void setClustersNumber_0(long nativeObj, int val);
+
+    // C++:  void cv::ml::EM::setCovarianceMatrixType(int val)
+    private static native void setCovarianceMatrixType_0(long nativeObj, int val);
+
+    // C++:  void cv::ml::EM::setTermCriteria(TermCriteria val)
+    private static native void setTermCriteria_0(long nativeObj, int val_type, int val_maxCount, double val_epsilon);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: app/src/main/java/com/maksym/findthis/OpenCVmagic/DetectionEngine.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- app/src/main/java/com/maksym/findthis/OpenCVmagic/DetectionEngine.java	(revision cc086539aa8e7367bf4575d32b97aa049d6faad9)
+++ app/src/main/java/com/maksym/findthis/OpenCVmagic/DetectionEngine.java	(date 1607973848905)
@@ -9,13 +9,17 @@
 import org.opencv.android.OpenCVLoader;
 import org.opencv.android.Utils;
 import org.opencv.calib3d.Calib3d;
+import org.opencv.core.Core;
+import org.opencv.core.CvType;
 import org.opencv.core.DMatch;
 import org.opencv.core.KeyPoint;
 import org.opencv.core.Mat;
+import org.opencv.core.MatOfByte;
 import org.opencv.core.MatOfDMatch;
 import org.opencv.core.MatOfKeyPoint;
 import org.opencv.core.MatOfPoint2f;
 import org.opencv.core.Point;
+import org.opencv.core.Scalar;
 import org.opencv.features2d.AKAZE;
 import org.opencv.features2d.BRISK;
 import org.opencv.features2d.DescriptorMatcher;
@@ -129,6 +133,19 @@
             return SIFT.create();///////////////////// TODO default detector
     }
 
+    // {"FAST","GFTT"};
+    public Feature2D selectTracker(int trackerId){
+        switch (trackerId){
+            case Constants.GFTT_DETECTOR_ID:
+                return GFTTDetector.create();
+            case Constants.FAST_DETECTOR_ID:
+                return FastFeatureDetector.create();
+            case Constants.ORB_DETECTOR_ID:
+            default:
+                return ORB.create();
+        }
+    }
+
 
 
     private MatOfKeyPoint doDetection(Feature2D detector, Mat inputImage){
@@ -142,7 +159,7 @@
 
 
 
-    public void matchObjects(Mat object, Mat objectInScene, Feature2D detector, MatchObjectsCallback callback){
+    public void matchObjects(Mat object, Mat objectInScene, Feature2D detector){
         Log.d(TAG, "trying to match...");
 
         //-- Step 1: Detect the keypoints using detector, compute the descriptors
@@ -158,7 +175,7 @@
 
         //-- Step 2: Matching descriptor vectors with a FLANN based matcher
         // Since SURF is a floating-point descriptor NORM_L2 is used
-        DescriptorMatcher matcher = DescriptorMatcher.create(DescriptorMatcher.FLANNBASED);
+        DescriptorMatcher matcher = DescriptorMatcher.create(DescriptorMatcher.BRUTEFORCE_HAMMING);
         List<MatOfDMatch> knnMatches = new ArrayList<>();
 
         matcher.knnMatch(descriptorsObject, descriptorsScene, knnMatches, 2);
@@ -175,13 +192,14 @@
             }
         }
         double mathcesRatio = (double) listOfGoodMatches.size() / knnMatches.size();
+
         if (mathcesRatio > Constants.MATCHED_FEATURES_THRESHOLD) {
             MatOfDMatch goodMatches = new MatOfDMatch();
             goodMatches.fromList(listOfGoodMatches);
 
-//        //-- Draw matches
-//        Mat imgMatches = new Mat();
-//        Features2d.drawMatches(imgObject, keypointsObject, imgScene, keypointsScene, goodMatches, imgMatches, Scalar.all(-1),
+        //-- Draw matches
+        Mat imgMatches = objectInScene;
+//        Features2d.drawMatches(object, keypointsObject, objectInScene, keypointsScene, goodMatches, imgMatches, Scalar.all(-1),
 //                Scalar.all(-1), new MatOfByte(), Features2d.NOT_DRAW_SINGLE_POINTS);
 
             //-- Localize the object
@@ -201,14 +219,72 @@
             sceneMat.fromList(scene);
             double ransacReprojThreshold = 3.0;
             Mat H = Calib3d.findHomography(objMat, sceneMat, Calib3d.RANSAC, ransacReprojThreshold);
+//            Mat fundamental = Calib3d.findFundamentalMat(objMat, sceneMat,Calib3d.RANSAC, ransacReprojThreshold );
 
             Log.d(TAG, "Done calculations!");
             Log.d(TAG, "good matches size: " + goodMatches.toList().size());
 
-            callback.matchObjectsCallback(H, true);
-        }
-        else {
-            callback.matchObjectsCallback(null, false);
+
+
+            //-- Get the corners from the image_1 ( the object to be "detected" )
+            Mat objCorners = new Mat(4, 1, CvType.CV_32FC2), sceneCorners = new Mat();
+            float[] objCornersData = new float[(int) (objCorners.total() * objCorners.channels())];
+            objCorners.get(0, 0, objCornersData);
+            objCornersData[0] = 0;
+            objCornersData[1] = 0;
+            objCornersData[2] = object.cols();
+            objCornersData[3] = 0;
+            objCornersData[4] = object.cols();
+            objCornersData[5] = object.rows();
+            objCornersData[6] = 0;
+            objCornersData[7] = object.rows();
+            objCorners.put(0, 0, objCornersData);
+            Core.perspectiveTransform(objCorners, sceneCorners, H);
+            float[] sceneCornersData = new float[(int) (sceneCorners.total() * sceneCorners.channels())];
+            sceneCorners.get(0, 0, sceneCornersData);
+            //-- Draw lines between the corners (the mapped object in the scene - image_2 )
+            Imgproc.line(imgMatches, new Point(sceneCornersData[0], sceneCornersData[1]),
+                    new Point(sceneCornersData[2], sceneCornersData[3]), new Scalar(0, 255, 0), 4);
+            Imgproc.line(imgMatches, new Point(sceneCornersData[2], sceneCornersData[3]),
+                    new Point(sceneCornersData[4], sceneCornersData[5]), new Scalar(0, 255, 0), 4);
+            Imgproc.line(imgMatches, new Point(sceneCornersData[4], sceneCornersData[5]),
+                    new Point(sceneCornersData[6], sceneCornersData[7]), new Scalar(0, 255, 0), 4);
+            Imgproc.line(imgMatches, new Point(sceneCornersData[6], sceneCornersData[7]),
+                    new Point(sceneCornersData[0], sceneCornersData[1]), new Scalar(0, 255, 0), 4);
+
+
+
+//            testing
+//            Imgproc.line(imgMatches,
+//                    new Point(0,0),
+//                    new Point(sceneCornersData[6], sceneCornersData[7] ),
+//                    new Scalar(255, 255, 0), 4);
+
+//            sceneCornersData[0], sceneCornersData[1]                                 // left top x left top y
+//            sceneCornersData[2], sceneCornersData[3]                                 // right top x right top y
+//
+//            sceneCornersData[2], sceneCornersData[3]                                 //  r t x, r t y
+//            sceneCornersData[4], sceneCornersData[5]                                 //  r b x, r b y
+//
+//            sceneCornersData[4], sceneCornersData[5]                                  //  r b x, r b y
+//            sceneCornersData[6], sceneCornersData[7]                                  //  l b x, l b y
+//
+//            sceneCornersData[6], sceneCornersData[7]                                  //  l b x, l b y
+//            sceneCornersData[0], sceneCornersData[1]                                  // l t x, l t y
+
+//            Log.d(TAG, "drawing imgMatches");
+//
+            Log.d(TAG, "objects cols: "+object.cols()+" rows: "+object.rows());
+            Log.d(TAG, "scene cols: "+objectInScene.cols()+" rows: "+objectInScene.rows());
+            Log.d(TAG, "matches cols: "+imgMatches.cols()+" rows: "+imgMatches.rows());
+            Log.d(TAG, "scene corners data: "+sceneCornersData[0]);
+            Log.d(TAG, "scene corners data: "+sceneCornersData[1]);
+            Log.d(TAG, "scene corners data: "+sceneCornersData[2]);
+            Log.d(TAG, "scene corners data: "+sceneCornersData[3]);
+            Log.d(TAG, "scene corners data: "+sceneCornersData[4]);
+            Log.d(TAG, "scene corners data: "+sceneCornersData[5]);
+            Log.d(TAG, "scene corners data: "+sceneCornersData[6]);
+            Log.d(TAG, "scene corners data: "+sceneCornersData[7]);
         }
     }
 
Index: openCVLibrary3411/src/main/java/org/opencv/ml/KNearest.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/ml/KNearest.java	(date 1605830247869)
+++ openCVLibrary3411/src/main/java/org/opencv/ml/KNearest.java	(date 1605830247869)
@@ -0,0 +1,309 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.ml;
+
+import org.opencv.core.Mat;
+import org.opencv.ml.KNearest;
+import org.opencv.ml.StatModel;
+
+// C++: class KNearest
+/**
+ * The class implements K-Nearest Neighbors model
+ *
+ * SEE: REF: ml_intro_knn
+ */
+public class KNearest extends StatModel {
+
+    protected KNearest(long addr) { super(addr); }
+
+    // internal usage only
+    public static KNearest __fromPtr__(long addr) { return new KNearest(addr); }
+
+    // C++: enum Types
+    public static final int
+            BRUTE_FORCE = 1,
+            KDTREE = 2;
+
+
+    //
+    // C++: static Ptr_KNearest cv::ml::KNearest::create()
+    //
+
+    /**
+     * Creates the empty model
+     *
+     *     The static method creates empty %KNearest classifier. It should be then trained using StatModel::train method.
+     * @return automatically generated
+     */
+    public static KNearest create() {
+        return KNearest.__fromPtr__(create_0());
+    }
+
+
+    //
+    // C++: static Ptr_KNearest cv::ml::KNearest::load(String filepath)
+    //
+
+    /**
+     * Loads and creates a serialized knearest from a file
+     *
+     * Use KNearest::save to serialize and store an KNearest to disk.
+     * Load the KNearest from this file again, by calling this function with the path to the file.
+     *
+     * @param filepath path to serialized KNearest
+     * @return automatically generated
+     */
+    public static KNearest load(String filepath) {
+        return KNearest.__fromPtr__(load_0(filepath));
+    }
+
+
+    //
+    // C++:  bool cv::ml::KNearest::getIsClassifier()
+    //
+
+    /**
+     * SEE: setIsClassifier
+     * @return automatically generated
+     */
+    public boolean getIsClassifier() {
+        return getIsClassifier_0(nativeObj);
+    }
+
+
+    //
+    // C++:  float cv::ml::KNearest::findNearest(Mat samples, int k, Mat& results, Mat& neighborResponses = Mat(), Mat& dist = Mat())
+    //
+
+    /**
+     * Finds the neighbors and predicts responses for input vectors.
+     *
+     *     @param samples Input samples stored by rows. It is a single-precision floating-point matrix of
+     *         {@code &lt;number_of_samples&gt; * k} size.
+     *     @param k Number of used nearest neighbors. Should be greater than 1.
+     *     @param results Vector with results of prediction (regression or classification) for each input
+     *         sample. It is a single-precision floating-point vector with {@code &lt;number_of_samples&gt;} elements.
+     *     @param neighborResponses Optional output values for corresponding neighbors. It is a single-
+     *         precision floating-point matrix of {@code &lt;number_of_samples&gt; * k} size.
+     *     @param dist Optional output distances from the input vectors to the corresponding neighbors. It
+     *         is a single-precision floating-point matrix of {@code &lt;number_of_samples&gt; * k} size.
+     *
+     *     For each input vector (a row of the matrix samples), the method finds the k nearest neighbors.
+     *     In case of regression, the predicted result is a mean value of the particular vector's neighbor
+     *     responses. In case of classification, the class is determined by voting.
+     *
+     *     For each input vector, the neighbors are sorted by their distances to the vector.
+     *
+     *     In case of C++ interface you can use output pointers to empty matrices and the function will
+     *     allocate memory itself.
+     *
+     *     If only a single input vector is passed, all output matrices are optional and the predicted
+     *     value is returned by the method.
+     *
+     *     The function is parallelized with the TBB library.
+     * @return automatically generated
+     */
+    public float findNearest(Mat samples, int k, Mat results, Mat neighborResponses, Mat dist) {
+        return findNearest_0(nativeObj, samples.nativeObj, k, results.nativeObj, neighborResponses.nativeObj, dist.nativeObj);
+    }
+
+    /**
+     * Finds the neighbors and predicts responses for input vectors.
+     *
+     *     @param samples Input samples stored by rows. It is a single-precision floating-point matrix of
+     *         {@code &lt;number_of_samples&gt; * k} size.
+     *     @param k Number of used nearest neighbors. Should be greater than 1.
+     *     @param results Vector with results of prediction (regression or classification) for each input
+     *         sample. It is a single-precision floating-point vector with {@code &lt;number_of_samples&gt;} elements.
+     *     @param neighborResponses Optional output values for corresponding neighbors. It is a single-
+     *         precision floating-point matrix of {@code &lt;number_of_samples&gt; * k} size.
+     *         is a single-precision floating-point matrix of {@code &lt;number_of_samples&gt; * k} size.
+     *
+     *     For each input vector (a row of the matrix samples), the method finds the k nearest neighbors.
+     *     In case of regression, the predicted result is a mean value of the particular vector's neighbor
+     *     responses. In case of classification, the class is determined by voting.
+     *
+     *     For each input vector, the neighbors are sorted by their distances to the vector.
+     *
+     *     In case of C++ interface you can use output pointers to empty matrices and the function will
+     *     allocate memory itself.
+     *
+     *     If only a single input vector is passed, all output matrices are optional and the predicted
+     *     value is returned by the method.
+     *
+     *     The function is parallelized with the TBB library.
+     * @return automatically generated
+     */
+    public float findNearest(Mat samples, int k, Mat results, Mat neighborResponses) {
+        return findNearest_1(nativeObj, samples.nativeObj, k, results.nativeObj, neighborResponses.nativeObj);
+    }
+
+    /**
+     * Finds the neighbors and predicts responses for input vectors.
+     *
+     *     @param samples Input samples stored by rows. It is a single-precision floating-point matrix of
+     *         {@code &lt;number_of_samples&gt; * k} size.
+     *     @param k Number of used nearest neighbors. Should be greater than 1.
+     *     @param results Vector with results of prediction (regression or classification) for each input
+     *         sample. It is a single-precision floating-point vector with {@code &lt;number_of_samples&gt;} elements.
+     *         precision floating-point matrix of {@code &lt;number_of_samples&gt; * k} size.
+     *         is a single-precision floating-point matrix of {@code &lt;number_of_samples&gt; * k} size.
+     *
+     *     For each input vector (a row of the matrix samples), the method finds the k nearest neighbors.
+     *     In case of regression, the predicted result is a mean value of the particular vector's neighbor
+     *     responses. In case of classification, the class is determined by voting.
+     *
+     *     For each input vector, the neighbors are sorted by their distances to the vector.
+     *
+     *     In case of C++ interface you can use output pointers to empty matrices and the function will
+     *     allocate memory itself.
+     *
+     *     If only a single input vector is passed, all output matrices are optional and the predicted
+     *     value is returned by the method.
+     *
+     *     The function is parallelized with the TBB library.
+     * @return automatically generated
+     */
+    public float findNearest(Mat samples, int k, Mat results) {
+        return findNearest_2(nativeObj, samples.nativeObj, k, results.nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::ml::KNearest::getAlgorithmType()
+    //
+
+    /**
+     * SEE: setAlgorithmType
+     * @return automatically generated
+     */
+    public int getAlgorithmType() {
+        return getAlgorithmType_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::ml::KNearest::getDefaultK()
+    //
+
+    /**
+     * SEE: setDefaultK
+     * @return automatically generated
+     */
+    public int getDefaultK() {
+        return getDefaultK_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::ml::KNearest::getEmax()
+    //
+
+    /**
+     * SEE: setEmax
+     * @return automatically generated
+     */
+    public int getEmax() {
+        return getEmax_0(nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::ml::KNearest::setAlgorithmType(int val)
+    //
+
+    /**
+     *  getAlgorithmType SEE: getAlgorithmType
+     * @param val automatically generated
+     */
+    public void setAlgorithmType(int val) {
+        setAlgorithmType_0(nativeObj, val);
+    }
+
+
+    //
+    // C++:  void cv::ml::KNearest::setDefaultK(int val)
+    //
+
+    /**
+     *  getDefaultK SEE: getDefaultK
+     * @param val automatically generated
+     */
+    public void setDefaultK(int val) {
+        setDefaultK_0(nativeObj, val);
+    }
+
+
+    //
+    // C++:  void cv::ml::KNearest::setEmax(int val)
+    //
+
+    /**
+     *  getEmax SEE: getEmax
+     * @param val automatically generated
+     */
+    public void setEmax(int val) {
+        setEmax_0(nativeObj, val);
+    }
+
+
+    //
+    // C++:  void cv::ml::KNearest::setIsClassifier(bool val)
+    //
+
+    /**
+     *  getIsClassifier SEE: getIsClassifier
+     * @param val automatically generated
+     */
+    public void setIsClassifier(boolean val) {
+        setIsClassifier_0(nativeObj, val);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++: static Ptr_KNearest cv::ml::KNearest::create()
+    private static native long create_0();
+
+    // C++: static Ptr_KNearest cv::ml::KNearest::load(String filepath)
+    private static native long load_0(String filepath);
+
+    // C++:  bool cv::ml::KNearest::getIsClassifier()
+    private static native boolean getIsClassifier_0(long nativeObj);
+
+    // C++:  float cv::ml::KNearest::findNearest(Mat samples, int k, Mat& results, Mat& neighborResponses = Mat(), Mat& dist = Mat())
+    private static native float findNearest_0(long nativeObj, long samples_nativeObj, int k, long results_nativeObj, long neighborResponses_nativeObj, long dist_nativeObj);
+    private static native float findNearest_1(long nativeObj, long samples_nativeObj, int k, long results_nativeObj, long neighborResponses_nativeObj);
+    private static native float findNearest_2(long nativeObj, long samples_nativeObj, int k, long results_nativeObj);
+
+    // C++:  int cv::ml::KNearest::getAlgorithmType()
+    private static native int getAlgorithmType_0(long nativeObj);
+
+    // C++:  int cv::ml::KNearest::getDefaultK()
+    private static native int getDefaultK_0(long nativeObj);
+
+    // C++:  int cv::ml::KNearest::getEmax()
+    private static native int getEmax_0(long nativeObj);
+
+    // C++:  void cv::ml::KNearest::setAlgorithmType(int val)
+    private static native void setAlgorithmType_0(long nativeObj, int val);
+
+    // C++:  void cv::ml::KNearest::setDefaultK(int val)
+    private static native void setDefaultK_0(long nativeObj, int val);
+
+    // C++:  void cv::ml::KNearest::setEmax(int val)
+    private static native void setEmax_0(long nativeObj, int val);
+
+    // C++:  void cv::ml::KNearest::setIsClassifier(bool val)
+    private static native void setIsClassifier_0(long nativeObj, boolean val);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/ml/LogisticRegression.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/ml/LogisticRegression.java	(date 1605830247872)
+++ openCVLibrary3411/src/main/java/org/opencv/ml/LogisticRegression.java	(date 1605830247872)
@@ -0,0 +1,360 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.ml;
+
+import org.opencv.core.Mat;
+import org.opencv.core.TermCriteria;
+import org.opencv.ml.LogisticRegression;
+import org.opencv.ml.StatModel;
+
+// C++: class LogisticRegression
+/**
+ * Implements Logistic Regression classifier.
+ *
+ * SEE: REF: ml_intro_lr
+ */
+public class LogisticRegression extends StatModel {
+
+    protected LogisticRegression(long addr) { super(addr); }
+
+    // internal usage only
+    public static LogisticRegression __fromPtr__(long addr) { return new LogisticRegression(addr); }
+
+    // C++: enum RegKinds
+    public static final int
+            REG_DISABLE = -1,
+            REG_L1 = 0,
+            REG_L2 = 1;
+
+
+    // C++: enum Methods
+    public static final int
+            BATCH = 0,
+            MINI_BATCH = 1;
+
+
+    //
+    // C++:  Mat cv::ml::LogisticRegression::get_learnt_thetas()
+    //
+
+    /**
+     * This function returns the trained parameters arranged across rows.
+     *
+     *     For a two class classification problem, it returns a row matrix. It returns learnt parameters of
+     *     the Logistic Regression as a matrix of type CV_32F.
+     * @return automatically generated
+     */
+    public Mat get_learnt_thetas() {
+        return new Mat(get_learnt_thetas_0(nativeObj));
+    }
+
+
+    //
+    // C++: static Ptr_LogisticRegression cv::ml::LogisticRegression::create()
+    //
+
+    /**
+     * Creates empty model.
+     *
+     *     Creates Logistic Regression model with parameters given.
+     * @return automatically generated
+     */
+    public static LogisticRegression create() {
+        return LogisticRegression.__fromPtr__(create_0());
+    }
+
+
+    //
+    // C++: static Ptr_LogisticRegression cv::ml::LogisticRegression::load(String filepath, String nodeName = String())
+    //
+
+    /**
+     * Loads and creates a serialized LogisticRegression from a file
+     *
+     * Use LogisticRegression::save to serialize and store an LogisticRegression to disk.
+     * Load the LogisticRegression from this file again, by calling this function with the path to the file.
+     * Optionally specify the node for the file containing the classifier
+     *
+     * @param filepath path to serialized LogisticRegression
+     * @param nodeName name of node containing the classifier
+     * @return automatically generated
+     */
+    public static LogisticRegression load(String filepath, String nodeName) {
+        return LogisticRegression.__fromPtr__(load_0(filepath, nodeName));
+    }
+
+    /**
+     * Loads and creates a serialized LogisticRegression from a file
+     *
+     * Use LogisticRegression::save to serialize and store an LogisticRegression to disk.
+     * Load the LogisticRegression from this file again, by calling this function with the path to the file.
+     * Optionally specify the node for the file containing the classifier
+     *
+     * @param filepath path to serialized LogisticRegression
+     * @return automatically generated
+     */
+    public static LogisticRegression load(String filepath) {
+        return LogisticRegression.__fromPtr__(load_1(filepath));
+    }
+
+
+    //
+    // C++:  TermCriteria cv::ml::LogisticRegression::getTermCriteria()
+    //
+
+    /**
+     * SEE: setTermCriteria
+     * @return automatically generated
+     */
+    public TermCriteria getTermCriteria() {
+        return new TermCriteria(getTermCriteria_0(nativeObj));
+    }
+
+
+    //
+    // C++:  double cv::ml::LogisticRegression::getLearningRate()
+    //
+
+    /**
+     * SEE: setLearningRate
+     * @return automatically generated
+     */
+    public double getLearningRate() {
+        return getLearningRate_0(nativeObj);
+    }
+
+
+    //
+    // C++:  float cv::ml::LogisticRegression::predict(Mat samples, Mat& results = Mat(), int flags = 0)
+    //
+
+    /**
+     * Predicts responses for input samples and returns a float type.
+     *
+     *     @param samples The input data for the prediction algorithm. Matrix [m x n], where each row
+     *         contains variables (features) of one object being classified. Should have data type CV_32F.
+     *     @param results Predicted labels as a column matrix of type CV_32S.
+     *     @param flags Not used.
+     * @return automatically generated
+     */
+    public float predict(Mat samples, Mat results, int flags) {
+        return predict_0(nativeObj, samples.nativeObj, results.nativeObj, flags);
+    }
+
+    /**
+     * Predicts responses for input samples and returns a float type.
+     *
+     *     @param samples The input data for the prediction algorithm. Matrix [m x n], where each row
+     *         contains variables (features) of one object being classified. Should have data type CV_32F.
+     *     @param results Predicted labels as a column matrix of type CV_32S.
+     * @return automatically generated
+     */
+    public float predict(Mat samples, Mat results) {
+        return predict_1(nativeObj, samples.nativeObj, results.nativeObj);
+    }
+
+    /**
+     * Predicts responses for input samples and returns a float type.
+     *
+     *     @param samples The input data for the prediction algorithm. Matrix [m x n], where each row
+     *         contains variables (features) of one object being classified. Should have data type CV_32F.
+     * @return automatically generated
+     */
+    public float predict(Mat samples) {
+        return predict_2(nativeObj, samples.nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::ml::LogisticRegression::getIterations()
+    //
+
+    /**
+     * SEE: setIterations
+     * @return automatically generated
+     */
+    public int getIterations() {
+        return getIterations_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::ml::LogisticRegression::getMiniBatchSize()
+    //
+
+    /**
+     * SEE: setMiniBatchSize
+     * @return automatically generated
+     */
+    public int getMiniBatchSize() {
+        return getMiniBatchSize_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::ml::LogisticRegression::getRegularization()
+    //
+
+    /**
+     * SEE: setRegularization
+     * @return automatically generated
+     */
+    public int getRegularization() {
+        return getRegularization_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::ml::LogisticRegression::getTrainMethod()
+    //
+
+    /**
+     * SEE: setTrainMethod
+     * @return automatically generated
+     */
+    public int getTrainMethod() {
+        return getTrainMethod_0(nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::ml::LogisticRegression::setIterations(int val)
+    //
+
+    /**
+     *  getIterations SEE: getIterations
+     * @param val automatically generated
+     */
+    public void setIterations(int val) {
+        setIterations_0(nativeObj, val);
+    }
+
+
+    //
+    // C++:  void cv::ml::LogisticRegression::setLearningRate(double val)
+    //
+
+    /**
+     *  getLearningRate SEE: getLearningRate
+     * @param val automatically generated
+     */
+    public void setLearningRate(double val) {
+        setLearningRate_0(nativeObj, val);
+    }
+
+
+    //
+    // C++:  void cv::ml::LogisticRegression::setMiniBatchSize(int val)
+    //
+
+    /**
+     *  getMiniBatchSize SEE: getMiniBatchSize
+     * @param val automatically generated
+     */
+    public void setMiniBatchSize(int val) {
+        setMiniBatchSize_0(nativeObj, val);
+    }
+
+
+    //
+    // C++:  void cv::ml::LogisticRegression::setRegularization(int val)
+    //
+
+    /**
+     *  getRegularization SEE: getRegularization
+     * @param val automatically generated
+     */
+    public void setRegularization(int val) {
+        setRegularization_0(nativeObj, val);
+    }
+
+
+    //
+    // C++:  void cv::ml::LogisticRegression::setTermCriteria(TermCriteria val)
+    //
+
+    /**
+     *  getTermCriteria SEE: getTermCriteria
+     * @param val automatically generated
+     */
+    public void setTermCriteria(TermCriteria val) {
+        setTermCriteria_0(nativeObj, val.type, val.maxCount, val.epsilon);
+    }
+
+
+    //
+    // C++:  void cv::ml::LogisticRegression::setTrainMethod(int val)
+    //
+
+    /**
+     *  getTrainMethod SEE: getTrainMethod
+     * @param val automatically generated
+     */
+    public void setTrainMethod(int val) {
+        setTrainMethod_0(nativeObj, val);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++:  Mat cv::ml::LogisticRegression::get_learnt_thetas()
+    private static native long get_learnt_thetas_0(long nativeObj);
+
+    // C++: static Ptr_LogisticRegression cv::ml::LogisticRegression::create()
+    private static native long create_0();
+
+    // C++: static Ptr_LogisticRegression cv::ml::LogisticRegression::load(String filepath, String nodeName = String())
+    private static native long load_0(String filepath, String nodeName);
+    private static native long load_1(String filepath);
+
+    // C++:  TermCriteria cv::ml::LogisticRegression::getTermCriteria()
+    private static native double[] getTermCriteria_0(long nativeObj);
+
+    // C++:  double cv::ml::LogisticRegression::getLearningRate()
+    private static native double getLearningRate_0(long nativeObj);
+
+    // C++:  float cv::ml::LogisticRegression::predict(Mat samples, Mat& results = Mat(), int flags = 0)
+    private static native float predict_0(long nativeObj, long samples_nativeObj, long results_nativeObj, int flags);
+    private static native float predict_1(long nativeObj, long samples_nativeObj, long results_nativeObj);
+    private static native float predict_2(long nativeObj, long samples_nativeObj);
+
+    // C++:  int cv::ml::LogisticRegression::getIterations()
+    private static native int getIterations_0(long nativeObj);
+
+    // C++:  int cv::ml::LogisticRegression::getMiniBatchSize()
+    private static native int getMiniBatchSize_0(long nativeObj);
+
+    // C++:  int cv::ml::LogisticRegression::getRegularization()
+    private static native int getRegularization_0(long nativeObj);
+
+    // C++:  int cv::ml::LogisticRegression::getTrainMethod()
+    private static native int getTrainMethod_0(long nativeObj);
+
+    // C++:  void cv::ml::LogisticRegression::setIterations(int val)
+    private static native void setIterations_0(long nativeObj, int val);
+
+    // C++:  void cv::ml::LogisticRegression::setLearningRate(double val)
+    private static native void setLearningRate_0(long nativeObj, double val);
+
+    // C++:  void cv::ml::LogisticRegression::setMiniBatchSize(int val)
+    private static native void setMiniBatchSize_0(long nativeObj, int val);
+
+    // C++:  void cv::ml::LogisticRegression::setRegularization(int val)
+    private static native void setRegularization_0(long nativeObj, int val);
+
+    // C++:  void cv::ml::LogisticRegression::setTermCriteria(TermCriteria val)
+    private static native void setTermCriteria_0(long nativeObj, int val_type, int val_maxCount, double val_epsilon);
+
+    // C++:  void cv::ml::LogisticRegression::setTrainMethod(int val)
+    private static native void setTrainMethod_0(long nativeObj, int val);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/aidl/org/opencv/engine/OpenCVEngineInterface.aidl
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/aidl/org/opencv/engine/OpenCVEngineInterface.aidl	(date 1605830247499)
+++ openCVLibrary3411/src/main/aidl/org/opencv/engine/OpenCVEngineInterface.aidl	(date 1605830247499)
@@ -0,0 +1,33 @@
+package org.opencv.engine;
+
+/**
+* Class provides a Java interface for OpenCV Engine Service. It's synchronous with native OpenCVEngine class.
+*/
+interface OpenCVEngineInterface
+{
+    /**
+    * @return Returns service version.
+    */
+    int getEngineVersion();
+
+    /**
+    * Finds an installed OpenCV library.
+    * @param OpenCV version.
+    * @return Returns path to OpenCV native libs or an empty string if OpenCV can not be found.
+    */
+    String getLibPathByVersion(String version);
+
+    /**
+    * Tries to install defined version of OpenCV from Google Play Market.
+    * @param OpenCV version.
+    * @return Returns true if installation was successful or OpenCV package has been already installed.
+    */
+    boolean installVersion(String version);
+
+    /**
+    * Returns list of libraries in loading order, separated by semicolon.
+    * @param OpenCV version.
+    * @return Returns names of OpenCV libraries, separated by semicolon.
+    */
+    String getLibraryList(String version);
+}
Index: openCVLibrary3411/src/main/java/org/opencv/ml/ANN_MLP.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/ml/ANN_MLP.java	(date 1605830247804)
+++ openCVLibrary3411/src/main/java/org/opencv/ml/ANN_MLP.java	(date 1605830247804)
@@ -0,0 +1,632 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.ml;
+
+import org.opencv.core.Mat;
+import org.opencv.core.TermCriteria;
+import org.opencv.ml.ANN_MLP;
+import org.opencv.ml.StatModel;
+
+// C++: class ANN_MLP
+/**
+ * Artificial Neural Networks - Multi-Layer Perceptrons.
+ *
+ * Unlike many other models in ML that are constructed and trained at once, in the MLP model these
+ * steps are separated. First, a network with the specified topology is created using the non-default
+ * constructor or the method ANN_MLP::create. All the weights are set to zeros. Then, the network is
+ * trained using a set of input and output vectors. The training procedure can be repeated more than
+ * once, that is, the weights can be adjusted based on the new training data.
+ *
+ * Additional flags for StatModel::train are available: ANN_MLP::TrainFlags.
+ *
+ * SEE: REF: ml_intro_ann
+ */
+public class ANN_MLP extends StatModel {
+
+    protected ANN_MLP(long addr) { super(addr); }
+
+    // internal usage only
+    public static ANN_MLP __fromPtr__(long addr) { return new ANN_MLP(addr); }
+
+    // C++: enum TrainingMethods
+    public static final int
+            BACKPROP = 0,
+            RPROP = 1,
+            ANNEAL = 2;
+
+
+    // C++: enum TrainFlags
+    public static final int
+            UPDATE_WEIGHTS = 1,
+            NO_INPUT_SCALE = 2,
+            NO_OUTPUT_SCALE = 4;
+
+
+    // C++: enum ActivationFunctions
+    public static final int
+            IDENTITY = 0,
+            SIGMOID_SYM = 1,
+            GAUSSIAN = 2,
+            RELU = 3,
+            LEAKYRELU = 4;
+
+
+    //
+    // C++:  Mat cv::ml::ANN_MLP::getLayerSizes()
+    //
+
+    /**
+     * Integer vector specifying the number of neurons in each layer including the input and output layers.
+     *     The very first element specifies the number of elements in the input layer.
+     *     The last element - number of elements in the output layer.
+     * SEE: setLayerSizes
+     * @return automatically generated
+     */
+    public Mat getLayerSizes() {
+        return new Mat(getLayerSizes_0(nativeObj));
+    }
+
+
+    //
+    // C++:  Mat cv::ml::ANN_MLP::getWeights(int layerIdx)
+    //
+
+    public Mat getWeights(int layerIdx) {
+        return new Mat(getWeights_0(nativeObj, layerIdx));
+    }
+
+
+    //
+    // C++: static Ptr_ANN_MLP cv::ml::ANN_MLP::create()
+    //
+
+    /**
+     * Creates empty model
+     *
+     *     Use StatModel::train to train the model, Algorithm::load&lt;ANN_MLP&gt;(filename) to load the pre-trained model.
+     *     Note that the train method has optional flags: ANN_MLP::TrainFlags.
+     * @return automatically generated
+     */
+    public static ANN_MLP create() {
+        return ANN_MLP.__fromPtr__(create_0());
+    }
+
+
+    //
+    // C++: static Ptr_ANN_MLP cv::ml::ANN_MLP::load(String filepath)
+    //
+
+    /**
+     * Loads and creates a serialized ANN from a file
+     *
+     * Use ANN::save to serialize and store an ANN to disk.
+     * Load the ANN from this file again, by calling this function with the path to the file.
+     *
+     * @param filepath path to serialized ANN
+     * @return automatically generated
+     */
+    public static ANN_MLP load(String filepath) {
+        return ANN_MLP.__fromPtr__(load_0(filepath));
+    }
+
+
+    //
+    // C++:  TermCriteria cv::ml::ANN_MLP::getTermCriteria()
+    //
+
+    /**
+     * SEE: setTermCriteria
+     * @return automatically generated
+     */
+    public TermCriteria getTermCriteria() {
+        return new TermCriteria(getTermCriteria_0(nativeObj));
+    }
+
+
+    //
+    // C++:  double cv::ml::ANN_MLP::getAnnealCoolingRatio()
+    //
+
+    /**
+     * SEE: setAnnealCoolingRatio
+     * @return automatically generated
+     */
+    public double getAnnealCoolingRatio() {
+        return getAnnealCoolingRatio_0(nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::ml::ANN_MLP::getAnnealFinalT()
+    //
+
+    /**
+     * SEE: setAnnealFinalT
+     * @return automatically generated
+     */
+    public double getAnnealFinalT() {
+        return getAnnealFinalT_0(nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::ml::ANN_MLP::getAnnealInitialT()
+    //
+
+    /**
+     * SEE: setAnnealInitialT
+     * @return automatically generated
+     */
+    public double getAnnealInitialT() {
+        return getAnnealInitialT_0(nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::ml::ANN_MLP::getBackpropMomentumScale()
+    //
+
+    /**
+     * SEE: setBackpropMomentumScale
+     * @return automatically generated
+     */
+    public double getBackpropMomentumScale() {
+        return getBackpropMomentumScale_0(nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::ml::ANN_MLP::getBackpropWeightScale()
+    //
+
+    /**
+     * SEE: setBackpropWeightScale
+     * @return automatically generated
+     */
+    public double getBackpropWeightScale() {
+        return getBackpropWeightScale_0(nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::ml::ANN_MLP::getRpropDW0()
+    //
+
+    /**
+     * SEE: setRpropDW0
+     * @return automatically generated
+     */
+    public double getRpropDW0() {
+        return getRpropDW0_0(nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::ml::ANN_MLP::getRpropDWMax()
+    //
+
+    /**
+     * SEE: setRpropDWMax
+     * @return automatically generated
+     */
+    public double getRpropDWMax() {
+        return getRpropDWMax_0(nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::ml::ANN_MLP::getRpropDWMin()
+    //
+
+    /**
+     * SEE: setRpropDWMin
+     * @return automatically generated
+     */
+    public double getRpropDWMin() {
+        return getRpropDWMin_0(nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::ml::ANN_MLP::getRpropDWMinus()
+    //
+
+    /**
+     * SEE: setRpropDWMinus
+     * @return automatically generated
+     */
+    public double getRpropDWMinus() {
+        return getRpropDWMinus_0(nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::ml::ANN_MLP::getRpropDWPlus()
+    //
+
+    /**
+     * SEE: setRpropDWPlus
+     * @return automatically generated
+     */
+    public double getRpropDWPlus() {
+        return getRpropDWPlus_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::ml::ANN_MLP::getAnnealItePerStep()
+    //
+
+    /**
+     * SEE: setAnnealItePerStep
+     * @return automatically generated
+     */
+    public int getAnnealItePerStep() {
+        return getAnnealItePerStep_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::ml::ANN_MLP::getTrainMethod()
+    //
+
+    /**
+     * Returns current training method
+     * @return automatically generated
+     */
+    public int getTrainMethod() {
+        return getTrainMethod_0(nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::ml::ANN_MLP::setActivationFunction(int type, double param1 = 0, double param2 = 0)
+    //
+
+    /**
+     * Initialize the activation function for each neuron.
+     *     Currently the default and the only fully supported activation function is ANN_MLP::SIGMOID_SYM.
+     *     @param type The type of activation function. See ANN_MLP::ActivationFunctions.
+     *     @param param1 The first parameter of the activation function, \(\alpha\). Default value is 0.
+     *     @param param2 The second parameter of the activation function, \(\beta\). Default value is 0.
+     */
+    public void setActivationFunction(int type, double param1, double param2) {
+        setActivationFunction_0(nativeObj, type, param1, param2);
+    }
+
+    /**
+     * Initialize the activation function for each neuron.
+     *     Currently the default and the only fully supported activation function is ANN_MLP::SIGMOID_SYM.
+     *     @param type The type of activation function. See ANN_MLP::ActivationFunctions.
+     *     @param param1 The first parameter of the activation function, \(\alpha\). Default value is 0.
+     */
+    public void setActivationFunction(int type, double param1) {
+        setActivationFunction_1(nativeObj, type, param1);
+    }
+
+    /**
+     * Initialize the activation function for each neuron.
+     *     Currently the default and the only fully supported activation function is ANN_MLP::SIGMOID_SYM.
+     *     @param type The type of activation function. See ANN_MLP::ActivationFunctions.
+     */
+    public void setActivationFunction(int type) {
+        setActivationFunction_2(nativeObj, type);
+    }
+
+
+    //
+    // C++:  void cv::ml::ANN_MLP::setAnnealCoolingRatio(double val)
+    //
+
+    /**
+     *  getAnnealCoolingRatio SEE: getAnnealCoolingRatio
+     * @param val automatically generated
+     */
+    public void setAnnealCoolingRatio(double val) {
+        setAnnealCoolingRatio_0(nativeObj, val);
+    }
+
+
+    //
+    // C++:  void cv::ml::ANN_MLP::setAnnealFinalT(double val)
+    //
+
+    /**
+     *  getAnnealFinalT SEE: getAnnealFinalT
+     * @param val automatically generated
+     */
+    public void setAnnealFinalT(double val) {
+        setAnnealFinalT_0(nativeObj, val);
+    }
+
+
+    //
+    // C++:  void cv::ml::ANN_MLP::setAnnealInitialT(double val)
+    //
+
+    /**
+     *  getAnnealInitialT SEE: getAnnealInitialT
+     * @param val automatically generated
+     */
+    public void setAnnealInitialT(double val) {
+        setAnnealInitialT_0(nativeObj, val);
+    }
+
+
+    //
+    // C++:  void cv::ml::ANN_MLP::setAnnealItePerStep(int val)
+    //
+
+    /**
+     *  getAnnealItePerStep SEE: getAnnealItePerStep
+     * @param val automatically generated
+     */
+    public void setAnnealItePerStep(int val) {
+        setAnnealItePerStep_0(nativeObj, val);
+    }
+
+
+    //
+    // C++:  void cv::ml::ANN_MLP::setBackpropMomentumScale(double val)
+    //
+
+    /**
+     *  getBackpropMomentumScale SEE: getBackpropMomentumScale
+     * @param val automatically generated
+     */
+    public void setBackpropMomentumScale(double val) {
+        setBackpropMomentumScale_0(nativeObj, val);
+    }
+
+
+    //
+    // C++:  void cv::ml::ANN_MLP::setBackpropWeightScale(double val)
+    //
+
+    /**
+     *  getBackpropWeightScale SEE: getBackpropWeightScale
+     * @param val automatically generated
+     */
+    public void setBackpropWeightScale(double val) {
+        setBackpropWeightScale_0(nativeObj, val);
+    }
+
+
+    //
+    // C++:  void cv::ml::ANN_MLP::setLayerSizes(Mat _layer_sizes)
+    //
+
+    /**
+     * Integer vector specifying the number of neurons in each layer including the input and output layers.
+     *     The very first element specifies the number of elements in the input layer.
+     *     The last element - number of elements in the output layer. Default value is empty Mat.
+     * SEE: getLayerSizes
+     * @param _layer_sizes automatically generated
+     */
+    public void setLayerSizes(Mat _layer_sizes) {
+        setLayerSizes_0(nativeObj, _layer_sizes.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::ml::ANN_MLP::setRpropDW0(double val)
+    //
+
+    /**
+     *  getRpropDW0 SEE: getRpropDW0
+     * @param val automatically generated
+     */
+    public void setRpropDW0(double val) {
+        setRpropDW0_0(nativeObj, val);
+    }
+
+
+    //
+    // C++:  void cv::ml::ANN_MLP::setRpropDWMax(double val)
+    //
+
+    /**
+     *  getRpropDWMax SEE: getRpropDWMax
+     * @param val automatically generated
+     */
+    public void setRpropDWMax(double val) {
+        setRpropDWMax_0(nativeObj, val);
+    }
+
+
+    //
+    // C++:  void cv::ml::ANN_MLP::setRpropDWMin(double val)
+    //
+
+    /**
+     *  getRpropDWMin SEE: getRpropDWMin
+     * @param val automatically generated
+     */
+    public void setRpropDWMin(double val) {
+        setRpropDWMin_0(nativeObj, val);
+    }
+
+
+    //
+    // C++:  void cv::ml::ANN_MLP::setRpropDWMinus(double val)
+    //
+
+    /**
+     *  getRpropDWMinus SEE: getRpropDWMinus
+     * @param val automatically generated
+     */
+    public void setRpropDWMinus(double val) {
+        setRpropDWMinus_0(nativeObj, val);
+    }
+
+
+    //
+    // C++:  void cv::ml::ANN_MLP::setRpropDWPlus(double val)
+    //
+
+    /**
+     *  getRpropDWPlus SEE: getRpropDWPlus
+     * @param val automatically generated
+     */
+    public void setRpropDWPlus(double val) {
+        setRpropDWPlus_0(nativeObj, val);
+    }
+
+
+    //
+    // C++:  void cv::ml::ANN_MLP::setTermCriteria(TermCriteria val)
+    //
+
+    /**
+     *  getTermCriteria SEE: getTermCriteria
+     * @param val automatically generated
+     */
+    public void setTermCriteria(TermCriteria val) {
+        setTermCriteria_0(nativeObj, val.type, val.maxCount, val.epsilon);
+    }
+
+
+    //
+    // C++:  void cv::ml::ANN_MLP::setTrainMethod(int method, double param1 = 0, double param2 = 0)
+    //
+
+    /**
+     * Sets training method and common parameters.
+     *     @param method Default value is ANN_MLP::RPROP. See ANN_MLP::TrainingMethods.
+     *     @param param1 passed to setRpropDW0 for ANN_MLP::RPROP and to setBackpropWeightScale for ANN_MLP::BACKPROP and to initialT for ANN_MLP::ANNEAL.
+     *     @param param2 passed to setRpropDWMin for ANN_MLP::RPROP and to setBackpropMomentumScale for ANN_MLP::BACKPROP and to finalT for ANN_MLP::ANNEAL.
+     */
+    public void setTrainMethod(int method, double param1, double param2) {
+        setTrainMethod_0(nativeObj, method, param1, param2);
+    }
+
+    /**
+     * Sets training method and common parameters.
+     *     @param method Default value is ANN_MLP::RPROP. See ANN_MLP::TrainingMethods.
+     *     @param param1 passed to setRpropDW0 for ANN_MLP::RPROP and to setBackpropWeightScale for ANN_MLP::BACKPROP and to initialT for ANN_MLP::ANNEAL.
+     */
+    public void setTrainMethod(int method, double param1) {
+        setTrainMethod_1(nativeObj, method, param1);
+    }
+
+    /**
+     * Sets training method and common parameters.
+     *     @param method Default value is ANN_MLP::RPROP. See ANN_MLP::TrainingMethods.
+     */
+    public void setTrainMethod(int method) {
+        setTrainMethod_2(nativeObj, method);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++:  Mat cv::ml::ANN_MLP::getLayerSizes()
+    private static native long getLayerSizes_0(long nativeObj);
+
+    // C++:  Mat cv::ml::ANN_MLP::getWeights(int layerIdx)
+    private static native long getWeights_0(long nativeObj, int layerIdx);
+
+    // C++: static Ptr_ANN_MLP cv::ml::ANN_MLP::create()
+    private static native long create_0();
+
+    // C++: static Ptr_ANN_MLP cv::ml::ANN_MLP::load(String filepath)
+    private static native long load_0(String filepath);
+
+    // C++:  TermCriteria cv::ml::ANN_MLP::getTermCriteria()
+    private static native double[] getTermCriteria_0(long nativeObj);
+
+    // C++:  double cv::ml::ANN_MLP::getAnnealCoolingRatio()
+    private static native double getAnnealCoolingRatio_0(long nativeObj);
+
+    // C++:  double cv::ml::ANN_MLP::getAnnealFinalT()
+    private static native double getAnnealFinalT_0(long nativeObj);
+
+    // C++:  double cv::ml::ANN_MLP::getAnnealInitialT()
+    private static native double getAnnealInitialT_0(long nativeObj);
+
+    // C++:  double cv::ml::ANN_MLP::getBackpropMomentumScale()
+    private static native double getBackpropMomentumScale_0(long nativeObj);
+
+    // C++:  double cv::ml::ANN_MLP::getBackpropWeightScale()
+    private static native double getBackpropWeightScale_0(long nativeObj);
+
+    // C++:  double cv::ml::ANN_MLP::getRpropDW0()
+    private static native double getRpropDW0_0(long nativeObj);
+
+    // C++:  double cv::ml::ANN_MLP::getRpropDWMax()
+    private static native double getRpropDWMax_0(long nativeObj);
+
+    // C++:  double cv::ml::ANN_MLP::getRpropDWMin()
+    private static native double getRpropDWMin_0(long nativeObj);
+
+    // C++:  double cv::ml::ANN_MLP::getRpropDWMinus()
+    private static native double getRpropDWMinus_0(long nativeObj);
+
+    // C++:  double cv::ml::ANN_MLP::getRpropDWPlus()
+    private static native double getRpropDWPlus_0(long nativeObj);
+
+    // C++:  int cv::ml::ANN_MLP::getAnnealItePerStep()
+    private static native int getAnnealItePerStep_0(long nativeObj);
+
+    // C++:  int cv::ml::ANN_MLP::getTrainMethod()
+    private static native int getTrainMethod_0(long nativeObj);
+
+    // C++:  void cv::ml::ANN_MLP::setActivationFunction(int type, double param1 = 0, double param2 = 0)
+    private static native void setActivationFunction_0(long nativeObj, int type, double param1, double param2);
+    private static native void setActivationFunction_1(long nativeObj, int type, double param1);
+    private static native void setActivationFunction_2(long nativeObj, int type);
+
+    // C++:  void cv::ml::ANN_MLP::setAnnealCoolingRatio(double val)
+    private static native void setAnnealCoolingRatio_0(long nativeObj, double val);
+
+    // C++:  void cv::ml::ANN_MLP::setAnnealFinalT(double val)
+    private static native void setAnnealFinalT_0(long nativeObj, double val);
+
+    // C++:  void cv::ml::ANN_MLP::setAnnealInitialT(double val)
+    private static native void setAnnealInitialT_0(long nativeObj, double val);
+
+    // C++:  void cv::ml::ANN_MLP::setAnnealItePerStep(int val)
+    private static native void setAnnealItePerStep_0(long nativeObj, int val);
+
+    // C++:  void cv::ml::ANN_MLP::setBackpropMomentumScale(double val)
+    private static native void setBackpropMomentumScale_0(long nativeObj, double val);
+
+    // C++:  void cv::ml::ANN_MLP::setBackpropWeightScale(double val)
+    private static native void setBackpropWeightScale_0(long nativeObj, double val);
+
+    // C++:  void cv::ml::ANN_MLP::setLayerSizes(Mat _layer_sizes)
+    private static native void setLayerSizes_0(long nativeObj, long _layer_sizes_nativeObj);
+
+    // C++:  void cv::ml::ANN_MLP::setRpropDW0(double val)
+    private static native void setRpropDW0_0(long nativeObj, double val);
+
+    // C++:  void cv::ml::ANN_MLP::setRpropDWMax(double val)
+    private static native void setRpropDWMax_0(long nativeObj, double val);
+
+    // C++:  void cv::ml::ANN_MLP::setRpropDWMin(double val)
+    private static native void setRpropDWMin_0(long nativeObj, double val);
+
+    // C++:  void cv::ml::ANN_MLP::setRpropDWMinus(double val)
+    private static native void setRpropDWMinus_0(long nativeObj, double val);
+
+    // C++:  void cv::ml::ANN_MLP::setRpropDWPlus(double val)
+    private static native void setRpropDWPlus_0(long nativeObj, double val);
+
+    // C++:  void cv::ml::ANN_MLP::setTermCriteria(TermCriteria val)
+    private static native void setTermCriteria_0(long nativeObj, int val_type, int val_maxCount, double val_epsilon);
+
+    // C++:  void cv::ml::ANN_MLP::setTrainMethod(int method, double param1 = 0, double param2 = 0)
+    private static native void setTrainMethod_0(long nativeObj, int method, double param1, double param2);
+    private static native void setTrainMethod_1(long nativeObj, int method, double param1);
+    private static native void setTrainMethod_2(long nativeObj, int method);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/ml/ANN_MLP_ANNEAL.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/ml/ANN_MLP_ANNEAL.java	(date 1605830247831)
+++ openCVLibrary3411/src/main/java/org/opencv/ml/ANN_MLP_ANNEAL.java	(date 1605830247831)
@@ -0,0 +1,159 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.ml;
+
+import org.opencv.ml.ANN_MLP;
+
+// C++: class ANN_MLP_ANNEAL
+/**
+ * Artificial Neural Networks - Multi-Layer Perceptrons.
+ *
+ * SEE: REF: ml_intro_ann
+ */
+public class ANN_MLP_ANNEAL extends ANN_MLP {
+
+    protected ANN_MLP_ANNEAL(long addr) { super(addr); }
+
+    // internal usage only
+    public static ANN_MLP_ANNEAL __fromPtr__(long addr) { return new ANN_MLP_ANNEAL(addr); }
+
+    //
+    // C++:  double cv::ml::ANN_MLP_ANNEAL::getAnnealCoolingRatio()
+    //
+
+    /**
+     * SEE: setAnnealCoolingRatio
+     * @return automatically generated
+     */
+    public double getAnnealCoolingRatio() {
+        return getAnnealCoolingRatio_0(nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::ml::ANN_MLP_ANNEAL::getAnnealFinalT()
+    //
+
+    /**
+     * SEE: setAnnealFinalT
+     * @return automatically generated
+     */
+    public double getAnnealFinalT() {
+        return getAnnealFinalT_0(nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::ml::ANN_MLP_ANNEAL::getAnnealInitialT()
+    //
+
+    /**
+     * SEE: setAnnealInitialT
+     * @return automatically generated
+     */
+    public double getAnnealInitialT() {
+        return getAnnealInitialT_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::ml::ANN_MLP_ANNEAL::getAnnealItePerStep()
+    //
+
+    /**
+     * SEE: setAnnealItePerStep
+     * @return automatically generated
+     */
+    public int getAnnealItePerStep() {
+        return getAnnealItePerStep_0(nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::ml::ANN_MLP_ANNEAL::setAnnealCoolingRatio(double val)
+    //
+
+    /**
+     *  getAnnealCoolingRatio SEE: getAnnealCoolingRatio
+     * @param val automatically generated
+     */
+    public void setAnnealCoolingRatio(double val) {
+        setAnnealCoolingRatio_0(nativeObj, val);
+    }
+
+
+    //
+    // C++:  void cv::ml::ANN_MLP_ANNEAL::setAnnealFinalT(double val)
+    //
+
+    /**
+     *  getAnnealFinalT SEE: getAnnealFinalT
+     * @param val automatically generated
+     */
+    public void setAnnealFinalT(double val) {
+        setAnnealFinalT_0(nativeObj, val);
+    }
+
+
+    //
+    // C++:  void cv::ml::ANN_MLP_ANNEAL::setAnnealInitialT(double val)
+    //
+
+    /**
+     *  getAnnealInitialT SEE: getAnnealInitialT
+     * @param val automatically generated
+     */
+    public void setAnnealInitialT(double val) {
+        setAnnealInitialT_0(nativeObj, val);
+    }
+
+
+    //
+    // C++:  void cv::ml::ANN_MLP_ANNEAL::setAnnealItePerStep(int val)
+    //
+
+    /**
+     *  getAnnealItePerStep SEE: getAnnealItePerStep
+     * @param val automatically generated
+     */
+    public void setAnnealItePerStep(int val) {
+        setAnnealItePerStep_0(nativeObj, val);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++:  double cv::ml::ANN_MLP_ANNEAL::getAnnealCoolingRatio()
+    private static native double getAnnealCoolingRatio_0(long nativeObj);
+
+    // C++:  double cv::ml::ANN_MLP_ANNEAL::getAnnealFinalT()
+    private static native double getAnnealFinalT_0(long nativeObj);
+
+    // C++:  double cv::ml::ANN_MLP_ANNEAL::getAnnealInitialT()
+    private static native double getAnnealInitialT_0(long nativeObj);
+
+    // C++:  int cv::ml::ANN_MLP_ANNEAL::getAnnealItePerStep()
+    private static native int getAnnealItePerStep_0(long nativeObj);
+
+    // C++:  void cv::ml::ANN_MLP_ANNEAL::setAnnealCoolingRatio(double val)
+    private static native void setAnnealCoolingRatio_0(long nativeObj, double val);
+
+    // C++:  void cv::ml::ANN_MLP_ANNEAL::setAnnealFinalT(double val)
+    private static native void setAnnealFinalT_0(long nativeObj, double val);
+
+    // C++:  void cv::ml::ANN_MLP_ANNEAL::setAnnealInitialT(double val)
+    private static native void setAnnealInitialT_0(long nativeObj, double val);
+
+    // C++:  void cv::ml::ANN_MLP_ANNEAL::setAnnealItePerStep(int val)
+    private static native void setAnnealItePerStep_0(long nativeObj, int val);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/ml/Boost.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/ml/Boost.java	(date 1605830247842)
+++ openCVLibrary3411/src/main/java/org/opencv/ml/Boost.java	(date 1605830247842)
@@ -0,0 +1,191 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.ml;
+
+import org.opencv.ml.Boost;
+import org.opencv.ml.DTrees;
+
+// C++: class Boost
+/**
+ * Boosted tree classifier derived from DTrees
+ *
+ * SEE: REF: ml_intro_boost
+ */
+public class Boost extends DTrees {
+
+    protected Boost(long addr) { super(addr); }
+
+    // internal usage only
+    public static Boost __fromPtr__(long addr) { return new Boost(addr); }
+
+    // C++: enum Types
+    public static final int
+            DISCRETE = 0,
+            REAL = 1,
+            LOGIT = 2,
+            GENTLE = 3;
+
+
+    //
+    // C++: static Ptr_Boost cv::ml::Boost::create()
+    //
+
+    /**
+     * Creates the empty model.
+     * Use StatModel::train to train the model, Algorithm::load&lt;Boost&gt;(filename) to load the pre-trained model.
+     * @return automatically generated
+     */
+    public static Boost create() {
+        return Boost.__fromPtr__(create_0());
+    }
+
+
+    //
+    // C++: static Ptr_Boost cv::ml::Boost::load(String filepath, String nodeName = String())
+    //
+
+    /**
+     * Loads and creates a serialized Boost from a file
+     *
+     * Use Boost::save to serialize and store an RTree to disk.
+     * Load the Boost from this file again, by calling this function with the path to the file.
+     * Optionally specify the node for the file containing the classifier
+     *
+     * @param filepath path to serialized Boost
+     * @param nodeName name of node containing the classifier
+     * @return automatically generated
+     */
+    public static Boost load(String filepath, String nodeName) {
+        return Boost.__fromPtr__(load_0(filepath, nodeName));
+    }
+
+    /**
+     * Loads and creates a serialized Boost from a file
+     *
+     * Use Boost::save to serialize and store an RTree to disk.
+     * Load the Boost from this file again, by calling this function with the path to the file.
+     * Optionally specify the node for the file containing the classifier
+     *
+     * @param filepath path to serialized Boost
+     * @return automatically generated
+     */
+    public static Boost load(String filepath) {
+        return Boost.__fromPtr__(load_1(filepath));
+    }
+
+
+    //
+    // C++:  double cv::ml::Boost::getWeightTrimRate()
+    //
+
+    /**
+     * SEE: setWeightTrimRate
+     * @return automatically generated
+     */
+    public double getWeightTrimRate() {
+        return getWeightTrimRate_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::ml::Boost::getBoostType()
+    //
+
+    /**
+     * SEE: setBoostType
+     * @return automatically generated
+     */
+    public int getBoostType() {
+        return getBoostType_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::ml::Boost::getWeakCount()
+    //
+
+    /**
+     * SEE: setWeakCount
+     * @return automatically generated
+     */
+    public int getWeakCount() {
+        return getWeakCount_0(nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::ml::Boost::setBoostType(int val)
+    //
+
+    /**
+     *  getBoostType SEE: getBoostType
+     * @param val automatically generated
+     */
+    public void setBoostType(int val) {
+        setBoostType_0(nativeObj, val);
+    }
+
+
+    //
+    // C++:  void cv::ml::Boost::setWeakCount(int val)
+    //
+
+    /**
+     *  getWeakCount SEE: getWeakCount
+     * @param val automatically generated
+     */
+    public void setWeakCount(int val) {
+        setWeakCount_0(nativeObj, val);
+    }
+
+
+    //
+    // C++:  void cv::ml::Boost::setWeightTrimRate(double val)
+    //
+
+    /**
+     *  getWeightTrimRate SEE: getWeightTrimRate
+     * @param val automatically generated
+     */
+    public void setWeightTrimRate(double val) {
+        setWeightTrimRate_0(nativeObj, val);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++: static Ptr_Boost cv::ml::Boost::create()
+    private static native long create_0();
+
+    // C++: static Ptr_Boost cv::ml::Boost::load(String filepath, String nodeName = String())
+    private static native long load_0(String filepath, String nodeName);
+    private static native long load_1(String filepath);
+
+    // C++:  double cv::ml::Boost::getWeightTrimRate()
+    private static native double getWeightTrimRate_0(long nativeObj);
+
+    // C++:  int cv::ml::Boost::getBoostType()
+    private static native int getBoostType_0(long nativeObj);
+
+    // C++:  int cv::ml::Boost::getWeakCount()
+    private static native int getWeakCount_0(long nativeObj);
+
+    // C++:  void cv::ml::Boost::setBoostType(int val)
+    private static native void setBoostType_0(long nativeObj, int val);
+
+    // C++:  void cv::ml::Boost::setWeakCount(int val)
+    private static native void setWeakCount_0(long nativeObj, int val);
+
+    // C++:  void cv::ml::Boost::setWeightTrimRate(double val)
+    private static native void setWeightTrimRate_0(long nativeObj, double val);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/res/values/attrs.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/res/values/attrs.xml	(date 1605830246966)
+++ openCVLibrary3411/src/main/res/values/attrs.xml	(date 1605830246966)
@@ -0,0 +1,11 @@
+<?xml version="1.0" encoding="utf-8"?>
+<resources>
+    <declare-styleable name = "CameraBridgeViewBase" >
+       <attr name="show_fps" format="boolean"/>
+       <attr name="camera_id" format="integer" >
+          <enum name="any" value="-1" />
+          <enum name="back" value="99" />
+          <enum name="front" value="98" />
+       </attr>
+    </declare-styleable>
+</resources>
Index: openCVLibrary3411/src/main/AndroidManifest.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/AndroidManifest.xml	(date 1605830282718)
+++ openCVLibrary3411/src/main/AndroidManifest.xml	(date 1605830282718)
@@ -0,0 +1,8 @@
+<?xml version="1.0" encoding="utf-8"?>
+<manifest xmlns:android="http://schemas.android.com/apk/res/android"
+      package="org.opencv"
+      android:versionCode="34110"
+      android:versionName="3.4.11">
+
+    <uses-sdk android:targetSdkVersion="21" />
+</manifest>
Index: openCVLibrary3411/build/generated/aidl_source_output_dir/debug/out/org/opencv/engine/OpenCVEngineInterface.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/build/generated/aidl_source_output_dir/debug/out/org/opencv/engine/OpenCVEngineInterface.java	(date 1605831104532)
+++ openCVLibrary3411/build/generated/aidl_source_output_dir/debug/out/org/opencv/engine/OpenCVEngineInterface.java	(date 1605831104532)
@@ -0,0 +1,289 @@
+/*
+ * This file is auto-generated.  DO NOT MODIFY.
+ */
+package org.opencv.engine;
+/**
+* Class provides a Java interface for OpenCV Engine Service. It's synchronous with native OpenCVEngine class.
+*/
+public interface OpenCVEngineInterface extends android.os.IInterface
+{
+  /** Default implementation for OpenCVEngineInterface. */
+  public static class Default implements org.opencv.engine.OpenCVEngineInterface
+  {
+    /**
+        * @return Returns service version.
+        */
+    @Override public int getEngineVersion() throws android.os.RemoteException
+    {
+      return 0;
+    }
+    /**
+        * Finds an installed OpenCV library.
+        * @param OpenCV version.
+        * @return Returns path to OpenCV native libs or an empty string if OpenCV can not be found.
+        */
+    @Override public java.lang.String getLibPathByVersion(java.lang.String version) throws android.os.RemoteException
+    {
+      return null;
+    }
+    /**
+        * Tries to install defined version of OpenCV from Google Play Market.
+        * @param OpenCV version.
+        * @return Returns true if installation was successful or OpenCV package has been already installed.
+        */
+    @Override public boolean installVersion(java.lang.String version) throws android.os.RemoteException
+    {
+      return false;
+    }
+    /**
+        * Returns list of libraries in loading order, separated by semicolon.
+        * @param OpenCV version.
+        * @return Returns names of OpenCV libraries, separated by semicolon.
+        */
+    @Override public java.lang.String getLibraryList(java.lang.String version) throws android.os.RemoteException
+    {
+      return null;
+    }
+    @Override
+    public android.os.IBinder asBinder() {
+      return null;
+    }
+  }
+  /** Local-side IPC implementation stub class. */
+  public static abstract class Stub extends android.os.Binder implements org.opencv.engine.OpenCVEngineInterface
+  {
+    private static final java.lang.String DESCRIPTOR = "org.opencv.engine.OpenCVEngineInterface";
+    /** Construct the stub at attach it to the interface. */
+    public Stub()
+    {
+      this.attachInterface(this, DESCRIPTOR);
+    }
+    /**
+     * Cast an IBinder object into an org.opencv.engine.OpenCVEngineInterface interface,
+     * generating a proxy if needed.
+     */
+    public static org.opencv.engine.OpenCVEngineInterface asInterface(android.os.IBinder obj)
+    {
+      if ((obj==null)) {
+        return null;
+      }
+      android.os.IInterface iin = obj.queryLocalInterface(DESCRIPTOR);
+      if (((iin!=null)&&(iin instanceof org.opencv.engine.OpenCVEngineInterface))) {
+        return ((org.opencv.engine.OpenCVEngineInterface)iin);
+      }
+      return new org.opencv.engine.OpenCVEngineInterface.Stub.Proxy(obj);
+    }
+    @Override public android.os.IBinder asBinder()
+    {
+      return this;
+    }
+    @Override public boolean onTransact(int code, android.os.Parcel data, android.os.Parcel reply, int flags) throws android.os.RemoteException
+    {
+      java.lang.String descriptor = DESCRIPTOR;
+      switch (code)
+      {
+        case INTERFACE_TRANSACTION:
+        {
+          reply.writeString(descriptor);
+          return true;
+        }
+        case TRANSACTION_getEngineVersion:
+        {
+          data.enforceInterface(descriptor);
+          int _result = this.getEngineVersion();
+          reply.writeNoException();
+          reply.writeInt(_result);
+          return true;
+        }
+        case TRANSACTION_getLibPathByVersion:
+        {
+          data.enforceInterface(descriptor);
+          java.lang.String _arg0;
+          _arg0 = data.readString();
+          java.lang.String _result = this.getLibPathByVersion(_arg0);
+          reply.writeNoException();
+          reply.writeString(_result);
+          return true;
+        }
+        case TRANSACTION_installVersion:
+        {
+          data.enforceInterface(descriptor);
+          java.lang.String _arg0;
+          _arg0 = data.readString();
+          boolean _result = this.installVersion(_arg0);
+          reply.writeNoException();
+          reply.writeInt(((_result)?(1):(0)));
+          return true;
+        }
+        case TRANSACTION_getLibraryList:
+        {
+          data.enforceInterface(descriptor);
+          java.lang.String _arg0;
+          _arg0 = data.readString();
+          java.lang.String _result = this.getLibraryList(_arg0);
+          reply.writeNoException();
+          reply.writeString(_result);
+          return true;
+        }
+        default:
+        {
+          return super.onTransact(code, data, reply, flags);
+        }
+      }
+    }
+    private static class Proxy implements org.opencv.engine.OpenCVEngineInterface
+    {
+      private android.os.IBinder mRemote;
+      Proxy(android.os.IBinder remote)
+      {
+        mRemote = remote;
+      }
+      @Override public android.os.IBinder asBinder()
+      {
+        return mRemote;
+      }
+      public java.lang.String getInterfaceDescriptor()
+      {
+        return DESCRIPTOR;
+      }
+      /**
+          * @return Returns service version.
+          */
+      @Override public int getEngineVersion() throws android.os.RemoteException
+      {
+        android.os.Parcel _data = android.os.Parcel.obtain();
+        android.os.Parcel _reply = android.os.Parcel.obtain();
+        int _result;
+        try {
+          _data.writeInterfaceToken(DESCRIPTOR);
+          boolean _status = mRemote.transact(Stub.TRANSACTION_getEngineVersion, _data, _reply, 0);
+          if (!_status && getDefaultImpl() != null) {
+            return getDefaultImpl().getEngineVersion();
+          }
+          _reply.readException();
+          _result = _reply.readInt();
+        }
+        finally {
+          _reply.recycle();
+          _data.recycle();
+        }
+        return _result;
+      }
+      /**
+          * Finds an installed OpenCV library.
+          * @param OpenCV version.
+          * @return Returns path to OpenCV native libs or an empty string if OpenCV can not be found.
+          */
+      @Override public java.lang.String getLibPathByVersion(java.lang.String version) throws android.os.RemoteException
+      {
+        android.os.Parcel _data = android.os.Parcel.obtain();
+        android.os.Parcel _reply = android.os.Parcel.obtain();
+        java.lang.String _result;
+        try {
+          _data.writeInterfaceToken(DESCRIPTOR);
+          _data.writeString(version);
+          boolean _status = mRemote.transact(Stub.TRANSACTION_getLibPathByVersion, _data, _reply, 0);
+          if (!_status && getDefaultImpl() != null) {
+            return getDefaultImpl().getLibPathByVersion(version);
+          }
+          _reply.readException();
+          _result = _reply.readString();
+        }
+        finally {
+          _reply.recycle();
+          _data.recycle();
+        }
+        return _result;
+      }
+      /**
+          * Tries to install defined version of OpenCV from Google Play Market.
+          * @param OpenCV version.
+          * @return Returns true if installation was successful or OpenCV package has been already installed.
+          */
+      @Override public boolean installVersion(java.lang.String version) throws android.os.RemoteException
+      {
+        android.os.Parcel _data = android.os.Parcel.obtain();
+        android.os.Parcel _reply = android.os.Parcel.obtain();
+        boolean _result;
+        try {
+          _data.writeInterfaceToken(DESCRIPTOR);
+          _data.writeString(version);
+          boolean _status = mRemote.transact(Stub.TRANSACTION_installVersion, _data, _reply, 0);
+          if (!_status && getDefaultImpl() != null) {
+            return getDefaultImpl().installVersion(version);
+          }
+          _reply.readException();
+          _result = (0!=_reply.readInt());
+        }
+        finally {
+          _reply.recycle();
+          _data.recycle();
+        }
+        return _result;
+      }
+      /**
+          * Returns list of libraries in loading order, separated by semicolon.
+          * @param OpenCV version.
+          * @return Returns names of OpenCV libraries, separated by semicolon.
+          */
+      @Override public java.lang.String getLibraryList(java.lang.String version) throws android.os.RemoteException
+      {
+        android.os.Parcel _data = android.os.Parcel.obtain();
+        android.os.Parcel _reply = android.os.Parcel.obtain();
+        java.lang.String _result;
+        try {
+          _data.writeInterfaceToken(DESCRIPTOR);
+          _data.writeString(version);
+          boolean _status = mRemote.transact(Stub.TRANSACTION_getLibraryList, _data, _reply, 0);
+          if (!_status && getDefaultImpl() != null) {
+            return getDefaultImpl().getLibraryList(version);
+          }
+          _reply.readException();
+          _result = _reply.readString();
+        }
+        finally {
+          _reply.recycle();
+          _data.recycle();
+        }
+        return _result;
+      }
+      public static org.opencv.engine.OpenCVEngineInterface sDefaultImpl;
+    }
+    static final int TRANSACTION_getEngineVersion = (android.os.IBinder.FIRST_CALL_TRANSACTION + 0);
+    static final int TRANSACTION_getLibPathByVersion = (android.os.IBinder.FIRST_CALL_TRANSACTION + 1);
+    static final int TRANSACTION_installVersion = (android.os.IBinder.FIRST_CALL_TRANSACTION + 2);
+    static final int TRANSACTION_getLibraryList = (android.os.IBinder.FIRST_CALL_TRANSACTION + 3);
+    public static boolean setDefaultImpl(org.opencv.engine.OpenCVEngineInterface impl) {
+      if (Stub.Proxy.sDefaultImpl == null && impl != null) {
+        Stub.Proxy.sDefaultImpl = impl;
+        return true;
+      }
+      return false;
+    }
+    public static org.opencv.engine.OpenCVEngineInterface getDefaultImpl() {
+      return Stub.Proxy.sDefaultImpl;
+    }
+  }
+  /**
+      * @return Returns service version.
+      */
+  public int getEngineVersion() throws android.os.RemoteException;
+  /**
+      * Finds an installed OpenCV library.
+      * @param OpenCV version.
+      * @return Returns path to OpenCV native libs or an empty string if OpenCV can not be found.
+      */
+  public java.lang.String getLibPathByVersion(java.lang.String version) throws android.os.RemoteException;
+  /**
+      * Tries to install defined version of OpenCV from Google Play Market.
+      * @param OpenCV version.
+      * @return Returns true if installation was successful or OpenCV package has been already installed.
+      */
+  public boolean installVersion(java.lang.String version) throws android.os.RemoteException;
+  /**
+      * Returns list of libraries in loading order, separated by semicolon.
+      * @param OpenCV version.
+      * @return Returns names of OpenCV libraries, separated by semicolon.
+      */
+  public java.lang.String getLibraryList(java.lang.String version) throws android.os.RemoteException;
+}
Index: openCVLibrary3411/lint.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/lint.xml	(date 1605830246968)
+++ openCVLibrary3411/lint.xml	(date 1605830246968)
@@ -0,0 +1,9 @@
+<?xml version="1.0" encoding="utf-8"?>
+<lint>
+    <issue id="InlinedApi">
+        <ignore path="src\org\opencv\android\JavaCameraView.java" />
+    </issue>
+    <issue id="NewApi">
+        <ignore path="src\org\opencv\android\JavaCameraView.java" />
+    </issue>
+</lint>
Index: openCVLibrary3411/build.gradle
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/build.gradle	(date 1605830282607)
+++ openCVLibrary3411/build.gradle	(date 1605830282607)
@@ -0,0 +1,18 @@
+apply plugin: 'com.android.library'
+
+android {
+    compileSdkVersion 29
+    buildToolsVersion "29.0.2"
+
+    defaultConfig {
+        targetSdkVersion 29
+        minSdkVersion 8
+    }
+
+    buildTypes {
+        release {
+            minifyEnabled false
+            proguardFiles getDefaultProguardFile('proguard-android.txt'), 'proguard-rules.txt'
+        }
+    }
+}
Index: openCVLibrary3411/build/generated/source/buildConfig/debug/org/opencv/BuildConfig.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/build/generated/source/buildConfig/debug/org/opencv/BuildConfig.java	(date 1605831106936)
+++ openCVLibrary3411/build/generated/source/buildConfig/debug/org/opencv/BuildConfig.java	(date 1605831106936)
@@ -0,0 +1,12 @@
+/**
+ * Automatically generated file. DO NOT MODIFY
+ */
+package org.opencv;
+
+public final class BuildConfig {
+  public static final boolean DEBUG = Boolean.parseBoolean("true");
+  public static final String LIBRARY_PACKAGE_NAME = "org.opencv";
+  public static final String BUILD_TYPE = "debug";
+  public static final int VERSION_CODE = 34110;
+  public static final String VERSION_NAME = "3.4.11";
+}
Index: openCVLibrary3411/src/main/java/org/opencv/videoio/VideoCapture.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/videoio/VideoCapture.java	(date 1605830248351)
+++ openCVLibrary3411/src/main/java/org/opencv/videoio/VideoCapture.java	(date 1605830248351)
@@ -0,0 +1,488 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.videoio;
+
+import org.opencv.core.Mat;
+
+// C++: class VideoCapture
+/**
+ * Class for video capturing from video files, image sequences or cameras.
+ *
+ * The class provides C++ API for capturing video from cameras or for reading video files and image sequences.
+ *
+ * Here is how the class can be used:
+ * INCLUDE: samples/cpp/videocapture_basic.cpp
+ *
+ * <b>Note:</b> In REF: videoio_c "C API" the black-box structure {@code CvCapture} is used instead of %VideoCapture.
+ * <b>Note:</b>
+ * <ul>
+ *   <li>
+ *    (C++) A basic sample on using the %VideoCapture interface can be found at
+ *     {@code OPENCV_SOURCE_CODE/samples/cpp/videocapture_starter.cpp}
+ *   </li>
+ *   <li>
+ *    (Python) A basic sample on using the %VideoCapture interface can be found at
+ *     {@code OPENCV_SOURCE_CODE/samples/python/video.py}
+ *   </li>
+ *   <li>
+ *    (Python) A multi threaded video processing sample can be found at
+ *     {@code OPENCV_SOURCE_CODE/samples/python/video_threaded.py}
+ *   </li>
+ *   <li>
+ *    (Python) %VideoCapture sample showcasing some features of the Video4Linux2 backend
+ *     {@code OPENCV_SOURCE_CODE/samples/python/video_v4l2.py}
+ *   </li>
+ * </ul>
+ */
+public class VideoCapture {
+
+    protected final long nativeObj;
+    protected VideoCapture(long addr) { nativeObj = addr; }
+
+    public long getNativeObjAddr() { return nativeObj; }
+
+    // internal usage only
+    public static VideoCapture __fromPtr__(long addr) { return new VideoCapture(addr); }
+
+    //
+    // C++:   cv::VideoCapture::VideoCapture(String filename, int apiPreference)
+    //
+
+    /**
+     *
+     *      Open video file or a capturing device or a IP video stream for video capturing with API Preference
+     *
+     *     @param filename it can be:
+     * <ul>
+     *   <li>
+     *      name of video file (eg. {@code video.avi})
+     *   </li>
+     *   <li>
+     *      or image sequence (eg. {@code img_%02d.jpg}, which will read samples like {@code img_00.jpg, img_01.jpg, img_02.jpg, ...})
+     *   </li>
+     *   <li>
+     *      or URL of video stream (eg. {@code protocol://host:port/script_name?script_params|auth})
+     *   </li>
+     *   <li>
+     *      or GStreamer pipeline string in gst-launch tool format in case if GStreamer is used as backend
+     *       Note that each video stream or IP camera feed has its own URL scheme. Please refer to the
+     *       documentation of source stream to know the right URL.
+     *     @param apiPreference preferred Capture API backends to use. Can be used to enforce a specific reader
+     *     implementation if multiple are available: e.g. cv::CAP_FFMPEG or cv::CAP_IMAGES or cv::CAP_DSHOW.
+     *   </li>
+     * </ul>
+     *
+     *     SEE: cv::VideoCaptureAPIs
+     */
+    public VideoCapture(String filename, int apiPreference) {
+        nativeObj = VideoCapture_0(filename, apiPreference);
+    }
+
+
+    //
+    // C++:   cv::VideoCapture::VideoCapture(String filename)
+    //
+
+    /**
+     *
+     *      Open video file or image file sequence or a capturing device or a IP video stream for video capturing
+     *
+     *     Same as VideoCapture(const String&amp; filename, int apiPreference) but using default Capture API backends
+     * @param filename automatically generated
+     */
+    public VideoCapture(String filename) {
+        nativeObj = VideoCapture_1(filename);
+    }
+
+
+    //
+    // C++:   cv::VideoCapture::VideoCapture(int index, int apiPreference)
+    //
+
+    /**
+     *
+     *      Opens a camera for video capturing
+     *
+     *     @param index id of the video capturing device to open. To open default camera using default backend just pass 0.
+     *     (to backward compatibility usage of camera_id + domain_offset (CAP_*) is valid when apiPreference is CAP_ANY)
+     *     @param apiPreference preferred Capture API backends to use. Can be used to enforce a specific reader
+     *     implementation if multiple are available: e.g. cv::CAP_DSHOW or cv::CAP_MSMF or cv::CAP_V4L2.
+     *
+     *     SEE: cv::VideoCaptureAPIs
+     */
+    public VideoCapture(int index, int apiPreference) {
+        nativeObj = VideoCapture_2(index, apiPreference);
+    }
+
+
+    //
+    // C++:   cv::VideoCapture::VideoCapture(int index)
+    //
+
+    /**
+     *
+     *      Open a camera for video capturing
+     *
+     *     @param index camera_id + domain_offset (CAP_*) id of the video capturing device to open. To open default camera using default backend just pass 0.
+     *     Use a {@code domain_offset} to enforce a specific reader implementation if multiple are available like cv::CAP_FFMPEG or cv::CAP_IMAGES or cv::CAP_DSHOW.
+     *     e.g. to open Camera 1 using the MS Media Foundation API use {@code index = 1 + cv::CAP_MSMF}
+     *
+     *     SEE: cv::VideoCaptureAPIs
+     */
+    public VideoCapture(int index) {
+        nativeObj = VideoCapture_3(index);
+    }
+
+
+    //
+    // C++:   cv::VideoCapture::VideoCapture()
+    //
+
+    /**
+     * Default constructor
+     *     <b>Note:</b> In REF: videoio_c "C API", when you finished working with video, release CvCapture structure with
+     *     cvReleaseCapture(), or use Ptr&lt;CvCapture&gt; that calls cvReleaseCapture() automatically in the
+     *     destructor.
+     */
+    public VideoCapture() {
+        nativeObj = VideoCapture_4();
+    }
+
+
+    //
+    // C++:  String cv::VideoCapture::getBackendName()
+    //
+
+    /**
+     * Returns used backend API name
+     *
+     *      <b>Note:</b> Stream should be opened.
+     * @return automatically generated
+     */
+    public String getBackendName() {
+        return getBackendName_0(nativeObj);
+    }
+
+
+    //
+    // C++:  bool cv::VideoCapture::grab()
+    //
+
+    /**
+     * Grabs the next frame from video file or capturing device.
+     *
+     *     @return {@code true} (non-zero) in the case of success.
+     *
+     *     The method/function grabs the next frame from video file or camera and returns true (non-zero) in
+     *     the case of success.
+     *
+     *     The primary use of the function is in multi-camera environments, especially when the cameras do not
+     *     have hardware synchronization. That is, you call VideoCapture::grab() for each camera and after that
+     *     call the slower method VideoCapture::retrieve() to decode and get frame from each camera. This way
+     *     the overhead on demosaicing or motion jpeg decompression etc. is eliminated and the retrieved frames
+     *     from different cameras will be closer in time.
+     *
+     *     Also, when a connected camera is multi-head (for example, a stereo camera or a Kinect device), the
+     *     correct way of retrieving data from it is to call VideoCapture::grab() first and then call
+     *     VideoCapture::retrieve() one or more times with different values of the channel parameter.
+     *
+     *     REF: tutorial_kinect_openni
+     */
+    public boolean grab() {
+        return grab_0(nativeObj);
+    }
+
+
+    //
+    // C++:  bool cv::VideoCapture::isOpened()
+    //
+
+    /**
+     * Returns true if video capturing has been initialized already.
+     *
+     *     If the previous call to VideoCapture constructor or VideoCapture::open() succeeded, the method returns
+     *     true.
+     * @return automatically generated
+     */
+    public boolean isOpened() {
+        return isOpened_0(nativeObj);
+    }
+
+
+    //
+    // C++:  bool cv::VideoCapture::open(String filename, int apiPreference)
+    //
+
+    /**
+     * Open video file or a capturing device or a IP video stream for video capturing with API Preference
+     *
+     *     
+     *
+     *     Parameters are same as the constructor VideoCapture(const String&amp; filename, int apiPreference)
+     *     @return {@code true} if the file has been successfully opened
+     *
+     *     The method first calls VideoCapture::release to close the already opened file or camera.
+     * @param filename automatically generated
+     * @param apiPreference automatically generated
+     */
+    public boolean open(String filename, int apiPreference) {
+        return open_0(nativeObj, filename, apiPreference);
+    }
+
+
+    //
+    // C++:  bool cv::VideoCapture::open(String filename)
+    //
+
+    /**
+     *  Open video file or a capturing device or a IP video stream for video capturing
+     *
+     *     
+     *
+     *     Parameters are same as the constructor VideoCapture(const String&amp; filename)
+     *     @return {@code true} if the file has been successfully opened
+     *
+     *     The method first calls VideoCapture::release to close the already opened file or camera.
+     * @param filename automatically generated
+     */
+    public boolean open(String filename) {
+        return open_1(nativeObj, filename);
+    }
+
+
+    //
+    // C++:  bool cv::VideoCapture::open(int cameraNum, int apiPreference)
+    //
+
+    /**
+     *  Open a camera for video capturing
+     *
+     *     
+     *
+     *     Parameters are similar as the constructor VideoCapture(int index),except it takes an additional argument apiPreference.
+     *     Definitely, is same as open(int index) where {@code index=cameraNum + apiPreference}
+     *     @return {@code true} if the camera has been successfully opened.
+     * @param cameraNum automatically generated
+     * @param apiPreference automatically generated
+     */
+    public boolean open(int cameraNum, int apiPreference) {
+        return open_2(nativeObj, cameraNum, apiPreference);
+    }
+
+
+    //
+    // C++:  bool cv::VideoCapture::open(int index)
+    //
+
+    /**
+     *  Open a camera for video capturing
+     *
+     *     
+     *
+     *     Parameters are same as the constructor VideoCapture(int index)
+     *     @return {@code true} if the camera has been successfully opened.
+     *
+     *     The method first calls VideoCapture::release to close the already opened file or camera.
+     * @param index automatically generated
+     */
+    public boolean open(int index) {
+        return open_3(nativeObj, index);
+    }
+
+
+    //
+    // C++:  bool cv::VideoCapture::read(Mat& image)
+    //
+
+    /**
+     * Grabs, decodes and returns the next video frame.
+     *
+     *     @return {@code false} if no frames has been grabbed
+     *
+     *     The method/function combines VideoCapture::grab() and VideoCapture::retrieve() in one call. This is the
+     *     most convenient method for reading video files or capturing data from decode and returns the just
+     *     grabbed frame. If no frames has been grabbed (camera has been disconnected, or there are no more
+     *     frames in video file), the method returns false and the function returns empty image (with %cv::Mat, test it with Mat::empty()).
+     *
+     *     <b>Note:</b> In REF: videoio_c "C API", functions cvRetrieveFrame() and cv.RetrieveFrame() return image stored inside the video
+     *     capturing structure. It is not allowed to modify or release the image! You can copy the frame using
+     *     cvCloneImage and then do whatever you want with the copy.
+     * @param image automatically generated
+     */
+    public boolean read(Mat image) {
+        return read_0(nativeObj, image.nativeObj);
+    }
+
+
+    //
+    // C++:  bool cv::VideoCapture::retrieve(Mat& image, int flag = 0)
+    //
+
+    /**
+     * Decodes and returns the grabbed video frame.
+     *
+     *     @param flag it could be a frame index or a driver specific flag
+     *     @return {@code false} if no frames has been grabbed
+     *
+     *     The method decodes and returns the just grabbed frame. If no frames has been grabbed
+     *     (camera has been disconnected, or there are no more frames in video file), the method returns false
+     *     and the function returns an empty image (with %cv::Mat, test it with Mat::empty()).
+     *
+     *     SEE: read()
+     *
+     *     <b>Note:</b> In REF: videoio_c "C API", functions cvRetrieveFrame() and cv.RetrieveFrame() return image stored inside the video
+     *     capturing structure. It is not allowed to modify or release the image! You can copy the frame using
+     *     cvCloneImage and then do whatever you want with the copy.
+     * @param image automatically generated
+     */
+    public boolean retrieve(Mat image, int flag) {
+        return retrieve_0(nativeObj, image.nativeObj, flag);
+    }
+
+    /**
+     * Decodes and returns the grabbed video frame.
+     *
+     *     @return {@code false} if no frames has been grabbed
+     *
+     *     The method decodes and returns the just grabbed frame. If no frames has been grabbed
+     *     (camera has been disconnected, or there are no more frames in video file), the method returns false
+     *     and the function returns an empty image (with %cv::Mat, test it with Mat::empty()).
+     *
+     *     SEE: read()
+     *
+     *     <b>Note:</b> In REF: videoio_c "C API", functions cvRetrieveFrame() and cv.RetrieveFrame() return image stored inside the video
+     *     capturing structure. It is not allowed to modify or release the image! You can copy the frame using
+     *     cvCloneImage and then do whatever you want with the copy.
+     * @param image automatically generated
+     */
+    public boolean retrieve(Mat image) {
+        return retrieve_1(nativeObj, image.nativeObj);
+    }
+
+
+    //
+    // C++:  bool cv::VideoCapture::set(int propId, double value)
+    //
+
+    /**
+     * Sets a property in the VideoCapture.
+     *
+     *     @param propId Property identifier from cv::VideoCaptureProperties (eg. cv::CAP_PROP_POS_MSEC, cv::CAP_PROP_POS_FRAMES, ...)
+     *     or one from REF: videoio_flags_others
+     *     @param value Value of the property.
+     *     @return {@code true} if the property is supported by backend used by the VideoCapture instance.
+     *     <b>Note:</b> Even if it returns {@code true} this doesn't ensure that the property
+     *     value has been accepted by the capture device. See note in VideoCapture::get()
+     */
+    public boolean set(int propId, double value) {
+        return set_0(nativeObj, propId, value);
+    }
+
+
+    //
+    // C++:  double cv::VideoCapture::get(int propId)
+    //
+
+    /**
+     * Returns the specified VideoCapture property
+     *
+     *     @param propId Property identifier from cv::VideoCaptureProperties (eg. cv::CAP_PROP_POS_MSEC, cv::CAP_PROP_POS_FRAMES, ...)
+     *     or one from REF: videoio_flags_others
+     *     @return Value for the specified property. Value 0 is returned when querying a property that is
+     *     not supported by the backend used by the VideoCapture instance.
+     *
+     *     <b>Note:</b> Reading / writing properties involves many layers. Some unexpected result might happens
+     *     along this chain.
+     *     <code>
+     *     VideoCapture -&gt; API Backend -&gt; Operating System -&gt; Device Driver -&gt; Device Hardware
+     *     </code>
+     *     The returned value might be different from what really used by the device or it could be encoded
+     *     using device dependent rules (eg. steps or percentage). Effective behaviour depends from device
+     *     driver and API Backend
+     */
+    public double get(int propId) {
+        return get_0(nativeObj, propId);
+    }
+
+
+    //
+    // C++:  void cv::VideoCapture::release()
+    //
+
+    /**
+     * Closes video file or capturing device.
+     *
+     *     The method is automatically called by subsequent VideoCapture::open and by VideoCapture
+     *     destructor.
+     *
+     *     The C function also deallocates memory and clears \*capture pointer.
+     */
+    public void release() {
+        release_0(nativeObj);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++:   cv::VideoCapture::VideoCapture(String filename, int apiPreference)
+    private static native long VideoCapture_0(String filename, int apiPreference);
+
+    // C++:   cv::VideoCapture::VideoCapture(String filename)
+    private static native long VideoCapture_1(String filename);
+
+    // C++:   cv::VideoCapture::VideoCapture(int index, int apiPreference)
+    private static native long VideoCapture_2(int index, int apiPreference);
+
+    // C++:   cv::VideoCapture::VideoCapture(int index)
+    private static native long VideoCapture_3(int index);
+
+    // C++:   cv::VideoCapture::VideoCapture()
+    private static native long VideoCapture_4();
+
+    // C++:  String cv::VideoCapture::getBackendName()
+    private static native String getBackendName_0(long nativeObj);
+
+    // C++:  bool cv::VideoCapture::grab()
+    private static native boolean grab_0(long nativeObj);
+
+    // C++:  bool cv::VideoCapture::isOpened()
+    private static native boolean isOpened_0(long nativeObj);
+
+    // C++:  bool cv::VideoCapture::open(String filename, int apiPreference)
+    private static native boolean open_0(long nativeObj, String filename, int apiPreference);
+
+    // C++:  bool cv::VideoCapture::open(String filename)
+    private static native boolean open_1(long nativeObj, String filename);
+
+    // C++:  bool cv::VideoCapture::open(int cameraNum, int apiPreference)
+    private static native boolean open_2(long nativeObj, int cameraNum, int apiPreference);
+
+    // C++:  bool cv::VideoCapture::open(int index)
+    private static native boolean open_3(long nativeObj, int index);
+
+    // C++:  bool cv::VideoCapture::read(Mat& image)
+    private static native boolean read_0(long nativeObj, long image_nativeObj);
+
+    // C++:  bool cv::VideoCapture::retrieve(Mat& image, int flag = 0)
+    private static native boolean retrieve_0(long nativeObj, long image_nativeObj, int flag);
+    private static native boolean retrieve_1(long nativeObj, long image_nativeObj);
+
+    // C++:  bool cv::VideoCapture::set(int propId, double value)
+    private static native boolean set_0(long nativeObj, int propId, double value);
+
+    // C++:  double cv::VideoCapture::get(int propId)
+    private static native double get_0(long nativeObj, int propId);
+
+    // C++:  void cv::VideoCapture::release()
+    private static native void release_0(long nativeObj);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/imgproc/Imgproc.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/imgproc/Imgproc.java	(date 1605830247764)
+++ openCVLibrary3411/src/main/java/org/opencv/imgproc/Imgproc.java	(date 1605830247764)
@@ -0,0 +1,10063 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.imgproc;
+
+import java.util.ArrayList;
+import java.util.List;
+import org.opencv.core.Mat;
+import org.opencv.core.MatOfFloat;
+import org.opencv.core.MatOfInt;
+import org.opencv.core.MatOfInt4;
+import org.opencv.core.MatOfPoint;
+import org.opencv.core.MatOfPoint2f;
+import org.opencv.core.Point;
+import org.opencv.core.Rect;
+import org.opencv.core.RotatedRect;
+import org.opencv.core.Scalar;
+import org.opencv.core.Size;
+import org.opencv.core.TermCriteria;
+import org.opencv.imgproc.CLAHE;
+import org.opencv.imgproc.GeneralizedHoughBallard;
+import org.opencv.imgproc.GeneralizedHoughGuil;
+import org.opencv.imgproc.LineSegmentDetector;
+import org.opencv.utils.Converters;
+
+// C++: class Imgproc
+
+public class Imgproc {
+
+    private static final int
+            IPL_BORDER_CONSTANT = 0,
+            IPL_BORDER_REPLICATE = 1,
+            IPL_BORDER_REFLECT = 2,
+            IPL_BORDER_WRAP = 3,
+            IPL_BORDER_REFLECT_101 = 4,
+            IPL_BORDER_TRANSPARENT = 5,
+            CV_INTER_NN = 0,
+            CV_INTER_LINEAR = 1,
+            CV_INTER_CUBIC = 2,
+            CV_INTER_AREA = 3,
+            CV_INTER_LANCZOS4 = 4,
+            CV_MOP_ERODE = 0,
+            CV_MOP_DILATE = 1,
+            CV_MOP_OPEN = 2,
+            CV_MOP_CLOSE = 3,
+            CV_MOP_GRADIENT = 4,
+            CV_MOP_TOPHAT = 5,
+            CV_MOP_BLACKHAT = 6,
+            CV_RETR_EXTERNAL = 0,
+            CV_RETR_LIST = 1,
+            CV_RETR_CCOMP = 2,
+            CV_RETR_TREE = 3,
+            CV_RETR_FLOODFILL = 4,
+            CV_CHAIN_APPROX_NONE = 1,
+            CV_CHAIN_APPROX_SIMPLE = 2,
+            CV_CHAIN_APPROX_TC89_L1 = 3,
+            CV_CHAIN_APPROX_TC89_KCOS = 4,
+            CV_THRESH_BINARY = 0,
+            CV_THRESH_BINARY_INV = 1,
+            CV_THRESH_TRUNC = 2,
+            CV_THRESH_TOZERO = 3,
+            CV_THRESH_TOZERO_INV = 4,
+            CV_THRESH_MASK = 7,
+            CV_THRESH_OTSU = 8,
+            CV_THRESH_TRIANGLE = 16;
+
+
+    // C++: enum InterpolationMasks
+    public static final int
+            INTER_BITS = 5,
+            INTER_BITS2 = INTER_BITS * 2,
+            INTER_TAB_SIZE = 1 << INTER_BITS,
+            INTER_TAB_SIZE2 = INTER_TAB_SIZE * INTER_TAB_SIZE;
+
+
+    // C++: enum MorphTypes
+    public static final int
+            MORPH_ERODE = 0,
+            MORPH_DILATE = 1,
+            MORPH_OPEN = 2,
+            MORPH_CLOSE = 3,
+            MORPH_GRADIENT = 4,
+            MORPH_TOPHAT = 5,
+            MORPH_BLACKHAT = 6,
+            MORPH_HITMISS = 7;
+
+
+    // C++: enum FloodFillFlags
+    public static final int
+            FLOODFILL_FIXED_RANGE = 1 << 16,
+            FLOODFILL_MASK_ONLY = 1 << 17;
+
+
+    // C++: enum HoughModes
+    public static final int
+            HOUGH_STANDARD = 0,
+            HOUGH_PROBABILISTIC = 1,
+            HOUGH_MULTI_SCALE = 2,
+            HOUGH_GRADIENT = 3;
+
+
+    // C++: enum ConnectedComponentsAlgorithmsTypes
+    public static final int
+            CCL_WU = 0,
+            CCL_DEFAULT = -1,
+            CCL_GRANA = 1;
+
+
+    // C++: enum GrabCutModes
+    public static final int
+            GC_INIT_WITH_RECT = 0,
+            GC_INIT_WITH_MASK = 1,
+            GC_EVAL = 2,
+            GC_EVAL_FREEZE_MODEL = 3;
+
+
+    // C++: enum GrabCutClasses
+    public static final int
+            GC_BGD = 0,
+            GC_FGD = 1,
+            GC_PR_BGD = 2,
+            GC_PR_FGD = 3;
+
+
+    // C++: enum ColormapTypes
+    public static final int
+            COLORMAP_AUTUMN = 0,
+            COLORMAP_BONE = 1,
+            COLORMAP_JET = 2,
+            COLORMAP_WINTER = 3,
+            COLORMAP_RAINBOW = 4,
+            COLORMAP_OCEAN = 5,
+            COLORMAP_SUMMER = 6,
+            COLORMAP_SPRING = 7,
+            COLORMAP_COOL = 8,
+            COLORMAP_HSV = 9,
+            COLORMAP_PINK = 10,
+            COLORMAP_HOT = 11,
+            COLORMAP_PARULA = 12,
+            COLORMAP_MAGMA = 13,
+            COLORMAP_INFERNO = 14,
+            COLORMAP_PLASMA = 15,
+            COLORMAP_VIRIDIS = 16,
+            COLORMAP_CIVIDIS = 17,
+            COLORMAP_TWILIGHT = 18,
+            COLORMAP_TWILIGHT_SHIFTED = 19,
+            COLORMAP_TURBO = 20,
+            COLORMAP_DEEPGREEN = 21;
+
+
+    // C++: enum HistCompMethods
+    public static final int
+            HISTCMP_CORREL = 0,
+            HISTCMP_CHISQR = 1,
+            HISTCMP_INTERSECT = 2,
+            HISTCMP_BHATTACHARYYA = 3,
+            HISTCMP_HELLINGER = 3,
+            HISTCMP_CHISQR_ALT = 4,
+            HISTCMP_KL_DIV = 5;
+
+
+    // C++: enum InterpolationFlags
+    public static final int
+            INTER_NEAREST = 0,
+            INTER_LINEAR = 1,
+            INTER_CUBIC = 2,
+            INTER_AREA = 3,
+            INTER_LANCZOS4 = 4,
+            INTER_LINEAR_EXACT = 5,
+            INTER_MAX = 7,
+            WARP_FILL_OUTLIERS = 8,
+            WARP_INVERSE_MAP = 16;
+
+
+    // C++: enum ContourApproximationModes
+    public static final int
+            CHAIN_APPROX_NONE = 1,
+            CHAIN_APPROX_SIMPLE = 2,
+            CHAIN_APPROX_TC89_L1 = 3,
+            CHAIN_APPROX_TC89_KCOS = 4;
+
+
+    // C++: enum ShapeMatchModes
+    public static final int
+            CONTOURS_MATCH_I1 = 1,
+            CONTOURS_MATCH_I2 = 2,
+            CONTOURS_MATCH_I3 = 3;
+
+
+    // C++: enum WarpPolarMode
+    public static final int
+            WARP_POLAR_LINEAR = 0,
+            WARP_POLAR_LOG = 256;
+
+
+    // C++: enum UndistortTypes
+    public static final int
+            PROJ_SPHERICAL_ORTHO = 0,
+            PROJ_SPHERICAL_EQRECT = 1;
+
+
+    // C++: enum ColorConversionCodes
+    public static final int
+            COLOR_BGR2BGRA = 0,
+            COLOR_RGB2RGBA = 0,
+            COLOR_BGRA2BGR = 1,
+            COLOR_RGBA2RGB = 1,
+            COLOR_BGR2RGBA = 2,
+            COLOR_RGB2BGRA = 2,
+            COLOR_RGBA2BGR = 3,
+            COLOR_BGRA2RGB = 3,
+            COLOR_BGR2RGB = 4,
+            COLOR_RGB2BGR = 4,
+            COLOR_BGRA2RGBA = 5,
+            COLOR_RGBA2BGRA = 5,
+            COLOR_BGR2GRAY = 6,
+            COLOR_RGB2GRAY = 7,
+            COLOR_GRAY2BGR = 8,
+            COLOR_GRAY2RGB = 8,
+            COLOR_GRAY2BGRA = 9,
+            COLOR_GRAY2RGBA = 9,
+            COLOR_BGRA2GRAY = 10,
+            COLOR_RGBA2GRAY = 11,
+            COLOR_BGR2BGR565 = 12,
+            COLOR_RGB2BGR565 = 13,
+            COLOR_BGR5652BGR = 14,
+            COLOR_BGR5652RGB = 15,
+            COLOR_BGRA2BGR565 = 16,
+            COLOR_RGBA2BGR565 = 17,
+            COLOR_BGR5652BGRA = 18,
+            COLOR_BGR5652RGBA = 19,
+            COLOR_GRAY2BGR565 = 20,
+            COLOR_BGR5652GRAY = 21,
+            COLOR_BGR2BGR555 = 22,
+            COLOR_RGB2BGR555 = 23,
+            COLOR_BGR5552BGR = 24,
+            COLOR_BGR5552RGB = 25,
+            COLOR_BGRA2BGR555 = 26,
+            COLOR_RGBA2BGR555 = 27,
+            COLOR_BGR5552BGRA = 28,
+            COLOR_BGR5552RGBA = 29,
+            COLOR_GRAY2BGR555 = 30,
+            COLOR_BGR5552GRAY = 31,
+            COLOR_BGR2XYZ = 32,
+            COLOR_RGB2XYZ = 33,
+            COLOR_XYZ2BGR = 34,
+            COLOR_XYZ2RGB = 35,
+            COLOR_BGR2YCrCb = 36,
+            COLOR_RGB2YCrCb = 37,
+            COLOR_YCrCb2BGR = 38,
+            COLOR_YCrCb2RGB = 39,
+            COLOR_BGR2HSV = 40,
+            COLOR_RGB2HSV = 41,
+            COLOR_BGR2Lab = 44,
+            COLOR_RGB2Lab = 45,
+            COLOR_BGR2Luv = 50,
+            COLOR_RGB2Luv = 51,
+            COLOR_BGR2HLS = 52,
+            COLOR_RGB2HLS = 53,
+            COLOR_HSV2BGR = 54,
+            COLOR_HSV2RGB = 55,
+            COLOR_Lab2BGR = 56,
+            COLOR_Lab2RGB = 57,
+            COLOR_Luv2BGR = 58,
+            COLOR_Luv2RGB = 59,
+            COLOR_HLS2BGR = 60,
+            COLOR_HLS2RGB = 61,
+            COLOR_BGR2HSV_FULL = 66,
+            COLOR_RGB2HSV_FULL = 67,
+            COLOR_BGR2HLS_FULL = 68,
+            COLOR_RGB2HLS_FULL = 69,
+            COLOR_HSV2BGR_FULL = 70,
+            COLOR_HSV2RGB_FULL = 71,
+            COLOR_HLS2BGR_FULL = 72,
+            COLOR_HLS2RGB_FULL = 73,
+            COLOR_LBGR2Lab = 74,
+            COLOR_LRGB2Lab = 75,
+            COLOR_LBGR2Luv = 76,
+            COLOR_LRGB2Luv = 77,
+            COLOR_Lab2LBGR = 78,
+            COLOR_Lab2LRGB = 79,
+            COLOR_Luv2LBGR = 80,
+            COLOR_Luv2LRGB = 81,
+            COLOR_BGR2YUV = 82,
+            COLOR_RGB2YUV = 83,
+            COLOR_YUV2BGR = 84,
+            COLOR_YUV2RGB = 85,
+            COLOR_YUV2RGB_NV12 = 90,
+            COLOR_YUV2BGR_NV12 = 91,
+            COLOR_YUV2RGB_NV21 = 92,
+            COLOR_YUV2BGR_NV21 = 93,
+            COLOR_YUV420sp2RGB = 92,
+            COLOR_YUV420sp2BGR = 93,
+            COLOR_YUV2RGBA_NV12 = 94,
+            COLOR_YUV2BGRA_NV12 = 95,
+            COLOR_YUV2RGBA_NV21 = 96,
+            COLOR_YUV2BGRA_NV21 = 97,
+            COLOR_YUV420sp2RGBA = 96,
+            COLOR_YUV420sp2BGRA = 97,
+            COLOR_YUV2RGB_YV12 = 98,
+            COLOR_YUV2BGR_YV12 = 99,
+            COLOR_YUV2RGB_IYUV = 100,
+            COLOR_YUV2BGR_IYUV = 101,
+            COLOR_YUV2RGB_I420 = 100,
+            COLOR_YUV2BGR_I420 = 101,
+            COLOR_YUV420p2RGB = 98,
+            COLOR_YUV420p2BGR = 99,
+            COLOR_YUV2RGBA_YV12 = 102,
+            COLOR_YUV2BGRA_YV12 = 103,
+            COLOR_YUV2RGBA_IYUV = 104,
+            COLOR_YUV2BGRA_IYUV = 105,
+            COLOR_YUV2RGBA_I420 = 104,
+            COLOR_YUV2BGRA_I420 = 105,
+            COLOR_YUV420p2RGBA = 102,
+            COLOR_YUV420p2BGRA = 103,
+            COLOR_YUV2GRAY_420 = 106,
+            COLOR_YUV2GRAY_NV21 = 106,
+            COLOR_YUV2GRAY_NV12 = 106,
+            COLOR_YUV2GRAY_YV12 = 106,
+            COLOR_YUV2GRAY_IYUV = 106,
+            COLOR_YUV2GRAY_I420 = 106,
+            COLOR_YUV420sp2GRAY = 106,
+            COLOR_YUV420p2GRAY = 106,
+            COLOR_YUV2RGB_UYVY = 107,
+            COLOR_YUV2BGR_UYVY = 108,
+            COLOR_YUV2RGB_Y422 = 107,
+            COLOR_YUV2BGR_Y422 = 108,
+            COLOR_YUV2RGB_UYNV = 107,
+            COLOR_YUV2BGR_UYNV = 108,
+            COLOR_YUV2RGBA_UYVY = 111,
+            COLOR_YUV2BGRA_UYVY = 112,
+            COLOR_YUV2RGBA_Y422 = 111,
+            COLOR_YUV2BGRA_Y422 = 112,
+            COLOR_YUV2RGBA_UYNV = 111,
+            COLOR_YUV2BGRA_UYNV = 112,
+            COLOR_YUV2RGB_YUY2 = 115,
+            COLOR_YUV2BGR_YUY2 = 116,
+            COLOR_YUV2RGB_YVYU = 117,
+            COLOR_YUV2BGR_YVYU = 118,
+            COLOR_YUV2RGB_YUYV = 115,
+            COLOR_YUV2BGR_YUYV = 116,
+            COLOR_YUV2RGB_YUNV = 115,
+            COLOR_YUV2BGR_YUNV = 116,
+            COLOR_YUV2RGBA_YUY2 = 119,
+            COLOR_YUV2BGRA_YUY2 = 120,
+            COLOR_YUV2RGBA_YVYU = 121,
+            COLOR_YUV2BGRA_YVYU = 122,
+            COLOR_YUV2RGBA_YUYV = 119,
+            COLOR_YUV2BGRA_YUYV = 120,
+            COLOR_YUV2RGBA_YUNV = 119,
+            COLOR_YUV2BGRA_YUNV = 120,
+            COLOR_YUV2GRAY_UYVY = 123,
+            COLOR_YUV2GRAY_YUY2 = 124,
+            COLOR_YUV2GRAY_Y422 = 123,
+            COLOR_YUV2GRAY_UYNV = 123,
+            COLOR_YUV2GRAY_YVYU = 124,
+            COLOR_YUV2GRAY_YUYV = 124,
+            COLOR_YUV2GRAY_YUNV = 124,
+            COLOR_RGBA2mRGBA = 125,
+            COLOR_mRGBA2RGBA = 126,
+            COLOR_RGB2YUV_I420 = 127,
+            COLOR_BGR2YUV_I420 = 128,
+            COLOR_RGB2YUV_IYUV = 127,
+            COLOR_BGR2YUV_IYUV = 128,
+            COLOR_RGBA2YUV_I420 = 129,
+            COLOR_BGRA2YUV_I420 = 130,
+            COLOR_RGBA2YUV_IYUV = 129,
+            COLOR_BGRA2YUV_IYUV = 130,
+            COLOR_RGB2YUV_YV12 = 131,
+            COLOR_BGR2YUV_YV12 = 132,
+            COLOR_RGBA2YUV_YV12 = 133,
+            COLOR_BGRA2YUV_YV12 = 134,
+            COLOR_BayerBG2BGR = 46,
+            COLOR_BayerGB2BGR = 47,
+            COLOR_BayerRG2BGR = 48,
+            COLOR_BayerGR2BGR = 49,
+            COLOR_BayerBG2RGB = 48,
+            COLOR_BayerGB2RGB = 49,
+            COLOR_BayerRG2RGB = 46,
+            COLOR_BayerGR2RGB = 47,
+            COLOR_BayerBG2GRAY = 86,
+            COLOR_BayerGB2GRAY = 87,
+            COLOR_BayerRG2GRAY = 88,
+            COLOR_BayerGR2GRAY = 89,
+            COLOR_BayerBG2BGR_VNG = 62,
+            COLOR_BayerGB2BGR_VNG = 63,
+            COLOR_BayerRG2BGR_VNG = 64,
+            COLOR_BayerGR2BGR_VNG = 65,
+            COLOR_BayerBG2RGB_VNG = 64,
+            COLOR_BayerGB2RGB_VNG = 65,
+            COLOR_BayerRG2RGB_VNG = 62,
+            COLOR_BayerGR2RGB_VNG = 63,
+            COLOR_BayerBG2BGR_EA = 135,
+            COLOR_BayerGB2BGR_EA = 136,
+            COLOR_BayerRG2BGR_EA = 137,
+            COLOR_BayerGR2BGR_EA = 138,
+            COLOR_BayerBG2RGB_EA = 137,
+            COLOR_BayerGB2RGB_EA = 138,
+            COLOR_BayerRG2RGB_EA = 135,
+            COLOR_BayerGR2RGB_EA = 136,
+            COLOR_BayerBG2BGRA = 139,
+            COLOR_BayerGB2BGRA = 140,
+            COLOR_BayerRG2BGRA = 141,
+            COLOR_BayerGR2BGRA = 142,
+            COLOR_BayerBG2RGBA = 141,
+            COLOR_BayerGB2RGBA = 142,
+            COLOR_BayerRG2RGBA = 139,
+            COLOR_BayerGR2RGBA = 140,
+            COLOR_COLORCVT_MAX = 143;
+
+
+    // C++: enum RectanglesIntersectTypes
+    public static final int
+            INTERSECT_NONE = 0,
+            INTERSECT_PARTIAL = 1,
+            INTERSECT_FULL = 2;
+
+
+    // C++: enum <unnamed>
+    public static final int
+            LINE_AA = 16,
+            LINE_8 = 8,
+            LINE_4 = 4,
+            CV_GAUSSIAN_5x5 = 7,
+            CV_SCHARR = -1,
+            CV_MAX_SOBEL_KSIZE = 7,
+            CV_RGBA2mRGBA = 125,
+            CV_mRGBA2RGBA = 126,
+            CV_WARP_FILL_OUTLIERS = 8,
+            CV_WARP_INVERSE_MAP = 16,
+            CV_CHAIN_CODE = 0,
+            CV_LINK_RUNS = 5,
+            CV_POLY_APPROX_DP = 0,
+            CV_CONTOURS_MATCH_I1 = 1,
+            CV_CONTOURS_MATCH_I2 = 2,
+            CV_CONTOURS_MATCH_I3 = 3,
+            CV_CLOCKWISE = 1,
+            CV_COUNTER_CLOCKWISE = 2,
+            CV_COMP_CORREL = 0,
+            CV_COMP_CHISQR = 1,
+            CV_COMP_INTERSECT = 2,
+            CV_COMP_BHATTACHARYYA = 3,
+            CV_COMP_HELLINGER = 3,
+            CV_COMP_CHISQR_ALT = 4,
+            CV_COMP_KL_DIV = 5,
+            CV_DIST_MASK_3 = 3,
+            CV_DIST_MASK_5 = 5,
+            CV_DIST_MASK_PRECISE = 0,
+            CV_DIST_LABEL_CCOMP = 0,
+            CV_DIST_LABEL_PIXEL = 1,
+            CV_DIST_USER = -1,
+            CV_DIST_L1 = 1,
+            CV_DIST_L2 = 2,
+            CV_DIST_C = 3,
+            CV_DIST_L12 = 4,
+            CV_DIST_FAIR = 5,
+            CV_DIST_WELSCH = 6,
+            CV_DIST_HUBER = 7,
+            CV_CANNY_L2_GRADIENT = (1 << 31),
+            CV_HOUGH_STANDARD = 0,
+            CV_HOUGH_PROBABILISTIC = 1,
+            CV_HOUGH_MULTI_SCALE = 2,
+            CV_HOUGH_GRADIENT = 3;
+
+
+    // C++: enum ThresholdTypes
+    public static final int
+            THRESH_BINARY = 0,
+            THRESH_BINARY_INV = 1,
+            THRESH_TRUNC = 2,
+            THRESH_TOZERO = 3,
+            THRESH_TOZERO_INV = 4,
+            THRESH_MASK = 7,
+            THRESH_OTSU = 8,
+            THRESH_TRIANGLE = 16;
+
+
+    // C++: enum AdaptiveThresholdTypes
+    public static final int
+            ADAPTIVE_THRESH_MEAN_C = 0,
+            ADAPTIVE_THRESH_GAUSSIAN_C = 1;
+
+
+    // C++: enum MorphShapes_c
+    public static final int
+            CV_SHAPE_RECT = 0,
+            CV_SHAPE_CROSS = 1,
+            CV_SHAPE_ELLIPSE = 2,
+            CV_SHAPE_CUSTOM = 100;
+
+
+    // C++: enum RetrievalModes
+    public static final int
+            RETR_EXTERNAL = 0,
+            RETR_LIST = 1,
+            RETR_CCOMP = 2,
+            RETR_TREE = 3,
+            RETR_FLOODFILL = 4;
+
+
+    // C++: enum MorphShapes
+    public static final int
+            MORPH_RECT = 0,
+            MORPH_CROSS = 1,
+            MORPH_ELLIPSE = 2;
+
+
+    // C++: enum DistanceTransformLabelTypes
+    public static final int
+            DIST_LABEL_CCOMP = 0,
+            DIST_LABEL_PIXEL = 1;
+
+
+    // C++: enum DistanceTypes
+    public static final int
+            DIST_USER = -1,
+            DIST_L1 = 1,
+            DIST_L2 = 2,
+            DIST_C = 3,
+            DIST_L12 = 4,
+            DIST_FAIR = 5,
+            DIST_WELSCH = 6,
+            DIST_HUBER = 7;
+
+
+    // C++: enum LineSegmentDetectorModes
+    public static final int
+            LSD_REFINE_NONE = 0,
+            LSD_REFINE_STD = 1,
+            LSD_REFINE_ADV = 2;
+
+
+    // C++: enum TemplateMatchModes
+    public static final int
+            TM_SQDIFF = 0,
+            TM_SQDIFF_NORMED = 1,
+            TM_CCORR = 2,
+            TM_CCORR_NORMED = 3,
+            TM_CCOEFF = 4,
+            TM_CCOEFF_NORMED = 5;
+
+
+    // C++: enum DistanceTransformMasks
+    public static final int
+            DIST_MASK_3 = 3,
+            DIST_MASK_5 = 5,
+            DIST_MASK_PRECISE = 0;
+
+
+    // C++: enum ConnectedComponentsTypes
+    public static final int
+            CC_STAT_LEFT = 0,
+            CC_STAT_TOP = 1,
+            CC_STAT_WIDTH = 2,
+            CC_STAT_HEIGHT = 3,
+            CC_STAT_AREA = 4,
+            CC_STAT_MAX = 5;
+
+
+    // C++: enum SmoothMethod_c
+    public static final int
+            CV_BLUR_NO_SCALE = 0,
+            CV_BLUR = 1,
+            CV_GAUSSIAN = 2,
+            CV_MEDIAN = 3,
+            CV_BILATERAL = 4;
+
+
+    // C++: enum MarkerTypes
+    public static final int
+            MARKER_CROSS = 0,
+            MARKER_TILTED_CROSS = 1,
+            MARKER_STAR = 2,
+            MARKER_DIAMOND = 3,
+            MARKER_SQUARE = 4,
+            MARKER_TRIANGLE_UP = 5,
+            MARKER_TRIANGLE_DOWN = 6;
+
+
+    //
+    // C++:  Mat cv::getAffineTransform(vector_Point2f src, vector_Point2f dst)
+    //
+
+    public static Mat getAffineTransform(MatOfPoint2f src, MatOfPoint2f dst) {
+        Mat src_mat = src;
+        Mat dst_mat = dst;
+        return new Mat(getAffineTransform_0(src_mat.nativeObj, dst_mat.nativeObj));
+    }
+
+
+    //
+    // C++:  Mat cv::getDefaultNewCameraMatrix(Mat cameraMatrix, Size imgsize = Size(), bool centerPrincipalPoint = false)
+    //
+
+    /**
+     * Returns the default new camera matrix.
+     *
+     * The function returns the camera matrix that is either an exact copy of the input cameraMatrix (when
+     * centerPrinicipalPoint=false ), or the modified one (when centerPrincipalPoint=true).
+     *
+     * In the latter case, the new camera matrix will be:
+     *
+     * \(\begin{bmatrix} f_x &amp;&amp; 0 &amp;&amp; ( \texttt{imgSize.width} -1)*0.5  \\ 0 &amp;&amp; f_y &amp;&amp; ( \texttt{imgSize.height} -1)*0.5  \\ 0 &amp;&amp; 0 &amp;&amp; 1 \end{bmatrix} ,\)
+     *
+     * where \(f_x\) and \(f_y\) are \((0,0)\) and \((1,1)\) elements of cameraMatrix, respectively.
+     *
+     * By default, the undistortion functions in OpenCV (see #initUndistortRectifyMap, #undistort) do not
+     * move the principal point. However, when you work with stereo, it is important to move the principal
+     * points in both views to the same y-coordinate (which is required by most of stereo correspondence
+     * algorithms), and may be to the same x-coordinate too. So, you can form the new camera matrix for
+     * each view where the principal points are located at the center.
+     *
+     * @param cameraMatrix Input camera matrix.
+     * @param imgsize Camera view image size in pixels.
+     * @param centerPrincipalPoint Location of the principal point in the new camera matrix. The
+     * parameter indicates whether this location should be at the image center or not.
+     * @return automatically generated
+     */
+    public static Mat getDefaultNewCameraMatrix(Mat cameraMatrix, Size imgsize, boolean centerPrincipalPoint) {
+        return new Mat(getDefaultNewCameraMatrix_0(cameraMatrix.nativeObj, imgsize.width, imgsize.height, centerPrincipalPoint));
+    }
+
+    /**
+     * Returns the default new camera matrix.
+     *
+     * The function returns the camera matrix that is either an exact copy of the input cameraMatrix (when
+     * centerPrinicipalPoint=false ), or the modified one (when centerPrincipalPoint=true).
+     *
+     * In the latter case, the new camera matrix will be:
+     *
+     * \(\begin{bmatrix} f_x &amp;&amp; 0 &amp;&amp; ( \texttt{imgSize.width} -1)*0.5  \\ 0 &amp;&amp; f_y &amp;&amp; ( \texttt{imgSize.height} -1)*0.5  \\ 0 &amp;&amp; 0 &amp;&amp; 1 \end{bmatrix} ,\)
+     *
+     * where \(f_x\) and \(f_y\) are \((0,0)\) and \((1,1)\) elements of cameraMatrix, respectively.
+     *
+     * By default, the undistortion functions in OpenCV (see #initUndistortRectifyMap, #undistort) do not
+     * move the principal point. However, when you work with stereo, it is important to move the principal
+     * points in both views to the same y-coordinate (which is required by most of stereo correspondence
+     * algorithms), and may be to the same x-coordinate too. So, you can form the new camera matrix for
+     * each view where the principal points are located at the center.
+     *
+     * @param cameraMatrix Input camera matrix.
+     * @param imgsize Camera view image size in pixels.
+     * parameter indicates whether this location should be at the image center or not.
+     * @return automatically generated
+     */
+    public static Mat getDefaultNewCameraMatrix(Mat cameraMatrix, Size imgsize) {
+        return new Mat(getDefaultNewCameraMatrix_1(cameraMatrix.nativeObj, imgsize.width, imgsize.height));
+    }
+
+    /**
+     * Returns the default new camera matrix.
+     *
+     * The function returns the camera matrix that is either an exact copy of the input cameraMatrix (when
+     * centerPrinicipalPoint=false ), or the modified one (when centerPrincipalPoint=true).
+     *
+     * In the latter case, the new camera matrix will be:
+     *
+     * \(\begin{bmatrix} f_x &amp;&amp; 0 &amp;&amp; ( \texttt{imgSize.width} -1)*0.5  \\ 0 &amp;&amp; f_y &amp;&amp; ( \texttt{imgSize.height} -1)*0.5  \\ 0 &amp;&amp; 0 &amp;&amp; 1 \end{bmatrix} ,\)
+     *
+     * where \(f_x\) and \(f_y\) are \((0,0)\) and \((1,1)\) elements of cameraMatrix, respectively.
+     *
+     * By default, the undistortion functions in OpenCV (see #initUndistortRectifyMap, #undistort) do not
+     * move the principal point. However, when you work with stereo, it is important to move the principal
+     * points in both views to the same y-coordinate (which is required by most of stereo correspondence
+     * algorithms), and may be to the same x-coordinate too. So, you can form the new camera matrix for
+     * each view where the principal points are located at the center.
+     *
+     * @param cameraMatrix Input camera matrix.
+     * parameter indicates whether this location should be at the image center or not.
+     * @return automatically generated
+     */
+    public static Mat getDefaultNewCameraMatrix(Mat cameraMatrix) {
+        return new Mat(getDefaultNewCameraMatrix_2(cameraMatrix.nativeObj));
+    }
+
+
+    //
+    // C++:  Mat cv::getGaborKernel(Size ksize, double sigma, double theta, double lambd, double gamma, double psi = CV_PI*0.5, int ktype = CV_64F)
+    //
+
+    /**
+     * Returns Gabor filter coefficients.
+     *
+     * For more details about gabor filter equations and parameters, see: [Gabor
+     * Filter](http://en.wikipedia.org/wiki/Gabor_filter).
+     *
+     * @param ksize Size of the filter returned.
+     * @param sigma Standard deviation of the gaussian envelope.
+     * @param theta Orientation of the normal to the parallel stripes of a Gabor function.
+     * @param lambd Wavelength of the sinusoidal factor.
+     * @param gamma Spatial aspect ratio.
+     * @param psi Phase offset.
+     * @param ktype Type of filter coefficients. It can be CV_32F or CV_64F .
+     * @return automatically generated
+     */
+    public static Mat getGaborKernel(Size ksize, double sigma, double theta, double lambd, double gamma, double psi, int ktype) {
+        return new Mat(getGaborKernel_0(ksize.width, ksize.height, sigma, theta, lambd, gamma, psi, ktype));
+    }
+
+    /**
+     * Returns Gabor filter coefficients.
+     *
+     * For more details about gabor filter equations and parameters, see: [Gabor
+     * Filter](http://en.wikipedia.org/wiki/Gabor_filter).
+     *
+     * @param ksize Size of the filter returned.
+     * @param sigma Standard deviation of the gaussian envelope.
+     * @param theta Orientation of the normal to the parallel stripes of a Gabor function.
+     * @param lambd Wavelength of the sinusoidal factor.
+     * @param gamma Spatial aspect ratio.
+     * @param psi Phase offset.
+     * @return automatically generated
+     */
+    public static Mat getGaborKernel(Size ksize, double sigma, double theta, double lambd, double gamma, double psi) {
+        return new Mat(getGaborKernel_1(ksize.width, ksize.height, sigma, theta, lambd, gamma, psi));
+    }
+
+    /**
+     * Returns Gabor filter coefficients.
+     *
+     * For more details about gabor filter equations and parameters, see: [Gabor
+     * Filter](http://en.wikipedia.org/wiki/Gabor_filter).
+     *
+     * @param ksize Size of the filter returned.
+     * @param sigma Standard deviation of the gaussian envelope.
+     * @param theta Orientation of the normal to the parallel stripes of a Gabor function.
+     * @param lambd Wavelength of the sinusoidal factor.
+     * @param gamma Spatial aspect ratio.
+     * @return automatically generated
+     */
+    public static Mat getGaborKernel(Size ksize, double sigma, double theta, double lambd, double gamma) {
+        return new Mat(getGaborKernel_2(ksize.width, ksize.height, sigma, theta, lambd, gamma));
+    }
+
+
+    //
+    // C++:  Mat cv::getGaussianKernel(int ksize, double sigma, int ktype = CV_64F)
+    //
+
+    /**
+     * Returns Gaussian filter coefficients.
+     *
+     * The function computes and returns the \(\texttt{ksize} \times 1\) matrix of Gaussian filter
+     * coefficients:
+     *
+     * \(G_i= \alpha *e^{-(i-( \texttt{ksize} -1)/2)^2/(2* \texttt{sigma}^2)},\)
+     *
+     * where \(i=0..\texttt{ksize}-1\) and \(\alpha\) is the scale factor chosen so that \(\sum_i G_i=1\).
+     *
+     * Two of such generated kernels can be passed to sepFilter2D. Those functions automatically recognize
+     * smoothing kernels (a symmetrical kernel with sum of weights equal to 1) and handle them accordingly.
+     * You may also use the higher-level GaussianBlur.
+     * @param ksize Aperture size. It should be odd ( \(\texttt{ksize} \mod 2 = 1\) ) and positive.
+     * @param sigma Gaussian standard deviation. If it is non-positive, it is computed from ksize as
+     * {@code sigma = 0.3*((ksize-1)*0.5 - 1) + 0.8}.
+     * @param ktype Type of filter coefficients. It can be CV_32F or CV_64F .
+     * SEE:  sepFilter2D, getDerivKernels, getStructuringElement, GaussianBlur
+     * @return automatically generated
+     */
+    public static Mat getGaussianKernel(int ksize, double sigma, int ktype) {
+        return new Mat(getGaussianKernel_0(ksize, sigma, ktype));
+    }
+
+    /**
+     * Returns Gaussian filter coefficients.
+     *
+     * The function computes and returns the \(\texttt{ksize} \times 1\) matrix of Gaussian filter
+     * coefficients:
+     *
+     * \(G_i= \alpha *e^{-(i-( \texttt{ksize} -1)/2)^2/(2* \texttt{sigma}^2)},\)
+     *
+     * where \(i=0..\texttt{ksize}-1\) and \(\alpha\) is the scale factor chosen so that \(\sum_i G_i=1\).
+     *
+     * Two of such generated kernels can be passed to sepFilter2D. Those functions automatically recognize
+     * smoothing kernels (a symmetrical kernel with sum of weights equal to 1) and handle them accordingly.
+     * You may also use the higher-level GaussianBlur.
+     * @param ksize Aperture size. It should be odd ( \(\texttt{ksize} \mod 2 = 1\) ) and positive.
+     * @param sigma Gaussian standard deviation. If it is non-positive, it is computed from ksize as
+     * {@code sigma = 0.3*((ksize-1)*0.5 - 1) + 0.8}.
+     * SEE:  sepFilter2D, getDerivKernels, getStructuringElement, GaussianBlur
+     * @return automatically generated
+     */
+    public static Mat getGaussianKernel(int ksize, double sigma) {
+        return new Mat(getGaussianKernel_1(ksize, sigma));
+    }
+
+
+    //
+    // C++:  Mat cv::getPerspectiveTransform(Mat src, Mat dst)
+    //
+
+    /**
+     * Calculates a perspective transform from four pairs of the corresponding points.
+     *
+     * The function calculates the \(3 \times 3\) matrix of a perspective transform so that:
+     *
+     * \(\begin{bmatrix} t_i x'_i \\ t_i y'_i \\ t_i \end{bmatrix} = \texttt{map_matrix} \cdot \begin{bmatrix} x_i \\ y_i \\ 1 \end{bmatrix}\)
+     *
+     * where
+     *
+     * \(dst(i)=(x'_i,y'_i), src(i)=(x_i, y_i), i=0,1,2,3\)
+     *
+     * @param src Coordinates of quadrangle vertices in the source image.
+     * @param dst Coordinates of the corresponding quadrangle vertices in the destination image.
+     *
+     * SEE:  findHomography, warpPerspective, perspectiveTransform
+     * @return automatically generated
+     */
+    public static Mat getPerspectiveTransform(Mat src, Mat dst) {
+        return new Mat(getPerspectiveTransform_0(src.nativeObj, dst.nativeObj));
+    }
+
+
+    //
+    // C++:  Mat cv::getRotationMatrix2D(Point2f center, double angle, double scale)
+    //
+
+    /**
+     * Calculates an affine matrix of 2D rotation.
+     *
+     * The function calculates the following matrix:
+     *
+     * \(\begin{bmatrix} \alpha &amp;  \beta &amp; (1- \alpha )  \cdot \texttt{center.x} -  \beta \cdot \texttt{center.y} \\ - \beta &amp;  \alpha &amp;  \beta \cdot \texttt{center.x} + (1- \alpha )  \cdot \texttt{center.y} \end{bmatrix}\)
+     *
+     * where
+     *
+     * \(\begin{array}{l} \alpha =  \texttt{scale} \cdot \cos \texttt{angle} , \\ \beta =  \texttt{scale} \cdot \sin \texttt{angle} \end{array}\)
+     *
+     * The transformation maps the rotation center to itself. If this is not the target, adjust the shift.
+     *
+     * @param center Center of the rotation in the source image.
+     * @param angle Rotation angle in degrees. Positive values mean counter-clockwise rotation (the
+     * coordinate origin is assumed to be the top-left corner).
+     * @param scale Isotropic scale factor.
+     *
+     * SEE:  getAffineTransform, warpAffine, transform
+     * @return automatically generated
+     */
+    public static Mat getRotationMatrix2D(Point center, double angle, double scale) {
+        return new Mat(getRotationMatrix2D_0(center.x, center.y, angle, scale));
+    }
+
+
+    //
+    // C++:  Mat cv::getStructuringElement(int shape, Size ksize, Point anchor = Point(-1,-1))
+    //
+
+    /**
+     * Returns a structuring element of the specified size and shape for morphological operations.
+     *
+     * The function constructs and returns the structuring element that can be further passed to #erode,
+     * #dilate or #morphologyEx. But you can also construct an arbitrary binary mask yourself and use it as
+     * the structuring element.
+     *
+     * @param shape Element shape that could be one of #MorphShapes
+     * @param ksize Size of the structuring element.
+     * @param anchor Anchor position within the element. The default value \((-1, -1)\) means that the
+     * anchor is at the center. Note that only the shape of a cross-shaped element depends on the anchor
+     * position. In other cases the anchor just regulates how much the result of the morphological
+     * operation is shifted.
+     * @return automatically generated
+     */
+    public static Mat getStructuringElement(int shape, Size ksize, Point anchor) {
+        return new Mat(getStructuringElement_0(shape, ksize.width, ksize.height, anchor.x, anchor.y));
+    }
+
+    /**
+     * Returns a structuring element of the specified size and shape for morphological operations.
+     *
+     * The function constructs and returns the structuring element that can be further passed to #erode,
+     * #dilate or #morphologyEx. But you can also construct an arbitrary binary mask yourself and use it as
+     * the structuring element.
+     *
+     * @param shape Element shape that could be one of #MorphShapes
+     * @param ksize Size of the structuring element.
+     * anchor is at the center. Note that only the shape of a cross-shaped element depends on the anchor
+     * position. In other cases the anchor just regulates how much the result of the morphological
+     * operation is shifted.
+     * @return automatically generated
+     */
+    public static Mat getStructuringElement(int shape, Size ksize) {
+        return new Mat(getStructuringElement_1(shape, ksize.width, ksize.height));
+    }
+
+
+    //
+    // C++:  Moments cv::moments(Mat array, bool binaryImage = false)
+    //
+
+    /**
+     * Calculates all of the moments up to the third order of a polygon or rasterized shape.
+     *
+     * The function computes moments, up to the 3rd order, of a vector shape or a rasterized shape. The
+     * results are returned in the structure cv::Moments.
+     *
+     * @param array Raster image (single-channel, 8-bit or floating-point 2D array) or an array (
+     * \(1 \times N\) or \(N \times 1\) ) of 2D points (Point or Point2f ).
+     * @param binaryImage If it is true, all non-zero image pixels are treated as 1's. The parameter is
+     * used for images only.
+     * @return moments.
+     *
+     * <b>Note:</b> Only applicable to contour moments calculations from Python bindings: Note that the numpy
+     * type for the input array should be either np.int32 or np.float32.
+     *
+     * SEE:  contourArea, arcLength
+     */
+    public static Moments moments(Mat array, boolean binaryImage) {
+        return new Moments(moments_0(array.nativeObj, binaryImage));
+    }
+
+    /**
+     * Calculates all of the moments up to the third order of a polygon or rasterized shape.
+     *
+     * The function computes moments, up to the 3rd order, of a vector shape or a rasterized shape. The
+     * results are returned in the structure cv::Moments.
+     *
+     * @param array Raster image (single-channel, 8-bit or floating-point 2D array) or an array (
+     * \(1 \times N\) or \(N \times 1\) ) of 2D points (Point or Point2f ).
+     * used for images only.
+     * @return moments.
+     *
+     * <b>Note:</b> Only applicable to contour moments calculations from Python bindings: Note that the numpy
+     * type for the input array should be either np.int32 or np.float32.
+     *
+     * SEE:  contourArea, arcLength
+     */
+    public static Moments moments(Mat array) {
+        return new Moments(moments_1(array.nativeObj));
+    }
+
+
+    //
+    // C++:  Point2d cv::phaseCorrelate(Mat src1, Mat src2, Mat window = Mat(), double* response = 0)
+    //
+
+    /**
+     * The function is used to detect translational shifts that occur between two images.
+     *
+     * The operation takes advantage of the Fourier shift theorem for detecting the translational shift in
+     * the frequency domain. It can be used for fast image registration as well as motion estimation. For
+     * more information please see &lt;http://en.wikipedia.org/wiki/Phase_correlation&gt;
+     *
+     * Calculates the cross-power spectrum of two supplied source arrays. The arrays are padded if needed
+     * with getOptimalDFTSize.
+     *
+     * The function performs the following equations:
+     * <ul>
+     *   <li>
+     *  First it applies a Hanning window (see &lt;http://en.wikipedia.org/wiki/Hann_function&gt;) to each
+     * image to remove possible edge effects. This window is cached until the array size changes to speed
+     * up processing time.
+     *   </li>
+     *   <li>
+     *  Next it computes the forward DFTs of each source array:
+     * \(\mathbf{G}_a = \mathcal{F}\{src_1\}, \; \mathbf{G}_b = \mathcal{F}\{src_2\}\)
+     * where \(\mathcal{F}\) is the forward DFT.
+     *   </li>
+     *   <li>
+     *  It then computes the cross-power spectrum of each frequency domain array:
+     * \(R = \frac{ \mathbf{G}_a \mathbf{G}_b^*}{|\mathbf{G}_a \mathbf{G}_b^*|}\)
+     *   </li>
+     *   <li>
+     *  Next the cross-correlation is converted back into the time domain via the inverse DFT:
+     * \(r = \mathcal{F}^{-1}\{R\}\)
+     *   </li>
+     *   <li>
+     *  Finally, it computes the peak location and computes a 5x5 weighted centroid around the peak to
+     * achieve sub-pixel accuracy.
+     * \((\Delta x, \Delta y) = \texttt{weightedCentroid} \{\arg \max_{(x, y)}\{r\}\}\)
+     *   </li>
+     *   <li>
+     *  If non-zero, the response parameter is computed as the sum of the elements of r within the 5x5
+     * centroid around the peak location. It is normalized to a maximum of 1 (meaning there is a single
+     * peak) and will be smaller when there are multiple peaks.
+     *   </li>
+     * </ul>
+     *
+     * @param src1 Source floating point array (CV_32FC1 or CV_64FC1)
+     * @param src2 Source floating point array (CV_32FC1 or CV_64FC1)
+     * @param window Floating point array with windowing coefficients to reduce edge effects (optional).
+     * @param response Signal power within the 5x5 centroid around the peak, between 0 and 1 (optional).
+     * @return detected phase shift (sub-pixel) between the two arrays.
+     *
+     * SEE: dft, getOptimalDFTSize, idft, mulSpectrums createHanningWindow
+     */
+    public static Point phaseCorrelate(Mat src1, Mat src2, Mat window, double[] response) {
+        double[] response_out = new double[1];
+        Point retVal = new Point(phaseCorrelate_0(src1.nativeObj, src2.nativeObj, window.nativeObj, response_out));
+        if(response!=null) response[0] = (double)response_out[0];
+        return retVal;
+    }
+
+    /**
+     * The function is used to detect translational shifts that occur between two images.
+     *
+     * The operation takes advantage of the Fourier shift theorem for detecting the translational shift in
+     * the frequency domain. It can be used for fast image registration as well as motion estimation. For
+     * more information please see &lt;http://en.wikipedia.org/wiki/Phase_correlation&gt;
+     *
+     * Calculates the cross-power spectrum of two supplied source arrays. The arrays are padded if needed
+     * with getOptimalDFTSize.
+     *
+     * The function performs the following equations:
+     * <ul>
+     *   <li>
+     *  First it applies a Hanning window (see &lt;http://en.wikipedia.org/wiki/Hann_function&gt;) to each
+     * image to remove possible edge effects. This window is cached until the array size changes to speed
+     * up processing time.
+     *   </li>
+     *   <li>
+     *  Next it computes the forward DFTs of each source array:
+     * \(\mathbf{G}_a = \mathcal{F}\{src_1\}, \; \mathbf{G}_b = \mathcal{F}\{src_2\}\)
+     * where \(\mathcal{F}\) is the forward DFT.
+     *   </li>
+     *   <li>
+     *  It then computes the cross-power spectrum of each frequency domain array:
+     * \(R = \frac{ \mathbf{G}_a \mathbf{G}_b^*}{|\mathbf{G}_a \mathbf{G}_b^*|}\)
+     *   </li>
+     *   <li>
+     *  Next the cross-correlation is converted back into the time domain via the inverse DFT:
+     * \(r = \mathcal{F}^{-1}\{R\}\)
+     *   </li>
+     *   <li>
+     *  Finally, it computes the peak location and computes a 5x5 weighted centroid around the peak to
+     * achieve sub-pixel accuracy.
+     * \((\Delta x, \Delta y) = \texttt{weightedCentroid} \{\arg \max_{(x, y)}\{r\}\}\)
+     *   </li>
+     *   <li>
+     *  If non-zero, the response parameter is computed as the sum of the elements of r within the 5x5
+     * centroid around the peak location. It is normalized to a maximum of 1 (meaning there is a single
+     * peak) and will be smaller when there are multiple peaks.
+     *   </li>
+     * </ul>
+     *
+     * @param src1 Source floating point array (CV_32FC1 or CV_64FC1)
+     * @param src2 Source floating point array (CV_32FC1 or CV_64FC1)
+     * @param window Floating point array with windowing coefficients to reduce edge effects (optional).
+     * @return detected phase shift (sub-pixel) between the two arrays.
+     *
+     * SEE: dft, getOptimalDFTSize, idft, mulSpectrums createHanningWindow
+     */
+    public static Point phaseCorrelate(Mat src1, Mat src2, Mat window) {
+        return new Point(phaseCorrelate_1(src1.nativeObj, src2.nativeObj, window.nativeObj));
+    }
+
+    /**
+     * The function is used to detect translational shifts that occur between two images.
+     *
+     * The operation takes advantage of the Fourier shift theorem for detecting the translational shift in
+     * the frequency domain. It can be used for fast image registration as well as motion estimation. For
+     * more information please see &lt;http://en.wikipedia.org/wiki/Phase_correlation&gt;
+     *
+     * Calculates the cross-power spectrum of two supplied source arrays. The arrays are padded if needed
+     * with getOptimalDFTSize.
+     *
+     * The function performs the following equations:
+     * <ul>
+     *   <li>
+     *  First it applies a Hanning window (see &lt;http://en.wikipedia.org/wiki/Hann_function&gt;) to each
+     * image to remove possible edge effects. This window is cached until the array size changes to speed
+     * up processing time.
+     *   </li>
+     *   <li>
+     *  Next it computes the forward DFTs of each source array:
+     * \(\mathbf{G}_a = \mathcal{F}\{src_1\}, \; \mathbf{G}_b = \mathcal{F}\{src_2\}\)
+     * where \(\mathcal{F}\) is the forward DFT.
+     *   </li>
+     *   <li>
+     *  It then computes the cross-power spectrum of each frequency domain array:
+     * \(R = \frac{ \mathbf{G}_a \mathbf{G}_b^*}{|\mathbf{G}_a \mathbf{G}_b^*|}\)
+     *   </li>
+     *   <li>
+     *  Next the cross-correlation is converted back into the time domain via the inverse DFT:
+     * \(r = \mathcal{F}^{-1}\{R\}\)
+     *   </li>
+     *   <li>
+     *  Finally, it computes the peak location and computes a 5x5 weighted centroid around the peak to
+     * achieve sub-pixel accuracy.
+     * \((\Delta x, \Delta y) = \texttt{weightedCentroid} \{\arg \max_{(x, y)}\{r\}\}\)
+     *   </li>
+     *   <li>
+     *  If non-zero, the response parameter is computed as the sum of the elements of r within the 5x5
+     * centroid around the peak location. It is normalized to a maximum of 1 (meaning there is a single
+     * peak) and will be smaller when there are multiple peaks.
+     *   </li>
+     * </ul>
+     *
+     * @param src1 Source floating point array (CV_32FC1 or CV_64FC1)
+     * @param src2 Source floating point array (CV_32FC1 or CV_64FC1)
+     * @return detected phase shift (sub-pixel) between the two arrays.
+     *
+     * SEE: dft, getOptimalDFTSize, idft, mulSpectrums createHanningWindow
+     */
+    public static Point phaseCorrelate(Mat src1, Mat src2) {
+        return new Point(phaseCorrelate_2(src1.nativeObj, src2.nativeObj));
+    }
+
+
+    //
+    // C++:  Ptr_CLAHE cv::createCLAHE(double clipLimit = 40.0, Size tileGridSize = Size(8, 8))
+    //
+
+    /**
+     * Creates a smart pointer to a cv::CLAHE class and initializes it.
+     *
+     * @param clipLimit Threshold for contrast limiting.
+     * @param tileGridSize Size of grid for histogram equalization. Input image will be divided into
+     * equally sized rectangular tiles. tileGridSize defines the number of tiles in row and column.
+     * @return automatically generated
+     */
+    public static CLAHE createCLAHE(double clipLimit, Size tileGridSize) {
+        return CLAHE.__fromPtr__(createCLAHE_0(clipLimit, tileGridSize.width, tileGridSize.height));
+    }
+
+    /**
+     * Creates a smart pointer to a cv::CLAHE class and initializes it.
+     *
+     * @param clipLimit Threshold for contrast limiting.
+     * equally sized rectangular tiles. tileGridSize defines the number of tiles in row and column.
+     * @return automatically generated
+     */
+    public static CLAHE createCLAHE(double clipLimit) {
+        return CLAHE.__fromPtr__(createCLAHE_1(clipLimit));
+    }
+
+    /**
+     * Creates a smart pointer to a cv::CLAHE class and initializes it.
+     *
+     * equally sized rectangular tiles. tileGridSize defines the number of tiles in row and column.
+     * @return automatically generated
+     */
+    public static CLAHE createCLAHE() {
+        return CLAHE.__fromPtr__(createCLAHE_2());
+    }
+
+
+    //
+    // C++:  Ptr_GeneralizedHoughBallard cv::createGeneralizedHoughBallard()
+    //
+
+    /**
+     * Creates a smart pointer to a cv::GeneralizedHoughBallard class and initializes it.
+     * @return automatically generated
+     */
+    public static GeneralizedHoughBallard createGeneralizedHoughBallard() {
+        return GeneralizedHoughBallard.__fromPtr__(createGeneralizedHoughBallard_0());
+    }
+
+
+    //
+    // C++:  Ptr_GeneralizedHoughGuil cv::createGeneralizedHoughGuil()
+    //
+
+    /**
+     * Creates a smart pointer to a cv::GeneralizedHoughGuil class and initializes it.
+     * @return automatically generated
+     */
+    public static GeneralizedHoughGuil createGeneralizedHoughGuil() {
+        return GeneralizedHoughGuil.__fromPtr__(createGeneralizedHoughGuil_0());
+    }
+
+
+    //
+    // C++:  Ptr_LineSegmentDetector cv::createLineSegmentDetector(int _refine = LSD_REFINE_STD, double _scale = 0.8, double _sigma_scale = 0.6, double _quant = 2.0, double _ang_th = 22.5, double _log_eps = 0, double _density_th = 0.7, int _n_bins = 1024)
+    //
+
+    /**
+     * Creates a smart pointer to a LineSegmentDetector object and initializes it.
+     *
+     * The LineSegmentDetector algorithm is defined using the standard values. Only advanced users may want
+     * to edit those, as to tailor it for their own application.
+     *
+     * @param _refine The way found lines will be refined, see #LineSegmentDetectorModes
+     * @param _scale The scale of the image that will be used to find the lines. Range (0..1].
+     * @param _sigma_scale Sigma for Gaussian filter. It is computed as sigma = _sigma_scale/_scale.
+     * @param _quant Bound to the quantization error on the gradient norm.
+     * @param _ang_th Gradient angle tolerance in degrees.
+     * @param _log_eps Detection threshold: -log10(NFA) &gt; log_eps. Used only when advance refinement
+     * is chosen.
+     * @param _density_th Minimal density of aligned region points in the enclosing rectangle.
+     * @param _n_bins Number of bins in pseudo-ordering of gradient modulus.
+     *
+     * <b>Note:</b> Implementation has been removed due original code license conflict
+     * @return automatically generated
+     */
+    public static LineSegmentDetector createLineSegmentDetector(int _refine, double _scale, double _sigma_scale, double _quant, double _ang_th, double _log_eps, double _density_th, int _n_bins) {
+        return LineSegmentDetector.__fromPtr__(createLineSegmentDetector_0(_refine, _scale, _sigma_scale, _quant, _ang_th, _log_eps, _density_th, _n_bins));
+    }
+
+    /**
+     * Creates a smart pointer to a LineSegmentDetector object and initializes it.
+     *
+     * The LineSegmentDetector algorithm is defined using the standard values. Only advanced users may want
+     * to edit those, as to tailor it for their own application.
+     *
+     * @param _refine The way found lines will be refined, see #LineSegmentDetectorModes
+     * @param _scale The scale of the image that will be used to find the lines. Range (0..1].
+     * @param _sigma_scale Sigma for Gaussian filter. It is computed as sigma = _sigma_scale/_scale.
+     * @param _quant Bound to the quantization error on the gradient norm.
+     * @param _ang_th Gradient angle tolerance in degrees.
+     * @param _log_eps Detection threshold: -log10(NFA) &gt; log_eps. Used only when advance refinement
+     * is chosen.
+     * @param _density_th Minimal density of aligned region points in the enclosing rectangle.
+     *
+     * <b>Note:</b> Implementation has been removed due original code license conflict
+     * @return automatically generated
+     */
+    public static LineSegmentDetector createLineSegmentDetector(int _refine, double _scale, double _sigma_scale, double _quant, double _ang_th, double _log_eps, double _density_th) {
+        return LineSegmentDetector.__fromPtr__(createLineSegmentDetector_1(_refine, _scale, _sigma_scale, _quant, _ang_th, _log_eps, _density_th));
+    }
+
+    /**
+     * Creates a smart pointer to a LineSegmentDetector object and initializes it.
+     *
+     * The LineSegmentDetector algorithm is defined using the standard values. Only advanced users may want
+     * to edit those, as to tailor it for their own application.
+     *
+     * @param _refine The way found lines will be refined, see #LineSegmentDetectorModes
+     * @param _scale The scale of the image that will be used to find the lines. Range (0..1].
+     * @param _sigma_scale Sigma for Gaussian filter. It is computed as sigma = _sigma_scale/_scale.
+     * @param _quant Bound to the quantization error on the gradient norm.
+     * @param _ang_th Gradient angle tolerance in degrees.
+     * @param _log_eps Detection threshold: -log10(NFA) &gt; log_eps. Used only when advance refinement
+     * is chosen.
+     *
+     * <b>Note:</b> Implementation has been removed due original code license conflict
+     * @return automatically generated
+     */
+    public static LineSegmentDetector createLineSegmentDetector(int _refine, double _scale, double _sigma_scale, double _quant, double _ang_th, double _log_eps) {
+        return LineSegmentDetector.__fromPtr__(createLineSegmentDetector_2(_refine, _scale, _sigma_scale, _quant, _ang_th, _log_eps));
+    }
+
+    /**
+     * Creates a smart pointer to a LineSegmentDetector object and initializes it.
+     *
+     * The LineSegmentDetector algorithm is defined using the standard values. Only advanced users may want
+     * to edit those, as to tailor it for their own application.
+     *
+     * @param _refine The way found lines will be refined, see #LineSegmentDetectorModes
+     * @param _scale The scale of the image that will be used to find the lines. Range (0..1].
+     * @param _sigma_scale Sigma for Gaussian filter. It is computed as sigma = _sigma_scale/_scale.
+     * @param _quant Bound to the quantization error on the gradient norm.
+     * @param _ang_th Gradient angle tolerance in degrees.
+     * is chosen.
+     *
+     * <b>Note:</b> Implementation has been removed due original code license conflict
+     * @return automatically generated
+     */
+    public static LineSegmentDetector createLineSegmentDetector(int _refine, double _scale, double _sigma_scale, double _quant, double _ang_th) {
+        return LineSegmentDetector.__fromPtr__(createLineSegmentDetector_3(_refine, _scale, _sigma_scale, _quant, _ang_th));
+    }
+
+    /**
+     * Creates a smart pointer to a LineSegmentDetector object and initializes it.
+     *
+     * The LineSegmentDetector algorithm is defined using the standard values. Only advanced users may want
+     * to edit those, as to tailor it for their own application.
+     *
+     * @param _refine The way found lines will be refined, see #LineSegmentDetectorModes
+     * @param _scale The scale of the image that will be used to find the lines. Range (0..1].
+     * @param _sigma_scale Sigma for Gaussian filter. It is computed as sigma = _sigma_scale/_scale.
+     * @param _quant Bound to the quantization error on the gradient norm.
+     * is chosen.
+     *
+     * <b>Note:</b> Implementation has been removed due original code license conflict
+     * @return automatically generated
+     */
+    public static LineSegmentDetector createLineSegmentDetector(int _refine, double _scale, double _sigma_scale, double _quant) {
+        return LineSegmentDetector.__fromPtr__(createLineSegmentDetector_4(_refine, _scale, _sigma_scale, _quant));
+    }
+
+    /**
+     * Creates a smart pointer to a LineSegmentDetector object and initializes it.
+     *
+     * The LineSegmentDetector algorithm is defined using the standard values. Only advanced users may want
+     * to edit those, as to tailor it for their own application.
+     *
+     * @param _refine The way found lines will be refined, see #LineSegmentDetectorModes
+     * @param _scale The scale of the image that will be used to find the lines. Range (0..1].
+     * @param _sigma_scale Sigma for Gaussian filter. It is computed as sigma = _sigma_scale/_scale.
+     * is chosen.
+     *
+     * <b>Note:</b> Implementation has been removed due original code license conflict
+     * @return automatically generated
+     */
+    public static LineSegmentDetector createLineSegmentDetector(int _refine, double _scale, double _sigma_scale) {
+        return LineSegmentDetector.__fromPtr__(createLineSegmentDetector_5(_refine, _scale, _sigma_scale));
+    }
+
+    /**
+     * Creates a smart pointer to a LineSegmentDetector object and initializes it.
+     *
+     * The LineSegmentDetector algorithm is defined using the standard values. Only advanced users may want
+     * to edit those, as to tailor it for their own application.
+     *
+     * @param _refine The way found lines will be refined, see #LineSegmentDetectorModes
+     * @param _scale The scale of the image that will be used to find the lines. Range (0..1].
+     * is chosen.
+     *
+     * <b>Note:</b> Implementation has been removed due original code license conflict
+     * @return automatically generated
+     */
+    public static LineSegmentDetector createLineSegmentDetector(int _refine, double _scale) {
+        return LineSegmentDetector.__fromPtr__(createLineSegmentDetector_6(_refine, _scale));
+    }
+
+    /**
+     * Creates a smart pointer to a LineSegmentDetector object and initializes it.
+     *
+     * The LineSegmentDetector algorithm is defined using the standard values. Only advanced users may want
+     * to edit those, as to tailor it for their own application.
+     *
+     * @param _refine The way found lines will be refined, see #LineSegmentDetectorModes
+     * is chosen.
+     *
+     * <b>Note:</b> Implementation has been removed due original code license conflict
+     * @return automatically generated
+     */
+    public static LineSegmentDetector createLineSegmentDetector(int _refine) {
+        return LineSegmentDetector.__fromPtr__(createLineSegmentDetector_7(_refine));
+    }
+
+    /**
+     * Creates a smart pointer to a LineSegmentDetector object and initializes it.
+     *
+     * The LineSegmentDetector algorithm is defined using the standard values. Only advanced users may want
+     * to edit those, as to tailor it for their own application.
+     *
+     * is chosen.
+     *
+     * <b>Note:</b> Implementation has been removed due original code license conflict
+     * @return automatically generated
+     */
+    public static LineSegmentDetector createLineSegmentDetector() {
+        return LineSegmentDetector.__fromPtr__(createLineSegmentDetector_8());
+    }
+
+
+    //
+    // C++:  Rect cv::boundingRect(Mat array)
+    //
+
+    /**
+     * Calculates the up-right bounding rectangle of a point set or non-zero pixels of gray-scale image.
+     *
+     * The function calculates and returns the minimal up-right bounding rectangle for the specified point set or
+     * non-zero pixels of gray-scale image.
+     *
+     * @param array Input gray-scale image or 2D point set, stored in std::vector or Mat.
+     * @return automatically generated
+     */
+    public static Rect boundingRect(Mat array) {
+        return new Rect(boundingRect_0(array.nativeObj));
+    }
+
+
+    //
+    // C++:  RotatedRect cv::fitEllipse(vector_Point2f points)
+    //
+
+    /**
+     * Fits an ellipse around a set of 2D points.
+     *
+     * The function calculates the ellipse that fits (in a least-squares sense) a set of 2D points best of
+     * all. It returns the rotated rectangle in which the ellipse is inscribed. The first algorithm described by CITE: Fitzgibbon95
+     * is used. Developer should keep in mind that it is possible that the returned
+     * ellipse/rotatedRect data contains negative indices, due to the data points being close to the
+     * border of the containing Mat element.
+     *
+     * @param points Input 2D point set, stored in std::vector&lt;&gt; or Mat
+     * @return automatically generated
+     */
+    public static RotatedRect fitEllipse(MatOfPoint2f points) {
+        Mat points_mat = points;
+        return new RotatedRect(fitEllipse_0(points_mat.nativeObj));
+    }
+
+
+    //
+    // C++:  RotatedRect cv::fitEllipseAMS(Mat points)
+    //
+
+    /**
+     * Fits an ellipse around a set of 2D points.
+     *
+     *  The function calculates the ellipse that fits a set of 2D points.
+     *  It returns the rotated rectangle in which the ellipse is inscribed.
+     *  The Approximate Mean Square (AMS) proposed by CITE: Taubin1991 is used.
+     *
+     *  For an ellipse, this basis set is \( \chi= \left(x^2, x y, y^2, x, y, 1\right) \),
+     *  which is a set of six free coefficients \( A^T=\left\{A_{\text{xx}},A_{\text{xy}},A_{\text{yy}},A_x,A_y,A_0\right\} \).
+     *  However, to specify an ellipse, all that is needed is five numbers; the major and minor axes lengths \( (a,b) \),
+     *  the position \( (x_0,y_0) \), and the orientation \( \theta \). This is because the basis set includes lines,
+     *  quadratics, parabolic and hyperbolic functions as well as elliptical functions as possible fits.
+     *  If the fit is found to be a parabolic or hyperbolic function then the standard #fitEllipse method is used.
+     *  The AMS method restricts the fit to parabolic, hyperbolic and elliptical curves
+     *  by imposing the condition that \( A^T ( D_x^T D_x  +   D_y^T D_y) A = 1 \) where
+     *  the matrices \( Dx \) and \( Dy \) are the partial derivatives of the design matrix \( D \) with
+     *  respect to x and y. The matrices are formed row by row applying the following to
+     *  each of the points in the set:
+     *  \(align*}{
+     *  D(i,:)&amp;=\left\{x_i^2, x_i y_i, y_i^2, x_i, y_i, 1\right\} &amp;
+     *  D_x(i,:)&amp;=\left\{2 x_i,y_i,0,1,0,0\right\} &amp;
+     *  D_y(i,:)&amp;=\left\{0,x_i,2 y_i,0,1,0\right\}
+     *  \)
+     *  The AMS method minimizes the cost function
+     *  \(equation*}{
+     *  \epsilon ^2=\frac{ A^T D^T D A }{ A^T (D_x^T D_x +  D_y^T D_y) A^T }
+     *  \)
+     *
+     *  The minimum cost is found by solving the generalized eigenvalue problem.
+     *
+     *  \(equation*}{
+     *  D^T D A = \lambda  \left( D_x^T D_x +  D_y^T D_y\right) A
+     *  \)
+     *
+     *  @param points Input 2D point set, stored in std::vector&lt;&gt; or Mat
+     * @return automatically generated
+     */
+    public static RotatedRect fitEllipseAMS(Mat points) {
+        return new RotatedRect(fitEllipseAMS_0(points.nativeObj));
+    }
+
+
+    //
+    // C++:  RotatedRect cv::fitEllipseDirect(Mat points)
+    //
+
+    /**
+     * Fits an ellipse around a set of 2D points.
+     *
+     *  The function calculates the ellipse that fits a set of 2D points.
+     *  It returns the rotated rectangle in which the ellipse is inscribed.
+     *  The Direct least square (Direct) method by CITE: Fitzgibbon1999 is used.
+     *
+     *  For an ellipse, this basis set is \( \chi= \left(x^2, x y, y^2, x, y, 1\right) \),
+     *  which is a set of six free coefficients \( A^T=\left\{A_{\text{xx}},A_{\text{xy}},A_{\text{yy}},A_x,A_y,A_0\right\} \).
+     *  However, to specify an ellipse, all that is needed is five numbers; the major and minor axes lengths \( (a,b) \),
+     *  the position \( (x_0,y_0) \), and the orientation \( \theta \). This is because the basis set includes lines,
+     *  quadratics, parabolic and hyperbolic functions as well as elliptical functions as possible fits.
+     *  The Direct method confines the fit to ellipses by ensuring that \( 4 A_{xx} A_{yy}- A_{xy}^2 &gt; 0 \).
+     *  The condition imposed is that \( 4 A_{xx} A_{yy}- A_{xy}^2=1 \) which satisfies the inequality
+     *  and as the coefficients can be arbitrarily scaled is not overly restrictive.
+     *
+     *  \(equation*}{
+     *  \epsilon ^2= A^T D^T D A \quad \text{with} \quad A^T C A =1 \quad \text{and} \quad C=\left(\begin{matrix}
+     *  0 &amp; 0  &amp; 2  &amp; 0  &amp; 0  &amp;  0  \\
+     *  0 &amp; -1  &amp; 0  &amp; 0  &amp; 0  &amp;  0 \\
+     *  2 &amp; 0  &amp; 0  &amp; 0  &amp; 0  &amp;  0 \\
+     *  0 &amp; 0  &amp; 0  &amp; 0  &amp; 0  &amp;  0 \\
+     *  0 &amp; 0  &amp; 0  &amp; 0  &amp; 0  &amp;  0 \\
+     *  0 &amp; 0  &amp; 0  &amp; 0  &amp; 0  &amp;  0
+     *  \end{matrix} \right)
+     *  \)
+     *
+     *  The minimum cost is found by solving the generalized eigenvalue problem.
+     *
+     *  \(equation*}{
+     *  D^T D A = \lambda  \left( C\right) A
+     *  \)
+     *
+     *  The system produces only one positive eigenvalue \( \lambda\) which is chosen as the solution
+     *  with its eigenvector \(\mathbf{u}\). These are used to find the coefficients
+     *
+     *  \(equation*}{
+     *  A = \sqrt{\frac{1}{\mathbf{u}^T C \mathbf{u}}}  \mathbf{u}
+     *  \)
+     *  The scaling factor guarantees that  \(A^T C A =1\).
+     *
+     *  @param points Input 2D point set, stored in std::vector&lt;&gt; or Mat
+     * @return automatically generated
+     */
+    public static RotatedRect fitEllipseDirect(Mat points) {
+        return new RotatedRect(fitEllipseDirect_0(points.nativeObj));
+    }
+
+
+    //
+    // C++:  RotatedRect cv::minAreaRect(vector_Point2f points)
+    //
+
+    /**
+     * Finds a rotated rectangle of the minimum area enclosing the input 2D point set.
+     *
+     * The function calculates and returns the minimum-area bounding rectangle (possibly rotated) for a
+     * specified point set. Developer should keep in mind that the returned RotatedRect can contain negative
+     * indices when data is close to the containing Mat element boundary.
+     *
+     * @param points Input vector of 2D points, stored in std::vector&lt;&gt; or Mat
+     * @return automatically generated
+     */
+    public static RotatedRect minAreaRect(MatOfPoint2f points) {
+        Mat points_mat = points;
+        return new RotatedRect(minAreaRect_0(points_mat.nativeObj));
+    }
+
+
+    //
+    // C++:  bool cv::clipLine(Rect imgRect, Point& pt1, Point& pt2)
+    //
+
+    /**
+     *
+     * @param imgRect Image rectangle.
+     * @param pt1 First line point.
+     * @param pt2 Second line point.
+     * @return automatically generated
+     */
+    public static boolean clipLine(Rect imgRect, Point pt1, Point pt2) {
+        double[] pt1_out = new double[2];
+        double[] pt2_out = new double[2];
+        boolean retVal = clipLine_0(imgRect.x, imgRect.y, imgRect.width, imgRect.height, pt1.x, pt1.y, pt1_out, pt2.x, pt2.y, pt2_out);
+        if(pt1!=null){ pt1.x = pt1_out[0]; pt1.y = pt1_out[1]; } 
+        if(pt2!=null){ pt2.x = pt2_out[0]; pt2.y = pt2_out[1]; } 
+        return retVal;
+    }
+
+
+    //
+    // C++:  bool cv::isContourConvex(vector_Point contour)
+    //
+
+    /**
+     * Tests a contour convexity.
+     *
+     * The function tests whether the input contour is convex or not. The contour must be simple, that is,
+     * without self-intersections. Otherwise, the function output is undefined.
+     *
+     * @param contour Input vector of 2D points, stored in std::vector&lt;&gt; or Mat
+     * @return automatically generated
+     */
+    public static boolean isContourConvex(MatOfPoint contour) {
+        Mat contour_mat = contour;
+        return isContourConvex_0(contour_mat.nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::arcLength(vector_Point2f curve, bool closed)
+    //
+
+    /**
+     * Calculates a contour perimeter or a curve length.
+     *
+     * The function computes a curve length or a closed contour perimeter.
+     *
+     * @param curve Input vector of 2D points, stored in std::vector or Mat.
+     * @param closed Flag indicating whether the curve is closed or not.
+     * @return automatically generated
+     */
+    public static double arcLength(MatOfPoint2f curve, boolean closed) {
+        Mat curve_mat = curve;
+        return arcLength_0(curve_mat.nativeObj, closed);
+    }
+
+
+    //
+    // C++:  double cv::compareHist(Mat H1, Mat H2, int method)
+    //
+
+    /**
+     * Compares two histograms.
+     *
+     * The function cv::compareHist compares two dense or two sparse histograms using the specified method.
+     *
+     * The function returns \(d(H_1, H_2)\) .
+     *
+     * While the function works well with 1-, 2-, 3-dimensional dense histograms, it may not be suitable
+     * for high-dimensional sparse histograms. In such histograms, because of aliasing and sampling
+     * problems, the coordinates of non-zero histogram bins can slightly shift. To compare such histograms
+     * or more general sparse configurations of weighted points, consider using the #EMD function.
+     *
+     * @param H1 First compared histogram.
+     * @param H2 Second compared histogram of the same size as H1 .
+     * @param method Comparison method, see #HistCompMethods
+     * @return automatically generated
+     */
+    public static double compareHist(Mat H1, Mat H2, int method) {
+        return compareHist_0(H1.nativeObj, H2.nativeObj, method);
+    }
+
+
+    //
+    // C++:  double cv::contourArea(Mat contour, bool oriented = false)
+    //
+
+    /**
+     * Calculates a contour area.
+     *
+     * The function computes a contour area. Similarly to moments , the area is computed using the Green
+     * formula. Thus, the returned area and the number of non-zero pixels, if you draw the contour using
+     * #drawContours or #fillPoly , can be different. Also, the function will most certainly give a wrong
+     * results for contours with self-intersections.
+     *
+     * Example:
+     * <code>
+     *     vector&lt;Point&gt; contour;
+     *     contour.push_back(Point2f(0, 0));
+     *     contour.push_back(Point2f(10, 0));
+     *     contour.push_back(Point2f(10, 10));
+     *     contour.push_back(Point2f(5, 4));
+     *
+     *     double area0 = contourArea(contour);
+     *     vector&lt;Point&gt; approx;
+     *     approxPolyDP(contour, approx, 5, true);
+     *     double area1 = contourArea(approx);
+     *
+     *     cout &lt;&lt; "area0 =" &lt;&lt; area0 &lt;&lt; endl &lt;&lt;
+     *             "area1 =" &lt;&lt; area1 &lt;&lt; endl &lt;&lt;
+     *             "approx poly vertices" &lt;&lt; approx.size() &lt;&lt; endl;
+     * </code>
+     * @param contour Input vector of 2D points (contour vertices), stored in std::vector or Mat.
+     * @param oriented Oriented area flag. If it is true, the function returns a signed area value,
+     * depending on the contour orientation (clockwise or counter-clockwise). Using this feature you can
+     * determine orientation of a contour by taking the sign of an area. By default, the parameter is
+     * false, which means that the absolute value is returned.
+     * @return automatically generated
+     */
+    public static double contourArea(Mat contour, boolean oriented) {
+        return contourArea_0(contour.nativeObj, oriented);
+    }
+
+    /**
+     * Calculates a contour area.
+     *
+     * The function computes a contour area. Similarly to moments , the area is computed using the Green
+     * formula. Thus, the returned area and the number of non-zero pixels, if you draw the contour using
+     * #drawContours or #fillPoly , can be different. Also, the function will most certainly give a wrong
+     * results for contours with self-intersections.
+     *
+     * Example:
+     * <code>
+     *     vector&lt;Point&gt; contour;
+     *     contour.push_back(Point2f(0, 0));
+     *     contour.push_back(Point2f(10, 0));
+     *     contour.push_back(Point2f(10, 10));
+     *     contour.push_back(Point2f(5, 4));
+     *
+     *     double area0 = contourArea(contour);
+     *     vector&lt;Point&gt; approx;
+     *     approxPolyDP(contour, approx, 5, true);
+     *     double area1 = contourArea(approx);
+     *
+     *     cout &lt;&lt; "area0 =" &lt;&lt; area0 &lt;&lt; endl &lt;&lt;
+     *             "area1 =" &lt;&lt; area1 &lt;&lt; endl &lt;&lt;
+     *             "approx poly vertices" &lt;&lt; approx.size() &lt;&lt; endl;
+     * </code>
+     * @param contour Input vector of 2D points (contour vertices), stored in std::vector or Mat.
+     * depending on the contour orientation (clockwise or counter-clockwise). Using this feature you can
+     * determine orientation of a contour by taking the sign of an area. By default, the parameter is
+     * false, which means that the absolute value is returned.
+     * @return automatically generated
+     */
+    public static double contourArea(Mat contour) {
+        return contourArea_1(contour.nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::getFontScaleFromHeight(int fontFace, int pixelHeight, int thickness = 1)
+    //
+
+    /**
+     * Calculates the font-specific size to use to achieve a given height in pixels.
+     *
+     * @param fontFace Font to use, see cv::HersheyFonts.
+     * @param pixelHeight Pixel height to compute the fontScale for
+     * @param thickness Thickness of lines used to render the text.See putText for details.
+     * @return The fontSize to use for cv::putText
+     *
+     * SEE: cv::putText
+     */
+    public static double getFontScaleFromHeight(int fontFace, int pixelHeight, int thickness) {
+        return getFontScaleFromHeight_0(fontFace, pixelHeight, thickness);
+    }
+
+    /**
+     * Calculates the font-specific size to use to achieve a given height in pixels.
+     *
+     * @param fontFace Font to use, see cv::HersheyFonts.
+     * @param pixelHeight Pixel height to compute the fontScale for
+     * @return The fontSize to use for cv::putText
+     *
+     * SEE: cv::putText
+     */
+    public static double getFontScaleFromHeight(int fontFace, int pixelHeight) {
+        return getFontScaleFromHeight_1(fontFace, pixelHeight);
+    }
+
+
+    //
+    // C++:  double cv::matchShapes(Mat contour1, Mat contour2, int method, double parameter)
+    //
+
+    /**
+     * Compares two shapes.
+     *
+     * The function compares two shapes. All three implemented methods use the Hu invariants (see #HuMoments)
+     *
+     * @param contour1 First contour or grayscale image.
+     * @param contour2 Second contour or grayscale image.
+     * @param method Comparison method, see #ShapeMatchModes
+     * @param parameter Method-specific parameter (not supported now).
+     * @return automatically generated
+     */
+    public static double matchShapes(Mat contour1, Mat contour2, int method, double parameter) {
+        return matchShapes_0(contour1.nativeObj, contour2.nativeObj, method, parameter);
+    }
+
+
+    //
+    // C++:  double cv::minEnclosingTriangle(Mat points, Mat& triangle)
+    //
+
+    /**
+     * Finds a triangle of minimum area enclosing a 2D point set and returns its area.
+     *
+     * The function finds a triangle of minimum area enclosing the given set of 2D points and returns its
+     * area. The output for a given 2D point set is shown in the image below. 2D points are depicted in
+     * red* and the enclosing triangle in *yellow*.
+     *
+     * ![Sample output of the minimum enclosing triangle function](pics/minenclosingtriangle.png)
+     *
+     * The implementation of the algorithm is based on O'Rourke's CITE: ORourke86 and Klee and Laskowski's
+     * CITE: KleeLaskowski85 papers. O'Rourke provides a \(\theta(n)\) algorithm for finding the minimal
+     * enclosing triangle of a 2D convex polygon with n vertices. Since the #minEnclosingTriangle function
+     * takes a 2D point set as input an additional preprocessing step of computing the convex hull of the
+     * 2D point set is required. The complexity of the #convexHull function is \(O(n log(n))\) which is higher
+     * than \(\theta(n)\). Thus the overall complexity of the function is \(O(n log(n))\).
+     *
+     * @param points Input vector of 2D points with depth CV_32S or CV_32F, stored in std::vector&lt;&gt; or Mat
+     * @param triangle Output vector of three 2D points defining the vertices of the triangle. The depth
+     * of the OutputArray must be CV_32F.
+     * @return automatically generated
+     */
+    public static double minEnclosingTriangle(Mat points, Mat triangle) {
+        return minEnclosingTriangle_0(points.nativeObj, triangle.nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::pointPolygonTest(vector_Point2f contour, Point2f pt, bool measureDist)
+    //
+
+    /**
+     * Performs a point-in-contour test.
+     *
+     * The function determines whether the point is inside a contour, outside, or lies on an edge (or
+     * coincides with a vertex). It returns positive (inside), negative (outside), or zero (on an edge)
+     * value, correspondingly. When measureDist=false , the return value is +1, -1, and 0, respectively.
+     * Otherwise, the return value is a signed distance between the point and the nearest contour edge.
+     *
+     * See below a sample output of the function where each image pixel is tested against the contour:
+     *
+     * ![sample output](pics/pointpolygon.png)
+     *
+     * @param contour Input contour.
+     * @param pt Point tested against the contour.
+     * @param measureDist If true, the function estimates the signed distance from the point to the
+     * nearest contour edge. Otherwise, the function only checks if the point is inside a contour or not.
+     * @return automatically generated
+     */
+    public static double pointPolygonTest(MatOfPoint2f contour, Point pt, boolean measureDist) {
+        Mat contour_mat = contour;
+        return pointPolygonTest_0(contour_mat.nativeObj, pt.x, pt.y, measureDist);
+    }
+
+
+    //
+    // C++:  double cv::threshold(Mat src, Mat& dst, double thresh, double maxval, int type)
+    //
+
+    /**
+     * Applies a fixed-level threshold to each array element.
+     *
+     * The function applies fixed-level thresholding to a multiple-channel array. The function is typically
+     * used to get a bi-level (binary) image out of a grayscale image ( #compare could be also used for
+     * this purpose) or for removing a noise, that is, filtering out pixels with too small or too large
+     * values. There are several types of thresholding supported by the function. They are determined by
+     * type parameter.
+     *
+     * Also, the special values #THRESH_OTSU or #THRESH_TRIANGLE may be combined with one of the
+     * above values. In these cases, the function determines the optimal threshold value using the Otsu's
+     * or Triangle algorithm and uses it instead of the specified thresh.
+     *
+     * <b>Note:</b> Currently, the Otsu's and Triangle methods are implemented only for 8-bit single-channel images.
+     *
+     * @param src input array (multiple-channel, 8-bit or 32-bit floating point).
+     * @param dst output array of the same size  and type and the same number of channels as src.
+     * @param thresh threshold value.
+     * @param maxval maximum value to use with the #THRESH_BINARY and #THRESH_BINARY_INV thresholding
+     * types.
+     * @param type thresholding type (see #ThresholdTypes).
+     * @return the computed threshold value if Otsu's or Triangle methods used.
+     *
+     * SEE:  adaptiveThreshold, findContours, compare, min, max
+     */
+    public static double threshold(Mat src, Mat dst, double thresh, double maxval, int type) {
+        return threshold_0(src.nativeObj, dst.nativeObj, thresh, maxval, type);
+    }
+
+
+    //
+    // C++:  float cv::initWideAngleProjMap(Mat cameraMatrix, Mat distCoeffs, Size imageSize, int destImageWidth, int m1type, Mat& map1, Mat& map2, int projType = PROJ_SPHERICAL_EQRECT, double alpha = 0)
+    //
+
+    public static float initWideAngleProjMap(Mat cameraMatrix, Mat distCoeffs, Size imageSize, int destImageWidth, int m1type, Mat map1, Mat map2, int projType, double alpha) {
+        return initWideAngleProjMap_0(cameraMatrix.nativeObj, distCoeffs.nativeObj, imageSize.width, imageSize.height, destImageWidth, m1type, map1.nativeObj, map2.nativeObj, projType, alpha);
+    }
+
+    public static float initWideAngleProjMap(Mat cameraMatrix, Mat distCoeffs, Size imageSize, int destImageWidth, int m1type, Mat map1, Mat map2, int projType) {
+        return initWideAngleProjMap_1(cameraMatrix.nativeObj, distCoeffs.nativeObj, imageSize.width, imageSize.height, destImageWidth, m1type, map1.nativeObj, map2.nativeObj, projType);
+    }
+
+    public static float initWideAngleProjMap(Mat cameraMatrix, Mat distCoeffs, Size imageSize, int destImageWidth, int m1type, Mat map1, Mat map2) {
+        return initWideAngleProjMap_2(cameraMatrix.nativeObj, distCoeffs.nativeObj, imageSize.width, imageSize.height, destImageWidth, m1type, map1.nativeObj, map2.nativeObj);
+    }
+
+
+    //
+    // C++:  float cv::intersectConvexConvex(Mat _p1, Mat _p2, Mat& _p12, bool handleNested = true)
+    //
+
+    /**
+     * Finds intersection of two convex polygons
+     *
+     * @param _p1 First polygon
+     * @param _p2 Second polygon
+     * @param _p12 Output polygon describing the intersecting area
+     * @param handleNested When true, an intersection is found if one of the polygons is fully enclosed in the other.
+     * When false, no intersection is found. If the polygons share a side or the vertex of one polygon lies on an edge
+     * of the other, they are not considered nested and an intersection will be found regardless of the value of handleNested.
+     *
+     * @return Absolute value of area of intersecting polygon
+     *
+     * <b>Note:</b> intersectConvexConvex doesn't confirm that both polygons are convex and will return invalid results if they aren't.
+     */
+    public static float intersectConvexConvex(Mat _p1, Mat _p2, Mat _p12, boolean handleNested) {
+        return intersectConvexConvex_0(_p1.nativeObj, _p2.nativeObj, _p12.nativeObj, handleNested);
+    }
+
+    /**
+     * Finds intersection of two convex polygons
+     *
+     * @param _p1 First polygon
+     * @param _p2 Second polygon
+     * @param _p12 Output polygon describing the intersecting area
+     * When false, no intersection is found. If the polygons share a side or the vertex of one polygon lies on an edge
+     * of the other, they are not considered nested and an intersection will be found regardless of the value of handleNested.
+     *
+     * @return Absolute value of area of intersecting polygon
+     *
+     * <b>Note:</b> intersectConvexConvex doesn't confirm that both polygons are convex and will return invalid results if they aren't.
+     */
+    public static float intersectConvexConvex(Mat _p1, Mat _p2, Mat _p12) {
+        return intersectConvexConvex_1(_p1.nativeObj, _p2.nativeObj, _p12.nativeObj);
+    }
+
+
+    //
+    // C++:  float cv::wrapperEMD(Mat signature1, Mat signature2, int distType, Mat cost = Mat(), Ptr_float& lowerBound = Ptr<float>(), Mat& flow = Mat())
+    //
+
+    /**
+     * Computes the "minimal work" distance between two weighted point configurations.
+     *
+     * The function computes the earth mover distance and/or a lower boundary of the distance between the
+     * two weighted point configurations. One of the applications described in CITE: RubnerSept98,
+     * CITE: Rubner2000 is multi-dimensional histogram comparison for image retrieval. EMD is a transportation
+     * problem that is solved using some modification of a simplex algorithm, thus the complexity is
+     * exponential in the worst case, though, on average it is much faster. In the case of a real metric
+     * the lower boundary can be calculated even faster (using linear-time algorithm) and it can be used
+     * to determine roughly whether the two signatures are far enough so that they cannot relate to the
+     * same object.
+     *
+     * @param signature1 First signature, a \(\texttt{size1}\times \texttt{dims}+1\) floating-point matrix.
+     * Each row stores the point weight followed by the point coordinates. The matrix is allowed to have
+     * a single column (weights only) if the user-defined cost matrix is used. The weights must be
+     * non-negative and have at least one non-zero value.
+     * @param signature2 Second signature of the same format as signature1 , though the number of rows
+     * may be different. The total weights may be different. In this case an extra "dummy" point is added
+     * to either signature1 or signature2. The weights must be non-negative and have at least one non-zero
+     * value.
+     * @param distType Used metric. See #DistanceTypes.
+     * @param cost User-defined \(\texttt{size1}\times \texttt{size2}\) cost matrix. Also, if a cost matrix
+     * is used, lower boundary lowerBound cannot be calculated because it needs a metric function.
+     * signatures that is a distance between mass centers. The lower boundary may not be calculated if
+     * the user-defined cost matrix is used, the total weights of point configurations are not equal, or
+     * if the signatures consist of weights only (the signature matrices have a single column). You
+     * <b>must</b> initialize \*lowerBound . If the calculated distance between mass centers is greater or
+     * equal to \*lowerBound (it means that the signatures are far enough), the function does not
+     * calculate EMD. In any case \*lowerBound is set to the calculated distance between mass centers on
+     * return. Thus, if you want to calculate both distance between mass centers and EMD, \*lowerBound
+     * should be set to 0.
+     * @param flow Resultant \(\texttt{size1} \times \texttt{size2}\) flow matrix: \(\texttt{flow}_{i,j}\) is
+     * a flow from \(i\) -th point of signature1 to \(j\) -th point of signature2 .
+     * @return automatically generated
+     */
+    public static float EMD(Mat signature1, Mat signature2, int distType, Mat cost, Mat flow) {
+        return EMD_0(signature1.nativeObj, signature2.nativeObj, distType, cost.nativeObj, flow.nativeObj);
+    }
+
+    /**
+     * Computes the "minimal work" distance between two weighted point configurations.
+     *
+     * The function computes the earth mover distance and/or a lower boundary of the distance between the
+     * two weighted point configurations. One of the applications described in CITE: RubnerSept98,
+     * CITE: Rubner2000 is multi-dimensional histogram comparison for image retrieval. EMD is a transportation
+     * problem that is solved using some modification of a simplex algorithm, thus the complexity is
+     * exponential in the worst case, though, on average it is much faster. In the case of a real metric
+     * the lower boundary can be calculated even faster (using linear-time algorithm) and it can be used
+     * to determine roughly whether the two signatures are far enough so that they cannot relate to the
+     * same object.
+     *
+     * @param signature1 First signature, a \(\texttt{size1}\times \texttt{dims}+1\) floating-point matrix.
+     * Each row stores the point weight followed by the point coordinates. The matrix is allowed to have
+     * a single column (weights only) if the user-defined cost matrix is used. The weights must be
+     * non-negative and have at least one non-zero value.
+     * @param signature2 Second signature of the same format as signature1 , though the number of rows
+     * may be different. The total weights may be different. In this case an extra "dummy" point is added
+     * to either signature1 or signature2. The weights must be non-negative and have at least one non-zero
+     * value.
+     * @param distType Used metric. See #DistanceTypes.
+     * @param cost User-defined \(\texttt{size1}\times \texttt{size2}\) cost matrix. Also, if a cost matrix
+     * is used, lower boundary lowerBound cannot be calculated because it needs a metric function.
+     * signatures that is a distance between mass centers. The lower boundary may not be calculated if
+     * the user-defined cost matrix is used, the total weights of point configurations are not equal, or
+     * if the signatures consist of weights only (the signature matrices have a single column). You
+     * <b>must</b> initialize \*lowerBound . If the calculated distance between mass centers is greater or
+     * equal to \*lowerBound (it means that the signatures are far enough), the function does not
+     * calculate EMD. In any case \*lowerBound is set to the calculated distance between mass centers on
+     * return. Thus, if you want to calculate both distance between mass centers and EMD, \*lowerBound
+     * should be set to 0.
+     * a flow from \(i\) -th point of signature1 to \(j\) -th point of signature2 .
+     * @return automatically generated
+     */
+    public static float EMD(Mat signature1, Mat signature2, int distType, Mat cost) {
+        return EMD_1(signature1.nativeObj, signature2.nativeObj, distType, cost.nativeObj);
+    }
+
+    /**
+     * Computes the "minimal work" distance between two weighted point configurations.
+     *
+     * The function computes the earth mover distance and/or a lower boundary of the distance between the
+     * two weighted point configurations. One of the applications described in CITE: RubnerSept98,
+     * CITE: Rubner2000 is multi-dimensional histogram comparison for image retrieval. EMD is a transportation
+     * problem that is solved using some modification of a simplex algorithm, thus the complexity is
+     * exponential in the worst case, though, on average it is much faster. In the case of a real metric
+     * the lower boundary can be calculated even faster (using linear-time algorithm) and it can be used
+     * to determine roughly whether the two signatures are far enough so that they cannot relate to the
+     * same object.
+     *
+     * @param signature1 First signature, a \(\texttt{size1}\times \texttt{dims}+1\) floating-point matrix.
+     * Each row stores the point weight followed by the point coordinates. The matrix is allowed to have
+     * a single column (weights only) if the user-defined cost matrix is used. The weights must be
+     * non-negative and have at least one non-zero value.
+     * @param signature2 Second signature of the same format as signature1 , though the number of rows
+     * may be different. The total weights may be different. In this case an extra "dummy" point is added
+     * to either signature1 or signature2. The weights must be non-negative and have at least one non-zero
+     * value.
+     * @param distType Used metric. See #DistanceTypes.
+     * is used, lower boundary lowerBound cannot be calculated because it needs a metric function.
+     * signatures that is a distance between mass centers. The lower boundary may not be calculated if
+     * the user-defined cost matrix is used, the total weights of point configurations are not equal, or
+     * if the signatures consist of weights only (the signature matrices have a single column). You
+     * <b>must</b> initialize \*lowerBound . If the calculated distance between mass centers is greater or
+     * equal to \*lowerBound (it means that the signatures are far enough), the function does not
+     * calculate EMD. In any case \*lowerBound is set to the calculated distance between mass centers on
+     * return. Thus, if you want to calculate both distance between mass centers and EMD, \*lowerBound
+     * should be set to 0.
+     * a flow from \(i\) -th point of signature1 to \(j\) -th point of signature2 .
+     * @return automatically generated
+     */
+    public static float EMD(Mat signature1, Mat signature2, int distType) {
+        return EMD_3(signature1.nativeObj, signature2.nativeObj, distType);
+    }
+
+
+    //
+    // C++:  int cv::connectedComponents(Mat image, Mat& labels, int connectivity, int ltype, int ccltype)
+    //
+
+    /**
+     * computes the connected components labeled image of boolean image
+     *
+     * image with 4 or 8 way connectivity - returns N, the total number of labels [0, N-1] where 0
+     * represents the background label. ltype specifies the output label image type, an important
+     * consideration based on the total number of labels or alternatively the total number of pixels in
+     * the source image. ccltype specifies the connected components labeling algorithm to use, currently
+     * Grana (BBDT) and Wu's (SAUF) algorithms are supported, see the #ConnectedComponentsAlgorithmsTypes
+     * for details. Note that SAUF algorithm forces a row major ordering of labels while BBDT does not.
+     * This function uses parallel version of both Grana and Wu's algorithms if at least one allowed
+     * parallel framework is enabled and if the rows of the image are at least twice the number returned by #getNumberOfCPUs.
+     *
+     * @param image the 8-bit single-channel image to be labeled
+     * @param labels destination labeled image
+     * @param connectivity 8 or 4 for 8-way or 4-way connectivity respectively
+     * @param ltype output image label type. Currently CV_32S and CV_16U are supported.
+     * @param ccltype connected components algorithm type (see the #ConnectedComponentsAlgorithmsTypes).
+     * @return automatically generated
+     */
+    public static int connectedComponentsWithAlgorithm(Mat image, Mat labels, int connectivity, int ltype, int ccltype) {
+        return connectedComponentsWithAlgorithm_0(image.nativeObj, labels.nativeObj, connectivity, ltype, ccltype);
+    }
+
+
+    //
+    // C++:  int cv::connectedComponents(Mat image, Mat& labels, int connectivity = 8, int ltype = CV_32S)
+    //
+
+    /**
+     *
+     *
+     * @param image the 8-bit single-channel image to be labeled
+     * @param labels destination labeled image
+     * @param connectivity 8 or 4 for 8-way or 4-way connectivity respectively
+     * @param ltype output image label type. Currently CV_32S and CV_16U are supported.
+     * @return automatically generated
+     */
+    public static int connectedComponents(Mat image, Mat labels, int connectivity, int ltype) {
+        return connectedComponents_0(image.nativeObj, labels.nativeObj, connectivity, ltype);
+    }
+
+    /**
+     *
+     *
+     * @param image the 8-bit single-channel image to be labeled
+     * @param labels destination labeled image
+     * @param connectivity 8 or 4 for 8-way or 4-way connectivity respectively
+     * @return automatically generated
+     */
+    public static int connectedComponents(Mat image, Mat labels, int connectivity) {
+        return connectedComponents_1(image.nativeObj, labels.nativeObj, connectivity);
+    }
+
+    /**
+     *
+     *
+     * @param image the 8-bit single-channel image to be labeled
+     * @param labels destination labeled image
+     * @return automatically generated
+     */
+    public static int connectedComponents(Mat image, Mat labels) {
+        return connectedComponents_2(image.nativeObj, labels.nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::connectedComponentsWithStats(Mat image, Mat& labels, Mat& stats, Mat& centroids, int connectivity, int ltype, int ccltype)
+    //
+
+    /**
+     * computes the connected components labeled image of boolean image and also produces a statistics output for each label
+     *
+     * image with 4 or 8 way connectivity - returns N, the total number of labels [0, N-1] where 0
+     * represents the background label. ltype specifies the output label image type, an important
+     * consideration based on the total number of labels or alternatively the total number of pixels in
+     * the source image. ccltype specifies the connected components labeling algorithm to use, currently
+     * Grana's (BBDT) and Wu's (SAUF) algorithms are supported, see the #ConnectedComponentsAlgorithmsTypes
+     * for details. Note that SAUF algorithm forces a row major ordering of labels while BBDT does not.
+     * This function uses parallel version of both Grana and Wu's algorithms (statistics included) if at least one allowed
+     * parallel framework is enabled and if the rows of the image are at least twice the number returned by #getNumberOfCPUs.
+     *
+     * @param image the 8-bit single-channel image to be labeled
+     * @param labels destination labeled image
+     * @param stats statistics output for each label, including the background label.
+     * Statistics are accessed via stats(label, COLUMN) where COLUMN is one of
+     * #ConnectedComponentsTypes, selecting the statistic. The data type is CV_32S.
+     * @param centroids centroid output for each label, including the background label. Centroids are
+     * accessed via centroids(label, 0) for x and centroids(label, 1) for y. The data type CV_64F.
+     * @param connectivity 8 or 4 for 8-way or 4-way connectivity respectively
+     * @param ltype output image label type. Currently CV_32S and CV_16U are supported.
+     * @param ccltype connected components algorithm type (see #ConnectedComponentsAlgorithmsTypes).
+     * @return automatically generated
+     */
+    public static int connectedComponentsWithStatsWithAlgorithm(Mat image, Mat labels, Mat stats, Mat centroids, int connectivity, int ltype, int ccltype) {
+        return connectedComponentsWithStatsWithAlgorithm_0(image.nativeObj, labels.nativeObj, stats.nativeObj, centroids.nativeObj, connectivity, ltype, ccltype);
+    }
+
+
+    //
+    // C++:  int cv::connectedComponentsWithStats(Mat image, Mat& labels, Mat& stats, Mat& centroids, int connectivity = 8, int ltype = CV_32S)
+    //
+
+    /**
+     *
+     * @param image the 8-bit single-channel image to be labeled
+     * @param labels destination labeled image
+     * @param stats statistics output for each label, including the background label.
+     * Statistics are accessed via stats(label, COLUMN) where COLUMN is one of
+     * #ConnectedComponentsTypes, selecting the statistic. The data type is CV_32S.
+     * @param centroids centroid output for each label, including the background label. Centroids are
+     * accessed via centroids(label, 0) for x and centroids(label, 1) for y. The data type CV_64F.
+     * @param connectivity 8 or 4 for 8-way or 4-way connectivity respectively
+     * @param ltype output image label type. Currently CV_32S and CV_16U are supported.
+     * @return automatically generated
+     */
+    public static int connectedComponentsWithStats(Mat image, Mat labels, Mat stats, Mat centroids, int connectivity, int ltype) {
+        return connectedComponentsWithStats_0(image.nativeObj, labels.nativeObj, stats.nativeObj, centroids.nativeObj, connectivity, ltype);
+    }
+
+    /**
+     *
+     * @param image the 8-bit single-channel image to be labeled
+     * @param labels destination labeled image
+     * @param stats statistics output for each label, including the background label.
+     * Statistics are accessed via stats(label, COLUMN) where COLUMN is one of
+     * #ConnectedComponentsTypes, selecting the statistic. The data type is CV_32S.
+     * @param centroids centroid output for each label, including the background label. Centroids are
+     * accessed via centroids(label, 0) for x and centroids(label, 1) for y. The data type CV_64F.
+     * @param connectivity 8 or 4 for 8-way or 4-way connectivity respectively
+     * @return automatically generated
+     */
+    public static int connectedComponentsWithStats(Mat image, Mat labels, Mat stats, Mat centroids, int connectivity) {
+        return connectedComponentsWithStats_1(image.nativeObj, labels.nativeObj, stats.nativeObj, centroids.nativeObj, connectivity);
+    }
+
+    /**
+     *
+     * @param image the 8-bit single-channel image to be labeled
+     * @param labels destination labeled image
+     * @param stats statistics output for each label, including the background label.
+     * Statistics are accessed via stats(label, COLUMN) where COLUMN is one of
+     * #ConnectedComponentsTypes, selecting the statistic. The data type is CV_32S.
+     * @param centroids centroid output for each label, including the background label. Centroids are
+     * accessed via centroids(label, 0) for x and centroids(label, 1) for y. The data type CV_64F.
+     * @return automatically generated
+     */
+    public static int connectedComponentsWithStats(Mat image, Mat labels, Mat stats, Mat centroids) {
+        return connectedComponentsWithStats_2(image.nativeObj, labels.nativeObj, stats.nativeObj, centroids.nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::floodFill(Mat& image, Mat& mask, Point seedPoint, Scalar newVal, Rect* rect = 0, Scalar loDiff = Scalar(), Scalar upDiff = Scalar(), int flags = 4)
+    //
+
+    /**
+     * Fills a connected component with the given color.
+     *
+     * The function cv::floodFill fills a connected component starting from the seed point with the specified
+     * color. The connectivity is determined by the color/brightness closeness of the neighbor pixels. The
+     * pixel at \((x,y)\) is considered to belong to the repainted domain if:
+     *
+     * <ul>
+     *   <li>
+     *  in case of a grayscale image and floating range
+     * \(\texttt{src} (x',y')- \texttt{loDiff} \leq \texttt{src} (x,y)  \leq \texttt{src} (x',y')+ \texttt{upDiff}\)
+     *   </li>
+     * </ul>
+     *
+     *
+     * <ul>
+     *   <li>
+     *  in case of a grayscale image and fixed range
+     * \(\texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)- \texttt{loDiff} \leq \texttt{src} (x,y)  \leq \texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)+ \texttt{upDiff}\)
+     *   </li>
+     * </ul>
+     *
+     *
+     * <ul>
+     *   <li>
+     *  in case of a color image and floating range
+     * \(\texttt{src} (x',y')_r- \texttt{loDiff} _r \leq \texttt{src} (x,y)_r \leq \texttt{src} (x',y')_r+ \texttt{upDiff} _r,\)
+     * \(\texttt{src} (x',y')_g- \texttt{loDiff} _g \leq \texttt{src} (x,y)_g \leq \texttt{src} (x',y')_g+ \texttt{upDiff} _g\)
+     * and
+     * \(\texttt{src} (x',y')_b- \texttt{loDiff} _b \leq \texttt{src} (x,y)_b \leq \texttt{src} (x',y')_b+ \texttt{upDiff} _b\)
+     *   </li>
+     * </ul>
+     *
+     *
+     * <ul>
+     *   <li>
+     *  in case of a color image and fixed range
+     * \(\texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)_r- \texttt{loDiff} _r \leq \texttt{src} (x,y)_r \leq \texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)_r+ \texttt{upDiff} _r,\)
+     * \(\texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)_g- \texttt{loDiff} _g \leq \texttt{src} (x,y)_g \leq \texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)_g+ \texttt{upDiff} _g\)
+     * and
+     * \(\texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)_b- \texttt{loDiff} _b \leq \texttt{src} (x,y)_b \leq \texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)_b+ \texttt{upDiff} _b\)
+     *   </li>
+     * </ul>
+     *
+     *
+     * where \(src(x',y')\) is the value of one of pixel neighbors that is already known to belong to the
+     * component. That is, to be added to the connected component, a color/brightness of the pixel should
+     * be close enough to:
+     * <ul>
+     *   <li>
+     *  Color/brightness of one of its neighbors that already belong to the connected component in case
+     * of a floating range.
+     *   </li>
+     *   <li>
+     *  Color/brightness of the seed point in case of a fixed range.
+     *   </li>
+     * </ul>
+     *
+     * Use these functions to either mark a connected component with the specified color in-place, or build
+     * a mask and then extract the contour, or copy the region to another image, and so on.
+     *
+     * @param image Input/output 1- or 3-channel, 8-bit, or floating-point image. It is modified by the
+     * function unless the #FLOODFILL_MASK_ONLY flag is set in the second variant of the function. See
+     * the details below.
+     * @param mask Operation mask that should be a single-channel 8-bit image, 2 pixels wider and 2 pixels
+     * taller than image. Since this is both an input and output parameter, you must take responsibility
+     * of initializing it. Flood-filling cannot go across non-zero pixels in the input mask. For example,
+     * an edge detector output can be used as a mask to stop filling at edges. On output, pixels in the
+     * mask corresponding to filled pixels in the image are set to 1 or to the a value specified in flags
+     * as described below. Additionally, the function fills the border of the mask with ones to simplify
+     * internal processing. It is therefore possible to use the same mask in multiple calls to the function
+     * to make sure the filled areas do not overlap.
+     * @param seedPoint Starting point.
+     * @param newVal New value of the repainted domain pixels.
+     * @param loDiff Maximal lower brightness/color difference between the currently observed pixel and
+     * one of its neighbors belonging to the component, or a seed pixel being added to the component.
+     * @param upDiff Maximal upper brightness/color difference between the currently observed pixel and
+     * one of its neighbors belonging to the component, or a seed pixel being added to the component.
+     * @param rect Optional output parameter set by the function to the minimum bounding rectangle of the
+     * repainted domain.
+     * @param flags Operation flags. The first 8 bits contain a connectivity value. The default value of
+     * 4 means that only the four nearest neighbor pixels (those that share an edge) are considered. A
+     * connectivity value of 8 means that the eight nearest neighbor pixels (those that share a corner)
+     * will be considered. The next 8 bits (8-16) contain a value between 1 and 255 with which to fill
+     * the mask (the default value is 1). For example, 4 | ( 255 &lt;&lt; 8 ) will consider 4 nearest
+     * neighbours and fill the mask with a value of 255. The following additional options occupy higher
+     * bits and therefore may be further combined with the connectivity and mask fill values using
+     * bit-wise or (|), see #FloodFillFlags.
+     *
+     * <b>Note:</b> Since the mask is larger than the filled image, a pixel \((x, y)\) in image corresponds to the
+     * pixel \((x+1, y+1)\) in the mask .
+     *
+     * SEE: findContours
+     * @return automatically generated
+     */
+    public static int floodFill(Mat image, Mat mask, Point seedPoint, Scalar newVal, Rect rect, Scalar loDiff, Scalar upDiff, int flags) {
+        double[] rect_out = new double[4];
+        int retVal = floodFill_0(image.nativeObj, mask.nativeObj, seedPoint.x, seedPoint.y, newVal.val[0], newVal.val[1], newVal.val[2], newVal.val[3], rect_out, loDiff.val[0], loDiff.val[1], loDiff.val[2], loDiff.val[3], upDiff.val[0], upDiff.val[1], upDiff.val[2], upDiff.val[3], flags);
+        if(rect!=null){ rect.x = (int)rect_out[0]; rect.y = (int)rect_out[1]; rect.width = (int)rect_out[2]; rect.height = (int)rect_out[3]; } 
+        return retVal;
+    }
+
+    /**
+     * Fills a connected component with the given color.
+     *
+     * The function cv::floodFill fills a connected component starting from the seed point with the specified
+     * color. The connectivity is determined by the color/brightness closeness of the neighbor pixels. The
+     * pixel at \((x,y)\) is considered to belong to the repainted domain if:
+     *
+     * <ul>
+     *   <li>
+     *  in case of a grayscale image and floating range
+     * \(\texttt{src} (x',y')- \texttt{loDiff} \leq \texttt{src} (x,y)  \leq \texttt{src} (x',y')+ \texttt{upDiff}\)
+     *   </li>
+     * </ul>
+     *
+     *
+     * <ul>
+     *   <li>
+     *  in case of a grayscale image and fixed range
+     * \(\texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)- \texttt{loDiff} \leq \texttt{src} (x,y)  \leq \texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)+ \texttt{upDiff}\)
+     *   </li>
+     * </ul>
+     *
+     *
+     * <ul>
+     *   <li>
+     *  in case of a color image and floating range
+     * \(\texttt{src} (x',y')_r- \texttt{loDiff} _r \leq \texttt{src} (x,y)_r \leq \texttt{src} (x',y')_r+ \texttt{upDiff} _r,\)
+     * \(\texttt{src} (x',y')_g- \texttt{loDiff} _g \leq \texttt{src} (x,y)_g \leq \texttt{src} (x',y')_g+ \texttt{upDiff} _g\)
+     * and
+     * \(\texttt{src} (x',y')_b- \texttt{loDiff} _b \leq \texttt{src} (x,y)_b \leq \texttt{src} (x',y')_b+ \texttt{upDiff} _b\)
+     *   </li>
+     * </ul>
+     *
+     *
+     * <ul>
+     *   <li>
+     *  in case of a color image and fixed range
+     * \(\texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)_r- \texttt{loDiff} _r \leq \texttt{src} (x,y)_r \leq \texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)_r+ \texttt{upDiff} _r,\)
+     * \(\texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)_g- \texttt{loDiff} _g \leq \texttt{src} (x,y)_g \leq \texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)_g+ \texttt{upDiff} _g\)
+     * and
+     * \(\texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)_b- \texttt{loDiff} _b \leq \texttt{src} (x,y)_b \leq \texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)_b+ \texttt{upDiff} _b\)
+     *   </li>
+     * </ul>
+     *
+     *
+     * where \(src(x',y')\) is the value of one of pixel neighbors that is already known to belong to the
+     * component. That is, to be added to the connected component, a color/brightness of the pixel should
+     * be close enough to:
+     * <ul>
+     *   <li>
+     *  Color/brightness of one of its neighbors that already belong to the connected component in case
+     * of a floating range.
+     *   </li>
+     *   <li>
+     *  Color/brightness of the seed point in case of a fixed range.
+     *   </li>
+     * </ul>
+     *
+     * Use these functions to either mark a connected component with the specified color in-place, or build
+     * a mask and then extract the contour, or copy the region to another image, and so on.
+     *
+     * @param image Input/output 1- or 3-channel, 8-bit, or floating-point image. It is modified by the
+     * function unless the #FLOODFILL_MASK_ONLY flag is set in the second variant of the function. See
+     * the details below.
+     * @param mask Operation mask that should be a single-channel 8-bit image, 2 pixels wider and 2 pixels
+     * taller than image. Since this is both an input and output parameter, you must take responsibility
+     * of initializing it. Flood-filling cannot go across non-zero pixels in the input mask. For example,
+     * an edge detector output can be used as a mask to stop filling at edges. On output, pixels in the
+     * mask corresponding to filled pixels in the image are set to 1 or to the a value specified in flags
+     * as described below. Additionally, the function fills the border of the mask with ones to simplify
+     * internal processing. It is therefore possible to use the same mask in multiple calls to the function
+     * to make sure the filled areas do not overlap.
+     * @param seedPoint Starting point.
+     * @param newVal New value of the repainted domain pixels.
+     * @param loDiff Maximal lower brightness/color difference between the currently observed pixel and
+     * one of its neighbors belonging to the component, or a seed pixel being added to the component.
+     * @param upDiff Maximal upper brightness/color difference between the currently observed pixel and
+     * one of its neighbors belonging to the component, or a seed pixel being added to the component.
+     * @param rect Optional output parameter set by the function to the minimum bounding rectangle of the
+     * repainted domain.
+     * 4 means that only the four nearest neighbor pixels (those that share an edge) are considered. A
+     * connectivity value of 8 means that the eight nearest neighbor pixels (those that share a corner)
+     * will be considered. The next 8 bits (8-16) contain a value between 1 and 255 with which to fill
+     * the mask (the default value is 1). For example, 4 | ( 255 &lt;&lt; 8 ) will consider 4 nearest
+     * neighbours and fill the mask with a value of 255. The following additional options occupy higher
+     * bits and therefore may be further combined with the connectivity and mask fill values using
+     * bit-wise or (|), see #FloodFillFlags.
+     *
+     * <b>Note:</b> Since the mask is larger than the filled image, a pixel \((x, y)\) in image corresponds to the
+     * pixel \((x+1, y+1)\) in the mask .
+     *
+     * SEE: findContours
+     * @return automatically generated
+     */
+    public static int floodFill(Mat image, Mat mask, Point seedPoint, Scalar newVal, Rect rect, Scalar loDiff, Scalar upDiff) {
+        double[] rect_out = new double[4];
+        int retVal = floodFill_1(image.nativeObj, mask.nativeObj, seedPoint.x, seedPoint.y, newVal.val[0], newVal.val[1], newVal.val[2], newVal.val[3], rect_out, loDiff.val[0], loDiff.val[1], loDiff.val[2], loDiff.val[3], upDiff.val[0], upDiff.val[1], upDiff.val[2], upDiff.val[3]);
+        if(rect!=null){ rect.x = (int)rect_out[0]; rect.y = (int)rect_out[1]; rect.width = (int)rect_out[2]; rect.height = (int)rect_out[3]; } 
+        return retVal;
+    }
+
+    /**
+     * Fills a connected component with the given color.
+     *
+     * The function cv::floodFill fills a connected component starting from the seed point with the specified
+     * color. The connectivity is determined by the color/brightness closeness of the neighbor pixels. The
+     * pixel at \((x,y)\) is considered to belong to the repainted domain if:
+     *
+     * <ul>
+     *   <li>
+     *  in case of a grayscale image and floating range
+     * \(\texttt{src} (x',y')- \texttt{loDiff} \leq \texttt{src} (x,y)  \leq \texttt{src} (x',y')+ \texttt{upDiff}\)
+     *   </li>
+     * </ul>
+     *
+     *
+     * <ul>
+     *   <li>
+     *  in case of a grayscale image and fixed range
+     * \(\texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)- \texttt{loDiff} \leq \texttt{src} (x,y)  \leq \texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)+ \texttt{upDiff}\)
+     *   </li>
+     * </ul>
+     *
+     *
+     * <ul>
+     *   <li>
+     *  in case of a color image and floating range
+     * \(\texttt{src} (x',y')_r- \texttt{loDiff} _r \leq \texttt{src} (x,y)_r \leq \texttt{src} (x',y')_r+ \texttt{upDiff} _r,\)
+     * \(\texttt{src} (x',y')_g- \texttt{loDiff} _g \leq \texttt{src} (x,y)_g \leq \texttt{src} (x',y')_g+ \texttt{upDiff} _g\)
+     * and
+     * \(\texttt{src} (x',y')_b- \texttt{loDiff} _b \leq \texttt{src} (x,y)_b \leq \texttt{src} (x',y')_b+ \texttt{upDiff} _b\)
+     *   </li>
+     * </ul>
+     *
+     *
+     * <ul>
+     *   <li>
+     *  in case of a color image and fixed range
+     * \(\texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)_r- \texttt{loDiff} _r \leq \texttt{src} (x,y)_r \leq \texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)_r+ \texttt{upDiff} _r,\)
+     * \(\texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)_g- \texttt{loDiff} _g \leq \texttt{src} (x,y)_g \leq \texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)_g+ \texttt{upDiff} _g\)
+     * and
+     * \(\texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)_b- \texttt{loDiff} _b \leq \texttt{src} (x,y)_b \leq \texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)_b+ \texttt{upDiff} _b\)
+     *   </li>
+     * </ul>
+     *
+     *
+     * where \(src(x',y')\) is the value of one of pixel neighbors that is already known to belong to the
+     * component. That is, to be added to the connected component, a color/brightness of the pixel should
+     * be close enough to:
+     * <ul>
+     *   <li>
+     *  Color/brightness of one of its neighbors that already belong to the connected component in case
+     * of a floating range.
+     *   </li>
+     *   <li>
+     *  Color/brightness of the seed point in case of a fixed range.
+     *   </li>
+     * </ul>
+     *
+     * Use these functions to either mark a connected component with the specified color in-place, or build
+     * a mask and then extract the contour, or copy the region to another image, and so on.
+     *
+     * @param image Input/output 1- or 3-channel, 8-bit, or floating-point image. It is modified by the
+     * function unless the #FLOODFILL_MASK_ONLY flag is set in the second variant of the function. See
+     * the details below.
+     * @param mask Operation mask that should be a single-channel 8-bit image, 2 pixels wider and 2 pixels
+     * taller than image. Since this is both an input and output parameter, you must take responsibility
+     * of initializing it. Flood-filling cannot go across non-zero pixels in the input mask. For example,
+     * an edge detector output can be used as a mask to stop filling at edges. On output, pixels in the
+     * mask corresponding to filled pixels in the image are set to 1 or to the a value specified in flags
+     * as described below. Additionally, the function fills the border of the mask with ones to simplify
+     * internal processing. It is therefore possible to use the same mask in multiple calls to the function
+     * to make sure the filled areas do not overlap.
+     * @param seedPoint Starting point.
+     * @param newVal New value of the repainted domain pixels.
+     * @param loDiff Maximal lower brightness/color difference between the currently observed pixel and
+     * one of its neighbors belonging to the component, or a seed pixel being added to the component.
+     * one of its neighbors belonging to the component, or a seed pixel being added to the component.
+     * @param rect Optional output parameter set by the function to the minimum bounding rectangle of the
+     * repainted domain.
+     * 4 means that only the four nearest neighbor pixels (those that share an edge) are considered. A
+     * connectivity value of 8 means that the eight nearest neighbor pixels (those that share a corner)
+     * will be considered. The next 8 bits (8-16) contain a value between 1 and 255 with which to fill
+     * the mask (the default value is 1). For example, 4 | ( 255 &lt;&lt; 8 ) will consider 4 nearest
+     * neighbours and fill the mask with a value of 255. The following additional options occupy higher
+     * bits and therefore may be further combined with the connectivity and mask fill values using
+     * bit-wise or (|), see #FloodFillFlags.
+     *
+     * <b>Note:</b> Since the mask is larger than the filled image, a pixel \((x, y)\) in image corresponds to the
+     * pixel \((x+1, y+1)\) in the mask .
+     *
+     * SEE: findContours
+     * @return automatically generated
+     */
+    public static int floodFill(Mat image, Mat mask, Point seedPoint, Scalar newVal, Rect rect, Scalar loDiff) {
+        double[] rect_out = new double[4];
+        int retVal = floodFill_2(image.nativeObj, mask.nativeObj, seedPoint.x, seedPoint.y, newVal.val[0], newVal.val[1], newVal.val[2], newVal.val[3], rect_out, loDiff.val[0], loDiff.val[1], loDiff.val[2], loDiff.val[3]);
+        if(rect!=null){ rect.x = (int)rect_out[0]; rect.y = (int)rect_out[1]; rect.width = (int)rect_out[2]; rect.height = (int)rect_out[3]; } 
+        return retVal;
+    }
+
+    /**
+     * Fills a connected component with the given color.
+     *
+     * The function cv::floodFill fills a connected component starting from the seed point with the specified
+     * color. The connectivity is determined by the color/brightness closeness of the neighbor pixels. The
+     * pixel at \((x,y)\) is considered to belong to the repainted domain if:
+     *
+     * <ul>
+     *   <li>
+     *  in case of a grayscale image and floating range
+     * \(\texttt{src} (x',y')- \texttt{loDiff} \leq \texttt{src} (x,y)  \leq \texttt{src} (x',y')+ \texttt{upDiff}\)
+     *   </li>
+     * </ul>
+     *
+     *
+     * <ul>
+     *   <li>
+     *  in case of a grayscale image and fixed range
+     * \(\texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)- \texttt{loDiff} \leq \texttt{src} (x,y)  \leq \texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)+ \texttt{upDiff}\)
+     *   </li>
+     * </ul>
+     *
+     *
+     * <ul>
+     *   <li>
+     *  in case of a color image and floating range
+     * \(\texttt{src} (x',y')_r- \texttt{loDiff} _r \leq \texttt{src} (x,y)_r \leq \texttt{src} (x',y')_r+ \texttt{upDiff} _r,\)
+     * \(\texttt{src} (x',y')_g- \texttt{loDiff} _g \leq \texttt{src} (x,y)_g \leq \texttt{src} (x',y')_g+ \texttt{upDiff} _g\)
+     * and
+     * \(\texttt{src} (x',y')_b- \texttt{loDiff} _b \leq \texttt{src} (x,y)_b \leq \texttt{src} (x',y')_b+ \texttt{upDiff} _b\)
+     *   </li>
+     * </ul>
+     *
+     *
+     * <ul>
+     *   <li>
+     *  in case of a color image and fixed range
+     * \(\texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)_r- \texttt{loDiff} _r \leq \texttt{src} (x,y)_r \leq \texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)_r+ \texttt{upDiff} _r,\)
+     * \(\texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)_g- \texttt{loDiff} _g \leq \texttt{src} (x,y)_g \leq \texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)_g+ \texttt{upDiff} _g\)
+     * and
+     * \(\texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)_b- \texttt{loDiff} _b \leq \texttt{src} (x,y)_b \leq \texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)_b+ \texttt{upDiff} _b\)
+     *   </li>
+     * </ul>
+     *
+     *
+     * where \(src(x',y')\) is the value of one of pixel neighbors that is already known to belong to the
+     * component. That is, to be added to the connected component, a color/brightness of the pixel should
+     * be close enough to:
+     * <ul>
+     *   <li>
+     *  Color/brightness of one of its neighbors that already belong to the connected component in case
+     * of a floating range.
+     *   </li>
+     *   <li>
+     *  Color/brightness of the seed point in case of a fixed range.
+     *   </li>
+     * </ul>
+     *
+     * Use these functions to either mark a connected component with the specified color in-place, or build
+     * a mask and then extract the contour, or copy the region to another image, and so on.
+     *
+     * @param image Input/output 1- or 3-channel, 8-bit, or floating-point image. It is modified by the
+     * function unless the #FLOODFILL_MASK_ONLY flag is set in the second variant of the function. See
+     * the details below.
+     * @param mask Operation mask that should be a single-channel 8-bit image, 2 pixels wider and 2 pixels
+     * taller than image. Since this is both an input and output parameter, you must take responsibility
+     * of initializing it. Flood-filling cannot go across non-zero pixels in the input mask. For example,
+     * an edge detector output can be used as a mask to stop filling at edges. On output, pixels in the
+     * mask corresponding to filled pixels in the image are set to 1 or to the a value specified in flags
+     * as described below. Additionally, the function fills the border of the mask with ones to simplify
+     * internal processing. It is therefore possible to use the same mask in multiple calls to the function
+     * to make sure the filled areas do not overlap.
+     * @param seedPoint Starting point.
+     * @param newVal New value of the repainted domain pixels.
+     * one of its neighbors belonging to the component, or a seed pixel being added to the component.
+     * one of its neighbors belonging to the component, or a seed pixel being added to the component.
+     * @param rect Optional output parameter set by the function to the minimum bounding rectangle of the
+     * repainted domain.
+     * 4 means that only the four nearest neighbor pixels (those that share an edge) are considered. A
+     * connectivity value of 8 means that the eight nearest neighbor pixels (those that share a corner)
+     * will be considered. The next 8 bits (8-16) contain a value between 1 and 255 with which to fill
+     * the mask (the default value is 1). For example, 4 | ( 255 &lt;&lt; 8 ) will consider 4 nearest
+     * neighbours and fill the mask with a value of 255. The following additional options occupy higher
+     * bits and therefore may be further combined with the connectivity and mask fill values using
+     * bit-wise or (|), see #FloodFillFlags.
+     *
+     * <b>Note:</b> Since the mask is larger than the filled image, a pixel \((x, y)\) in image corresponds to the
+     * pixel \((x+1, y+1)\) in the mask .
+     *
+     * SEE: findContours
+     * @return automatically generated
+     */
+    public static int floodFill(Mat image, Mat mask, Point seedPoint, Scalar newVal, Rect rect) {
+        double[] rect_out = new double[4];
+        int retVal = floodFill_3(image.nativeObj, mask.nativeObj, seedPoint.x, seedPoint.y, newVal.val[0], newVal.val[1], newVal.val[2], newVal.val[3], rect_out);
+        if(rect!=null){ rect.x = (int)rect_out[0]; rect.y = (int)rect_out[1]; rect.width = (int)rect_out[2]; rect.height = (int)rect_out[3]; } 
+        return retVal;
+    }
+
+    /**
+     * Fills a connected component with the given color.
+     *
+     * The function cv::floodFill fills a connected component starting from the seed point with the specified
+     * color. The connectivity is determined by the color/brightness closeness of the neighbor pixels. The
+     * pixel at \((x,y)\) is considered to belong to the repainted domain if:
+     *
+     * <ul>
+     *   <li>
+     *  in case of a grayscale image and floating range
+     * \(\texttt{src} (x',y')- \texttt{loDiff} \leq \texttt{src} (x,y)  \leq \texttt{src} (x',y')+ \texttt{upDiff}\)
+     *   </li>
+     * </ul>
+     *
+     *
+     * <ul>
+     *   <li>
+     *  in case of a grayscale image and fixed range
+     * \(\texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)- \texttt{loDiff} \leq \texttt{src} (x,y)  \leq \texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)+ \texttt{upDiff}\)
+     *   </li>
+     * </ul>
+     *
+     *
+     * <ul>
+     *   <li>
+     *  in case of a color image and floating range
+     * \(\texttt{src} (x',y')_r- \texttt{loDiff} _r \leq \texttt{src} (x,y)_r \leq \texttt{src} (x',y')_r+ \texttt{upDiff} _r,\)
+     * \(\texttt{src} (x',y')_g- \texttt{loDiff} _g \leq \texttt{src} (x,y)_g \leq \texttt{src} (x',y')_g+ \texttt{upDiff} _g\)
+     * and
+     * \(\texttt{src} (x',y')_b- \texttt{loDiff} _b \leq \texttt{src} (x,y)_b \leq \texttt{src} (x',y')_b+ \texttt{upDiff} _b\)
+     *   </li>
+     * </ul>
+     *
+     *
+     * <ul>
+     *   <li>
+     *  in case of a color image and fixed range
+     * \(\texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)_r- \texttt{loDiff} _r \leq \texttt{src} (x,y)_r \leq \texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)_r+ \texttt{upDiff} _r,\)
+     * \(\texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)_g- \texttt{loDiff} _g \leq \texttt{src} (x,y)_g \leq \texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)_g+ \texttt{upDiff} _g\)
+     * and
+     * \(\texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)_b- \texttt{loDiff} _b \leq \texttt{src} (x,y)_b \leq \texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)_b+ \texttt{upDiff} _b\)
+     *   </li>
+     * </ul>
+     *
+     *
+     * where \(src(x',y')\) is the value of one of pixel neighbors that is already known to belong to the
+     * component. That is, to be added to the connected component, a color/brightness of the pixel should
+     * be close enough to:
+     * <ul>
+     *   <li>
+     *  Color/brightness of one of its neighbors that already belong to the connected component in case
+     * of a floating range.
+     *   </li>
+     *   <li>
+     *  Color/brightness of the seed point in case of a fixed range.
+     *   </li>
+     * </ul>
+     *
+     * Use these functions to either mark a connected component with the specified color in-place, or build
+     * a mask and then extract the contour, or copy the region to another image, and so on.
+     *
+     * @param image Input/output 1- or 3-channel, 8-bit, or floating-point image. It is modified by the
+     * function unless the #FLOODFILL_MASK_ONLY flag is set in the second variant of the function. See
+     * the details below.
+     * @param mask Operation mask that should be a single-channel 8-bit image, 2 pixels wider and 2 pixels
+     * taller than image. Since this is both an input and output parameter, you must take responsibility
+     * of initializing it. Flood-filling cannot go across non-zero pixels in the input mask. For example,
+     * an edge detector output can be used as a mask to stop filling at edges. On output, pixels in the
+     * mask corresponding to filled pixels in the image are set to 1 or to the a value specified in flags
+     * as described below. Additionally, the function fills the border of the mask with ones to simplify
+     * internal processing. It is therefore possible to use the same mask in multiple calls to the function
+     * to make sure the filled areas do not overlap.
+     * @param seedPoint Starting point.
+     * @param newVal New value of the repainted domain pixels.
+     * one of its neighbors belonging to the component, or a seed pixel being added to the component.
+     * one of its neighbors belonging to the component, or a seed pixel being added to the component.
+     * repainted domain.
+     * 4 means that only the four nearest neighbor pixels (those that share an edge) are considered. A
+     * connectivity value of 8 means that the eight nearest neighbor pixels (those that share a corner)
+     * will be considered. The next 8 bits (8-16) contain a value between 1 and 255 with which to fill
+     * the mask (the default value is 1). For example, 4 | ( 255 &lt;&lt; 8 ) will consider 4 nearest
+     * neighbours and fill the mask with a value of 255. The following additional options occupy higher
+     * bits and therefore may be further combined with the connectivity and mask fill values using
+     * bit-wise or (|), see #FloodFillFlags.
+     *
+     * <b>Note:</b> Since the mask is larger than the filled image, a pixel \((x, y)\) in image corresponds to the
+     * pixel \((x+1, y+1)\) in the mask .
+     *
+     * SEE: findContours
+     * @return automatically generated
+     */
+    public static int floodFill(Mat image, Mat mask, Point seedPoint, Scalar newVal) {
+        return floodFill_4(image.nativeObj, mask.nativeObj, seedPoint.x, seedPoint.y, newVal.val[0], newVal.val[1], newVal.val[2], newVal.val[3]);
+    }
+
+
+    //
+    // C++:  int cv::rotatedRectangleIntersection(RotatedRect rect1, RotatedRect rect2, Mat& intersectingRegion)
+    //
+
+    /**
+     * Finds out if there is any intersection between two rotated rectangles.
+     *
+     * If there is then the vertices of the intersecting region are returned as well.
+     *
+     * Below are some examples of intersection configurations. The hatched pattern indicates the
+     * intersecting region and the red vertices are returned by the function.
+     *
+     * ![intersection examples](pics/intersection.png)
+     *
+     * @param rect1 First rectangle
+     * @param rect2 Second rectangle
+     * @param intersectingRegion The output array of the vertices of the intersecting region. It returns
+     * at most 8 vertices. Stored as std::vector&lt;cv::Point2f&gt; or cv::Mat as Mx1 of type CV_32FC2.
+     * @return One of #RectanglesIntersectTypes
+     */
+    public static int rotatedRectangleIntersection(RotatedRect rect1, RotatedRect rect2, Mat intersectingRegion) {
+        return rotatedRectangleIntersection_0(rect1.center.x, rect1.center.y, rect1.size.width, rect1.size.height, rect1.angle, rect2.center.x, rect2.center.y, rect2.size.width, rect2.size.height, rect2.angle, intersectingRegion.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::Canny(Mat dx, Mat dy, Mat& edges, double threshold1, double threshold2, bool L2gradient = false)
+    //
+
+    /**
+     * \overload
+     *
+     * Finds edges in an image using the Canny algorithm with custom image gradient.
+     *
+     * @param dx 16-bit x derivative of input image (CV_16SC1 or CV_16SC3).
+     * @param dy 16-bit y derivative of input image (same type as dx).
+     * @param edges output edge map; single channels 8-bit image, which has the same size as image .
+     * @param threshold1 first threshold for the hysteresis procedure.
+     * @param threshold2 second threshold for the hysteresis procedure.
+     * @param L2gradient a flag, indicating whether a more accurate \(L_2\) norm
+     * \(=\sqrt{(dI/dx)^2 + (dI/dy)^2}\) should be used to calculate the image gradient magnitude (
+     * L2gradient=true ), or whether the default \(L_1\) norm \(=|dI/dx|+|dI/dy|\) is enough (
+     * L2gradient=false ).
+     */
+    public static void Canny(Mat dx, Mat dy, Mat edges, double threshold1, double threshold2, boolean L2gradient) {
+        Canny_0(dx.nativeObj, dy.nativeObj, edges.nativeObj, threshold1, threshold2, L2gradient);
+    }
+
+    /**
+     * \overload
+     *
+     * Finds edges in an image using the Canny algorithm with custom image gradient.
+     *
+     * @param dx 16-bit x derivative of input image (CV_16SC1 or CV_16SC3).
+     * @param dy 16-bit y derivative of input image (same type as dx).
+     * @param edges output edge map; single channels 8-bit image, which has the same size as image .
+     * @param threshold1 first threshold for the hysteresis procedure.
+     * @param threshold2 second threshold for the hysteresis procedure.
+     * \(=\sqrt{(dI/dx)^2 + (dI/dy)^2}\) should be used to calculate the image gradient magnitude (
+     * L2gradient=true ), or whether the default \(L_1\) norm \(=|dI/dx|+|dI/dy|\) is enough (
+     * L2gradient=false ).
+     */
+    public static void Canny(Mat dx, Mat dy, Mat edges, double threshold1, double threshold2) {
+        Canny_1(dx.nativeObj, dy.nativeObj, edges.nativeObj, threshold1, threshold2);
+    }
+
+
+    //
+    // C++:  void cv::Canny(Mat image, Mat& edges, double threshold1, double threshold2, int apertureSize = 3, bool L2gradient = false)
+    //
+
+    /**
+     * Finds edges in an image using the Canny algorithm CITE: Canny86 .
+     *
+     * The function finds edges in the input image and marks them in the output map edges using the
+     * Canny algorithm. The smallest value between threshold1 and threshold2 is used for edge linking. The
+     * largest value is used to find initial segments of strong edges. See
+     * &lt;http://en.wikipedia.org/wiki/Canny_edge_detector&gt;
+     *
+     * @param image 8-bit input image.
+     * @param edges output edge map; single channels 8-bit image, which has the same size as image .
+     * @param threshold1 first threshold for the hysteresis procedure.
+     * @param threshold2 second threshold for the hysteresis procedure.
+     * @param apertureSize aperture size for the Sobel operator.
+     * @param L2gradient a flag, indicating whether a more accurate \(L_2\) norm
+     * \(=\sqrt{(dI/dx)^2 + (dI/dy)^2}\) should be used to calculate the image gradient magnitude (
+     * L2gradient=true ), or whether the default \(L_1\) norm \(=|dI/dx|+|dI/dy|\) is enough (
+     * L2gradient=false ).
+     */
+    public static void Canny(Mat image, Mat edges, double threshold1, double threshold2, int apertureSize, boolean L2gradient) {
+        Canny_2(image.nativeObj, edges.nativeObj, threshold1, threshold2, apertureSize, L2gradient);
+    }
+
+    /**
+     * Finds edges in an image using the Canny algorithm CITE: Canny86 .
+     *
+     * The function finds edges in the input image and marks them in the output map edges using the
+     * Canny algorithm. The smallest value between threshold1 and threshold2 is used for edge linking. The
+     * largest value is used to find initial segments of strong edges. See
+     * &lt;http://en.wikipedia.org/wiki/Canny_edge_detector&gt;
+     *
+     * @param image 8-bit input image.
+     * @param edges output edge map; single channels 8-bit image, which has the same size as image .
+     * @param threshold1 first threshold for the hysteresis procedure.
+     * @param threshold2 second threshold for the hysteresis procedure.
+     * @param apertureSize aperture size for the Sobel operator.
+     * \(=\sqrt{(dI/dx)^2 + (dI/dy)^2}\) should be used to calculate the image gradient magnitude (
+     * L2gradient=true ), or whether the default \(L_1\) norm \(=|dI/dx|+|dI/dy|\) is enough (
+     * L2gradient=false ).
+     */
+    public static void Canny(Mat image, Mat edges, double threshold1, double threshold2, int apertureSize) {
+        Canny_3(image.nativeObj, edges.nativeObj, threshold1, threshold2, apertureSize);
+    }
+
+    /**
+     * Finds edges in an image using the Canny algorithm CITE: Canny86 .
+     *
+     * The function finds edges in the input image and marks them in the output map edges using the
+     * Canny algorithm. The smallest value between threshold1 and threshold2 is used for edge linking. The
+     * largest value is used to find initial segments of strong edges. See
+     * &lt;http://en.wikipedia.org/wiki/Canny_edge_detector&gt;
+     *
+     * @param image 8-bit input image.
+     * @param edges output edge map; single channels 8-bit image, which has the same size as image .
+     * @param threshold1 first threshold for the hysteresis procedure.
+     * @param threshold2 second threshold for the hysteresis procedure.
+     * \(=\sqrt{(dI/dx)^2 + (dI/dy)^2}\) should be used to calculate the image gradient magnitude (
+     * L2gradient=true ), or whether the default \(L_1\) norm \(=|dI/dx|+|dI/dy|\) is enough (
+     * L2gradient=false ).
+     */
+    public static void Canny(Mat image, Mat edges, double threshold1, double threshold2) {
+        Canny_4(image.nativeObj, edges.nativeObj, threshold1, threshold2);
+    }
+
+
+    //
+    // C++:  void cv::GaussianBlur(Mat src, Mat& dst, Size ksize, double sigmaX, double sigmaY = 0, int borderType = BORDER_DEFAULT)
+    //
+
+    /**
+     * Blurs an image using a Gaussian filter.
+     *
+     * The function convolves the source image with the specified Gaussian kernel. In-place filtering is
+     * supported.
+     *
+     * @param src input image; the image can have any number of channels, which are processed
+     * independently, but the depth should be CV_8U, CV_16U, CV_16S, CV_32F or CV_64F.
+     * @param dst output image of the same size and type as src.
+     * @param ksize Gaussian kernel size. ksize.width and ksize.height can differ but they both must be
+     * positive and odd. Or, they can be zero's and then they are computed from sigma.
+     * @param sigmaX Gaussian kernel standard deviation in X direction.
+     * @param sigmaY Gaussian kernel standard deviation in Y direction; if sigmaY is zero, it is set to be
+     * equal to sigmaX, if both sigmas are zeros, they are computed from ksize.width and ksize.height,
+     * respectively (see #getGaussianKernel for details); to fully control the result regardless of
+     * possible future modifications of all this semantics, it is recommended to specify all of ksize,
+     * sigmaX, and sigmaY.
+     * @param borderType pixel extrapolation method, see #BorderTypes. #BORDER_WRAP is not supported.
+     *
+     * SEE:  sepFilter2D, filter2D, blur, boxFilter, bilateralFilter, medianBlur
+     */
+    public static void GaussianBlur(Mat src, Mat dst, Size ksize, double sigmaX, double sigmaY, int borderType) {
+        GaussianBlur_0(src.nativeObj, dst.nativeObj, ksize.width, ksize.height, sigmaX, sigmaY, borderType);
+    }
+
+    /**
+     * Blurs an image using a Gaussian filter.
+     *
+     * The function convolves the source image with the specified Gaussian kernel. In-place filtering is
+     * supported.
+     *
+     * @param src input image; the image can have any number of channels, which are processed
+     * independently, but the depth should be CV_8U, CV_16U, CV_16S, CV_32F or CV_64F.
+     * @param dst output image of the same size and type as src.
+     * @param ksize Gaussian kernel size. ksize.width and ksize.height can differ but they both must be
+     * positive and odd. Or, they can be zero's and then they are computed from sigma.
+     * @param sigmaX Gaussian kernel standard deviation in X direction.
+     * @param sigmaY Gaussian kernel standard deviation in Y direction; if sigmaY is zero, it is set to be
+     * equal to sigmaX, if both sigmas are zeros, they are computed from ksize.width and ksize.height,
+     * respectively (see #getGaussianKernel for details); to fully control the result regardless of
+     * possible future modifications of all this semantics, it is recommended to specify all of ksize,
+     * sigmaX, and sigmaY.
+     *
+     * SEE:  sepFilter2D, filter2D, blur, boxFilter, bilateralFilter, medianBlur
+     */
+    public static void GaussianBlur(Mat src, Mat dst, Size ksize, double sigmaX, double sigmaY) {
+        GaussianBlur_1(src.nativeObj, dst.nativeObj, ksize.width, ksize.height, sigmaX, sigmaY);
+    }
+
+    /**
+     * Blurs an image using a Gaussian filter.
+     *
+     * The function convolves the source image with the specified Gaussian kernel. In-place filtering is
+     * supported.
+     *
+     * @param src input image; the image can have any number of channels, which are processed
+     * independently, but the depth should be CV_8U, CV_16U, CV_16S, CV_32F or CV_64F.
+     * @param dst output image of the same size and type as src.
+     * @param ksize Gaussian kernel size. ksize.width and ksize.height can differ but they both must be
+     * positive and odd. Or, they can be zero's and then they are computed from sigma.
+     * @param sigmaX Gaussian kernel standard deviation in X direction.
+     * equal to sigmaX, if both sigmas are zeros, they are computed from ksize.width and ksize.height,
+     * respectively (see #getGaussianKernel for details); to fully control the result regardless of
+     * possible future modifications of all this semantics, it is recommended to specify all of ksize,
+     * sigmaX, and sigmaY.
+     *
+     * SEE:  sepFilter2D, filter2D, blur, boxFilter, bilateralFilter, medianBlur
+     */
+    public static void GaussianBlur(Mat src, Mat dst, Size ksize, double sigmaX) {
+        GaussianBlur_2(src.nativeObj, dst.nativeObj, ksize.width, ksize.height, sigmaX);
+    }
+
+
+    //
+    // C++:  void cv::HoughCircles(Mat image, Mat& circles, int method, double dp, double minDist, double param1 = 100, double param2 = 100, int minRadius = 0, int maxRadius = 0)
+    //
+
+    /**
+     * Finds circles in a grayscale image using the Hough transform.
+     *
+     * The function finds circles in a grayscale image using a modification of the Hough transform.
+     *
+     * Example: :
+     * INCLUDE: snippets/imgproc_HoughLinesCircles.cpp
+     *
+     * <b>Note:</b> Usually the function detects the centers of circles well. However, it may fail to find correct
+     * radii. You can assist to the function by specifying the radius range ( minRadius and maxRadius ) if
+     * you know it. Or, you may set maxRadius to a negative number to return centers only without radius
+     * search, and find the correct radius using an additional procedure.
+     *
+     * @param image 8-bit, single-channel, grayscale input image.
+     * @param circles Output vector of found circles. Each vector is encoded as  3 or 4 element
+     * floating-point vector \((x, y, radius)\) or \((x, y, radius, votes)\) .
+     * @param method Detection method, see #HoughModes. Currently, the only implemented method is #HOUGH_GRADIENT
+     * @param dp Inverse ratio of the accumulator resolution to the image resolution. For example, if
+     * dp=1 , the accumulator has the same resolution as the input image. If dp=2 , the accumulator has
+     * half as big width and height.
+     * @param minDist Minimum distance between the centers of the detected circles. If the parameter is
+     * too small, multiple neighbor circles may be falsely detected in addition to a true one. If it is
+     * too large, some circles may be missed.
+     * @param param1 First method-specific parameter. In case of #HOUGH_GRADIENT , it is the higher
+     * threshold of the two passed to the Canny edge detector (the lower one is twice smaller).
+     * @param param2 Second method-specific parameter. In case of #HOUGH_GRADIENT , it is the
+     * accumulator threshold for the circle centers at the detection stage. The smaller it is, the more
+     * false circles may be detected. Circles, corresponding to the larger accumulator values, will be
+     * returned first.
+     * @param minRadius Minimum circle radius.
+     * @param maxRadius Maximum circle radius. If &lt;= 0, uses the maximum image dimension. If &lt; 0, returns
+     * centers without finding the radius.
+     *
+     * SEE: fitEllipse, minEnclosingCircle
+     */
+    public static void HoughCircles(Mat image, Mat circles, int method, double dp, double minDist, double param1, double param2, int minRadius, int maxRadius) {
+        HoughCircles_0(image.nativeObj, circles.nativeObj, method, dp, minDist, param1, param2, minRadius, maxRadius);
+    }
+
+    /**
+     * Finds circles in a grayscale image using the Hough transform.
+     *
+     * The function finds circles in a grayscale image using a modification of the Hough transform.
+     *
+     * Example: :
+     * INCLUDE: snippets/imgproc_HoughLinesCircles.cpp
+     *
+     * <b>Note:</b> Usually the function detects the centers of circles well. However, it may fail to find correct
+     * radii. You can assist to the function by specifying the radius range ( minRadius and maxRadius ) if
+     * you know it. Or, you may set maxRadius to a negative number to return centers only without radius
+     * search, and find the correct radius using an additional procedure.
+     *
+     * @param image 8-bit, single-channel, grayscale input image.
+     * @param circles Output vector of found circles. Each vector is encoded as  3 or 4 element
+     * floating-point vector \((x, y, radius)\) or \((x, y, radius, votes)\) .
+     * @param method Detection method, see #HoughModes. Currently, the only implemented method is #HOUGH_GRADIENT
+     * @param dp Inverse ratio of the accumulator resolution to the image resolution. For example, if
+     * dp=1 , the accumulator has the same resolution as the input image. If dp=2 , the accumulator has
+     * half as big width and height.
+     * @param minDist Minimum distance between the centers of the detected circles. If the parameter is
+     * too small, multiple neighbor circles may be falsely detected in addition to a true one. If it is
+     * too large, some circles may be missed.
+     * @param param1 First method-specific parameter. In case of #HOUGH_GRADIENT , it is the higher
+     * threshold of the two passed to the Canny edge detector (the lower one is twice smaller).
+     * @param param2 Second method-specific parameter. In case of #HOUGH_GRADIENT , it is the
+     * accumulator threshold for the circle centers at the detection stage. The smaller it is, the more
+     * false circles may be detected. Circles, corresponding to the larger accumulator values, will be
+     * returned first.
+     * @param minRadius Minimum circle radius.
+     * centers without finding the radius.
+     *
+     * SEE: fitEllipse, minEnclosingCircle
+     */
+    public static void HoughCircles(Mat image, Mat circles, int method, double dp, double minDist, double param1, double param2, int minRadius) {
+        HoughCircles_1(image.nativeObj, circles.nativeObj, method, dp, minDist, param1, param2, minRadius);
+    }
+
+    /**
+     * Finds circles in a grayscale image using the Hough transform.
+     *
+     * The function finds circles in a grayscale image using a modification of the Hough transform.
+     *
+     * Example: :
+     * INCLUDE: snippets/imgproc_HoughLinesCircles.cpp
+     *
+     * <b>Note:</b> Usually the function detects the centers of circles well. However, it may fail to find correct
+     * radii. You can assist to the function by specifying the radius range ( minRadius and maxRadius ) if
+     * you know it. Or, you may set maxRadius to a negative number to return centers only without radius
+     * search, and find the correct radius using an additional procedure.
+     *
+     * @param image 8-bit, single-channel, grayscale input image.
+     * @param circles Output vector of found circles. Each vector is encoded as  3 or 4 element
+     * floating-point vector \((x, y, radius)\) or \((x, y, radius, votes)\) .
+     * @param method Detection method, see #HoughModes. Currently, the only implemented method is #HOUGH_GRADIENT
+     * @param dp Inverse ratio of the accumulator resolution to the image resolution. For example, if
+     * dp=1 , the accumulator has the same resolution as the input image. If dp=2 , the accumulator has
+     * half as big width and height.
+     * @param minDist Minimum distance between the centers of the detected circles. If the parameter is
+     * too small, multiple neighbor circles may be falsely detected in addition to a true one. If it is
+     * too large, some circles may be missed.
+     * @param param1 First method-specific parameter. In case of #HOUGH_GRADIENT , it is the higher
+     * threshold of the two passed to the Canny edge detector (the lower one is twice smaller).
+     * @param param2 Second method-specific parameter. In case of #HOUGH_GRADIENT , it is the
+     * accumulator threshold for the circle centers at the detection stage. The smaller it is, the more
+     * false circles may be detected. Circles, corresponding to the larger accumulator values, will be
+     * returned first.
+     * centers without finding the radius.
+     *
+     * SEE: fitEllipse, minEnclosingCircle
+     */
+    public static void HoughCircles(Mat image, Mat circles, int method, double dp, double minDist, double param1, double param2) {
+        HoughCircles_2(image.nativeObj, circles.nativeObj, method, dp, minDist, param1, param2);
+    }
+
+    /**
+     * Finds circles in a grayscale image using the Hough transform.
+     *
+     * The function finds circles in a grayscale image using a modification of the Hough transform.
+     *
+     * Example: :
+     * INCLUDE: snippets/imgproc_HoughLinesCircles.cpp
+     *
+     * <b>Note:</b> Usually the function detects the centers of circles well. However, it may fail to find correct
+     * radii. You can assist to the function by specifying the radius range ( minRadius and maxRadius ) if
+     * you know it. Or, you may set maxRadius to a negative number to return centers only without radius
+     * search, and find the correct radius using an additional procedure.
+     *
+     * @param image 8-bit, single-channel, grayscale input image.
+     * @param circles Output vector of found circles. Each vector is encoded as  3 or 4 element
+     * floating-point vector \((x, y, radius)\) or \((x, y, radius, votes)\) .
+     * @param method Detection method, see #HoughModes. Currently, the only implemented method is #HOUGH_GRADIENT
+     * @param dp Inverse ratio of the accumulator resolution to the image resolution. For example, if
+     * dp=1 , the accumulator has the same resolution as the input image. If dp=2 , the accumulator has
+     * half as big width and height.
+     * @param minDist Minimum distance between the centers of the detected circles. If the parameter is
+     * too small, multiple neighbor circles may be falsely detected in addition to a true one. If it is
+     * too large, some circles may be missed.
+     * @param param1 First method-specific parameter. In case of #HOUGH_GRADIENT , it is the higher
+     * threshold of the two passed to the Canny edge detector (the lower one is twice smaller).
+     * accumulator threshold for the circle centers at the detection stage. The smaller it is, the more
+     * false circles may be detected. Circles, corresponding to the larger accumulator values, will be
+     * returned first.
+     * centers without finding the radius.
+     *
+     * SEE: fitEllipse, minEnclosingCircle
+     */
+    public static void HoughCircles(Mat image, Mat circles, int method, double dp, double minDist, double param1) {
+        HoughCircles_3(image.nativeObj, circles.nativeObj, method, dp, minDist, param1);
+    }
+
+    /**
+     * Finds circles in a grayscale image using the Hough transform.
+     *
+     * The function finds circles in a grayscale image using a modification of the Hough transform.
+     *
+     * Example: :
+     * INCLUDE: snippets/imgproc_HoughLinesCircles.cpp
+     *
+     * <b>Note:</b> Usually the function detects the centers of circles well. However, it may fail to find correct
+     * radii. You can assist to the function by specifying the radius range ( minRadius and maxRadius ) if
+     * you know it. Or, you may set maxRadius to a negative number to return centers only without radius
+     * search, and find the correct radius using an additional procedure.
+     *
+     * @param image 8-bit, single-channel, grayscale input image.
+     * @param circles Output vector of found circles. Each vector is encoded as  3 or 4 element
+     * floating-point vector \((x, y, radius)\) or \((x, y, radius, votes)\) .
+     * @param method Detection method, see #HoughModes. Currently, the only implemented method is #HOUGH_GRADIENT
+     * @param dp Inverse ratio of the accumulator resolution to the image resolution. For example, if
+     * dp=1 , the accumulator has the same resolution as the input image. If dp=2 , the accumulator has
+     * half as big width and height.
+     * @param minDist Minimum distance between the centers of the detected circles. If the parameter is
+     * too small, multiple neighbor circles may be falsely detected in addition to a true one. If it is
+     * too large, some circles may be missed.
+     * threshold of the two passed to the Canny edge detector (the lower one is twice smaller).
+     * accumulator threshold for the circle centers at the detection stage. The smaller it is, the more
+     * false circles may be detected. Circles, corresponding to the larger accumulator values, will be
+     * returned first.
+     * centers without finding the radius.
+     *
+     * SEE: fitEllipse, minEnclosingCircle
+     */
+    public static void HoughCircles(Mat image, Mat circles, int method, double dp, double minDist) {
+        HoughCircles_4(image.nativeObj, circles.nativeObj, method, dp, minDist);
+    }
+
+
+    //
+    // C++:  void cv::HoughLines(Mat image, Mat& lines, double rho, double theta, int threshold, double srn = 0, double stn = 0, double min_theta = 0, double max_theta = CV_PI)
+    //
+
+    /**
+     * Finds lines in a binary image using the standard Hough transform.
+     *
+     * The function implements the standard or standard multi-scale Hough transform algorithm for line
+     * detection. See &lt;http://homepages.inf.ed.ac.uk/rbf/HIPR2/hough.htm&gt; for a good explanation of Hough
+     * transform.
+     *
+     * @param image 8-bit, single-channel binary source image. The image may be modified by the function.
+     * @param lines Output vector of lines. Each line is represented by a 2 or 3 element vector
+     * \((\rho, \theta)\) or \((\rho, \theta, \textrm{votes})\) . \(\rho\) is the distance from the coordinate origin \((0,0)\) (top-left corner of
+     * the image). \(\theta\) is the line rotation angle in radians (
+     * \(0 \sim \textrm{vertical line}, \pi/2 \sim \textrm{horizontal line}\) ).
+     * \(\textrm{votes}\) is the value of accumulator.
+     * @param rho Distance resolution of the accumulator in pixels.
+     * @param theta Angle resolution of the accumulator in radians.
+     * @param threshold Accumulator threshold parameter. Only those lines are returned that get enough
+     * votes ( \(&gt;\texttt{threshold}\) ).
+     * @param srn For the multi-scale Hough transform, it is a divisor for the distance resolution rho .
+     * The coarse accumulator distance resolution is rho and the accurate accumulator resolution is
+     * rho/srn . If both srn=0 and stn=0 , the classical Hough transform is used. Otherwise, both these
+     * parameters should be positive.
+     * @param stn For the multi-scale Hough transform, it is a divisor for the distance resolution theta.
+     * @param min_theta For standard and multi-scale Hough transform, minimum angle to check for lines.
+     * Must fall between 0 and max_theta.
+     * @param max_theta For standard and multi-scale Hough transform, maximum angle to check for lines.
+     * Must fall between min_theta and CV_PI.
+     */
+    public static void HoughLines(Mat image, Mat lines, double rho, double theta, int threshold, double srn, double stn, double min_theta, double max_theta) {
+        HoughLines_0(image.nativeObj, lines.nativeObj, rho, theta, threshold, srn, stn, min_theta, max_theta);
+    }
+
+    /**
+     * Finds lines in a binary image using the standard Hough transform.
+     *
+     * The function implements the standard or standard multi-scale Hough transform algorithm for line
+     * detection. See &lt;http://homepages.inf.ed.ac.uk/rbf/HIPR2/hough.htm&gt; for a good explanation of Hough
+     * transform.
+     *
+     * @param image 8-bit, single-channel binary source image. The image may be modified by the function.
+     * @param lines Output vector of lines. Each line is represented by a 2 or 3 element vector
+     * \((\rho, \theta)\) or \((\rho, \theta, \textrm{votes})\) . \(\rho\) is the distance from the coordinate origin \((0,0)\) (top-left corner of
+     * the image). \(\theta\) is the line rotation angle in radians (
+     * \(0 \sim \textrm{vertical line}, \pi/2 \sim \textrm{horizontal line}\) ).
+     * \(\textrm{votes}\) is the value of accumulator.
+     * @param rho Distance resolution of the accumulator in pixels.
+     * @param theta Angle resolution of the accumulator in radians.
+     * @param threshold Accumulator threshold parameter. Only those lines are returned that get enough
+     * votes ( \(&gt;\texttt{threshold}\) ).
+     * @param srn For the multi-scale Hough transform, it is a divisor for the distance resolution rho .
+     * The coarse accumulator distance resolution is rho and the accurate accumulator resolution is
+     * rho/srn . If both srn=0 and stn=0 , the classical Hough transform is used. Otherwise, both these
+     * parameters should be positive.
+     * @param stn For the multi-scale Hough transform, it is a divisor for the distance resolution theta.
+     * @param min_theta For standard and multi-scale Hough transform, minimum angle to check for lines.
+     * Must fall between 0 and max_theta.
+     * Must fall between min_theta and CV_PI.
+     */
+    public static void HoughLines(Mat image, Mat lines, double rho, double theta, int threshold, double srn, double stn, double min_theta) {
+        HoughLines_1(image.nativeObj, lines.nativeObj, rho, theta, threshold, srn, stn, min_theta);
+    }
+
+    /**
+     * Finds lines in a binary image using the standard Hough transform.
+     *
+     * The function implements the standard or standard multi-scale Hough transform algorithm for line
+     * detection. See &lt;http://homepages.inf.ed.ac.uk/rbf/HIPR2/hough.htm&gt; for a good explanation of Hough
+     * transform.
+     *
+     * @param image 8-bit, single-channel binary source image. The image may be modified by the function.
+     * @param lines Output vector of lines. Each line is represented by a 2 or 3 element vector
+     * \((\rho, \theta)\) or \((\rho, \theta, \textrm{votes})\) . \(\rho\) is the distance from the coordinate origin \((0,0)\) (top-left corner of
+     * the image). \(\theta\) is the line rotation angle in radians (
+     * \(0 \sim \textrm{vertical line}, \pi/2 \sim \textrm{horizontal line}\) ).
+     * \(\textrm{votes}\) is the value of accumulator.
+     * @param rho Distance resolution of the accumulator in pixels.
+     * @param theta Angle resolution of the accumulator in radians.
+     * @param threshold Accumulator threshold parameter. Only those lines are returned that get enough
+     * votes ( \(&gt;\texttt{threshold}\) ).
+     * @param srn For the multi-scale Hough transform, it is a divisor for the distance resolution rho .
+     * The coarse accumulator distance resolution is rho and the accurate accumulator resolution is
+     * rho/srn . If both srn=0 and stn=0 , the classical Hough transform is used. Otherwise, both these
+     * parameters should be positive.
+     * @param stn For the multi-scale Hough transform, it is a divisor for the distance resolution theta.
+     * Must fall between 0 and max_theta.
+     * Must fall between min_theta and CV_PI.
+     */
+    public static void HoughLines(Mat image, Mat lines, double rho, double theta, int threshold, double srn, double stn) {
+        HoughLines_2(image.nativeObj, lines.nativeObj, rho, theta, threshold, srn, stn);
+    }
+
+    /**
+     * Finds lines in a binary image using the standard Hough transform.
+     *
+     * The function implements the standard or standard multi-scale Hough transform algorithm for line
+     * detection. See &lt;http://homepages.inf.ed.ac.uk/rbf/HIPR2/hough.htm&gt; for a good explanation of Hough
+     * transform.
+     *
+     * @param image 8-bit, single-channel binary source image. The image may be modified by the function.
+     * @param lines Output vector of lines. Each line is represented by a 2 or 3 element vector
+     * \((\rho, \theta)\) or \((\rho, \theta, \textrm{votes})\) . \(\rho\) is the distance from the coordinate origin \((0,0)\) (top-left corner of
+     * the image). \(\theta\) is the line rotation angle in radians (
+     * \(0 \sim \textrm{vertical line}, \pi/2 \sim \textrm{horizontal line}\) ).
+     * \(\textrm{votes}\) is the value of accumulator.
+     * @param rho Distance resolution of the accumulator in pixels.
+     * @param theta Angle resolution of the accumulator in radians.
+     * @param threshold Accumulator threshold parameter. Only those lines are returned that get enough
+     * votes ( \(&gt;\texttt{threshold}\) ).
+     * @param srn For the multi-scale Hough transform, it is a divisor for the distance resolution rho .
+     * The coarse accumulator distance resolution is rho and the accurate accumulator resolution is
+     * rho/srn . If both srn=0 and stn=0 , the classical Hough transform is used. Otherwise, both these
+     * parameters should be positive.
+     * Must fall between 0 and max_theta.
+     * Must fall between min_theta and CV_PI.
+     */
+    public static void HoughLines(Mat image, Mat lines, double rho, double theta, int threshold, double srn) {
+        HoughLines_3(image.nativeObj, lines.nativeObj, rho, theta, threshold, srn);
+    }
+
+    /**
+     * Finds lines in a binary image using the standard Hough transform.
+     *
+     * The function implements the standard or standard multi-scale Hough transform algorithm for line
+     * detection. See &lt;http://homepages.inf.ed.ac.uk/rbf/HIPR2/hough.htm&gt; for a good explanation of Hough
+     * transform.
+     *
+     * @param image 8-bit, single-channel binary source image. The image may be modified by the function.
+     * @param lines Output vector of lines. Each line is represented by a 2 or 3 element vector
+     * \((\rho, \theta)\) or \((\rho, \theta, \textrm{votes})\) . \(\rho\) is the distance from the coordinate origin \((0,0)\) (top-left corner of
+     * the image). \(\theta\) is the line rotation angle in radians (
+     * \(0 \sim \textrm{vertical line}, \pi/2 \sim \textrm{horizontal line}\) ).
+     * \(\textrm{votes}\) is the value of accumulator.
+     * @param rho Distance resolution of the accumulator in pixels.
+     * @param theta Angle resolution of the accumulator in radians.
+     * @param threshold Accumulator threshold parameter. Only those lines are returned that get enough
+     * votes ( \(&gt;\texttt{threshold}\) ).
+     * The coarse accumulator distance resolution is rho and the accurate accumulator resolution is
+     * rho/srn . If both srn=0 and stn=0 , the classical Hough transform is used. Otherwise, both these
+     * parameters should be positive.
+     * Must fall between 0 and max_theta.
+     * Must fall between min_theta and CV_PI.
+     */
+    public static void HoughLines(Mat image, Mat lines, double rho, double theta, int threshold) {
+        HoughLines_4(image.nativeObj, lines.nativeObj, rho, theta, threshold);
+    }
+
+
+    //
+    // C++:  void cv::HoughLinesP(Mat image, Mat& lines, double rho, double theta, int threshold, double minLineLength = 0, double maxLineGap = 0)
+    //
+
+    /**
+     * Finds line segments in a binary image using the probabilistic Hough transform.
+     *
+     * The function implements the probabilistic Hough transform algorithm for line detection, described
+     * in CITE: Matas00
+     *
+     * See the line detection example below:
+     * INCLUDE: snippets/imgproc_HoughLinesP.cpp
+     * This is a sample picture the function parameters have been tuned for:
+     *
+     * ![image](pics/building.jpg)
+     *
+     * And this is the output of the above program in case of the probabilistic Hough transform:
+     *
+     * ![image](pics/houghp.png)
+     *
+     * @param image 8-bit, single-channel binary source image. The image may be modified by the function.
+     * @param lines Output vector of lines. Each line is represented by a 4-element vector
+     * \((x_1, y_1, x_2, y_2)\) , where \((x_1,y_1)\) and \((x_2, y_2)\) are the ending points of each detected
+     * line segment.
+     * @param rho Distance resolution of the accumulator in pixels.
+     * @param theta Angle resolution of the accumulator in radians.
+     * @param threshold Accumulator threshold parameter. Only those lines are returned that get enough
+     * votes ( \(&gt;\texttt{threshold}\) ).
+     * @param minLineLength Minimum line length. Line segments shorter than that are rejected.
+     * @param maxLineGap Maximum allowed gap between points on the same line to link them.
+     *
+     * SEE: LineSegmentDetector
+     */
+    public static void HoughLinesP(Mat image, Mat lines, double rho, double theta, int threshold, double minLineLength, double maxLineGap) {
+        HoughLinesP_0(image.nativeObj, lines.nativeObj, rho, theta, threshold, minLineLength, maxLineGap);
+    }
+
+    /**
+     * Finds line segments in a binary image using the probabilistic Hough transform.
+     *
+     * The function implements the probabilistic Hough transform algorithm for line detection, described
+     * in CITE: Matas00
+     *
+     * See the line detection example below:
+     * INCLUDE: snippets/imgproc_HoughLinesP.cpp
+     * This is a sample picture the function parameters have been tuned for:
+     *
+     * ![image](pics/building.jpg)
+     *
+     * And this is the output of the above program in case of the probabilistic Hough transform:
+     *
+     * ![image](pics/houghp.png)
+     *
+     * @param image 8-bit, single-channel binary source image. The image may be modified by the function.
+     * @param lines Output vector of lines. Each line is represented by a 4-element vector
+     * \((x_1, y_1, x_2, y_2)\) , where \((x_1,y_1)\) and \((x_2, y_2)\) are the ending points of each detected
+     * line segment.
+     * @param rho Distance resolution of the accumulator in pixels.
+     * @param theta Angle resolution of the accumulator in radians.
+     * @param threshold Accumulator threshold parameter. Only those lines are returned that get enough
+     * votes ( \(&gt;\texttt{threshold}\) ).
+     * @param minLineLength Minimum line length. Line segments shorter than that are rejected.
+     *
+     * SEE: LineSegmentDetector
+     */
+    public static void HoughLinesP(Mat image, Mat lines, double rho, double theta, int threshold, double minLineLength) {
+        HoughLinesP_1(image.nativeObj, lines.nativeObj, rho, theta, threshold, minLineLength);
+    }
+
+    /**
+     * Finds line segments in a binary image using the probabilistic Hough transform.
+     *
+     * The function implements the probabilistic Hough transform algorithm for line detection, described
+     * in CITE: Matas00
+     *
+     * See the line detection example below:
+     * INCLUDE: snippets/imgproc_HoughLinesP.cpp
+     * This is a sample picture the function parameters have been tuned for:
+     *
+     * ![image](pics/building.jpg)
+     *
+     * And this is the output of the above program in case of the probabilistic Hough transform:
+     *
+     * ![image](pics/houghp.png)
+     *
+     * @param image 8-bit, single-channel binary source image. The image may be modified by the function.
+     * @param lines Output vector of lines. Each line is represented by a 4-element vector
+     * \((x_1, y_1, x_2, y_2)\) , where \((x_1,y_1)\) and \((x_2, y_2)\) are the ending points of each detected
+     * line segment.
+     * @param rho Distance resolution of the accumulator in pixels.
+     * @param theta Angle resolution of the accumulator in radians.
+     * @param threshold Accumulator threshold parameter. Only those lines are returned that get enough
+     * votes ( \(&gt;\texttt{threshold}\) ).
+     *
+     * SEE: LineSegmentDetector
+     */
+    public static void HoughLinesP(Mat image, Mat lines, double rho, double theta, int threshold) {
+        HoughLinesP_2(image.nativeObj, lines.nativeObj, rho, theta, threshold);
+    }
+
+
+    //
+    // C++:  void cv::HoughLinesPointSet(Mat _point, Mat& _lines, int lines_max, int threshold, double min_rho, double max_rho, double rho_step, double min_theta, double max_theta, double theta_step)
+    //
+
+    /**
+     * Finds lines in a set of points using the standard Hough transform.
+     *
+     * The function finds lines in a set of points using a modification of the Hough transform.
+     * INCLUDE: snippets/imgproc_HoughLinesPointSet.cpp
+     * @param _point Input vector of points. Each vector must be encoded as a Point vector \((x,y)\). Type must be CV_32FC2 or CV_32SC2.
+     * @param _lines Output vector of found lines. Each vector is encoded as a vector&lt;Vec3d&gt; \((votes, rho, theta)\).
+     * The larger the value of 'votes', the higher the reliability of the Hough line.
+     * @param lines_max Max count of hough lines.
+     * @param threshold Accumulator threshold parameter. Only those lines are returned that get enough
+     * votes ( \(&gt;\texttt{threshold}\) )
+     * @param min_rho Minimum Distance value of the accumulator in pixels.
+     * @param max_rho Maximum Distance value of the accumulator in pixels.
+     * @param rho_step Distance resolution of the accumulator in pixels.
+     * @param min_theta Minimum angle value of the accumulator in radians.
+     * @param max_theta Maximum angle value of the accumulator in radians.
+     * @param theta_step Angle resolution of the accumulator in radians.
+     */
+    public static void HoughLinesPointSet(Mat _point, Mat _lines, int lines_max, int threshold, double min_rho, double max_rho, double rho_step, double min_theta, double max_theta, double theta_step) {
+        HoughLinesPointSet_0(_point.nativeObj, _lines.nativeObj, lines_max, threshold, min_rho, max_rho, rho_step, min_theta, max_theta, theta_step);
+    }
+
+
+    //
+    // C++:  void cv::HuMoments(Moments m, Mat& hu)
+    //
+
+    public static void HuMoments(Moments m, Mat hu) {
+        HuMoments_0(m.m00, m.m10, m.m01, m.m20, m.m11, m.m02, m.m30, m.m21, m.m12, m.m03, hu.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::Laplacian(Mat src, Mat& dst, int ddepth, int ksize = 1, double scale = 1, double delta = 0, int borderType = BORDER_DEFAULT)
+    //
+
+    /**
+     * Calculates the Laplacian of an image.
+     *
+     * The function calculates the Laplacian of the source image by adding up the second x and y
+     * derivatives calculated using the Sobel operator:
+     *
+     * \(\texttt{dst} =  \Delta \texttt{src} =  \frac{\partial^2 \texttt{src}}{\partial x^2} +  \frac{\partial^2 \texttt{src}}{\partial y^2}\)
+     *
+     * This is done when {@code ksize &gt; 1}. When {@code ksize == 1}, the Laplacian is computed by filtering the image
+     * with the following \(3 \times 3\) aperture:
+     *
+     * \(\vecthreethree {0}{1}{0}{1}{-4}{1}{0}{1}{0}\)
+     *
+     * @param src Source image.
+     * @param dst Destination image of the same size and the same number of channels as src .
+     * @param ddepth Desired depth of the destination image.
+     * @param ksize Aperture size used to compute the second-derivative filters. See #getDerivKernels for
+     * details. The size must be positive and odd.
+     * @param scale Optional scale factor for the computed Laplacian values. By default, no scaling is
+     * applied. See #getDerivKernels for details.
+     * @param delta Optional delta value that is added to the results prior to storing them in dst .
+     * @param borderType Pixel extrapolation method, see #BorderTypes. #BORDER_WRAP is not supported.
+     * SEE:  Sobel, Scharr
+     */
+    public static void Laplacian(Mat src, Mat dst, int ddepth, int ksize, double scale, double delta, int borderType) {
+        Laplacian_0(src.nativeObj, dst.nativeObj, ddepth, ksize, scale, delta, borderType);
+    }
+
+    /**
+     * Calculates the Laplacian of an image.
+     *
+     * The function calculates the Laplacian of the source image by adding up the second x and y
+     * derivatives calculated using the Sobel operator:
+     *
+     * \(\texttt{dst} =  \Delta \texttt{src} =  \frac{\partial^2 \texttt{src}}{\partial x^2} +  \frac{\partial^2 \texttt{src}}{\partial y^2}\)
+     *
+     * This is done when {@code ksize &gt; 1}. When {@code ksize == 1}, the Laplacian is computed by filtering the image
+     * with the following \(3 \times 3\) aperture:
+     *
+     * \(\vecthreethree {0}{1}{0}{1}{-4}{1}{0}{1}{0}\)
+     *
+     * @param src Source image.
+     * @param dst Destination image of the same size and the same number of channels as src .
+     * @param ddepth Desired depth of the destination image.
+     * @param ksize Aperture size used to compute the second-derivative filters. See #getDerivKernels for
+     * details. The size must be positive and odd.
+     * @param scale Optional scale factor for the computed Laplacian values. By default, no scaling is
+     * applied. See #getDerivKernels for details.
+     * @param delta Optional delta value that is added to the results prior to storing them in dst .
+     * SEE:  Sobel, Scharr
+     */
+    public static void Laplacian(Mat src, Mat dst, int ddepth, int ksize, double scale, double delta) {
+        Laplacian_1(src.nativeObj, dst.nativeObj, ddepth, ksize, scale, delta);
+    }
+
+    /**
+     * Calculates the Laplacian of an image.
+     *
+     * The function calculates the Laplacian of the source image by adding up the second x and y
+     * derivatives calculated using the Sobel operator:
+     *
+     * \(\texttt{dst} =  \Delta \texttt{src} =  \frac{\partial^2 \texttt{src}}{\partial x^2} +  \frac{\partial^2 \texttt{src}}{\partial y^2}\)
+     *
+     * This is done when {@code ksize &gt; 1}. When {@code ksize == 1}, the Laplacian is computed by filtering the image
+     * with the following \(3 \times 3\) aperture:
+     *
+     * \(\vecthreethree {0}{1}{0}{1}{-4}{1}{0}{1}{0}\)
+     *
+     * @param src Source image.
+     * @param dst Destination image of the same size and the same number of channels as src .
+     * @param ddepth Desired depth of the destination image.
+     * @param ksize Aperture size used to compute the second-derivative filters. See #getDerivKernels for
+     * details. The size must be positive and odd.
+     * @param scale Optional scale factor for the computed Laplacian values. By default, no scaling is
+     * applied. See #getDerivKernels for details.
+     * SEE:  Sobel, Scharr
+     */
+    public static void Laplacian(Mat src, Mat dst, int ddepth, int ksize, double scale) {
+        Laplacian_2(src.nativeObj, dst.nativeObj, ddepth, ksize, scale);
+    }
+
+    /**
+     * Calculates the Laplacian of an image.
+     *
+     * The function calculates the Laplacian of the source image by adding up the second x and y
+     * derivatives calculated using the Sobel operator:
+     *
+     * \(\texttt{dst} =  \Delta \texttt{src} =  \frac{\partial^2 \texttt{src}}{\partial x^2} +  \frac{\partial^2 \texttt{src}}{\partial y^2}\)
+     *
+     * This is done when {@code ksize &gt; 1}. When {@code ksize == 1}, the Laplacian is computed by filtering the image
+     * with the following \(3 \times 3\) aperture:
+     *
+     * \(\vecthreethree {0}{1}{0}{1}{-4}{1}{0}{1}{0}\)
+     *
+     * @param src Source image.
+     * @param dst Destination image of the same size and the same number of channels as src .
+     * @param ddepth Desired depth of the destination image.
+     * @param ksize Aperture size used to compute the second-derivative filters. See #getDerivKernels for
+     * details. The size must be positive and odd.
+     * applied. See #getDerivKernels for details.
+     * SEE:  Sobel, Scharr
+     */
+    public static void Laplacian(Mat src, Mat dst, int ddepth, int ksize) {
+        Laplacian_3(src.nativeObj, dst.nativeObj, ddepth, ksize);
+    }
+
+    /**
+     * Calculates the Laplacian of an image.
+     *
+     * The function calculates the Laplacian of the source image by adding up the second x and y
+     * derivatives calculated using the Sobel operator:
+     *
+     * \(\texttt{dst} =  \Delta \texttt{src} =  \frac{\partial^2 \texttt{src}}{\partial x^2} +  \frac{\partial^2 \texttt{src}}{\partial y^2}\)
+     *
+     * This is done when {@code ksize &gt; 1}. When {@code ksize == 1}, the Laplacian is computed by filtering the image
+     * with the following \(3 \times 3\) aperture:
+     *
+     * \(\vecthreethree {0}{1}{0}{1}{-4}{1}{0}{1}{0}\)
+     *
+     * @param src Source image.
+     * @param dst Destination image of the same size and the same number of channels as src .
+     * @param ddepth Desired depth of the destination image.
+     * details. The size must be positive and odd.
+     * applied. See #getDerivKernels for details.
+     * SEE:  Sobel, Scharr
+     */
+    public static void Laplacian(Mat src, Mat dst, int ddepth) {
+        Laplacian_4(src.nativeObj, dst.nativeObj, ddepth);
+    }
+
+
+    //
+    // C++:  void cv::Scharr(Mat src, Mat& dst, int ddepth, int dx, int dy, double scale = 1, double delta = 0, int borderType = BORDER_DEFAULT)
+    //
+
+    /**
+     * Calculates the first x- or y- image derivative using Scharr operator.
+     *
+     * The function computes the first x- or y- spatial image derivative using the Scharr operator. The
+     * call
+     *
+     * \(\texttt{Scharr(src, dst, ddepth, dx, dy, scale, delta, borderType)}\)
+     *
+     * is equivalent to
+     *
+     * \(\texttt{Sobel(src, dst, ddepth, dx, dy, CV_SCHARR, scale, delta, borderType)} .\)
+     *
+     * @param src input image.
+     * @param dst output image of the same size and the same number of channels as src.
+     * @param ddepth output image depth, see REF: filter_depths "combinations"
+     * @param dx order of the derivative x.
+     * @param dy order of the derivative y.
+     * @param scale optional scale factor for the computed derivative values; by default, no scaling is
+     * applied (see #getDerivKernels for details).
+     * @param delta optional delta value that is added to the results prior to storing them in dst.
+     * @param borderType pixel extrapolation method, see #BorderTypes. #BORDER_WRAP is not supported.
+     * SEE:  cartToPolar
+     */
+    public static void Scharr(Mat src, Mat dst, int ddepth, int dx, int dy, double scale, double delta, int borderType) {
+        Scharr_0(src.nativeObj, dst.nativeObj, ddepth, dx, dy, scale, delta, borderType);
+    }
+
+    /**
+     * Calculates the first x- or y- image derivative using Scharr operator.
+     *
+     * The function computes the first x- or y- spatial image derivative using the Scharr operator. The
+     * call
+     *
+     * \(\texttt{Scharr(src, dst, ddepth, dx, dy, scale, delta, borderType)}\)
+     *
+     * is equivalent to
+     *
+     * \(\texttt{Sobel(src, dst, ddepth, dx, dy, CV_SCHARR, scale, delta, borderType)} .\)
+     *
+     * @param src input image.
+     * @param dst output image of the same size and the same number of channels as src.
+     * @param ddepth output image depth, see REF: filter_depths "combinations"
+     * @param dx order of the derivative x.
+     * @param dy order of the derivative y.
+     * @param scale optional scale factor for the computed derivative values; by default, no scaling is
+     * applied (see #getDerivKernels for details).
+     * @param delta optional delta value that is added to the results prior to storing them in dst.
+     * SEE:  cartToPolar
+     */
+    public static void Scharr(Mat src, Mat dst, int ddepth, int dx, int dy, double scale, double delta) {
+        Scharr_1(src.nativeObj, dst.nativeObj, ddepth, dx, dy, scale, delta);
+    }
+
+    /**
+     * Calculates the first x- or y- image derivative using Scharr operator.
+     *
+     * The function computes the first x- or y- spatial image derivative using the Scharr operator. The
+     * call
+     *
+     * \(\texttt{Scharr(src, dst, ddepth, dx, dy, scale, delta, borderType)}\)
+     *
+     * is equivalent to
+     *
+     * \(\texttt{Sobel(src, dst, ddepth, dx, dy, CV_SCHARR, scale, delta, borderType)} .\)
+     *
+     * @param src input image.
+     * @param dst output image of the same size and the same number of channels as src.
+     * @param ddepth output image depth, see REF: filter_depths "combinations"
+     * @param dx order of the derivative x.
+     * @param dy order of the derivative y.
+     * @param scale optional scale factor for the computed derivative values; by default, no scaling is
+     * applied (see #getDerivKernels for details).
+     * SEE:  cartToPolar
+     */
+    public static void Scharr(Mat src, Mat dst, int ddepth, int dx, int dy, double scale) {
+        Scharr_2(src.nativeObj, dst.nativeObj, ddepth, dx, dy, scale);
+    }
+
+    /**
+     * Calculates the first x- or y- image derivative using Scharr operator.
+     *
+     * The function computes the first x- or y- spatial image derivative using the Scharr operator. The
+     * call
+     *
+     * \(\texttt{Scharr(src, dst, ddepth, dx, dy, scale, delta, borderType)}\)
+     *
+     * is equivalent to
+     *
+     * \(\texttt{Sobel(src, dst, ddepth, dx, dy, CV_SCHARR, scale, delta, borderType)} .\)
+     *
+     * @param src input image.
+     * @param dst output image of the same size and the same number of channels as src.
+     * @param ddepth output image depth, see REF: filter_depths "combinations"
+     * @param dx order of the derivative x.
+     * @param dy order of the derivative y.
+     * applied (see #getDerivKernels for details).
+     * SEE:  cartToPolar
+     */
+    public static void Scharr(Mat src, Mat dst, int ddepth, int dx, int dy) {
+        Scharr_3(src.nativeObj, dst.nativeObj, ddepth, dx, dy);
+    }
+
+
+    //
+    // C++:  void cv::Sobel(Mat src, Mat& dst, int ddepth, int dx, int dy, int ksize = 3, double scale = 1, double delta = 0, int borderType = BORDER_DEFAULT)
+    //
+
+    /**
+     * Calculates the first, second, third, or mixed image derivatives using an extended Sobel operator.
+     *
+     * In all cases except one, the \(\texttt{ksize} \times \texttt{ksize}\) separable kernel is used to
+     * calculate the derivative. When \(\texttt{ksize = 1}\), the \(3 \times 1\) or \(1 \times 3\)
+     * kernel is used (that is, no Gaussian smoothing is done). {@code ksize = 1} can only be used for the first
+     * or the second x- or y- derivatives.
+     *
+     * There is also the special value {@code ksize = #CV_SCHARR (-1)} that corresponds to the \(3\times3\) Scharr
+     * filter that may give more accurate results than the \(3\times3\) Sobel. The Scharr aperture is
+     *
+     * \(\vecthreethree{-3}{0}{3}{-10}{0}{10}{-3}{0}{3}\)
+     *
+     * for the x-derivative, or transposed for the y-derivative.
+     *
+     * The function calculates an image derivative by convolving the image with the appropriate kernel:
+     *
+     * \(\texttt{dst} =  \frac{\partial^{xorder+yorder} \texttt{src}}{\partial x^{xorder} \partial y^{yorder}}\)
+     *
+     * The Sobel operators combine Gaussian smoothing and differentiation, so the result is more or less
+     * resistant to the noise. Most often, the function is called with ( xorder = 1, yorder = 0, ksize = 3)
+     * or ( xorder = 0, yorder = 1, ksize = 3) to calculate the first x- or y- image derivative. The first
+     * case corresponds to a kernel of:
+     *
+     * \(\vecthreethree{-1}{0}{1}{-2}{0}{2}{-1}{0}{1}\)
+     *
+     * The second case corresponds to a kernel of:
+     *
+     * \(\vecthreethree{-1}{-2}{-1}{0}{0}{0}{1}{2}{1}\)
+     *
+     * @param src input image.
+     * @param dst output image of the same size and the same number of channels as src .
+     * @param ddepth output image depth, see REF: filter_depths "combinations"; in the case of
+     *     8-bit input images it will result in truncated derivatives.
+     * @param dx order of the derivative x.
+     * @param dy order of the derivative y.
+     * @param ksize size of the extended Sobel kernel; it must be 1, 3, 5, or 7.
+     * @param scale optional scale factor for the computed derivative values; by default, no scaling is
+     * applied (see #getDerivKernels for details).
+     * @param delta optional delta value that is added to the results prior to storing them in dst.
+     * @param borderType pixel extrapolation method, see #BorderTypes. #BORDER_WRAP is not supported.
+     * SEE:  Scharr, Laplacian, sepFilter2D, filter2D, GaussianBlur, cartToPolar
+     */
+    public static void Sobel(Mat src, Mat dst, int ddepth, int dx, int dy, int ksize, double scale, double delta, int borderType) {
+        Sobel_0(src.nativeObj, dst.nativeObj, ddepth, dx, dy, ksize, scale, delta, borderType);
+    }
+
+    /**
+     * Calculates the first, second, third, or mixed image derivatives using an extended Sobel operator.
+     *
+     * In all cases except one, the \(\texttt{ksize} \times \texttt{ksize}\) separable kernel is used to
+     * calculate the derivative. When \(\texttt{ksize = 1}\), the \(3 \times 1\) or \(1 \times 3\)
+     * kernel is used (that is, no Gaussian smoothing is done). {@code ksize = 1} can only be used for the first
+     * or the second x- or y- derivatives.
+     *
+     * There is also the special value {@code ksize = #CV_SCHARR (-1)} that corresponds to the \(3\times3\) Scharr
+     * filter that may give more accurate results than the \(3\times3\) Sobel. The Scharr aperture is
+     *
+     * \(\vecthreethree{-3}{0}{3}{-10}{0}{10}{-3}{0}{3}\)
+     *
+     * for the x-derivative, or transposed for the y-derivative.
+     *
+     * The function calculates an image derivative by convolving the image with the appropriate kernel:
+     *
+     * \(\texttt{dst} =  \frac{\partial^{xorder+yorder} \texttt{src}}{\partial x^{xorder} \partial y^{yorder}}\)
+     *
+     * The Sobel operators combine Gaussian smoothing and differentiation, so the result is more or less
+     * resistant to the noise. Most often, the function is called with ( xorder = 1, yorder = 0, ksize = 3)
+     * or ( xorder = 0, yorder = 1, ksize = 3) to calculate the first x- or y- image derivative. The first
+     * case corresponds to a kernel of:
+     *
+     * \(\vecthreethree{-1}{0}{1}{-2}{0}{2}{-1}{0}{1}\)
+     *
+     * The second case corresponds to a kernel of:
+     *
+     * \(\vecthreethree{-1}{-2}{-1}{0}{0}{0}{1}{2}{1}\)
+     *
+     * @param src input image.
+     * @param dst output image of the same size and the same number of channels as src .
+     * @param ddepth output image depth, see REF: filter_depths "combinations"; in the case of
+     *     8-bit input images it will result in truncated derivatives.
+     * @param dx order of the derivative x.
+     * @param dy order of the derivative y.
+     * @param ksize size of the extended Sobel kernel; it must be 1, 3, 5, or 7.
+     * @param scale optional scale factor for the computed derivative values; by default, no scaling is
+     * applied (see #getDerivKernels for details).
+     * @param delta optional delta value that is added to the results prior to storing them in dst.
+     * SEE:  Scharr, Laplacian, sepFilter2D, filter2D, GaussianBlur, cartToPolar
+     */
+    public static void Sobel(Mat src, Mat dst, int ddepth, int dx, int dy, int ksize, double scale, double delta) {
+        Sobel_1(src.nativeObj, dst.nativeObj, ddepth, dx, dy, ksize, scale, delta);
+    }
+
+    /**
+     * Calculates the first, second, third, or mixed image derivatives using an extended Sobel operator.
+     *
+     * In all cases except one, the \(\texttt{ksize} \times \texttt{ksize}\) separable kernel is used to
+     * calculate the derivative. When \(\texttt{ksize = 1}\), the \(3 \times 1\) or \(1 \times 3\)
+     * kernel is used (that is, no Gaussian smoothing is done). {@code ksize = 1} can only be used for the first
+     * or the second x- or y- derivatives.
+     *
+     * There is also the special value {@code ksize = #CV_SCHARR (-1)} that corresponds to the \(3\times3\) Scharr
+     * filter that may give more accurate results than the \(3\times3\) Sobel. The Scharr aperture is
+     *
+     * \(\vecthreethree{-3}{0}{3}{-10}{0}{10}{-3}{0}{3}\)
+     *
+     * for the x-derivative, or transposed for the y-derivative.
+     *
+     * The function calculates an image derivative by convolving the image with the appropriate kernel:
+     *
+     * \(\texttt{dst} =  \frac{\partial^{xorder+yorder} \texttt{src}}{\partial x^{xorder} \partial y^{yorder}}\)
+     *
+     * The Sobel operators combine Gaussian smoothing and differentiation, so the result is more or less
+     * resistant to the noise. Most often, the function is called with ( xorder = 1, yorder = 0, ksize = 3)
+     * or ( xorder = 0, yorder = 1, ksize = 3) to calculate the first x- or y- image derivative. The first
+     * case corresponds to a kernel of:
+     *
+     * \(\vecthreethree{-1}{0}{1}{-2}{0}{2}{-1}{0}{1}\)
+     *
+     * The second case corresponds to a kernel of:
+     *
+     * \(\vecthreethree{-1}{-2}{-1}{0}{0}{0}{1}{2}{1}\)
+     *
+     * @param src input image.
+     * @param dst output image of the same size and the same number of channels as src .
+     * @param ddepth output image depth, see REF: filter_depths "combinations"; in the case of
+     *     8-bit input images it will result in truncated derivatives.
+     * @param dx order of the derivative x.
+     * @param dy order of the derivative y.
+     * @param ksize size of the extended Sobel kernel; it must be 1, 3, 5, or 7.
+     * @param scale optional scale factor for the computed derivative values; by default, no scaling is
+     * applied (see #getDerivKernels for details).
+     * SEE:  Scharr, Laplacian, sepFilter2D, filter2D, GaussianBlur, cartToPolar
+     */
+    public static void Sobel(Mat src, Mat dst, int ddepth, int dx, int dy, int ksize, double scale) {
+        Sobel_2(src.nativeObj, dst.nativeObj, ddepth, dx, dy, ksize, scale);
+    }
+
+    /**
+     * Calculates the first, second, third, or mixed image derivatives using an extended Sobel operator.
+     *
+     * In all cases except one, the \(\texttt{ksize} \times \texttt{ksize}\) separable kernel is used to
+     * calculate the derivative. When \(\texttt{ksize = 1}\), the \(3 \times 1\) or \(1 \times 3\)
+     * kernel is used (that is, no Gaussian smoothing is done). {@code ksize = 1} can only be used for the first
+     * or the second x- or y- derivatives.
+     *
+     * There is also the special value {@code ksize = #CV_SCHARR (-1)} that corresponds to the \(3\times3\) Scharr
+     * filter that may give more accurate results than the \(3\times3\) Sobel. The Scharr aperture is
+     *
+     * \(\vecthreethree{-3}{0}{3}{-10}{0}{10}{-3}{0}{3}\)
+     *
+     * for the x-derivative, or transposed for the y-derivative.
+     *
+     * The function calculates an image derivative by convolving the image with the appropriate kernel:
+     *
+     * \(\texttt{dst} =  \frac{\partial^{xorder+yorder} \texttt{src}}{\partial x^{xorder} \partial y^{yorder}}\)
+     *
+     * The Sobel operators combine Gaussian smoothing and differentiation, so the result is more or less
+     * resistant to the noise. Most often, the function is called with ( xorder = 1, yorder = 0, ksize = 3)
+     * or ( xorder = 0, yorder = 1, ksize = 3) to calculate the first x- or y- image derivative. The first
+     * case corresponds to a kernel of:
+     *
+     * \(\vecthreethree{-1}{0}{1}{-2}{0}{2}{-1}{0}{1}\)
+     *
+     * The second case corresponds to a kernel of:
+     *
+     * \(\vecthreethree{-1}{-2}{-1}{0}{0}{0}{1}{2}{1}\)
+     *
+     * @param src input image.
+     * @param dst output image of the same size and the same number of channels as src .
+     * @param ddepth output image depth, see REF: filter_depths "combinations"; in the case of
+     *     8-bit input images it will result in truncated derivatives.
+     * @param dx order of the derivative x.
+     * @param dy order of the derivative y.
+     * @param ksize size of the extended Sobel kernel; it must be 1, 3, 5, or 7.
+     * applied (see #getDerivKernels for details).
+     * SEE:  Scharr, Laplacian, sepFilter2D, filter2D, GaussianBlur, cartToPolar
+     */
+    public static void Sobel(Mat src, Mat dst, int ddepth, int dx, int dy, int ksize) {
+        Sobel_3(src.nativeObj, dst.nativeObj, ddepth, dx, dy, ksize);
+    }
+
+    /**
+     * Calculates the first, second, third, or mixed image derivatives using an extended Sobel operator.
+     *
+     * In all cases except one, the \(\texttt{ksize} \times \texttt{ksize}\) separable kernel is used to
+     * calculate the derivative. When \(\texttt{ksize = 1}\), the \(3 \times 1\) or \(1 \times 3\)
+     * kernel is used (that is, no Gaussian smoothing is done). {@code ksize = 1} can only be used for the first
+     * or the second x- or y- derivatives.
+     *
+     * There is also the special value {@code ksize = #CV_SCHARR (-1)} that corresponds to the \(3\times3\) Scharr
+     * filter that may give more accurate results than the \(3\times3\) Sobel. The Scharr aperture is
+     *
+     * \(\vecthreethree{-3}{0}{3}{-10}{0}{10}{-3}{0}{3}\)
+     *
+     * for the x-derivative, or transposed for the y-derivative.
+     *
+     * The function calculates an image derivative by convolving the image with the appropriate kernel:
+     *
+     * \(\texttt{dst} =  \frac{\partial^{xorder+yorder} \texttt{src}}{\partial x^{xorder} \partial y^{yorder}}\)
+     *
+     * The Sobel operators combine Gaussian smoothing and differentiation, so the result is more or less
+     * resistant to the noise. Most often, the function is called with ( xorder = 1, yorder = 0, ksize = 3)
+     * or ( xorder = 0, yorder = 1, ksize = 3) to calculate the first x- or y- image derivative. The first
+     * case corresponds to a kernel of:
+     *
+     * \(\vecthreethree{-1}{0}{1}{-2}{0}{2}{-1}{0}{1}\)
+     *
+     * The second case corresponds to a kernel of:
+     *
+     * \(\vecthreethree{-1}{-2}{-1}{0}{0}{0}{1}{2}{1}\)
+     *
+     * @param src input image.
+     * @param dst output image of the same size and the same number of channels as src .
+     * @param ddepth output image depth, see REF: filter_depths "combinations"; in the case of
+     *     8-bit input images it will result in truncated derivatives.
+     * @param dx order of the derivative x.
+     * @param dy order of the derivative y.
+     * applied (see #getDerivKernels for details).
+     * SEE:  Scharr, Laplacian, sepFilter2D, filter2D, GaussianBlur, cartToPolar
+     */
+    public static void Sobel(Mat src, Mat dst, int ddepth, int dx, int dy) {
+        Sobel_4(src.nativeObj, dst.nativeObj, ddepth, dx, dy);
+    }
+
+
+    //
+    // C++:  void cv::accumulate(Mat src, Mat& dst, Mat mask = Mat())
+    //
+
+    /**
+     * Adds an image to the accumulator image.
+     *
+     * The function adds src or some of its elements to dst :
+     *
+     * \(\texttt{dst} (x,y)  \leftarrow \texttt{dst} (x,y) +  \texttt{src} (x,y)  \quad \text{if} \quad \texttt{mask} (x,y)  \ne 0\)
+     *
+     * The function supports multi-channel images. Each channel is processed independently.
+     *
+     * The function cv::accumulate can be used, for example, to collect statistics of a scene background
+     * viewed by a still camera and for the further foreground-background segmentation.
+     *
+     * @param src Input image of type CV_8UC(n), CV_16UC(n), CV_32FC(n) or CV_64FC(n), where n is a positive integer.
+     * @param dst %Accumulator image with the same number of channels as input image, and a depth of CV_32F or CV_64F.
+     * @param mask Optional operation mask.
+     *
+     * SEE:  accumulateSquare, accumulateProduct, accumulateWeighted
+     */
+    public static void accumulate(Mat src, Mat dst, Mat mask) {
+        accumulate_0(src.nativeObj, dst.nativeObj, mask.nativeObj);
+    }
+
+    /**
+     * Adds an image to the accumulator image.
+     *
+     * The function adds src or some of its elements to dst :
+     *
+     * \(\texttt{dst} (x,y)  \leftarrow \texttt{dst} (x,y) +  \texttt{src} (x,y)  \quad \text{if} \quad \texttt{mask} (x,y)  \ne 0\)
+     *
+     * The function supports multi-channel images. Each channel is processed independently.
+     *
+     * The function cv::accumulate can be used, for example, to collect statistics of a scene background
+     * viewed by a still camera and for the further foreground-background segmentation.
+     *
+     * @param src Input image of type CV_8UC(n), CV_16UC(n), CV_32FC(n) or CV_64FC(n), where n is a positive integer.
+     * @param dst %Accumulator image with the same number of channels as input image, and a depth of CV_32F or CV_64F.
+     *
+     * SEE:  accumulateSquare, accumulateProduct, accumulateWeighted
+     */
+    public static void accumulate(Mat src, Mat dst) {
+        accumulate_1(src.nativeObj, dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::accumulateProduct(Mat src1, Mat src2, Mat& dst, Mat mask = Mat())
+    //
+
+    /**
+     * Adds the per-element product of two input images to the accumulator image.
+     *
+     * The function adds the product of two images or their selected regions to the accumulator dst :
+     *
+     * \(\texttt{dst} (x,y)  \leftarrow \texttt{dst} (x,y) +  \texttt{src1} (x,y)  \cdot \texttt{src2} (x,y)  \quad \text{if} \quad \texttt{mask} (x,y)  \ne 0\)
+     *
+     * The function supports multi-channel images. Each channel is processed independently.
+     *
+     * @param src1 First input image, 1- or 3-channel, 8-bit or 32-bit floating point.
+     * @param src2 Second input image of the same type and the same size as src1 .
+     * @param dst %Accumulator image with the same number of channels as input images, 32-bit or 64-bit
+     * floating-point.
+     * @param mask Optional operation mask.
+     *
+     * SEE:  accumulate, accumulateSquare, accumulateWeighted
+     */
+    public static void accumulateProduct(Mat src1, Mat src2, Mat dst, Mat mask) {
+        accumulateProduct_0(src1.nativeObj, src2.nativeObj, dst.nativeObj, mask.nativeObj);
+    }
+
+    /**
+     * Adds the per-element product of two input images to the accumulator image.
+     *
+     * The function adds the product of two images or their selected regions to the accumulator dst :
+     *
+     * \(\texttt{dst} (x,y)  \leftarrow \texttt{dst} (x,y) +  \texttt{src1} (x,y)  \cdot \texttt{src2} (x,y)  \quad \text{if} \quad \texttt{mask} (x,y)  \ne 0\)
+     *
+     * The function supports multi-channel images. Each channel is processed independently.
+     *
+     * @param src1 First input image, 1- or 3-channel, 8-bit or 32-bit floating point.
+     * @param src2 Second input image of the same type and the same size as src1 .
+     * @param dst %Accumulator image with the same number of channels as input images, 32-bit or 64-bit
+     * floating-point.
+     *
+     * SEE:  accumulate, accumulateSquare, accumulateWeighted
+     */
+    public static void accumulateProduct(Mat src1, Mat src2, Mat dst) {
+        accumulateProduct_1(src1.nativeObj, src2.nativeObj, dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::accumulateSquare(Mat src, Mat& dst, Mat mask = Mat())
+    //
+
+    /**
+     * Adds the square of a source image to the accumulator image.
+     *
+     * The function adds the input image src or its selected region, raised to a power of 2, to the
+     * accumulator dst :
+     *
+     * \(\texttt{dst} (x,y)  \leftarrow \texttt{dst} (x,y) +  \texttt{src} (x,y)^2  \quad \text{if} \quad \texttt{mask} (x,y)  \ne 0\)
+     *
+     * The function supports multi-channel images. Each channel is processed independently.
+     *
+     * @param src Input image as 1- or 3-channel, 8-bit or 32-bit floating point.
+     * @param dst %Accumulator image with the same number of channels as input image, 32-bit or 64-bit
+     * floating-point.
+     * @param mask Optional operation mask.
+     *
+     * SEE:  accumulateSquare, accumulateProduct, accumulateWeighted
+     */
+    public static void accumulateSquare(Mat src, Mat dst, Mat mask) {
+        accumulateSquare_0(src.nativeObj, dst.nativeObj, mask.nativeObj);
+    }
+
+    /**
+     * Adds the square of a source image to the accumulator image.
+     *
+     * The function adds the input image src or its selected region, raised to a power of 2, to the
+     * accumulator dst :
+     *
+     * \(\texttt{dst} (x,y)  \leftarrow \texttt{dst} (x,y) +  \texttt{src} (x,y)^2  \quad \text{if} \quad \texttt{mask} (x,y)  \ne 0\)
+     *
+     * The function supports multi-channel images. Each channel is processed independently.
+     *
+     * @param src Input image as 1- or 3-channel, 8-bit or 32-bit floating point.
+     * @param dst %Accumulator image with the same number of channels as input image, 32-bit or 64-bit
+     * floating-point.
+     *
+     * SEE:  accumulateSquare, accumulateProduct, accumulateWeighted
+     */
+    public static void accumulateSquare(Mat src, Mat dst) {
+        accumulateSquare_1(src.nativeObj, dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::accumulateWeighted(Mat src, Mat& dst, double alpha, Mat mask = Mat())
+    //
+
+    /**
+     * Updates a running average.
+     *
+     * The function calculates the weighted sum of the input image src and the accumulator dst so that dst
+     * becomes a running average of a frame sequence:
+     *
+     * \(\texttt{dst} (x,y)  \leftarrow (1- \texttt{alpha} )  \cdot \texttt{dst} (x,y) +  \texttt{alpha} \cdot \texttt{src} (x,y)  \quad \text{if} \quad \texttt{mask} (x,y)  \ne 0\)
+     *
+     * That is, alpha regulates the update speed (how fast the accumulator "forgets" about earlier images).
+     * The function supports multi-channel images. Each channel is processed independently.
+     *
+     * @param src Input image as 1- or 3-channel, 8-bit or 32-bit floating point.
+     * @param dst %Accumulator image with the same number of channels as input image, 32-bit or 64-bit
+     * floating-point.
+     * @param alpha Weight of the input image.
+     * @param mask Optional operation mask.
+     *
+     * SEE:  accumulate, accumulateSquare, accumulateProduct
+     */
+    public static void accumulateWeighted(Mat src, Mat dst, double alpha, Mat mask) {
+        accumulateWeighted_0(src.nativeObj, dst.nativeObj, alpha, mask.nativeObj);
+    }
+
+    /**
+     * Updates a running average.
+     *
+     * The function calculates the weighted sum of the input image src and the accumulator dst so that dst
+     * becomes a running average of a frame sequence:
+     *
+     * \(\texttt{dst} (x,y)  \leftarrow (1- \texttt{alpha} )  \cdot \texttt{dst} (x,y) +  \texttt{alpha} \cdot \texttt{src} (x,y)  \quad \text{if} \quad \texttt{mask} (x,y)  \ne 0\)
+     *
+     * That is, alpha regulates the update speed (how fast the accumulator "forgets" about earlier images).
+     * The function supports multi-channel images. Each channel is processed independently.
+     *
+     * @param src Input image as 1- or 3-channel, 8-bit or 32-bit floating point.
+     * @param dst %Accumulator image with the same number of channels as input image, 32-bit or 64-bit
+     * floating-point.
+     * @param alpha Weight of the input image.
+     *
+     * SEE:  accumulate, accumulateSquare, accumulateProduct
+     */
+    public static void accumulateWeighted(Mat src, Mat dst, double alpha) {
+        accumulateWeighted_1(src.nativeObj, dst.nativeObj, alpha);
+    }
+
+
+    //
+    // C++:  void cv::adaptiveThreshold(Mat src, Mat& dst, double maxValue, int adaptiveMethod, int thresholdType, int blockSize, double C)
+    //
+
+    /**
+     * Applies an adaptive threshold to an array.
+     *
+     * The function transforms a grayscale image to a binary image according to the formulae:
+     * <ul>
+     *   <li>
+     *    <b>THRESH_BINARY</b>
+     *     \(dst(x,y) =  \fork{\texttt{maxValue}}{if \(src(x,y) &gt; T(x,y)\)}{0}{otherwise}\)
+     *   </li>
+     *   <li>
+     *    <b>THRESH_BINARY_INV</b>
+     *     \(dst(x,y) =  \fork{0}{if \(src(x,y) &gt; T(x,y)\)}{\texttt{maxValue}}{otherwise}\)
+     * where \(T(x,y)\) is a threshold calculated individually for each pixel (see adaptiveMethod parameter).
+     *   </li>
+     * </ul>
+     *
+     * The function can process the image in-place.
+     *
+     * @param src Source 8-bit single-channel image.
+     * @param dst Destination image of the same size and the same type as src.
+     * @param maxValue Non-zero value assigned to the pixels for which the condition is satisfied
+     * @param adaptiveMethod Adaptive thresholding algorithm to use, see #AdaptiveThresholdTypes.
+     * The #BORDER_REPLICATE | #BORDER_ISOLATED is used to process boundaries.
+     * @param thresholdType Thresholding type that must be either #THRESH_BINARY or #THRESH_BINARY_INV,
+     * see #ThresholdTypes.
+     * @param blockSize Size of a pixel neighborhood that is used to calculate a threshold value for the
+     * pixel: 3, 5, 7, and so on.
+     * @param C Constant subtracted from the mean or weighted mean (see the details below). Normally, it
+     * is positive but may be zero or negative as well.
+     *
+     * SEE:  threshold, blur, GaussianBlur
+     */
+    public static void adaptiveThreshold(Mat src, Mat dst, double maxValue, int adaptiveMethod, int thresholdType, int blockSize, double C) {
+        adaptiveThreshold_0(src.nativeObj, dst.nativeObj, maxValue, adaptiveMethod, thresholdType, blockSize, C);
+    }
+
+
+    //
+    // C++:  void cv::applyColorMap(Mat src, Mat& dst, Mat userColor)
+    //
+
+    /**
+     * Applies a user colormap on a given image.
+     *
+     * @param src The source image, grayscale or colored of type CV_8UC1 or CV_8UC3.
+     * @param dst The result is the colormapped source image. Note: Mat::create is called on dst.
+     * @param userColor The colormap to apply of type CV_8UC1 or CV_8UC3 and size 256
+     */
+    public static void applyColorMap(Mat src, Mat dst, Mat userColor) {
+        applyColorMap_0(src.nativeObj, dst.nativeObj, userColor.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::applyColorMap(Mat src, Mat& dst, int colormap)
+    //
+
+    /**
+     * Applies a GNU Octave/MATLAB equivalent colormap on a given image.
+     *
+     * @param src The source image, grayscale or colored of type CV_8UC1 or CV_8UC3.
+     * @param dst The result is the colormapped source image. Note: Mat::create is called on dst.
+     * @param colormap The colormap to apply, see #ColormapTypes
+     */
+    public static void applyColorMap(Mat src, Mat dst, int colormap) {
+        applyColorMap_1(src.nativeObj, dst.nativeObj, colormap);
+    }
+
+
+    //
+    // C++:  void cv::approxPolyDP(vector_Point2f curve, vector_Point2f& approxCurve, double epsilon, bool closed)
+    //
+
+    /**
+     * Approximates a polygonal curve(s) with the specified precision.
+     *
+     * The function cv::approxPolyDP approximates a curve or a polygon with another curve/polygon with less
+     * vertices so that the distance between them is less or equal to the specified precision. It uses the
+     * Douglas-Peucker algorithm &lt;http://en.wikipedia.org/wiki/Ramer-Douglas-Peucker_algorithm&gt;
+     *
+     * @param curve Input vector of a 2D point stored in std::vector or Mat
+     * @param approxCurve Result of the approximation. The type should match the type of the input curve.
+     * @param epsilon Parameter specifying the approximation accuracy. This is the maximum distance
+     * between the original curve and its approximation.
+     * @param closed If true, the approximated curve is closed (its first and last vertices are
+     * connected). Otherwise, it is not closed.
+     */
+    public static void approxPolyDP(MatOfPoint2f curve, MatOfPoint2f approxCurve, double epsilon, boolean closed) {
+        Mat curve_mat = curve;
+        Mat approxCurve_mat = approxCurve;
+        approxPolyDP_0(curve_mat.nativeObj, approxCurve_mat.nativeObj, epsilon, closed);
+    }
+
+
+    //
+    // C++:  void cv::arrowedLine(Mat& img, Point pt1, Point pt2, Scalar color, int thickness = 1, int line_type = 8, int shift = 0, double tipLength = 0.1)
+    //
+
+    /**
+     * Draws a arrow segment pointing from the first point to the second one.
+     *
+     * The function cv::arrowedLine draws an arrow between pt1 and pt2 points in the image. See also #line.
+     *
+     * @param img Image.
+     * @param pt1 The point the arrow starts from.
+     * @param pt2 The point the arrow points to.
+     * @param color Line color.
+     * @param thickness Line thickness.
+     * @param line_type Type of the line. See #LineTypes
+     * @param shift Number of fractional bits in the point coordinates.
+     * @param tipLength The length of the arrow tip in relation to the arrow length
+     */
+    public static void arrowedLine(Mat img, Point pt1, Point pt2, Scalar color, int thickness, int line_type, int shift, double tipLength) {
+        arrowedLine_0(img.nativeObj, pt1.x, pt1.y, pt2.x, pt2.y, color.val[0], color.val[1], color.val[2], color.val[3], thickness, line_type, shift, tipLength);
+    }
+
+    /**
+     * Draws a arrow segment pointing from the first point to the second one.
+     *
+     * The function cv::arrowedLine draws an arrow between pt1 and pt2 points in the image. See also #line.
+     *
+     * @param img Image.
+     * @param pt1 The point the arrow starts from.
+     * @param pt2 The point the arrow points to.
+     * @param color Line color.
+     * @param thickness Line thickness.
+     * @param line_type Type of the line. See #LineTypes
+     * @param shift Number of fractional bits in the point coordinates.
+     */
+    public static void arrowedLine(Mat img, Point pt1, Point pt2, Scalar color, int thickness, int line_type, int shift) {
+        arrowedLine_1(img.nativeObj, pt1.x, pt1.y, pt2.x, pt2.y, color.val[0], color.val[1], color.val[2], color.val[3], thickness, line_type, shift);
+    }
+
+    /**
+     * Draws a arrow segment pointing from the first point to the second one.
+     *
+     * The function cv::arrowedLine draws an arrow between pt1 and pt2 points in the image. See also #line.
+     *
+     * @param img Image.
+     * @param pt1 The point the arrow starts from.
+     * @param pt2 The point the arrow points to.
+     * @param color Line color.
+     * @param thickness Line thickness.
+     * @param line_type Type of the line. See #LineTypes
+     */
+    public static void arrowedLine(Mat img, Point pt1, Point pt2, Scalar color, int thickness, int line_type) {
+        arrowedLine_2(img.nativeObj, pt1.x, pt1.y, pt2.x, pt2.y, color.val[0], color.val[1], color.val[2], color.val[3], thickness, line_type);
+    }
+
+    /**
+     * Draws a arrow segment pointing from the first point to the second one.
+     *
+     * The function cv::arrowedLine draws an arrow between pt1 and pt2 points in the image. See also #line.
+     *
+     * @param img Image.
+     * @param pt1 The point the arrow starts from.
+     * @param pt2 The point the arrow points to.
+     * @param color Line color.
+     * @param thickness Line thickness.
+     */
+    public static void arrowedLine(Mat img, Point pt1, Point pt2, Scalar color, int thickness) {
+        arrowedLine_3(img.nativeObj, pt1.x, pt1.y, pt2.x, pt2.y, color.val[0], color.val[1], color.val[2], color.val[3], thickness);
+    }
+
+    /**
+     * Draws a arrow segment pointing from the first point to the second one.
+     *
+     * The function cv::arrowedLine draws an arrow between pt1 and pt2 points in the image. See also #line.
+     *
+     * @param img Image.
+     * @param pt1 The point the arrow starts from.
+     * @param pt2 The point the arrow points to.
+     * @param color Line color.
+     */
+    public static void arrowedLine(Mat img, Point pt1, Point pt2, Scalar color) {
+        arrowedLine_4(img.nativeObj, pt1.x, pt1.y, pt2.x, pt2.y, color.val[0], color.val[1], color.val[2], color.val[3]);
+    }
+
+
+    //
+    // C++:  void cv::bilateralFilter(Mat src, Mat& dst, int d, double sigmaColor, double sigmaSpace, int borderType = BORDER_DEFAULT)
+    //
+
+    /**
+     * Applies the bilateral filter to an image.
+     *
+     * The function applies bilateral filtering to the input image, as described in
+     * http://www.dai.ed.ac.uk/CVonline/LOCAL_COPIES/MANDUCHI1/Bilateral_Filtering.html
+     * bilateralFilter can reduce unwanted noise very well while keeping edges fairly sharp. However, it is
+     * very slow compared to most filters.
+     *
+     * _Sigma values_: For simplicity, you can set the 2 sigma values to be the same. If they are small (&lt;
+     * 10), the filter will not have much effect, whereas if they are large (&gt; 150), they will have a very
+     * strong effect, making the image look "cartoonish".
+     *
+     * _Filter size_: Large filters (d &gt; 5) are very slow, so it is recommended to use d=5 for real-time
+     * applications, and perhaps d=9 for offline applications that need heavy noise filtering.
+     *
+     * This filter does not work inplace.
+     * @param src Source 8-bit or floating-point, 1-channel or 3-channel image.
+     * @param dst Destination image of the same size and type as src .
+     * @param d Diameter of each pixel neighborhood that is used during filtering. If it is non-positive,
+     * it is computed from sigmaSpace.
+     * @param sigmaColor Filter sigma in the color space. A larger value of the parameter means that
+     * farther colors within the pixel neighborhood (see sigmaSpace) will be mixed together, resulting
+     * in larger areas of semi-equal color.
+     * @param sigmaSpace Filter sigma in the coordinate space. A larger value of the parameter means that
+     * farther pixels will influence each other as long as their colors are close enough (see sigmaColor
+     * ). When d&gt;0, it specifies the neighborhood size regardless of sigmaSpace. Otherwise, d is
+     * proportional to sigmaSpace.
+     * @param borderType border mode used to extrapolate pixels outside of the image, see #BorderTypes
+     */
+    public static void bilateralFilter(Mat src, Mat dst, int d, double sigmaColor, double sigmaSpace, int borderType) {
+        bilateralFilter_0(src.nativeObj, dst.nativeObj, d, sigmaColor, sigmaSpace, borderType);
+    }
+
+    /**
+     * Applies the bilateral filter to an image.
+     *
+     * The function applies bilateral filtering to the input image, as described in
+     * http://www.dai.ed.ac.uk/CVonline/LOCAL_COPIES/MANDUCHI1/Bilateral_Filtering.html
+     * bilateralFilter can reduce unwanted noise very well while keeping edges fairly sharp. However, it is
+     * very slow compared to most filters.
+     *
+     * _Sigma values_: For simplicity, you can set the 2 sigma values to be the same. If they are small (&lt;
+     * 10), the filter will not have much effect, whereas if they are large (&gt; 150), they will have a very
+     * strong effect, making the image look "cartoonish".
+     *
+     * _Filter size_: Large filters (d &gt; 5) are very slow, so it is recommended to use d=5 for real-time
+     * applications, and perhaps d=9 for offline applications that need heavy noise filtering.
+     *
+     * This filter does not work inplace.
+     * @param src Source 8-bit or floating-point, 1-channel or 3-channel image.
+     * @param dst Destination image of the same size and type as src .
+     * @param d Diameter of each pixel neighborhood that is used during filtering. If it is non-positive,
+     * it is computed from sigmaSpace.
+     * @param sigmaColor Filter sigma in the color space. A larger value of the parameter means that
+     * farther colors within the pixel neighborhood (see sigmaSpace) will be mixed together, resulting
+     * in larger areas of semi-equal color.
+     * @param sigmaSpace Filter sigma in the coordinate space. A larger value of the parameter means that
+     * farther pixels will influence each other as long as their colors are close enough (see sigmaColor
+     * ). When d&gt;0, it specifies the neighborhood size regardless of sigmaSpace. Otherwise, d is
+     * proportional to sigmaSpace.
+     */
+    public static void bilateralFilter(Mat src, Mat dst, int d, double sigmaColor, double sigmaSpace) {
+        bilateralFilter_1(src.nativeObj, dst.nativeObj, d, sigmaColor, sigmaSpace);
+    }
+
+
+    //
+    // C++:  void cv::blur(Mat src, Mat& dst, Size ksize, Point anchor = Point(-1,-1), int borderType = BORDER_DEFAULT)
+    //
+
+    /**
+     * Blurs an image using the normalized box filter.
+     *
+     * The function smooths an image using the kernel:
+     *
+     * \(\texttt{K} =  \frac{1}{\texttt{ksize.width*ksize.height}} \begin{bmatrix} 1 &amp; 1 &amp; 1 &amp;  \cdots &amp; 1 &amp; 1  \\ 1 &amp; 1 &amp; 1 &amp;  \cdots &amp; 1 &amp; 1  \\ \hdotsfor{6} \\ 1 &amp; 1 &amp; 1 &amp;  \cdots &amp; 1 &amp; 1  \\ \end{bmatrix}\)
+     *
+     * The call {@code blur(src, dst, ksize, anchor, borderType)} is equivalent to `boxFilter(src, dst, src.type(), ksize,
+     * anchor, true, borderType)`.
+     *
+     * @param src input image; it can have any number of channels, which are processed independently, but
+     * the depth should be CV_8U, CV_16U, CV_16S, CV_32F or CV_64F.
+     * @param dst output image of the same size and type as src.
+     * @param ksize blurring kernel size.
+     * @param anchor anchor point; default value Point(-1,-1) means that the anchor is at the kernel
+     * center.
+     * @param borderType border mode used to extrapolate pixels outside of the image, see #BorderTypes. #BORDER_WRAP is not supported.
+     * SEE:  boxFilter, bilateralFilter, GaussianBlur, medianBlur
+     */
+    public static void blur(Mat src, Mat dst, Size ksize, Point anchor, int borderType) {
+        blur_0(src.nativeObj, dst.nativeObj, ksize.width, ksize.height, anchor.x, anchor.y, borderType);
+    }
+
+    /**
+     * Blurs an image using the normalized box filter.
+     *
+     * The function smooths an image using the kernel:
+     *
+     * \(\texttt{K} =  \frac{1}{\texttt{ksize.width*ksize.height}} \begin{bmatrix} 1 &amp; 1 &amp; 1 &amp;  \cdots &amp; 1 &amp; 1  \\ 1 &amp; 1 &amp; 1 &amp;  \cdots &amp; 1 &amp; 1  \\ \hdotsfor{6} \\ 1 &amp; 1 &amp; 1 &amp;  \cdots &amp; 1 &amp; 1  \\ \end{bmatrix}\)
+     *
+     * The call {@code blur(src, dst, ksize, anchor, borderType)} is equivalent to `boxFilter(src, dst, src.type(), ksize,
+     * anchor, true, borderType)`.
+     *
+     * @param src input image; it can have any number of channels, which are processed independently, but
+     * the depth should be CV_8U, CV_16U, CV_16S, CV_32F or CV_64F.
+     * @param dst output image of the same size and type as src.
+     * @param ksize blurring kernel size.
+     * @param anchor anchor point; default value Point(-1,-1) means that the anchor is at the kernel
+     * center.
+     * SEE:  boxFilter, bilateralFilter, GaussianBlur, medianBlur
+     */
+    public static void blur(Mat src, Mat dst, Size ksize, Point anchor) {
+        blur_1(src.nativeObj, dst.nativeObj, ksize.width, ksize.height, anchor.x, anchor.y);
+    }
+
+    /**
+     * Blurs an image using the normalized box filter.
+     *
+     * The function smooths an image using the kernel:
+     *
+     * \(\texttt{K} =  \frac{1}{\texttt{ksize.width*ksize.height}} \begin{bmatrix} 1 &amp; 1 &amp; 1 &amp;  \cdots &amp; 1 &amp; 1  \\ 1 &amp; 1 &amp; 1 &amp;  \cdots &amp; 1 &amp; 1  \\ \hdotsfor{6} \\ 1 &amp; 1 &amp; 1 &amp;  \cdots &amp; 1 &amp; 1  \\ \end{bmatrix}\)
+     *
+     * The call {@code blur(src, dst, ksize, anchor, borderType)} is equivalent to `boxFilter(src, dst, src.type(), ksize,
+     * anchor, true, borderType)`.
+     *
+     * @param src input image; it can have any number of channels, which are processed independently, but
+     * the depth should be CV_8U, CV_16U, CV_16S, CV_32F or CV_64F.
+     * @param dst output image of the same size and type as src.
+     * @param ksize blurring kernel size.
+     * center.
+     * SEE:  boxFilter, bilateralFilter, GaussianBlur, medianBlur
+     */
+    public static void blur(Mat src, Mat dst, Size ksize) {
+        blur_2(src.nativeObj, dst.nativeObj, ksize.width, ksize.height);
+    }
+
+
+    //
+    // C++:  void cv::boxFilter(Mat src, Mat& dst, int ddepth, Size ksize, Point anchor = Point(-1,-1), bool normalize = true, int borderType = BORDER_DEFAULT)
+    //
+
+    /**
+     * Blurs an image using the box filter.
+     *
+     * The function smooths an image using the kernel:
+     *
+     * \(\texttt{K} =  \alpha \begin{bmatrix} 1 &amp; 1 &amp; 1 &amp;  \cdots &amp; 1 &amp; 1  \\ 1 &amp; 1 &amp; 1 &amp;  \cdots &amp; 1 &amp; 1  \\ \hdotsfor{6} \\ 1 &amp; 1 &amp; 1 &amp;  \cdots &amp; 1 &amp; 1 \end{bmatrix}\)
+     *
+     * where
+     *
+     * \(\alpha = \begin{cases} \frac{1}{\texttt{ksize.width*ksize.height}} &amp; \texttt{when } \texttt{normalize=true}  \\1 &amp; \texttt{otherwise}\end{cases}\)
+     *
+     * Unnormalized box filter is useful for computing various integral characteristics over each pixel
+     * neighborhood, such as covariance matrices of image derivatives (used in dense optical flow
+     * algorithms, and so on). If you need to compute pixel sums over variable-size windows, use #integral.
+     *
+     * @param src input image.
+     * @param dst output image of the same size and type as src.
+     * @param ddepth the output image depth (-1 to use src.depth()).
+     * @param ksize blurring kernel size.
+     * @param anchor anchor point; default value Point(-1,-1) means that the anchor is at the kernel
+     * center.
+     * @param normalize flag, specifying whether the kernel is normalized by its area or not.
+     * @param borderType border mode used to extrapolate pixels outside of the image, see #BorderTypes. #BORDER_WRAP is not supported.
+     * SEE:  blur, bilateralFilter, GaussianBlur, medianBlur, integral
+     */
+    public static void boxFilter(Mat src, Mat dst, int ddepth, Size ksize, Point anchor, boolean normalize, int borderType) {
+        boxFilter_0(src.nativeObj, dst.nativeObj, ddepth, ksize.width, ksize.height, anchor.x, anchor.y, normalize, borderType);
+    }
+
+    /**
+     * Blurs an image using the box filter.
+     *
+     * The function smooths an image using the kernel:
+     *
+     * \(\texttt{K} =  \alpha \begin{bmatrix} 1 &amp; 1 &amp; 1 &amp;  \cdots &amp; 1 &amp; 1  \\ 1 &amp; 1 &amp; 1 &amp;  \cdots &amp; 1 &amp; 1  \\ \hdotsfor{6} \\ 1 &amp; 1 &amp; 1 &amp;  \cdots &amp; 1 &amp; 1 \end{bmatrix}\)
+     *
+     * where
+     *
+     * \(\alpha = \begin{cases} \frac{1}{\texttt{ksize.width*ksize.height}} &amp; \texttt{when } \texttt{normalize=true}  \\1 &amp; \texttt{otherwise}\end{cases}\)
+     *
+     * Unnormalized box filter is useful for computing various integral characteristics over each pixel
+     * neighborhood, such as covariance matrices of image derivatives (used in dense optical flow
+     * algorithms, and so on). If you need to compute pixel sums over variable-size windows, use #integral.
+     *
+     * @param src input image.
+     * @param dst output image of the same size and type as src.
+     * @param ddepth the output image depth (-1 to use src.depth()).
+     * @param ksize blurring kernel size.
+     * @param anchor anchor point; default value Point(-1,-1) means that the anchor is at the kernel
+     * center.
+     * @param normalize flag, specifying whether the kernel is normalized by its area or not.
+     * SEE:  blur, bilateralFilter, GaussianBlur, medianBlur, integral
+     */
+    public static void boxFilter(Mat src, Mat dst, int ddepth, Size ksize, Point anchor, boolean normalize) {
+        boxFilter_1(src.nativeObj, dst.nativeObj, ddepth, ksize.width, ksize.height, anchor.x, anchor.y, normalize);
+    }
+
+    /**
+     * Blurs an image using the box filter.
+     *
+     * The function smooths an image using the kernel:
+     *
+     * \(\texttt{K} =  \alpha \begin{bmatrix} 1 &amp; 1 &amp; 1 &amp;  \cdots &amp; 1 &amp; 1  \\ 1 &amp; 1 &amp; 1 &amp;  \cdots &amp; 1 &amp; 1  \\ \hdotsfor{6} \\ 1 &amp; 1 &amp; 1 &amp;  \cdots &amp; 1 &amp; 1 \end{bmatrix}\)
+     *
+     * where
+     *
+     * \(\alpha = \begin{cases} \frac{1}{\texttt{ksize.width*ksize.height}} &amp; \texttt{when } \texttt{normalize=true}  \\1 &amp; \texttt{otherwise}\end{cases}\)
+     *
+     * Unnormalized box filter is useful for computing various integral characteristics over each pixel
+     * neighborhood, such as covariance matrices of image derivatives (used in dense optical flow
+     * algorithms, and so on). If you need to compute pixel sums over variable-size windows, use #integral.
+     *
+     * @param src input image.
+     * @param dst output image of the same size and type as src.
+     * @param ddepth the output image depth (-1 to use src.depth()).
+     * @param ksize blurring kernel size.
+     * @param anchor anchor point; default value Point(-1,-1) means that the anchor is at the kernel
+     * center.
+     * SEE:  blur, bilateralFilter, GaussianBlur, medianBlur, integral
+     */
+    public static void boxFilter(Mat src, Mat dst, int ddepth, Size ksize, Point anchor) {
+        boxFilter_2(src.nativeObj, dst.nativeObj, ddepth, ksize.width, ksize.height, anchor.x, anchor.y);
+    }
+
+    /**
+     * Blurs an image using the box filter.
+     *
+     * The function smooths an image using the kernel:
+     *
+     * \(\texttt{K} =  \alpha \begin{bmatrix} 1 &amp; 1 &amp; 1 &amp;  \cdots &amp; 1 &amp; 1  \\ 1 &amp; 1 &amp; 1 &amp;  \cdots &amp; 1 &amp; 1  \\ \hdotsfor{6} \\ 1 &amp; 1 &amp; 1 &amp;  \cdots &amp; 1 &amp; 1 \end{bmatrix}\)
+     *
+     * where
+     *
+     * \(\alpha = \begin{cases} \frac{1}{\texttt{ksize.width*ksize.height}} &amp; \texttt{when } \texttt{normalize=true}  \\1 &amp; \texttt{otherwise}\end{cases}\)
+     *
+     * Unnormalized box filter is useful for computing various integral characteristics over each pixel
+     * neighborhood, such as covariance matrices of image derivatives (used in dense optical flow
+     * algorithms, and so on). If you need to compute pixel sums over variable-size windows, use #integral.
+     *
+     * @param src input image.
+     * @param dst output image of the same size and type as src.
+     * @param ddepth the output image depth (-1 to use src.depth()).
+     * @param ksize blurring kernel size.
+     * center.
+     * SEE:  blur, bilateralFilter, GaussianBlur, medianBlur, integral
+     */
+    public static void boxFilter(Mat src, Mat dst, int ddepth, Size ksize) {
+        boxFilter_3(src.nativeObj, dst.nativeObj, ddepth, ksize.width, ksize.height);
+    }
+
+
+    //
+    // C++:  void cv::boxPoints(RotatedRect box, Mat& points)
+    //
+
+    /**
+     * Finds the four vertices of a rotated rect. Useful to draw the rotated rectangle.
+     *
+     * The function finds the four vertices of a rotated rectangle. This function is useful to draw the
+     * rectangle. In C++, instead of using this function, you can directly use RotatedRect::points method. Please
+     * visit the REF: tutorial_bounding_rotated_ellipses "tutorial on Creating Bounding rotated boxes and ellipses for contours" for more information.
+     *
+     * @param box The input rotated rectangle. It may be the output of
+     * @param points The output array of four vertices of rectangles.
+     */
+    public static void boxPoints(RotatedRect box, Mat points) {
+        boxPoints_0(box.center.x, box.center.y, box.size.width, box.size.height, box.angle, points.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::calcBackProject(vector_Mat images, vector_int channels, Mat hist, Mat& dst, vector_float ranges, double scale)
+    //
+
+    public static void calcBackProject(List<Mat> images, MatOfInt channels, Mat hist, Mat dst, MatOfFloat ranges, double scale) {
+        Mat images_mat = Converters.vector_Mat_to_Mat(images);
+        Mat channels_mat = channels;
+        Mat ranges_mat = ranges;
+        calcBackProject_0(images_mat.nativeObj, channels_mat.nativeObj, hist.nativeObj, dst.nativeObj, ranges_mat.nativeObj, scale);
+    }
+
+
+    //
+    // C++:  void cv::calcHist(vector_Mat images, vector_int channels, Mat mask, Mat& hist, vector_int histSize, vector_float ranges, bool accumulate = false)
+    //
+
+    public static void calcHist(List<Mat> images, MatOfInt channels, Mat mask, Mat hist, MatOfInt histSize, MatOfFloat ranges, boolean accumulate) {
+        Mat images_mat = Converters.vector_Mat_to_Mat(images);
+        Mat channels_mat = channels;
+        Mat histSize_mat = histSize;
+        Mat ranges_mat = ranges;
+        calcHist_0(images_mat.nativeObj, channels_mat.nativeObj, mask.nativeObj, hist.nativeObj, histSize_mat.nativeObj, ranges_mat.nativeObj, accumulate);
+    }
+
+    public static void calcHist(List<Mat> images, MatOfInt channels, Mat mask, Mat hist, MatOfInt histSize, MatOfFloat ranges) {
+        Mat images_mat = Converters.vector_Mat_to_Mat(images);
+        Mat channels_mat = channels;
+        Mat histSize_mat = histSize;
+        Mat ranges_mat = ranges;
+        calcHist_1(images_mat.nativeObj, channels_mat.nativeObj, mask.nativeObj, hist.nativeObj, histSize_mat.nativeObj, ranges_mat.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::circle(Mat& img, Point center, int radius, Scalar color, int thickness = 1, int lineType = LINE_8, int shift = 0)
+    //
+
+    /**
+     * Draws a circle.
+     *
+     * The function cv::circle draws a simple or filled circle with a given center and radius.
+     * @param img Image where the circle is drawn.
+     * @param center Center of the circle.
+     * @param radius Radius of the circle.
+     * @param color Circle color.
+     * @param thickness Thickness of the circle outline, if positive. Negative values, like #FILLED,
+     * mean that a filled circle is to be drawn.
+     * @param lineType Type of the circle boundary. See #LineTypes
+     * @param shift Number of fractional bits in the coordinates of the center and in the radius value.
+     */
+    public static void circle(Mat img, Point center, int radius, Scalar color, int thickness, int lineType, int shift) {
+        circle_0(img.nativeObj, center.x, center.y, radius, color.val[0], color.val[1], color.val[2], color.val[3], thickness, lineType, shift);
+    }
+
+    /**
+     * Draws a circle.
+     *
+     * The function cv::circle draws a simple or filled circle with a given center and radius.
+     * @param img Image where the circle is drawn.
+     * @param center Center of the circle.
+     * @param radius Radius of the circle.
+     * @param color Circle color.
+     * @param thickness Thickness of the circle outline, if positive. Negative values, like #FILLED,
+     * mean that a filled circle is to be drawn.
+     * @param lineType Type of the circle boundary. See #LineTypes
+     */
+    public static void circle(Mat img, Point center, int radius, Scalar color, int thickness, int lineType) {
+        circle_1(img.nativeObj, center.x, center.y, radius, color.val[0], color.val[1], color.val[2], color.val[3], thickness, lineType);
+    }
+
+    /**
+     * Draws a circle.
+     *
+     * The function cv::circle draws a simple or filled circle with a given center and radius.
+     * @param img Image where the circle is drawn.
+     * @param center Center of the circle.
+     * @param radius Radius of the circle.
+     * @param color Circle color.
+     * @param thickness Thickness of the circle outline, if positive. Negative values, like #FILLED,
+     * mean that a filled circle is to be drawn.
+     */
+    public static void circle(Mat img, Point center, int radius, Scalar color, int thickness) {
+        circle_2(img.nativeObj, center.x, center.y, radius, color.val[0], color.val[1], color.val[2], color.val[3], thickness);
+    }
+
+    /**
+     * Draws a circle.
+     *
+     * The function cv::circle draws a simple or filled circle with a given center and radius.
+     * @param img Image where the circle is drawn.
+     * @param center Center of the circle.
+     * @param radius Radius of the circle.
+     * @param color Circle color.
+     * mean that a filled circle is to be drawn.
+     */
+    public static void circle(Mat img, Point center, int radius, Scalar color) {
+        circle_3(img.nativeObj, center.x, center.y, radius, color.val[0], color.val[1], color.val[2], color.val[3]);
+    }
+
+
+    //
+    // C++:  void cv::convertMaps(Mat map1, Mat map2, Mat& dstmap1, Mat& dstmap2, int dstmap1type, bool nninterpolation = false)
+    //
+
+    /**
+     * Converts image transformation maps from one representation to another.
+     *
+     * The function converts a pair of maps for remap from one representation to another. The following
+     * options ( (map1.type(), map2.type()) \(\rightarrow\) (dstmap1.type(), dstmap2.type()) ) are
+     * supported:
+     *
+     * <ul>
+     *   <li>
+     *  \(\texttt{(CV_32FC1, CV_32FC1)} \rightarrow \texttt{(CV_16SC2, CV_16UC1)}\). This is the
+     * most frequently used conversion operation, in which the original floating-point maps (see remap )
+     * are converted to a more compact and much faster fixed-point representation. The first output array
+     * contains the rounded coordinates and the second array (created only when nninterpolation=false )
+     * contains indices in the interpolation tables.
+     *   </li>
+     * </ul>
+     *
+     * <ul>
+     *   <li>
+     *  \(\texttt{(CV_32FC2)} \rightarrow \texttt{(CV_16SC2, CV_16UC1)}\). The same as above but
+     * the original maps are stored in one 2-channel matrix.
+     *   </li>
+     * </ul>
+     *
+     * <ul>
+     *   <li>
+     *  Reverse conversion. Obviously, the reconstructed floating-point maps will not be exactly the same
+     * as the originals.
+     *   </li>
+     * </ul>
+     *
+     * @param map1 The first input map of type CV_16SC2, CV_32FC1, or CV_32FC2 .
+     * @param map2 The second input map of type CV_16UC1, CV_32FC1, or none (empty matrix),
+     * respectively.
+     * @param dstmap1 The first output map that has the type dstmap1type and the same size as src .
+     * @param dstmap2 The second output map.
+     * @param dstmap1type Type of the first output map that should be CV_16SC2, CV_32FC1, or
+     * CV_32FC2 .
+     * @param nninterpolation Flag indicating whether the fixed-point maps are used for the
+     * nearest-neighbor or for a more complex interpolation.
+     *
+     * SEE:  remap, undistort, initUndistortRectifyMap
+     */
+    public static void convertMaps(Mat map1, Mat map2, Mat dstmap1, Mat dstmap2, int dstmap1type, boolean nninterpolation) {
+        convertMaps_0(map1.nativeObj, map2.nativeObj, dstmap1.nativeObj, dstmap2.nativeObj, dstmap1type, nninterpolation);
+    }
+
+    /**
+     * Converts image transformation maps from one representation to another.
+     *
+     * The function converts a pair of maps for remap from one representation to another. The following
+     * options ( (map1.type(), map2.type()) \(\rightarrow\) (dstmap1.type(), dstmap2.type()) ) are
+     * supported:
+     *
+     * <ul>
+     *   <li>
+     *  \(\texttt{(CV_32FC1, CV_32FC1)} \rightarrow \texttt{(CV_16SC2, CV_16UC1)}\). This is the
+     * most frequently used conversion operation, in which the original floating-point maps (see remap )
+     * are converted to a more compact and much faster fixed-point representation. The first output array
+     * contains the rounded coordinates and the second array (created only when nninterpolation=false )
+     * contains indices in the interpolation tables.
+     *   </li>
+     * </ul>
+     *
+     * <ul>
+     *   <li>
+     *  \(\texttt{(CV_32FC2)} \rightarrow \texttt{(CV_16SC2, CV_16UC1)}\). The same as above but
+     * the original maps are stored in one 2-channel matrix.
+     *   </li>
+     * </ul>
+     *
+     * <ul>
+     *   <li>
+     *  Reverse conversion. Obviously, the reconstructed floating-point maps will not be exactly the same
+     * as the originals.
+     *   </li>
+     * </ul>
+     *
+     * @param map1 The first input map of type CV_16SC2, CV_32FC1, or CV_32FC2 .
+     * @param map2 The second input map of type CV_16UC1, CV_32FC1, or none (empty matrix),
+     * respectively.
+     * @param dstmap1 The first output map that has the type dstmap1type and the same size as src .
+     * @param dstmap2 The second output map.
+     * @param dstmap1type Type of the first output map that should be CV_16SC2, CV_32FC1, or
+     * CV_32FC2 .
+     * nearest-neighbor or for a more complex interpolation.
+     *
+     * SEE:  remap, undistort, initUndistortRectifyMap
+     */
+    public static void convertMaps(Mat map1, Mat map2, Mat dstmap1, Mat dstmap2, int dstmap1type) {
+        convertMaps_1(map1.nativeObj, map2.nativeObj, dstmap1.nativeObj, dstmap2.nativeObj, dstmap1type);
+    }
+
+
+    //
+    // C++:  void cv::convexHull(vector_Point points, vector_int& hull, bool clockwise = false,  _hidden_  returnPoints = true)
+    //
+
+    /**
+     * Finds the convex hull of a point set.
+     *
+     * The function cv::convexHull finds the convex hull of a 2D point set using the Sklansky's algorithm CITE: Sklansky82
+     * that has *O(N logN)* complexity in the current implementation.
+     *
+     * @param points Input 2D point set, stored in std::vector or Mat.
+     * @param hull Output convex hull. It is either an integer vector of indices or vector of points. In
+     * the first case, the hull elements are 0-based indices of the convex hull points in the original
+     * array (since the set of convex hull points is a subset of the original point set). In the second
+     * case, hull elements are the convex hull points themselves.
+     * @param clockwise Orientation flag. If it is true, the output convex hull is oriented clockwise.
+     * Otherwise, it is oriented counter-clockwise. The assumed coordinate system has its X axis pointing
+     * to the right, and its Y axis pointing upwards.
+     * returns convex hull points. Otherwise, it returns indices of the convex hull points. When the
+     * output array is std::vector, the flag is ignored, and the output depends on the type of the
+     * vector: std::vector&lt;int&gt; implies returnPoints=false, std::vector&lt;Point&gt; implies
+     * returnPoints=true.
+     *
+     * <b>Note:</b> {@code points} and {@code hull} should be different arrays, inplace processing isn't supported.
+     *
+     * Check REF: tutorial_hull "the corresponding tutorial" for more details.
+     *
+     * useful links:
+     *
+     * https://www.learnopencv.com/convex-hull-using-opencv-in-python-and-c/
+     */
+    public static void convexHull(MatOfPoint points, MatOfInt hull, boolean clockwise) {
+        Mat points_mat = points;
+        Mat hull_mat = hull;
+        convexHull_0(points_mat.nativeObj, hull_mat.nativeObj, clockwise);
+    }
+
+    /**
+     * Finds the convex hull of a point set.
+     *
+     * The function cv::convexHull finds the convex hull of a 2D point set using the Sklansky's algorithm CITE: Sklansky82
+     * that has *O(N logN)* complexity in the current implementation.
+     *
+     * @param points Input 2D point set, stored in std::vector or Mat.
+     * @param hull Output convex hull. It is either an integer vector of indices or vector of points. In
+     * the first case, the hull elements are 0-based indices of the convex hull points in the original
+     * array (since the set of convex hull points is a subset of the original point set). In the second
+     * case, hull elements are the convex hull points themselves.
+     * Otherwise, it is oriented counter-clockwise. The assumed coordinate system has its X axis pointing
+     * to the right, and its Y axis pointing upwards.
+     * returns convex hull points. Otherwise, it returns indices of the convex hull points. When the
+     * output array is std::vector, the flag is ignored, and the output depends on the type of the
+     * vector: std::vector&lt;int&gt; implies returnPoints=false, std::vector&lt;Point&gt; implies
+     * returnPoints=true.
+     *
+     * <b>Note:</b> {@code points} and {@code hull} should be different arrays, inplace processing isn't supported.
+     *
+     * Check REF: tutorial_hull "the corresponding tutorial" for more details.
+     *
+     * useful links:
+     *
+     * https://www.learnopencv.com/convex-hull-using-opencv-in-python-and-c/
+     */
+    public static void convexHull(MatOfPoint points, MatOfInt hull) {
+        Mat points_mat = points;
+        Mat hull_mat = hull;
+        convexHull_2(points_mat.nativeObj, hull_mat.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::convexityDefects(vector_Point contour, vector_int convexhull, vector_Vec4i& convexityDefects)
+    //
+
+    /**
+     * Finds the convexity defects of a contour.
+     *
+     * The figure below displays convexity defects of a hand contour:
+     *
+     * ![image](pics/defects.png)
+     *
+     * @param contour Input contour.
+     * @param convexhull Convex hull obtained using convexHull that should contain indices of the contour
+     * points that make the hull.
+     * @param convexityDefects The output vector of convexity defects. In C++ and the new Python/Java
+     * interface each convexity defect is represented as 4-element integer vector (a.k.a. #Vec4i):
+     * (start_index, end_index, farthest_pt_index, fixpt_depth), where indices are 0-based indices
+     * in the original contour of the convexity defect beginning, end and the farthest point, and
+     * fixpt_depth is fixed-point approximation (with 8 fractional bits) of the distance between the
+     * farthest contour point and the hull. That is, to get the floating-point value of the depth will be
+     * fixpt_depth/256.0.
+     */
+    public static void convexityDefects(MatOfPoint contour, MatOfInt convexhull, MatOfInt4 convexityDefects) {
+        Mat contour_mat = contour;
+        Mat convexhull_mat = convexhull;
+        Mat convexityDefects_mat = convexityDefects;
+        convexityDefects_0(contour_mat.nativeObj, convexhull_mat.nativeObj, convexityDefects_mat.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::cornerEigenValsAndVecs(Mat src, Mat& dst, int blockSize, int ksize, int borderType = BORDER_DEFAULT)
+    //
+
+    /**
+     * Calculates eigenvalues and eigenvectors of image blocks for corner detection.
+     *
+     * For every pixel \(p\) , the function cornerEigenValsAndVecs considers a blockSize \(\times\) blockSize
+     * neighborhood \(S(p)\) . It calculates the covariation matrix of derivatives over the neighborhood as:
+     *
+     * \(M =  \begin{bmatrix} \sum _{S(p)}(dI/dx)^2 &amp;  \sum _{S(p)}dI/dx dI/dy  \\ \sum _{S(p)}dI/dx dI/dy &amp;  \sum _{S(p)}(dI/dy)^2 \end{bmatrix}\)
+     *
+     * where the derivatives are computed using the Sobel operator.
+     *
+     * After that, it finds eigenvectors and eigenvalues of \(M\) and stores them in the destination image as
+     * \((\lambda_1, \lambda_2, x_1, y_1, x_2, y_2)\) where
+     *
+     * <ul>
+     *   <li>
+     *    \(\lambda_1, \lambda_2\) are the non-sorted eigenvalues of \(M\)
+     *   </li>
+     *   <li>
+     *    \(x_1, y_1\) are the eigenvectors corresponding to \(\lambda_1\)
+     *   </li>
+     *   <li>
+     *    \(x_2, y_2\) are the eigenvectors corresponding to \(\lambda_2\)
+     *   </li>
+     * </ul>
+     *
+     * The output of the function can be used for robust edge or corner detection.
+     *
+     * @param src Input single-channel 8-bit or floating-point image.
+     * @param dst Image to store the results. It has the same size as src and the type CV_32FC(6) .
+     * @param blockSize Neighborhood size (see details below).
+     * @param ksize Aperture parameter for the Sobel operator.
+     * @param borderType Pixel extrapolation method. See #BorderTypes. #BORDER_WRAP is not supported.
+     *
+     * SEE:  cornerMinEigenVal, cornerHarris, preCornerDetect
+     */
+    public static void cornerEigenValsAndVecs(Mat src, Mat dst, int blockSize, int ksize, int borderType) {
+        cornerEigenValsAndVecs_0(src.nativeObj, dst.nativeObj, blockSize, ksize, borderType);
+    }
+
+    /**
+     * Calculates eigenvalues and eigenvectors of image blocks for corner detection.
+     *
+     * For every pixel \(p\) , the function cornerEigenValsAndVecs considers a blockSize \(\times\) blockSize
+     * neighborhood \(S(p)\) . It calculates the covariation matrix of derivatives over the neighborhood as:
+     *
+     * \(M =  \begin{bmatrix} \sum _{S(p)}(dI/dx)^2 &amp;  \sum _{S(p)}dI/dx dI/dy  \\ \sum _{S(p)}dI/dx dI/dy &amp;  \sum _{S(p)}(dI/dy)^2 \end{bmatrix}\)
+     *
+     * where the derivatives are computed using the Sobel operator.
+     *
+     * After that, it finds eigenvectors and eigenvalues of \(M\) and stores them in the destination image as
+     * \((\lambda_1, \lambda_2, x_1, y_1, x_2, y_2)\) where
+     *
+     * <ul>
+     *   <li>
+     *    \(\lambda_1, \lambda_2\) are the non-sorted eigenvalues of \(M\)
+     *   </li>
+     *   <li>
+     *    \(x_1, y_1\) are the eigenvectors corresponding to \(\lambda_1\)
+     *   </li>
+     *   <li>
+     *    \(x_2, y_2\) are the eigenvectors corresponding to \(\lambda_2\)
+     *   </li>
+     * </ul>
+     *
+     * The output of the function can be used for robust edge or corner detection.
+     *
+     * @param src Input single-channel 8-bit or floating-point image.
+     * @param dst Image to store the results. It has the same size as src and the type CV_32FC(6) .
+     * @param blockSize Neighborhood size (see details below).
+     * @param ksize Aperture parameter for the Sobel operator.
+     *
+     * SEE:  cornerMinEigenVal, cornerHarris, preCornerDetect
+     */
+    public static void cornerEigenValsAndVecs(Mat src, Mat dst, int blockSize, int ksize) {
+        cornerEigenValsAndVecs_1(src.nativeObj, dst.nativeObj, blockSize, ksize);
+    }
+
+
+    //
+    // C++:  void cv::cornerHarris(Mat src, Mat& dst, int blockSize, int ksize, double k, int borderType = BORDER_DEFAULT)
+    //
+
+    /**
+     * Harris corner detector.
+     *
+     * The function runs the Harris corner detector on the image. Similarly to cornerMinEigenVal and
+     * cornerEigenValsAndVecs , for each pixel \((x, y)\) it calculates a \(2\times2\) gradient covariance
+     * matrix \(M^{(x,y)}\) over a \(\texttt{blockSize} \times \texttt{blockSize}\) neighborhood. Then, it
+     * computes the following characteristic:
+     *
+     * \(\texttt{dst} (x,y) =  \mathrm{det} M^{(x,y)} - k  \cdot \left ( \mathrm{tr} M^{(x,y)} \right )^2\)
+     *
+     * Corners in the image can be found as the local maxima of this response map.
+     *
+     * @param src Input single-channel 8-bit or floating-point image.
+     * @param dst Image to store the Harris detector responses. It has the type CV_32FC1 and the same
+     * size as src .
+     * @param blockSize Neighborhood size (see the details on #cornerEigenValsAndVecs ).
+     * @param ksize Aperture parameter for the Sobel operator.
+     * @param k Harris detector free parameter. See the formula above.
+     * @param borderType Pixel extrapolation method. See #BorderTypes. #BORDER_WRAP is not supported.
+     */
+    public static void cornerHarris(Mat src, Mat dst, int blockSize, int ksize, double k, int borderType) {
+        cornerHarris_0(src.nativeObj, dst.nativeObj, blockSize, ksize, k, borderType);
+    }
+
+    /**
+     * Harris corner detector.
+     *
+     * The function runs the Harris corner detector on the image. Similarly to cornerMinEigenVal and
+     * cornerEigenValsAndVecs , for each pixel \((x, y)\) it calculates a \(2\times2\) gradient covariance
+     * matrix \(M^{(x,y)}\) over a \(\texttt{blockSize} \times \texttt{blockSize}\) neighborhood. Then, it
+     * computes the following characteristic:
+     *
+     * \(\texttt{dst} (x,y) =  \mathrm{det} M^{(x,y)} - k  \cdot \left ( \mathrm{tr} M^{(x,y)} \right )^2\)
+     *
+     * Corners in the image can be found as the local maxima of this response map.
+     *
+     * @param src Input single-channel 8-bit or floating-point image.
+     * @param dst Image to store the Harris detector responses. It has the type CV_32FC1 and the same
+     * size as src .
+     * @param blockSize Neighborhood size (see the details on #cornerEigenValsAndVecs ).
+     * @param ksize Aperture parameter for the Sobel operator.
+     * @param k Harris detector free parameter. See the formula above.
+     */
+    public static void cornerHarris(Mat src, Mat dst, int blockSize, int ksize, double k) {
+        cornerHarris_1(src.nativeObj, dst.nativeObj, blockSize, ksize, k);
+    }
+
+
+    //
+    // C++:  void cv::cornerMinEigenVal(Mat src, Mat& dst, int blockSize, int ksize = 3, int borderType = BORDER_DEFAULT)
+    //
+
+    /**
+     * Calculates the minimal eigenvalue of gradient matrices for corner detection.
+     *
+     * The function is similar to cornerEigenValsAndVecs but it calculates and stores only the minimal
+     * eigenvalue of the covariance matrix of derivatives, that is, \(\min(\lambda_1, \lambda_2)\) in terms
+     * of the formulae in the cornerEigenValsAndVecs description.
+     *
+     * @param src Input single-channel 8-bit or floating-point image.
+     * @param dst Image to store the minimal eigenvalues. It has the type CV_32FC1 and the same size as
+     * src .
+     * @param blockSize Neighborhood size (see the details on #cornerEigenValsAndVecs ).
+     * @param ksize Aperture parameter for the Sobel operator.
+     * @param borderType Pixel extrapolation method. See #BorderTypes. #BORDER_WRAP is not supported.
+     */
+    public static void cornerMinEigenVal(Mat src, Mat dst, int blockSize, int ksize, int borderType) {
+        cornerMinEigenVal_0(src.nativeObj, dst.nativeObj, blockSize, ksize, borderType);
+    }
+
+    /**
+     * Calculates the minimal eigenvalue of gradient matrices for corner detection.
+     *
+     * The function is similar to cornerEigenValsAndVecs but it calculates and stores only the minimal
+     * eigenvalue of the covariance matrix of derivatives, that is, \(\min(\lambda_1, \lambda_2)\) in terms
+     * of the formulae in the cornerEigenValsAndVecs description.
+     *
+     * @param src Input single-channel 8-bit or floating-point image.
+     * @param dst Image to store the minimal eigenvalues. It has the type CV_32FC1 and the same size as
+     * src .
+     * @param blockSize Neighborhood size (see the details on #cornerEigenValsAndVecs ).
+     * @param ksize Aperture parameter for the Sobel operator.
+     */
+    public static void cornerMinEigenVal(Mat src, Mat dst, int blockSize, int ksize) {
+        cornerMinEigenVal_1(src.nativeObj, dst.nativeObj, blockSize, ksize);
+    }
+
+    /**
+     * Calculates the minimal eigenvalue of gradient matrices for corner detection.
+     *
+     * The function is similar to cornerEigenValsAndVecs but it calculates and stores only the minimal
+     * eigenvalue of the covariance matrix of derivatives, that is, \(\min(\lambda_1, \lambda_2)\) in terms
+     * of the formulae in the cornerEigenValsAndVecs description.
+     *
+     * @param src Input single-channel 8-bit or floating-point image.
+     * @param dst Image to store the minimal eigenvalues. It has the type CV_32FC1 and the same size as
+     * src .
+     * @param blockSize Neighborhood size (see the details on #cornerEigenValsAndVecs ).
+     */
+    public static void cornerMinEigenVal(Mat src, Mat dst, int blockSize) {
+        cornerMinEigenVal_2(src.nativeObj, dst.nativeObj, blockSize);
+    }
+
+
+    //
+    // C++:  void cv::cornerSubPix(Mat image, Mat& corners, Size winSize, Size zeroZone, TermCriteria criteria)
+    //
+
+    /**
+     * Refines the corner locations.
+     *
+     * The function iterates to find the sub-pixel accurate location of corners or radial saddle points, as
+     * shown on the figure below.
+     *
+     * ![image](pics/cornersubpix.png)
+     *
+     * Sub-pixel accurate corner locator is based on the observation that every vector from the center \(q\)
+     * to a point \(p\) located within a neighborhood of \(q\) is orthogonal to the image gradient at \(p\)
+     * subject to image and measurement noise. Consider the expression:
+     *
+     * \(\epsilon _i = {DI_{p_i}}^T  \cdot (q - p_i)\)
+     *
+     * where \({DI_{p_i}}\) is an image gradient at one of the points \(p_i\) in a neighborhood of \(q\) . The
+     * value of \(q\) is to be found so that \(\epsilon_i\) is minimized. A system of equations may be set up
+     * with \(\epsilon_i\) set to zero:
+     *
+     * \(\sum _i(DI_{p_i}  \cdot {DI_{p_i}}^T) \cdot q -  \sum _i(DI_{p_i}  \cdot {DI_{p_i}}^T  \cdot p_i)\)
+     *
+     * where the gradients are summed within a neighborhood ("search window") of \(q\) . Calling the first
+     * gradient term \(G\) and the second gradient term \(b\) gives:
+     *
+     * \(q = G^{-1}  \cdot b\)
+     *
+     * The algorithm sets the center of the neighborhood window at this new center \(q\) and then iterates
+     * until the center stays within a set threshold.
+     *
+     * @param image Input single-channel, 8-bit or float image.
+     * @param corners Initial coordinates of the input corners and refined coordinates provided for
+     * output.
+     * @param winSize Half of the side length of the search window. For example, if winSize=Size(5,5) ,
+     * then a \((5*2+1) \times (5*2+1) = 11 \times 11\) search window is used.
+     * @param zeroZone Half of the size of the dead region in the middle of the search zone over which
+     * the summation in the formula below is not done. It is used sometimes to avoid possible
+     * singularities of the autocorrelation matrix. The value of (-1,-1) indicates that there is no such
+     * a size.
+     * @param criteria Criteria for termination of the iterative process of corner refinement. That is,
+     * the process of corner position refinement stops either after criteria.maxCount iterations or when
+     * the corner position moves by less than criteria.epsilon on some iteration.
+     */
+    public static void cornerSubPix(Mat image, Mat corners, Size winSize, Size zeroZone, TermCriteria criteria) {
+        cornerSubPix_0(image.nativeObj, corners.nativeObj, winSize.width, winSize.height, zeroZone.width, zeroZone.height, criteria.type, criteria.maxCount, criteria.epsilon);
+    }
+
+
+    //
+    // C++:  void cv::createHanningWindow(Mat& dst, Size winSize, int type)
+    //
+
+    /**
+     * This function computes a Hanning window coefficients in two dimensions.
+     *
+     * See (http://en.wikipedia.org/wiki/Hann_function) and (http://en.wikipedia.org/wiki/Window_function)
+     * for more information.
+     *
+     * An example is shown below:
+     * <code>
+     *     // create hanning window of size 100x100 and type CV_32F
+     *     Mat hann;
+     *     createHanningWindow(hann, Size(100, 100), CV_32F);
+     * </code>
+     * @param dst Destination array to place Hann coefficients in
+     * @param winSize The window size specifications (both width and height must be &gt; 1)
+     * @param type Created array type
+     */
+    public static void createHanningWindow(Mat dst, Size winSize, int type) {
+        createHanningWindow_0(dst.nativeObj, winSize.width, winSize.height, type);
+    }
+
+
+    //
+    // C++:  void cv::cvtColor(Mat src, Mat& dst, int code, int dstCn = 0)
+    //
+
+    /**
+     * Converts an image from one color space to another.
+     *
+     * The function converts an input image from one color space to another. In case of a transformation
+     * to-from RGB color space, the order of the channels should be specified explicitly (RGB or BGR). Note
+     * that the default color format in OpenCV is often referred to as RGB but it is actually BGR (the
+     * bytes are reversed). So the first byte in a standard (24-bit) color image will be an 8-bit Blue
+     * component, the second byte will be Green, and the third byte will be Red. The fourth, fifth, and
+     * sixth bytes would then be the second pixel (Blue, then Green, then Red), and so on.
+     *
+     * The conventional ranges for R, G, and B channel values are:
+     * <ul>
+     *   <li>
+     *    0 to 255 for CV_8U images
+     *   </li>
+     *   <li>
+     *    0 to 65535 for CV_16U images
+     *   </li>
+     *   <li>
+     *    0 to 1 for CV_32F images
+     *   </li>
+     * </ul>
+     *
+     * In case of linear transformations, the range does not matter. But in case of a non-linear
+     * transformation, an input RGB image should be normalized to the proper value range to get the correct
+     * results, for example, for RGB \(\rightarrow\) L\*u\*v\* transformation. For example, if you have a
+     * 32-bit floating-point image directly converted from an 8-bit image without any scaling, then it will
+     * have the 0..255 value range instead of 0..1 assumed by the function. So, before calling #cvtColor ,
+     * you need first to scale the image down:
+     * <code>
+     *     img *= 1./255;
+     *     cvtColor(img, img, COLOR_BGR2Luv);
+     * </code>
+     * If you use #cvtColor with 8-bit images, the conversion will have some information lost. For many
+     * applications, this will not be noticeable but it is recommended to use 32-bit images in applications
+     * that need the full range of colors or that convert an image before an operation and then convert
+     * back.
+     *
+     * If conversion adds the alpha channel, its value will set to the maximum of corresponding channel
+     * range: 255 for CV_8U, 65535 for CV_16U, 1 for CV_32F.
+     *
+     * @param src input image: 8-bit unsigned, 16-bit unsigned ( CV_16UC... ), or single-precision
+     * floating-point.
+     * @param dst output image of the same size and depth as src.
+     * @param code color space conversion code (see #ColorConversionCodes).
+     * @param dstCn number of channels in the destination image; if the parameter is 0, the number of the
+     * channels is derived automatically from src and code.
+     *
+     * SEE: REF: imgproc_color_conversions
+     */
+    public static void cvtColor(Mat src, Mat dst, int code, int dstCn) {
+        cvtColor_0(src.nativeObj, dst.nativeObj, code, dstCn);
+    }
+
+    /**
+     * Converts an image from one color space to another.
+     *
+     * The function converts an input image from one color space to another. In case of a transformation
+     * to-from RGB color space, the order of the channels should be specified explicitly (RGB or BGR). Note
+     * that the default color format in OpenCV is often referred to as RGB but it is actually BGR (the
+     * bytes are reversed). So the first byte in a standard (24-bit) color image will be an 8-bit Blue
+     * component, the second byte will be Green, and the third byte will be Red. The fourth, fifth, and
+     * sixth bytes would then be the second pixel (Blue, then Green, then Red), and so on.
+     *
+     * The conventional ranges for R, G, and B channel values are:
+     * <ul>
+     *   <li>
+     *    0 to 255 for CV_8U images
+     *   </li>
+     *   <li>
+     *    0 to 65535 for CV_16U images
+     *   </li>
+     *   <li>
+     *    0 to 1 for CV_32F images
+     *   </li>
+     * </ul>
+     *
+     * In case of linear transformations, the range does not matter. But in case of a non-linear
+     * transformation, an input RGB image should be normalized to the proper value range to get the correct
+     * results, for example, for RGB \(\rightarrow\) L\*u\*v\* transformation. For example, if you have a
+     * 32-bit floating-point image directly converted from an 8-bit image without any scaling, then it will
+     * have the 0..255 value range instead of 0..1 assumed by the function. So, before calling #cvtColor ,
+     * you need first to scale the image down:
+     * <code>
+     *     img *= 1./255;
+     *     cvtColor(img, img, COLOR_BGR2Luv);
+     * </code>
+     * If you use #cvtColor with 8-bit images, the conversion will have some information lost. For many
+     * applications, this will not be noticeable but it is recommended to use 32-bit images in applications
+     * that need the full range of colors or that convert an image before an operation and then convert
+     * back.
+     *
+     * If conversion adds the alpha channel, its value will set to the maximum of corresponding channel
+     * range: 255 for CV_8U, 65535 for CV_16U, 1 for CV_32F.
+     *
+     * @param src input image: 8-bit unsigned, 16-bit unsigned ( CV_16UC... ), or single-precision
+     * floating-point.
+     * @param dst output image of the same size and depth as src.
+     * @param code color space conversion code (see #ColorConversionCodes).
+     * channels is derived automatically from src and code.
+     *
+     * SEE: REF: imgproc_color_conversions
+     */
+    public static void cvtColor(Mat src, Mat dst, int code) {
+        cvtColor_1(src.nativeObj, dst.nativeObj, code);
+    }
+
+
+    //
+    // C++:  void cv::cvtColorTwoPlane(Mat src1, Mat src2, Mat& dst, int code)
+    //
+
+    /**
+     * Converts an image from one color space to another where the source image is
+     * stored in two planes.
+     *
+     * This function only supports YUV420 to RGB conversion as of now.
+     *
+     * <ul>
+     *   <li>
+     *  #COLOR_YUV2BGR_NV12
+     *   </li>
+     *   <li>
+     *  #COLOR_YUV2RGB_NV12
+     *   </li>
+     *   <li>
+     *  #COLOR_YUV2BGRA_NV12
+     *   </li>
+     *   <li>
+     *  #COLOR_YUV2RGBA_NV12
+     *   </li>
+     *   <li>
+     *  #COLOR_YUV2BGR_NV21
+     *   </li>
+     *   <li>
+     *  #COLOR_YUV2RGB_NV21
+     *   </li>
+     *   <li>
+     *  #COLOR_YUV2BGRA_NV21
+     *   </li>
+     *   <li>
+     *  #COLOR_YUV2RGBA_NV21
+     *   </li>
+     * </ul>
+     * @param src1 automatically generated
+     * @param src2 automatically generated
+     * @param dst automatically generated
+     * @param code automatically generated
+     */
+    public static void cvtColorTwoPlane(Mat src1, Mat src2, Mat dst, int code) {
+        cvtColorTwoPlane_0(src1.nativeObj, src2.nativeObj, dst.nativeObj, code);
+    }
+
+
+    //
+    // C++:  void cv::demosaicing(Mat src, Mat& dst, int code, int dstCn = 0)
+    //
+
+    /**
+     * main function for all demosaicing processes
+     *
+     * @param src input image: 8-bit unsigned or 16-bit unsigned.
+     * @param dst output image of the same size and depth as src.
+     * @param code Color space conversion code (see the description below).
+     * @param dstCn number of channels in the destination image; if the parameter is 0, the number of the
+     * channels is derived automatically from src and code.
+     *
+     * The function can do the following transformations:
+     *
+     * <ul>
+     *   <li>
+     *    Demosaicing using bilinear interpolation
+     *   </li>
+     * </ul>
+     *
+     *     #COLOR_BayerBG2BGR , #COLOR_BayerGB2BGR , #COLOR_BayerRG2BGR , #COLOR_BayerGR2BGR
+     *
+     *     #COLOR_BayerBG2GRAY , #COLOR_BayerGB2GRAY , #COLOR_BayerRG2GRAY , #COLOR_BayerGR2GRAY
+     *
+     * <ul>
+     *   <li>
+     *    Demosaicing using Variable Number of Gradients.
+     *   </li>
+     * </ul>
+     *
+     *     #COLOR_BayerBG2BGR_VNG , #COLOR_BayerGB2BGR_VNG , #COLOR_BayerRG2BGR_VNG , #COLOR_BayerGR2BGR_VNG
+     *
+     * <ul>
+     *   <li>
+     *    Edge-Aware Demosaicing.
+     *   </li>
+     * </ul>
+     *
+     *     #COLOR_BayerBG2BGR_EA , #COLOR_BayerGB2BGR_EA , #COLOR_BayerRG2BGR_EA , #COLOR_BayerGR2BGR_EA
+     *
+     * <ul>
+     *   <li>
+     *    Demosaicing with alpha channel
+     *   </li>
+     * </ul>
+     *
+     *     #COLOR_BayerBG2BGRA , #COLOR_BayerGB2BGRA , #COLOR_BayerRG2BGRA , #COLOR_BayerGR2BGRA
+     *
+     * SEE: cvtColor
+     */
+    public static void demosaicing(Mat src, Mat dst, int code, int dstCn) {
+        demosaicing_0(src.nativeObj, dst.nativeObj, code, dstCn);
+    }
+
+    /**
+     * main function for all demosaicing processes
+     *
+     * @param src input image: 8-bit unsigned or 16-bit unsigned.
+     * @param dst output image of the same size and depth as src.
+     * @param code Color space conversion code (see the description below).
+     * channels is derived automatically from src and code.
+     *
+     * The function can do the following transformations:
+     *
+     * <ul>
+     *   <li>
+     *    Demosaicing using bilinear interpolation
+     *   </li>
+     * </ul>
+     *
+     *     #COLOR_BayerBG2BGR , #COLOR_BayerGB2BGR , #COLOR_BayerRG2BGR , #COLOR_BayerGR2BGR
+     *
+     *     #COLOR_BayerBG2GRAY , #COLOR_BayerGB2GRAY , #COLOR_BayerRG2GRAY , #COLOR_BayerGR2GRAY
+     *
+     * <ul>
+     *   <li>
+     *    Demosaicing using Variable Number of Gradients.
+     *   </li>
+     * </ul>
+     *
+     *     #COLOR_BayerBG2BGR_VNG , #COLOR_BayerGB2BGR_VNG , #COLOR_BayerRG2BGR_VNG , #COLOR_BayerGR2BGR_VNG
+     *
+     * <ul>
+     *   <li>
+     *    Edge-Aware Demosaicing.
+     *   </li>
+     * </ul>
+     *
+     *     #COLOR_BayerBG2BGR_EA , #COLOR_BayerGB2BGR_EA , #COLOR_BayerRG2BGR_EA , #COLOR_BayerGR2BGR_EA
+     *
+     * <ul>
+     *   <li>
+     *    Demosaicing with alpha channel
+     *   </li>
+     * </ul>
+     *
+     *     #COLOR_BayerBG2BGRA , #COLOR_BayerGB2BGRA , #COLOR_BayerRG2BGRA , #COLOR_BayerGR2BGRA
+     *
+     * SEE: cvtColor
+     */
+    public static void demosaicing(Mat src, Mat dst, int code) {
+        demosaicing_1(src.nativeObj, dst.nativeObj, code);
+    }
+
+
+    //
+    // C++:  void cv::dilate(Mat src, Mat& dst, Mat kernel, Point anchor = Point(-1,-1), int iterations = 1, int borderType = BORDER_CONSTANT, Scalar borderValue = morphologyDefaultBorderValue())
+    //
+
+    /**
+     * Dilates an image by using a specific structuring element.
+     *
+     * The function dilates the source image using the specified structuring element that determines the
+     * shape of a pixel neighborhood over which the maximum is taken:
+     * \(\texttt{dst} (x,y) =  \max _{(x',y'):  \, \texttt{element} (x',y') \ne0 } \texttt{src} (x+x',y+y')\)
+     *
+     * The function supports the in-place mode. Dilation can be applied several ( iterations ) times. In
+     * case of multi-channel images, each channel is processed independently.
+     *
+     * @param src input image; the number of channels can be arbitrary, but the depth should be one of
+     * CV_8U, CV_16U, CV_16S, CV_32F or CV_64F.
+     * @param dst output image of the same size and type as src.
+     * @param kernel structuring element used for dilation; if elemenat=Mat(), a 3 x 3 rectangular
+     * structuring element is used. Kernel can be created using #getStructuringElement
+     * @param anchor position of the anchor within the element; default value (-1, -1) means that the
+     * anchor is at the element center.
+     * @param iterations number of times dilation is applied.
+     * @param borderType pixel extrapolation method, see #BorderTypes. #BORDER_WRAP is not suported.
+     * @param borderValue border value in case of a constant border
+     * SEE:  erode, morphologyEx, getStructuringElement
+     */
+    public static void dilate(Mat src, Mat dst, Mat kernel, Point anchor, int iterations, int borderType, Scalar borderValue) {
+        dilate_0(src.nativeObj, dst.nativeObj, kernel.nativeObj, anchor.x, anchor.y, iterations, borderType, borderValue.val[0], borderValue.val[1], borderValue.val[2], borderValue.val[3]);
+    }
+
+    /**
+     * Dilates an image by using a specific structuring element.
+     *
+     * The function dilates the source image using the specified structuring element that determines the
+     * shape of a pixel neighborhood over which the maximum is taken:
+     * \(\texttt{dst} (x,y) =  \max _{(x',y'):  \, \texttt{element} (x',y') \ne0 } \texttt{src} (x+x',y+y')\)
+     *
+     * The function supports the in-place mode. Dilation can be applied several ( iterations ) times. In
+     * case of multi-channel images, each channel is processed independently.
+     *
+     * @param src input image; the number of channels can be arbitrary, but the depth should be one of
+     * CV_8U, CV_16U, CV_16S, CV_32F or CV_64F.
+     * @param dst output image of the same size and type as src.
+     * @param kernel structuring element used for dilation; if elemenat=Mat(), a 3 x 3 rectangular
+     * structuring element is used. Kernel can be created using #getStructuringElement
+     * @param anchor position of the anchor within the element; default value (-1, -1) means that the
+     * anchor is at the element center.
+     * @param iterations number of times dilation is applied.
+     * @param borderType pixel extrapolation method, see #BorderTypes. #BORDER_WRAP is not suported.
+     * SEE:  erode, morphologyEx, getStructuringElement
+     */
+    public static void dilate(Mat src, Mat dst, Mat kernel, Point anchor, int iterations, int borderType) {
+        dilate_1(src.nativeObj, dst.nativeObj, kernel.nativeObj, anchor.x, anchor.y, iterations, borderType);
+    }
+
+    /**
+     * Dilates an image by using a specific structuring element.
+     *
+     * The function dilates the source image using the specified structuring element that determines the
+     * shape of a pixel neighborhood over which the maximum is taken:
+     * \(\texttt{dst} (x,y) =  \max _{(x',y'):  \, \texttt{element} (x',y') \ne0 } \texttt{src} (x+x',y+y')\)
+     *
+     * The function supports the in-place mode. Dilation can be applied several ( iterations ) times. In
+     * case of multi-channel images, each channel is processed independently.
+     *
+     * @param src input image; the number of channels can be arbitrary, but the depth should be one of
+     * CV_8U, CV_16U, CV_16S, CV_32F or CV_64F.
+     * @param dst output image of the same size and type as src.
+     * @param kernel structuring element used for dilation; if elemenat=Mat(), a 3 x 3 rectangular
+     * structuring element is used. Kernel can be created using #getStructuringElement
+     * @param anchor position of the anchor within the element; default value (-1, -1) means that the
+     * anchor is at the element center.
+     * @param iterations number of times dilation is applied.
+     * SEE:  erode, morphologyEx, getStructuringElement
+     */
+    public static void dilate(Mat src, Mat dst, Mat kernel, Point anchor, int iterations) {
+        dilate_2(src.nativeObj, dst.nativeObj, kernel.nativeObj, anchor.x, anchor.y, iterations);
+    }
+
+    /**
+     * Dilates an image by using a specific structuring element.
+     *
+     * The function dilates the source image using the specified structuring element that determines the
+     * shape of a pixel neighborhood over which the maximum is taken:
+     * \(\texttt{dst} (x,y) =  \max _{(x',y'):  \, \texttt{element} (x',y') \ne0 } \texttt{src} (x+x',y+y')\)
+     *
+     * The function supports the in-place mode. Dilation can be applied several ( iterations ) times. In
+     * case of multi-channel images, each channel is processed independently.
+     *
+     * @param src input image; the number of channels can be arbitrary, but the depth should be one of
+     * CV_8U, CV_16U, CV_16S, CV_32F or CV_64F.
+     * @param dst output image of the same size and type as src.
+     * @param kernel structuring element used for dilation; if elemenat=Mat(), a 3 x 3 rectangular
+     * structuring element is used. Kernel can be created using #getStructuringElement
+     * @param anchor position of the anchor within the element; default value (-1, -1) means that the
+     * anchor is at the element center.
+     * SEE:  erode, morphologyEx, getStructuringElement
+     */
+    public static void dilate(Mat src, Mat dst, Mat kernel, Point anchor) {
+        dilate_3(src.nativeObj, dst.nativeObj, kernel.nativeObj, anchor.x, anchor.y);
+    }
+
+    /**
+     * Dilates an image by using a specific structuring element.
+     *
+     * The function dilates the source image using the specified structuring element that determines the
+     * shape of a pixel neighborhood over which the maximum is taken:
+     * \(\texttt{dst} (x,y) =  \max _{(x',y'):  \, \texttt{element} (x',y') \ne0 } \texttt{src} (x+x',y+y')\)
+     *
+     * The function supports the in-place mode. Dilation can be applied several ( iterations ) times. In
+     * case of multi-channel images, each channel is processed independently.
+     *
+     * @param src input image; the number of channels can be arbitrary, but the depth should be one of
+     * CV_8U, CV_16U, CV_16S, CV_32F or CV_64F.
+     * @param dst output image of the same size and type as src.
+     * @param kernel structuring element used for dilation; if elemenat=Mat(), a 3 x 3 rectangular
+     * structuring element is used. Kernel can be created using #getStructuringElement
+     * anchor is at the element center.
+     * SEE:  erode, morphologyEx, getStructuringElement
+     */
+    public static void dilate(Mat src, Mat dst, Mat kernel) {
+        dilate_4(src.nativeObj, dst.nativeObj, kernel.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::distanceTransform(Mat src, Mat& dst, Mat& labels, int distanceType, int maskSize, int labelType = DIST_LABEL_CCOMP)
+    //
+
+    /**
+     * Calculates the distance to the closest zero pixel for each pixel of the source image.
+     *
+     * The function cv::distanceTransform calculates the approximate or precise distance from every binary
+     * image pixel to the nearest zero pixel. For zero image pixels, the distance will obviously be zero.
+     *
+     * When maskSize == #DIST_MASK_PRECISE and distanceType == #DIST_L2 , the function runs the
+     * algorithm described in CITE: Felzenszwalb04 . This algorithm is parallelized with the TBB library.
+     *
+     * In other cases, the algorithm CITE: Borgefors86 is used. This means that for a pixel the function
+     * finds the shortest path to the nearest zero pixel consisting of basic shifts: horizontal, vertical,
+     * diagonal, or knight's move (the latest is available for a \(5\times 5\) mask). The overall
+     * distance is calculated as a sum of these basic distances. Since the distance function should be
+     * symmetric, all of the horizontal and vertical shifts must have the same cost (denoted as a ), all
+     * the diagonal shifts must have the same cost (denoted as {@code b}), and all knight's moves must have the
+     * same cost (denoted as {@code c}). For the #DIST_C and #DIST_L1 types, the distance is calculated
+     * precisely, whereas for #DIST_L2 (Euclidean distance) the distance can be calculated only with a
+     * relative error (a \(5\times 5\) mask gives more accurate results). For {@code a},{@code b}, and {@code c}, OpenCV
+     * uses the values suggested in the original paper:
+     * <ul>
+     *   <li>
+     *  DIST_L1: {@code a = 1, b = 2}
+     *   </li>
+     *   <li>
+     *  DIST_L2:
+     *   <ul>
+     *     <li>
+     *      {@code 3 x 3}: {@code a=0.955, b=1.3693}
+     *     </li>
+     *     <li>
+     *      {@code 5 x 5}: {@code a=1, b=1.4, c=2.1969}
+     *     </li>
+     *   </ul>
+     *   <li>
+     *  DIST_C: {@code a = 1, b = 1}
+     *   </li>
+     * </ul>
+     *
+     * Typically, for a fast, coarse distance estimation #DIST_L2, a \(3\times 3\) mask is used. For a
+     * more accurate distance estimation #DIST_L2, a \(5\times 5\) mask or the precise algorithm is used.
+     * Note that both the precise and the approximate algorithms are linear on the number of pixels.
+     *
+     * This variant of the function does not only compute the minimum distance for each pixel \((x, y)\)
+     * but also identifies the nearest connected component consisting of zero pixels
+     * (labelType==#DIST_LABEL_CCOMP) or the nearest zero pixel (labelType==#DIST_LABEL_PIXEL). Index of the
+     * component/pixel is stored in {@code labels(x, y)}. When labelType==#DIST_LABEL_CCOMP, the function
+     * automatically finds connected components of zero pixels in the input image and marks them with
+     * distinct labels. When labelType==#DIST_LABEL_CCOMP, the function scans through the input image and
+     * marks all the zero pixels with distinct labels.
+     *
+     * In this mode, the complexity is still linear. That is, the function provides a very fast way to
+     * compute the Voronoi diagram for a binary image. Currently, the second variant can use only the
+     * approximate distance transform algorithm, i.e. maskSize=#DIST_MASK_PRECISE is not supported
+     * yet.
+     *
+     * @param src 8-bit, single-channel (binary) source image.
+     * @param dst Output image with calculated distances. It is a 8-bit or 32-bit floating-point,
+     * single-channel image of the same size as src.
+     * @param labels Output 2D array of labels (the discrete Voronoi diagram). It has the type
+     * CV_32SC1 and the same size as src.
+     * @param distanceType Type of distance, see #DistanceTypes
+     * @param maskSize Size of the distance transform mask, see #DistanceTransformMasks.
+     * #DIST_MASK_PRECISE is not supported by this variant. In case of the #DIST_L1 or #DIST_C distance type,
+     * the parameter is forced to 3 because a \(3\times 3\) mask gives the same result as \(5\times
+     * 5\) or any larger aperture.
+     * @param labelType Type of the label array to build, see #DistanceTransformLabelTypes.
+     */
+    public static void distanceTransformWithLabels(Mat src, Mat dst, Mat labels, int distanceType, int maskSize, int labelType) {
+        distanceTransformWithLabels_0(src.nativeObj, dst.nativeObj, labels.nativeObj, distanceType, maskSize, labelType);
+    }
+
+    /**
+     * Calculates the distance to the closest zero pixel for each pixel of the source image.
+     *
+     * The function cv::distanceTransform calculates the approximate or precise distance from every binary
+     * image pixel to the nearest zero pixel. For zero image pixels, the distance will obviously be zero.
+     *
+     * When maskSize == #DIST_MASK_PRECISE and distanceType == #DIST_L2 , the function runs the
+     * algorithm described in CITE: Felzenszwalb04 . This algorithm is parallelized with the TBB library.
+     *
+     * In other cases, the algorithm CITE: Borgefors86 is used. This means that for a pixel the function
+     * finds the shortest path to the nearest zero pixel consisting of basic shifts: horizontal, vertical,
+     * diagonal, or knight's move (the latest is available for a \(5\times 5\) mask). The overall
+     * distance is calculated as a sum of these basic distances. Since the distance function should be
+     * symmetric, all of the horizontal and vertical shifts must have the same cost (denoted as a ), all
+     * the diagonal shifts must have the same cost (denoted as {@code b}), and all knight's moves must have the
+     * same cost (denoted as {@code c}). For the #DIST_C and #DIST_L1 types, the distance is calculated
+     * precisely, whereas for #DIST_L2 (Euclidean distance) the distance can be calculated only with a
+     * relative error (a \(5\times 5\) mask gives more accurate results). For {@code a},{@code b}, and {@code c}, OpenCV
+     * uses the values suggested in the original paper:
+     * <ul>
+     *   <li>
+     *  DIST_L1: {@code a = 1, b = 2}
+     *   </li>
+     *   <li>
+     *  DIST_L2:
+     *   <ul>
+     *     <li>
+     *      {@code 3 x 3}: {@code a=0.955, b=1.3693}
+     *     </li>
+     *     <li>
+     *      {@code 5 x 5}: {@code a=1, b=1.4, c=2.1969}
+     *     </li>
+     *   </ul>
+     *   <li>
+     *  DIST_C: {@code a = 1, b = 1}
+     *   </li>
+     * </ul>
+     *
+     * Typically, for a fast, coarse distance estimation #DIST_L2, a \(3\times 3\) mask is used. For a
+     * more accurate distance estimation #DIST_L2, a \(5\times 5\) mask or the precise algorithm is used.
+     * Note that both the precise and the approximate algorithms are linear on the number of pixels.
+     *
+     * This variant of the function does not only compute the minimum distance for each pixel \((x, y)\)
+     * but also identifies the nearest connected component consisting of zero pixels
+     * (labelType==#DIST_LABEL_CCOMP) or the nearest zero pixel (labelType==#DIST_LABEL_PIXEL). Index of the
+     * component/pixel is stored in {@code labels(x, y)}. When labelType==#DIST_LABEL_CCOMP, the function
+     * automatically finds connected components of zero pixels in the input image and marks them with
+     * distinct labels. When labelType==#DIST_LABEL_CCOMP, the function scans through the input image and
+     * marks all the zero pixels with distinct labels.
+     *
+     * In this mode, the complexity is still linear. That is, the function provides a very fast way to
+     * compute the Voronoi diagram for a binary image. Currently, the second variant can use only the
+     * approximate distance transform algorithm, i.e. maskSize=#DIST_MASK_PRECISE is not supported
+     * yet.
+     *
+     * @param src 8-bit, single-channel (binary) source image.
+     * @param dst Output image with calculated distances. It is a 8-bit or 32-bit floating-point,
+     * single-channel image of the same size as src.
+     * @param labels Output 2D array of labels (the discrete Voronoi diagram). It has the type
+     * CV_32SC1 and the same size as src.
+     * @param distanceType Type of distance, see #DistanceTypes
+     * @param maskSize Size of the distance transform mask, see #DistanceTransformMasks.
+     * #DIST_MASK_PRECISE is not supported by this variant. In case of the #DIST_L1 or #DIST_C distance type,
+     * the parameter is forced to 3 because a \(3\times 3\) mask gives the same result as \(5\times
+     * 5\) or any larger aperture.
+     */
+    public static void distanceTransformWithLabels(Mat src, Mat dst, Mat labels, int distanceType, int maskSize) {
+        distanceTransformWithLabels_1(src.nativeObj, dst.nativeObj, labels.nativeObj, distanceType, maskSize);
+    }
+
+
+    //
+    // C++:  void cv::distanceTransform(Mat src, Mat& dst, int distanceType, int maskSize, int dstType = CV_32F)
+    //
+
+    /**
+     *
+     * @param src 8-bit, single-channel (binary) source image.
+     * @param dst Output image with calculated distances. It is a 8-bit or 32-bit floating-point,
+     * single-channel image of the same size as src .
+     * @param distanceType Type of distance, see #DistanceTypes
+     * @param maskSize Size of the distance transform mask, see #DistanceTransformMasks. In case of the
+     * #DIST_L1 or #DIST_C distance type, the parameter is forced to 3 because a \(3\times 3\) mask gives
+     * the same result as \(5\times 5\) or any larger aperture.
+     * @param dstType Type of output image. It can be CV_8U or CV_32F. Type CV_8U can be used only for
+     * the first variant of the function and distanceType == #DIST_L1.
+     */
+    public static void distanceTransform(Mat src, Mat dst, int distanceType, int maskSize, int dstType) {
+        distanceTransform_0(src.nativeObj, dst.nativeObj, distanceType, maskSize, dstType);
+    }
+
+    /**
+     *
+     * @param src 8-bit, single-channel (binary) source image.
+     * @param dst Output image with calculated distances. It is a 8-bit or 32-bit floating-point,
+     * single-channel image of the same size as src .
+     * @param distanceType Type of distance, see #DistanceTypes
+     * @param maskSize Size of the distance transform mask, see #DistanceTransformMasks. In case of the
+     * #DIST_L1 or #DIST_C distance type, the parameter is forced to 3 because a \(3\times 3\) mask gives
+     * the same result as \(5\times 5\) or any larger aperture.
+     * the first variant of the function and distanceType == #DIST_L1.
+     */
+    public static void distanceTransform(Mat src, Mat dst, int distanceType, int maskSize) {
+        distanceTransform_1(src.nativeObj, dst.nativeObj, distanceType, maskSize);
+    }
+
+
+    //
+    // C++:  void cv::drawContours(Mat& image, vector_vector_Point contours, int contourIdx, Scalar color, int thickness = 1, int lineType = LINE_8, Mat hierarchy = Mat(), int maxLevel = INT_MAX, Point offset = Point())
+    //
+
+    /**
+     * Draws contours outlines or filled contours.
+     *
+     * The function draws contour outlines in the image if \(\texttt{thickness} \ge 0\) or fills the area
+     * bounded by the contours if \(\texttt{thickness}&lt;0\) . The example below shows how to retrieve
+     * connected components from the binary image and label them: :
+     * INCLUDE: snippets/imgproc_drawContours.cpp
+     *
+     * @param image Destination image.
+     * @param contours All the input contours. Each contour is stored as a point vector.
+     * @param contourIdx Parameter indicating a contour to draw. If it is negative, all the contours are drawn.
+     * @param color Color of the contours.
+     * @param thickness Thickness of lines the contours are drawn with. If it is negative (for example,
+     * thickness=#FILLED ), the contour interiors are drawn.
+     * @param lineType Line connectivity. See #LineTypes
+     * @param hierarchy Optional information about hierarchy. It is only needed if you want to draw only
+     * some of the contours (see maxLevel ).
+     * @param maxLevel Maximal level for drawn contours. If it is 0, only the specified contour is drawn.
+     * If it is 1, the function draws the contour(s) and all the nested contours. If it is 2, the function
+     * draws the contours, all the nested contours, all the nested-to-nested contours, and so on. This
+     * parameter is only taken into account when there is hierarchy available.
+     * @param offset Optional contour shift parameter. Shift all the drawn contours by the specified
+     * \(\texttt{offset}=(dx,dy)\) .
+     * <b>Note:</b> When thickness=#FILLED, the function is designed to handle connected components with holes correctly
+     * even when no hierarchy date is provided. This is done by analyzing all the outlines together
+     * using even-odd rule. This may give incorrect results if you have a joint collection of separately retrieved
+     * contours. In order to solve this problem, you need to call #drawContours separately for each sub-group
+     * of contours, or iterate over the collection using contourIdx parameter.
+     */
+    public static void drawContours(Mat image, List<MatOfPoint> contours, int contourIdx, Scalar color, int thickness, int lineType, Mat hierarchy, int maxLevel, Point offset) {
+        List<Mat> contours_tmplm = new ArrayList<Mat>((contours != null) ? contours.size() : 0);
+        Mat contours_mat = Converters.vector_vector_Point_to_Mat(contours, contours_tmplm);
+        drawContours_0(image.nativeObj, contours_mat.nativeObj, contourIdx, color.val[0], color.val[1], color.val[2], color.val[3], thickness, lineType, hierarchy.nativeObj, maxLevel, offset.x, offset.y);
+    }
+
+    /**
+     * Draws contours outlines or filled contours.
+     *
+     * The function draws contour outlines in the image if \(\texttt{thickness} \ge 0\) or fills the area
+     * bounded by the contours if \(\texttt{thickness}&lt;0\) . The example below shows how to retrieve
+     * connected components from the binary image and label them: :
+     * INCLUDE: snippets/imgproc_drawContours.cpp
+     *
+     * @param image Destination image.
+     * @param contours All the input contours. Each contour is stored as a point vector.
+     * @param contourIdx Parameter indicating a contour to draw. If it is negative, all the contours are drawn.
+     * @param color Color of the contours.
+     * @param thickness Thickness of lines the contours are drawn with. If it is negative (for example,
+     * thickness=#FILLED ), the contour interiors are drawn.
+     * @param lineType Line connectivity. See #LineTypes
+     * @param hierarchy Optional information about hierarchy. It is only needed if you want to draw only
+     * some of the contours (see maxLevel ).
+     * @param maxLevel Maximal level for drawn contours. If it is 0, only the specified contour is drawn.
+     * If it is 1, the function draws the contour(s) and all the nested contours. If it is 2, the function
+     * draws the contours, all the nested contours, all the nested-to-nested contours, and so on. This
+     * parameter is only taken into account when there is hierarchy available.
+     * \(\texttt{offset}=(dx,dy)\) .
+     * <b>Note:</b> When thickness=#FILLED, the function is designed to handle connected components with holes correctly
+     * even when no hierarchy date is provided. This is done by analyzing all the outlines together
+     * using even-odd rule. This may give incorrect results if you have a joint collection of separately retrieved
+     * contours. In order to solve this problem, you need to call #drawContours separately for each sub-group
+     * of contours, or iterate over the collection using contourIdx parameter.
+     */
+    public static void drawContours(Mat image, List<MatOfPoint> contours, int contourIdx, Scalar color, int thickness, int lineType, Mat hierarchy, int maxLevel) {
+        List<Mat> contours_tmplm = new ArrayList<Mat>((contours != null) ? contours.size() : 0);
+        Mat contours_mat = Converters.vector_vector_Point_to_Mat(contours, contours_tmplm);
+        drawContours_1(image.nativeObj, contours_mat.nativeObj, contourIdx, color.val[0], color.val[1], color.val[2], color.val[3], thickness, lineType, hierarchy.nativeObj, maxLevel);
+    }
+
+    /**
+     * Draws contours outlines or filled contours.
+     *
+     * The function draws contour outlines in the image if \(\texttt{thickness} \ge 0\) or fills the area
+     * bounded by the contours if \(\texttt{thickness}&lt;0\) . The example below shows how to retrieve
+     * connected components from the binary image and label them: :
+     * INCLUDE: snippets/imgproc_drawContours.cpp
+     *
+     * @param image Destination image.
+     * @param contours All the input contours. Each contour is stored as a point vector.
+     * @param contourIdx Parameter indicating a contour to draw. If it is negative, all the contours are drawn.
+     * @param color Color of the contours.
+     * @param thickness Thickness of lines the contours are drawn with. If it is negative (for example,
+     * thickness=#FILLED ), the contour interiors are drawn.
+     * @param lineType Line connectivity. See #LineTypes
+     * @param hierarchy Optional information about hierarchy. It is only needed if you want to draw only
+     * some of the contours (see maxLevel ).
+     * If it is 1, the function draws the contour(s) and all the nested contours. If it is 2, the function
+     * draws the contours, all the nested contours, all the nested-to-nested contours, and so on. This
+     * parameter is only taken into account when there is hierarchy available.
+     * \(\texttt{offset}=(dx,dy)\) .
+     * <b>Note:</b> When thickness=#FILLED, the function is designed to handle connected components with holes correctly
+     * even when no hierarchy date is provided. This is done by analyzing all the outlines together
+     * using even-odd rule. This may give incorrect results if you have a joint collection of separately retrieved
+     * contours. In order to solve this problem, you need to call #drawContours separately for each sub-group
+     * of contours, or iterate over the collection using contourIdx parameter.
+     */
+    public static void drawContours(Mat image, List<MatOfPoint> contours, int contourIdx, Scalar color, int thickness, int lineType, Mat hierarchy) {
+        List<Mat> contours_tmplm = new ArrayList<Mat>((contours != null) ? contours.size() : 0);
+        Mat contours_mat = Converters.vector_vector_Point_to_Mat(contours, contours_tmplm);
+        drawContours_2(image.nativeObj, contours_mat.nativeObj, contourIdx, color.val[0], color.val[1], color.val[2], color.val[3], thickness, lineType, hierarchy.nativeObj);
+    }
+
+    /**
+     * Draws contours outlines or filled contours.
+     *
+     * The function draws contour outlines in the image if \(\texttt{thickness} \ge 0\) or fills the area
+     * bounded by the contours if \(\texttt{thickness}&lt;0\) . The example below shows how to retrieve
+     * connected components from the binary image and label them: :
+     * INCLUDE: snippets/imgproc_drawContours.cpp
+     *
+     * @param image Destination image.
+     * @param contours All the input contours. Each contour is stored as a point vector.
+     * @param contourIdx Parameter indicating a contour to draw. If it is negative, all the contours are drawn.
+     * @param color Color of the contours.
+     * @param thickness Thickness of lines the contours are drawn with. If it is negative (for example,
+     * thickness=#FILLED ), the contour interiors are drawn.
+     * @param lineType Line connectivity. See #LineTypes
+     * some of the contours (see maxLevel ).
+     * If it is 1, the function draws the contour(s) and all the nested contours. If it is 2, the function
+     * draws the contours, all the nested contours, all the nested-to-nested contours, and so on. This
+     * parameter is only taken into account when there is hierarchy available.
+     * \(\texttt{offset}=(dx,dy)\) .
+     * <b>Note:</b> When thickness=#FILLED, the function is designed to handle connected components with holes correctly
+     * even when no hierarchy date is provided. This is done by analyzing all the outlines together
+     * using even-odd rule. This may give incorrect results if you have a joint collection of separately retrieved
+     * contours. In order to solve this problem, you need to call #drawContours separately for each sub-group
+     * of contours, or iterate over the collection using contourIdx parameter.
+     */
+    public static void drawContours(Mat image, List<MatOfPoint> contours, int contourIdx, Scalar color, int thickness, int lineType) {
+        List<Mat> contours_tmplm = new ArrayList<Mat>((contours != null) ? contours.size() : 0);
+        Mat contours_mat = Converters.vector_vector_Point_to_Mat(contours, contours_tmplm);
+        drawContours_3(image.nativeObj, contours_mat.nativeObj, contourIdx, color.val[0], color.val[1], color.val[2], color.val[3], thickness, lineType);
+    }
+
+    /**
+     * Draws contours outlines or filled contours.
+     *
+     * The function draws contour outlines in the image if \(\texttt{thickness} \ge 0\) or fills the area
+     * bounded by the contours if \(\texttt{thickness}&lt;0\) . The example below shows how to retrieve
+     * connected components from the binary image and label them: :
+     * INCLUDE: snippets/imgproc_drawContours.cpp
+     *
+     * @param image Destination image.
+     * @param contours All the input contours. Each contour is stored as a point vector.
+     * @param contourIdx Parameter indicating a contour to draw. If it is negative, all the contours are drawn.
+     * @param color Color of the contours.
+     * @param thickness Thickness of lines the contours are drawn with. If it is negative (for example,
+     * thickness=#FILLED ), the contour interiors are drawn.
+     * some of the contours (see maxLevel ).
+     * If it is 1, the function draws the contour(s) and all the nested contours. If it is 2, the function
+     * draws the contours, all the nested contours, all the nested-to-nested contours, and so on. This
+     * parameter is only taken into account when there is hierarchy available.
+     * \(\texttt{offset}=(dx,dy)\) .
+     * <b>Note:</b> When thickness=#FILLED, the function is designed to handle connected components with holes correctly
+     * even when no hierarchy date is provided. This is done by analyzing all the outlines together
+     * using even-odd rule. This may give incorrect results if you have a joint collection of separately retrieved
+     * contours. In order to solve this problem, you need to call #drawContours separately for each sub-group
+     * of contours, or iterate over the collection using contourIdx parameter.
+     */
+    public static void drawContours(Mat image, List<MatOfPoint> contours, int contourIdx, Scalar color, int thickness) {
+        List<Mat> contours_tmplm = new ArrayList<Mat>((contours != null) ? contours.size() : 0);
+        Mat contours_mat = Converters.vector_vector_Point_to_Mat(contours, contours_tmplm);
+        drawContours_4(image.nativeObj, contours_mat.nativeObj, contourIdx, color.val[0], color.val[1], color.val[2], color.val[3], thickness);
+    }
+
+    /**
+     * Draws contours outlines or filled contours.
+     *
+     * The function draws contour outlines in the image if \(\texttt{thickness} \ge 0\) or fills the area
+     * bounded by the contours if \(\texttt{thickness}&lt;0\) . The example below shows how to retrieve
+     * connected components from the binary image and label them: :
+     * INCLUDE: snippets/imgproc_drawContours.cpp
+     *
+     * @param image Destination image.
+     * @param contours All the input contours. Each contour is stored as a point vector.
+     * @param contourIdx Parameter indicating a contour to draw. If it is negative, all the contours are drawn.
+     * @param color Color of the contours.
+     * thickness=#FILLED ), the contour interiors are drawn.
+     * some of the contours (see maxLevel ).
+     * If it is 1, the function draws the contour(s) and all the nested contours. If it is 2, the function
+     * draws the contours, all the nested contours, all the nested-to-nested contours, and so on. This
+     * parameter is only taken into account when there is hierarchy available.
+     * \(\texttt{offset}=(dx,dy)\) .
+     * <b>Note:</b> When thickness=#FILLED, the function is designed to handle connected components with holes correctly
+     * even when no hierarchy date is provided. This is done by analyzing all the outlines together
+     * using even-odd rule. This may give incorrect results if you have a joint collection of separately retrieved
+     * contours. In order to solve this problem, you need to call #drawContours separately for each sub-group
+     * of contours, or iterate over the collection using contourIdx parameter.
+     */
+    public static void drawContours(Mat image, List<MatOfPoint> contours, int contourIdx, Scalar color) {
+        List<Mat> contours_tmplm = new ArrayList<Mat>((contours != null) ? contours.size() : 0);
+        Mat contours_mat = Converters.vector_vector_Point_to_Mat(contours, contours_tmplm);
+        drawContours_5(image.nativeObj, contours_mat.nativeObj, contourIdx, color.val[0], color.val[1], color.val[2], color.val[3]);
+    }
+
+
+    //
+    // C++:  void cv::drawMarker(Mat& img, Point position, Scalar color, int markerType = MARKER_CROSS, int markerSize = 20, int thickness = 1, int line_type = 8)
+    //
+
+    /**
+     * Draws a marker on a predefined position in an image.
+     *
+     * The function cv::drawMarker draws a marker on a given position in the image. For the moment several
+     * marker types are supported, see #MarkerTypes for more information.
+     *
+     * @param img Image.
+     * @param position The point where the crosshair is positioned.
+     * @param color Line color.
+     * @param markerType The specific type of marker you want to use, see #MarkerTypes
+     * @param thickness Line thickness.
+     * @param line_type Type of the line, See #LineTypes
+     * @param markerSize The length of the marker axis [default = 20 pixels]
+     */
+    public static void drawMarker(Mat img, Point position, Scalar color, int markerType, int markerSize, int thickness, int line_type) {
+        drawMarker_0(img.nativeObj, position.x, position.y, color.val[0], color.val[1], color.val[2], color.val[3], markerType, markerSize, thickness, line_type);
+    }
+
+    /**
+     * Draws a marker on a predefined position in an image.
+     *
+     * The function cv::drawMarker draws a marker on a given position in the image. For the moment several
+     * marker types are supported, see #MarkerTypes for more information.
+     *
+     * @param img Image.
+     * @param position The point where the crosshair is positioned.
+     * @param color Line color.
+     * @param markerType The specific type of marker you want to use, see #MarkerTypes
+     * @param thickness Line thickness.
+     * @param markerSize The length of the marker axis [default = 20 pixels]
+     */
+    public static void drawMarker(Mat img, Point position, Scalar color, int markerType, int markerSize, int thickness) {
+        drawMarker_1(img.nativeObj, position.x, position.y, color.val[0], color.val[1], color.val[2], color.val[3], markerType, markerSize, thickness);
+    }
+
+    /**
+     * Draws a marker on a predefined position in an image.
+     *
+     * The function cv::drawMarker draws a marker on a given position in the image. For the moment several
+     * marker types are supported, see #MarkerTypes for more information.
+     *
+     * @param img Image.
+     * @param position The point where the crosshair is positioned.
+     * @param color Line color.
+     * @param markerType The specific type of marker you want to use, see #MarkerTypes
+     * @param markerSize The length of the marker axis [default = 20 pixels]
+     */
+    public static void drawMarker(Mat img, Point position, Scalar color, int markerType, int markerSize) {
+        drawMarker_2(img.nativeObj, position.x, position.y, color.val[0], color.val[1], color.val[2], color.val[3], markerType, markerSize);
+    }
+
+    /**
+     * Draws a marker on a predefined position in an image.
+     *
+     * The function cv::drawMarker draws a marker on a given position in the image. For the moment several
+     * marker types are supported, see #MarkerTypes for more information.
+     *
+     * @param img Image.
+     * @param position The point where the crosshair is positioned.
+     * @param color Line color.
+     * @param markerType The specific type of marker you want to use, see #MarkerTypes
+     */
+    public static void drawMarker(Mat img, Point position, Scalar color, int markerType) {
+        drawMarker_3(img.nativeObj, position.x, position.y, color.val[0], color.val[1], color.val[2], color.val[3], markerType);
+    }
+
+    /**
+     * Draws a marker on a predefined position in an image.
+     *
+     * The function cv::drawMarker draws a marker on a given position in the image. For the moment several
+     * marker types are supported, see #MarkerTypes for more information.
+     *
+     * @param img Image.
+     * @param position The point where the crosshair is positioned.
+     * @param color Line color.
+     */
+    public static void drawMarker(Mat img, Point position, Scalar color) {
+        drawMarker_4(img.nativeObj, position.x, position.y, color.val[0], color.val[1], color.val[2], color.val[3]);
+    }
+
+
+    //
+    // C++:  void cv::ellipse(Mat& img, Point center, Size axes, double angle, double startAngle, double endAngle, Scalar color, int thickness = 1, int lineType = LINE_8, int shift = 0)
+    //
+
+    /**
+     * Draws a simple or thick elliptic arc or fills an ellipse sector.
+     *
+     * The function cv::ellipse with more parameters draws an ellipse outline, a filled ellipse, an elliptic
+     * arc, or a filled ellipse sector. The drawing code uses general parametric form.
+     * A piecewise-linear curve is used to approximate the elliptic arc
+     * boundary. If you need more control of the ellipse rendering, you can retrieve the curve using
+     * #ellipse2Poly and then render it with #polylines or fill it with #fillPoly. If you use the first
+     * variant of the function and want to draw the whole ellipse, not an arc, pass {@code startAngle=0} and
+     * {@code endAngle=360}. If {@code startAngle} is greater than {@code endAngle}, they are swapped. The figure below explains
+     * the meaning of the parameters to draw the blue arc.
+     *
+     * ![Parameters of Elliptic Arc](pics/ellipse.svg)
+     *
+     * @param img Image.
+     * @param center Center of the ellipse.
+     * @param axes Half of the size of the ellipse main axes.
+     * @param angle Ellipse rotation angle in degrees.
+     * @param startAngle Starting angle of the elliptic arc in degrees.
+     * @param endAngle Ending angle of the elliptic arc in degrees.
+     * @param color Ellipse color.
+     * @param thickness Thickness of the ellipse arc outline, if positive. Otherwise, this indicates that
+     * a filled ellipse sector is to be drawn.
+     * @param lineType Type of the ellipse boundary. See #LineTypes
+     * @param shift Number of fractional bits in the coordinates of the center and values of axes.
+     */
+    public static void ellipse(Mat img, Point center, Size axes, double angle, double startAngle, double endAngle, Scalar color, int thickness, int lineType, int shift) {
+        ellipse_0(img.nativeObj, center.x, center.y, axes.width, axes.height, angle, startAngle, endAngle, color.val[0], color.val[1], color.val[2], color.val[3], thickness, lineType, shift);
+    }
+
+    /**
+     * Draws a simple or thick elliptic arc or fills an ellipse sector.
+     *
+     * The function cv::ellipse with more parameters draws an ellipse outline, a filled ellipse, an elliptic
+     * arc, or a filled ellipse sector. The drawing code uses general parametric form.
+     * A piecewise-linear curve is used to approximate the elliptic arc
+     * boundary. If you need more control of the ellipse rendering, you can retrieve the curve using
+     * #ellipse2Poly and then render it with #polylines or fill it with #fillPoly. If you use the first
+     * variant of the function and want to draw the whole ellipse, not an arc, pass {@code startAngle=0} and
+     * {@code endAngle=360}. If {@code startAngle} is greater than {@code endAngle}, they are swapped. The figure below explains
+     * the meaning of the parameters to draw the blue arc.
+     *
+     * ![Parameters of Elliptic Arc](pics/ellipse.svg)
+     *
+     * @param img Image.
+     * @param center Center of the ellipse.
+     * @param axes Half of the size of the ellipse main axes.
+     * @param angle Ellipse rotation angle in degrees.
+     * @param startAngle Starting angle of the elliptic arc in degrees.
+     * @param endAngle Ending angle of the elliptic arc in degrees.
+     * @param color Ellipse color.
+     * @param thickness Thickness of the ellipse arc outline, if positive. Otherwise, this indicates that
+     * a filled ellipse sector is to be drawn.
+     * @param lineType Type of the ellipse boundary. See #LineTypes
+     */
+    public static void ellipse(Mat img, Point center, Size axes, double angle, double startAngle, double endAngle, Scalar color, int thickness, int lineType) {
+        ellipse_1(img.nativeObj, center.x, center.y, axes.width, axes.height, angle, startAngle, endAngle, color.val[0], color.val[1], color.val[2], color.val[3], thickness, lineType);
+    }
+
+    /**
+     * Draws a simple or thick elliptic arc or fills an ellipse sector.
+     *
+     * The function cv::ellipse with more parameters draws an ellipse outline, a filled ellipse, an elliptic
+     * arc, or a filled ellipse sector. The drawing code uses general parametric form.
+     * A piecewise-linear curve is used to approximate the elliptic arc
+     * boundary. If you need more control of the ellipse rendering, you can retrieve the curve using
+     * #ellipse2Poly and then render it with #polylines or fill it with #fillPoly. If you use the first
+     * variant of the function and want to draw the whole ellipse, not an arc, pass {@code startAngle=0} and
+     * {@code endAngle=360}. If {@code startAngle} is greater than {@code endAngle}, they are swapped. The figure below explains
+     * the meaning of the parameters to draw the blue arc.
+     *
+     * ![Parameters of Elliptic Arc](pics/ellipse.svg)
+     *
+     * @param img Image.
+     * @param center Center of the ellipse.
+     * @param axes Half of the size of the ellipse main axes.
+     * @param angle Ellipse rotation angle in degrees.
+     * @param startAngle Starting angle of the elliptic arc in degrees.
+     * @param endAngle Ending angle of the elliptic arc in degrees.
+     * @param color Ellipse color.
+     * @param thickness Thickness of the ellipse arc outline, if positive. Otherwise, this indicates that
+     * a filled ellipse sector is to be drawn.
+     */
+    public static void ellipse(Mat img, Point center, Size axes, double angle, double startAngle, double endAngle, Scalar color, int thickness) {
+        ellipse_2(img.nativeObj, center.x, center.y, axes.width, axes.height, angle, startAngle, endAngle, color.val[0], color.val[1], color.val[2], color.val[3], thickness);
+    }
+
+    /**
+     * Draws a simple or thick elliptic arc or fills an ellipse sector.
+     *
+     * The function cv::ellipse with more parameters draws an ellipse outline, a filled ellipse, an elliptic
+     * arc, or a filled ellipse sector. The drawing code uses general parametric form.
+     * A piecewise-linear curve is used to approximate the elliptic arc
+     * boundary. If you need more control of the ellipse rendering, you can retrieve the curve using
+     * #ellipse2Poly and then render it with #polylines or fill it with #fillPoly. If you use the first
+     * variant of the function and want to draw the whole ellipse, not an arc, pass {@code startAngle=0} and
+     * {@code endAngle=360}. If {@code startAngle} is greater than {@code endAngle}, they are swapped. The figure below explains
+     * the meaning of the parameters to draw the blue arc.
+     *
+     * ![Parameters of Elliptic Arc](pics/ellipse.svg)
+     *
+     * @param img Image.
+     * @param center Center of the ellipse.
+     * @param axes Half of the size of the ellipse main axes.
+     * @param angle Ellipse rotation angle in degrees.
+     * @param startAngle Starting angle of the elliptic arc in degrees.
+     * @param endAngle Ending angle of the elliptic arc in degrees.
+     * @param color Ellipse color.
+     * a filled ellipse sector is to be drawn.
+     */
+    public static void ellipse(Mat img, Point center, Size axes, double angle, double startAngle, double endAngle, Scalar color) {
+        ellipse_3(img.nativeObj, center.x, center.y, axes.width, axes.height, angle, startAngle, endAngle, color.val[0], color.val[1], color.val[2], color.val[3]);
+    }
+
+
+    //
+    // C++:  void cv::ellipse(Mat& img, RotatedRect box, Scalar color, int thickness = 1, int lineType = LINE_8)
+    //
+
+    /**
+     *
+     * @param img Image.
+     * @param box Alternative ellipse representation via RotatedRect. This means that the function draws
+     * an ellipse inscribed in the rotated rectangle.
+     * @param color Ellipse color.
+     * @param thickness Thickness of the ellipse arc outline, if positive. Otherwise, this indicates that
+     * a filled ellipse sector is to be drawn.
+     * @param lineType Type of the ellipse boundary. See #LineTypes
+     */
+    public static void ellipse(Mat img, RotatedRect box, Scalar color, int thickness, int lineType) {
+        ellipse_4(img.nativeObj, box.center.x, box.center.y, box.size.width, box.size.height, box.angle, color.val[0], color.val[1], color.val[2], color.val[3], thickness, lineType);
+    }
+
+    /**
+     *
+     * @param img Image.
+     * @param box Alternative ellipse representation via RotatedRect. This means that the function draws
+     * an ellipse inscribed in the rotated rectangle.
+     * @param color Ellipse color.
+     * @param thickness Thickness of the ellipse arc outline, if positive. Otherwise, this indicates that
+     * a filled ellipse sector is to be drawn.
+     */
+    public static void ellipse(Mat img, RotatedRect box, Scalar color, int thickness) {
+        ellipse_5(img.nativeObj, box.center.x, box.center.y, box.size.width, box.size.height, box.angle, color.val[0], color.val[1], color.val[2], color.val[3], thickness);
+    }
+
+    /**
+     *
+     * @param img Image.
+     * @param box Alternative ellipse representation via RotatedRect. This means that the function draws
+     * an ellipse inscribed in the rotated rectangle.
+     * @param color Ellipse color.
+     * a filled ellipse sector is to be drawn.
+     */
+    public static void ellipse(Mat img, RotatedRect box, Scalar color) {
+        ellipse_6(img.nativeObj, box.center.x, box.center.y, box.size.width, box.size.height, box.angle, color.val[0], color.val[1], color.val[2], color.val[3]);
+    }
+
+
+    //
+    // C++:  void cv::ellipse2Poly(Point center, Size axes, int angle, int arcStart, int arcEnd, int delta, vector_Point& pts)
+    //
+
+    /**
+     * Approximates an elliptic arc with a polyline.
+     *
+     * The function ellipse2Poly computes the vertices of a polyline that approximates the specified
+     * elliptic arc. It is used by #ellipse. If {@code arcStart} is greater than {@code arcEnd}, they are swapped.
+     *
+     * @param center Center of the arc.
+     * @param axes Half of the size of the ellipse main axes. See #ellipse for details.
+     * @param angle Rotation angle of the ellipse in degrees. See #ellipse for details.
+     * @param arcStart Starting angle of the elliptic arc in degrees.
+     * @param arcEnd Ending angle of the elliptic arc in degrees.
+     * @param delta Angle between the subsequent polyline vertices. It defines the approximation
+     * accuracy.
+     * @param pts Output vector of polyline vertices.
+     */
+    public static void ellipse2Poly(Point center, Size axes, int angle, int arcStart, int arcEnd, int delta, MatOfPoint pts) {
+        Mat pts_mat = pts;
+        ellipse2Poly_0(center.x, center.y, axes.width, axes.height, angle, arcStart, arcEnd, delta, pts_mat.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::equalizeHist(Mat src, Mat& dst)
+    //
+
+    /**
+     * Equalizes the histogram of a grayscale image.
+     *
+     * The function equalizes the histogram of the input image using the following algorithm:
+     *
+     * <ul>
+     *   <li>
+     *  Calculate the histogram \(H\) for src .
+     *   </li>
+     *   <li>
+     *  Normalize the histogram so that the sum of histogram bins is 255.
+     *   </li>
+     *   <li>
+     *  Compute the integral of the histogram:
+     * \(H'_i =  \sum _{0  \le j &lt; i} H(j)\)
+     *   </li>
+     *   <li>
+     *  Transform the image using \(H'\) as a look-up table: \(\texttt{dst}(x,y) = H'(\texttt{src}(x,y))\)
+     *   </li>
+     * </ul>
+     *
+     * The algorithm normalizes the brightness and increases the contrast of the image.
+     *
+     * @param src Source 8-bit single channel image.
+     * @param dst Destination image of the same size and type as src .
+     */
+    public static void equalizeHist(Mat src, Mat dst) {
+        equalizeHist_0(src.nativeObj, dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::erode(Mat src, Mat& dst, Mat kernel, Point anchor = Point(-1,-1), int iterations = 1, int borderType = BORDER_CONSTANT, Scalar borderValue = morphologyDefaultBorderValue())
+    //
+
+    /**
+     * Erodes an image by using a specific structuring element.
+     *
+     * The function erodes the source image using the specified structuring element that determines the
+     * shape of a pixel neighborhood over which the minimum is taken:
+     *
+     * \(\texttt{dst} (x,y) =  \min _{(x',y'):  \, \texttt{element} (x',y') \ne0 } \texttt{src} (x+x',y+y')\)
+     *
+     * The function supports the in-place mode. Erosion can be applied several ( iterations ) times. In
+     * case of multi-channel images, each channel is processed independently.
+     *
+     * @param src input image; the number of channels can be arbitrary, but the depth should be one of
+     * CV_8U, CV_16U, CV_16S, CV_32F or CV_64F.
+     * @param dst output image of the same size and type as src.
+     * @param kernel structuring element used for erosion; if {@code element=Mat()}, a {@code 3 x 3} rectangular
+     * structuring element is used. Kernel can be created using #getStructuringElement.
+     * @param anchor position of the anchor within the element; default value (-1, -1) means that the
+     * anchor is at the element center.
+     * @param iterations number of times erosion is applied.
+     * @param borderType pixel extrapolation method, see #BorderTypes. #BORDER_WRAP is not supported.
+     * @param borderValue border value in case of a constant border
+     * SEE:  dilate, morphologyEx, getStructuringElement
+     */
+    public static void erode(Mat src, Mat dst, Mat kernel, Point anchor, int iterations, int borderType, Scalar borderValue) {
+        erode_0(src.nativeObj, dst.nativeObj, kernel.nativeObj, anchor.x, anchor.y, iterations, borderType, borderValue.val[0], borderValue.val[1], borderValue.val[2], borderValue.val[3]);
+    }
+
+    /**
+     * Erodes an image by using a specific structuring element.
+     *
+     * The function erodes the source image using the specified structuring element that determines the
+     * shape of a pixel neighborhood over which the minimum is taken:
+     *
+     * \(\texttt{dst} (x,y) =  \min _{(x',y'):  \, \texttt{element} (x',y') \ne0 } \texttt{src} (x+x',y+y')\)
+     *
+     * The function supports the in-place mode. Erosion can be applied several ( iterations ) times. In
+     * case of multi-channel images, each channel is processed independently.
+     *
+     * @param src input image; the number of channels can be arbitrary, but the depth should be one of
+     * CV_8U, CV_16U, CV_16S, CV_32F or CV_64F.
+     * @param dst output image of the same size and type as src.
+     * @param kernel structuring element used for erosion; if {@code element=Mat()}, a {@code 3 x 3} rectangular
+     * structuring element is used. Kernel can be created using #getStructuringElement.
+     * @param anchor position of the anchor within the element; default value (-1, -1) means that the
+     * anchor is at the element center.
+     * @param iterations number of times erosion is applied.
+     * @param borderType pixel extrapolation method, see #BorderTypes. #BORDER_WRAP is not supported.
+     * SEE:  dilate, morphologyEx, getStructuringElement
+     */
+    public static void erode(Mat src, Mat dst, Mat kernel, Point anchor, int iterations, int borderType) {
+        erode_1(src.nativeObj, dst.nativeObj, kernel.nativeObj, anchor.x, anchor.y, iterations, borderType);
+    }
+
+    /**
+     * Erodes an image by using a specific structuring element.
+     *
+     * The function erodes the source image using the specified structuring element that determines the
+     * shape of a pixel neighborhood over which the minimum is taken:
+     *
+     * \(\texttt{dst} (x,y) =  \min _{(x',y'):  \, \texttt{element} (x',y') \ne0 } \texttt{src} (x+x',y+y')\)
+     *
+     * The function supports the in-place mode. Erosion can be applied several ( iterations ) times. In
+     * case of multi-channel images, each channel is processed independently.
+     *
+     * @param src input image; the number of channels can be arbitrary, but the depth should be one of
+     * CV_8U, CV_16U, CV_16S, CV_32F or CV_64F.
+     * @param dst output image of the same size and type as src.
+     * @param kernel structuring element used for erosion; if {@code element=Mat()}, a {@code 3 x 3} rectangular
+     * structuring element is used. Kernel can be created using #getStructuringElement.
+     * @param anchor position of the anchor within the element; default value (-1, -1) means that the
+     * anchor is at the element center.
+     * @param iterations number of times erosion is applied.
+     * SEE:  dilate, morphologyEx, getStructuringElement
+     */
+    public static void erode(Mat src, Mat dst, Mat kernel, Point anchor, int iterations) {
+        erode_2(src.nativeObj, dst.nativeObj, kernel.nativeObj, anchor.x, anchor.y, iterations);
+    }
+
+    /**
+     * Erodes an image by using a specific structuring element.
+     *
+     * The function erodes the source image using the specified structuring element that determines the
+     * shape of a pixel neighborhood over which the minimum is taken:
+     *
+     * \(\texttt{dst} (x,y) =  \min _{(x',y'):  \, \texttt{element} (x',y') \ne0 } \texttt{src} (x+x',y+y')\)
+     *
+     * The function supports the in-place mode. Erosion can be applied several ( iterations ) times. In
+     * case of multi-channel images, each channel is processed independently.
+     *
+     * @param src input image; the number of channels can be arbitrary, but the depth should be one of
+     * CV_8U, CV_16U, CV_16S, CV_32F or CV_64F.
+     * @param dst output image of the same size and type as src.
+     * @param kernel structuring element used for erosion; if {@code element=Mat()}, a {@code 3 x 3} rectangular
+     * structuring element is used. Kernel can be created using #getStructuringElement.
+     * @param anchor position of the anchor within the element; default value (-1, -1) means that the
+     * anchor is at the element center.
+     * SEE:  dilate, morphologyEx, getStructuringElement
+     */
+    public static void erode(Mat src, Mat dst, Mat kernel, Point anchor) {
+        erode_3(src.nativeObj, dst.nativeObj, kernel.nativeObj, anchor.x, anchor.y);
+    }
+
+    /**
+     * Erodes an image by using a specific structuring element.
+     *
+     * The function erodes the source image using the specified structuring element that determines the
+     * shape of a pixel neighborhood over which the minimum is taken:
+     *
+     * \(\texttt{dst} (x,y) =  \min _{(x',y'):  \, \texttt{element} (x',y') \ne0 } \texttt{src} (x+x',y+y')\)
+     *
+     * The function supports the in-place mode. Erosion can be applied several ( iterations ) times. In
+     * case of multi-channel images, each channel is processed independently.
+     *
+     * @param src input image; the number of channels can be arbitrary, but the depth should be one of
+     * CV_8U, CV_16U, CV_16S, CV_32F or CV_64F.
+     * @param dst output image of the same size and type as src.
+     * @param kernel structuring element used for erosion; if {@code element=Mat()}, a {@code 3 x 3} rectangular
+     * structuring element is used. Kernel can be created using #getStructuringElement.
+     * anchor is at the element center.
+     * SEE:  dilate, morphologyEx, getStructuringElement
+     */
+    public static void erode(Mat src, Mat dst, Mat kernel) {
+        erode_4(src.nativeObj, dst.nativeObj, kernel.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::fillConvexPoly(Mat& img, vector_Point points, Scalar color, int lineType = LINE_8, int shift = 0)
+    //
+
+    /**
+     * Fills a convex polygon.
+     *
+     * The function cv::fillConvexPoly draws a filled convex polygon. This function is much faster than the
+     * function #fillPoly . It can fill not only convex polygons but any monotonic polygon without
+     * self-intersections, that is, a polygon whose contour intersects every horizontal line (scan line)
+     * twice at the most (though, its top-most and/or the bottom edge could be horizontal).
+     *
+     * @param img Image.
+     * @param points Polygon vertices.
+     * @param color Polygon color.
+     * @param lineType Type of the polygon boundaries. See #LineTypes
+     * @param shift Number of fractional bits in the vertex coordinates.
+     */
+    public static void fillConvexPoly(Mat img, MatOfPoint points, Scalar color, int lineType, int shift) {
+        Mat points_mat = points;
+        fillConvexPoly_0(img.nativeObj, points_mat.nativeObj, color.val[0], color.val[1], color.val[2], color.val[3], lineType, shift);
+    }
+
+    /**
+     * Fills a convex polygon.
+     *
+     * The function cv::fillConvexPoly draws a filled convex polygon. This function is much faster than the
+     * function #fillPoly . It can fill not only convex polygons but any monotonic polygon without
+     * self-intersections, that is, a polygon whose contour intersects every horizontal line (scan line)
+     * twice at the most (though, its top-most and/or the bottom edge could be horizontal).
+     *
+     * @param img Image.
+     * @param points Polygon vertices.
+     * @param color Polygon color.
+     * @param lineType Type of the polygon boundaries. See #LineTypes
+     */
+    public static void fillConvexPoly(Mat img, MatOfPoint points, Scalar color, int lineType) {
+        Mat points_mat = points;
+        fillConvexPoly_1(img.nativeObj, points_mat.nativeObj, color.val[0], color.val[1], color.val[2], color.val[3], lineType);
+    }
+
+    /**
+     * Fills a convex polygon.
+     *
+     * The function cv::fillConvexPoly draws a filled convex polygon. This function is much faster than the
+     * function #fillPoly . It can fill not only convex polygons but any monotonic polygon without
+     * self-intersections, that is, a polygon whose contour intersects every horizontal line (scan line)
+     * twice at the most (though, its top-most and/or the bottom edge could be horizontal).
+     *
+     * @param img Image.
+     * @param points Polygon vertices.
+     * @param color Polygon color.
+     */
+    public static void fillConvexPoly(Mat img, MatOfPoint points, Scalar color) {
+        Mat points_mat = points;
+        fillConvexPoly_2(img.nativeObj, points_mat.nativeObj, color.val[0], color.val[1], color.val[2], color.val[3]);
+    }
+
+
+    //
+    // C++:  void cv::fillPoly(Mat& img, vector_vector_Point pts, Scalar color, int lineType = LINE_8, int shift = 0, Point offset = Point())
+    //
+
+    /**
+     * Fills the area bounded by one or more polygons.
+     *
+     * The function cv::fillPoly fills an area bounded by several polygonal contours. The function can fill
+     * complex areas, for example, areas with holes, contours with self-intersections (some of their
+     * parts), and so forth.
+     *
+     * @param img Image.
+     * @param pts Array of polygons where each polygon is represented as an array of points.
+     * @param color Polygon color.
+     * @param lineType Type of the polygon boundaries. See #LineTypes
+     * @param shift Number of fractional bits in the vertex coordinates.
+     * @param offset Optional offset of all points of the contours.
+     */
+    public static void fillPoly(Mat img, List<MatOfPoint> pts, Scalar color, int lineType, int shift, Point offset) {
+        List<Mat> pts_tmplm = new ArrayList<Mat>((pts != null) ? pts.size() : 0);
+        Mat pts_mat = Converters.vector_vector_Point_to_Mat(pts, pts_tmplm);
+        fillPoly_0(img.nativeObj, pts_mat.nativeObj, color.val[0], color.val[1], color.val[2], color.val[3], lineType, shift, offset.x, offset.y);
+    }
+
+    /**
+     * Fills the area bounded by one or more polygons.
+     *
+     * The function cv::fillPoly fills an area bounded by several polygonal contours. The function can fill
+     * complex areas, for example, areas with holes, contours with self-intersections (some of their
+     * parts), and so forth.
+     *
+     * @param img Image.
+     * @param pts Array of polygons where each polygon is represented as an array of points.
+     * @param color Polygon color.
+     * @param lineType Type of the polygon boundaries. See #LineTypes
+     * @param shift Number of fractional bits in the vertex coordinates.
+     */
+    public static void fillPoly(Mat img, List<MatOfPoint> pts, Scalar color, int lineType, int shift) {
+        List<Mat> pts_tmplm = new ArrayList<Mat>((pts != null) ? pts.size() : 0);
+        Mat pts_mat = Converters.vector_vector_Point_to_Mat(pts, pts_tmplm);
+        fillPoly_1(img.nativeObj, pts_mat.nativeObj, color.val[0], color.val[1], color.val[2], color.val[3], lineType, shift);
+    }
+
+    /**
+     * Fills the area bounded by one or more polygons.
+     *
+     * The function cv::fillPoly fills an area bounded by several polygonal contours. The function can fill
+     * complex areas, for example, areas with holes, contours with self-intersections (some of their
+     * parts), and so forth.
+     *
+     * @param img Image.
+     * @param pts Array of polygons where each polygon is represented as an array of points.
+     * @param color Polygon color.
+     * @param lineType Type of the polygon boundaries. See #LineTypes
+     */
+    public static void fillPoly(Mat img, List<MatOfPoint> pts, Scalar color, int lineType) {
+        List<Mat> pts_tmplm = new ArrayList<Mat>((pts != null) ? pts.size() : 0);
+        Mat pts_mat = Converters.vector_vector_Point_to_Mat(pts, pts_tmplm);
+        fillPoly_2(img.nativeObj, pts_mat.nativeObj, color.val[0], color.val[1], color.val[2], color.val[3], lineType);
+    }
+
+    /**
+     * Fills the area bounded by one or more polygons.
+     *
+     * The function cv::fillPoly fills an area bounded by several polygonal contours. The function can fill
+     * complex areas, for example, areas with holes, contours with self-intersections (some of their
+     * parts), and so forth.
+     *
+     * @param img Image.
+     * @param pts Array of polygons where each polygon is represented as an array of points.
+     * @param color Polygon color.
+     */
+    public static void fillPoly(Mat img, List<MatOfPoint> pts, Scalar color) {
+        List<Mat> pts_tmplm = new ArrayList<Mat>((pts != null) ? pts.size() : 0);
+        Mat pts_mat = Converters.vector_vector_Point_to_Mat(pts, pts_tmplm);
+        fillPoly_3(img.nativeObj, pts_mat.nativeObj, color.val[0], color.val[1], color.val[2], color.val[3]);
+    }
+
+
+    //
+    // C++:  void cv::filter2D(Mat src, Mat& dst, int ddepth, Mat kernel, Point anchor = Point(-1,-1), double delta = 0, int borderType = BORDER_DEFAULT)
+    //
+
+    /**
+     * Convolves an image with the kernel.
+     *
+     * The function applies an arbitrary linear filter to an image. In-place operation is supported. When
+     * the aperture is partially outside the image, the function interpolates outlier pixel values
+     * according to the specified border mode.
+     *
+     * The function does actually compute correlation, not the convolution:
+     *
+     * \(\texttt{dst} (x,y) =  \sum _{ \substack{0\leq x' &lt; \texttt{kernel.cols}\\{0\leq y' &lt; \texttt{kernel.rows}}}}  \texttt{kernel} (x',y')* \texttt{src} (x+x'- \texttt{anchor.x} ,y+y'- \texttt{anchor.y} )\)
+     *
+     * That is, the kernel is not mirrored around the anchor point. If you need a real convolution, flip
+     * the kernel using #flip and set the new anchor to `(kernel.cols - anchor.x - 1, kernel.rows -
+     * anchor.y - 1)`.
+     *
+     * The function uses the DFT-based algorithm in case of sufficiently large kernels (~{@code 11 x 11} or
+     * larger) and the direct algorithm for small kernels.
+     *
+     * @param src input image.
+     * @param dst output image of the same size and the same number of channels as src.
+     * @param ddepth desired depth of the destination image, see REF: filter_depths "combinations"
+     * @param kernel convolution kernel (or rather a correlation kernel), a single-channel floating point
+     * matrix; if you want to apply different kernels to different channels, split the image into
+     * separate color planes using split and process them individually.
+     * @param anchor anchor of the kernel that indicates the relative position of a filtered point within
+     * the kernel; the anchor should lie within the kernel; default value (-1,-1) means that the anchor
+     * is at the kernel center.
+     * @param delta optional value added to the filtered pixels before storing them in dst.
+     * @param borderType pixel extrapolation method, see #BorderTypes. #BORDER_WRAP is not supported.
+     * SEE:  sepFilter2D, dft, matchTemplate
+     */
+    public static void filter2D(Mat src, Mat dst, int ddepth, Mat kernel, Point anchor, double delta, int borderType) {
+        filter2D_0(src.nativeObj, dst.nativeObj, ddepth, kernel.nativeObj, anchor.x, anchor.y, delta, borderType);
+    }
+
+    /**
+     * Convolves an image with the kernel.
+     *
+     * The function applies an arbitrary linear filter to an image. In-place operation is supported. When
+     * the aperture is partially outside the image, the function interpolates outlier pixel values
+     * according to the specified border mode.
+     *
+     * The function does actually compute correlation, not the convolution:
+     *
+     * \(\texttt{dst} (x,y) =  \sum _{ \substack{0\leq x' &lt; \texttt{kernel.cols}\\{0\leq y' &lt; \texttt{kernel.rows}}}}  \texttt{kernel} (x',y')* \texttt{src} (x+x'- \texttt{anchor.x} ,y+y'- \texttt{anchor.y} )\)
+     *
+     * That is, the kernel is not mirrored around the anchor point. If you need a real convolution, flip
+     * the kernel using #flip and set the new anchor to `(kernel.cols - anchor.x - 1, kernel.rows -
+     * anchor.y - 1)`.
+     *
+     * The function uses the DFT-based algorithm in case of sufficiently large kernels (~{@code 11 x 11} or
+     * larger) and the direct algorithm for small kernels.
+     *
+     * @param src input image.
+     * @param dst output image of the same size and the same number of channels as src.
+     * @param ddepth desired depth of the destination image, see REF: filter_depths "combinations"
+     * @param kernel convolution kernel (or rather a correlation kernel), a single-channel floating point
+     * matrix; if you want to apply different kernels to different channels, split the image into
+     * separate color planes using split and process them individually.
+     * @param anchor anchor of the kernel that indicates the relative position of a filtered point within
+     * the kernel; the anchor should lie within the kernel; default value (-1,-1) means that the anchor
+     * is at the kernel center.
+     * @param delta optional value added to the filtered pixels before storing them in dst.
+     * SEE:  sepFilter2D, dft, matchTemplate
+     */
+    public static void filter2D(Mat src, Mat dst, int ddepth, Mat kernel, Point anchor, double delta) {
+        filter2D_1(src.nativeObj, dst.nativeObj, ddepth, kernel.nativeObj, anchor.x, anchor.y, delta);
+    }
+
+    /**
+     * Convolves an image with the kernel.
+     *
+     * The function applies an arbitrary linear filter to an image. In-place operation is supported. When
+     * the aperture is partially outside the image, the function interpolates outlier pixel values
+     * according to the specified border mode.
+     *
+     * The function does actually compute correlation, not the convolution:
+     *
+     * \(\texttt{dst} (x,y) =  \sum _{ \substack{0\leq x' &lt; \texttt{kernel.cols}\\{0\leq y' &lt; \texttt{kernel.rows}}}}  \texttt{kernel} (x',y')* \texttt{src} (x+x'- \texttt{anchor.x} ,y+y'- \texttt{anchor.y} )\)
+     *
+     * That is, the kernel is not mirrored around the anchor point. If you need a real convolution, flip
+     * the kernel using #flip and set the new anchor to `(kernel.cols - anchor.x - 1, kernel.rows -
+     * anchor.y - 1)`.
+     *
+     * The function uses the DFT-based algorithm in case of sufficiently large kernels (~{@code 11 x 11} or
+     * larger) and the direct algorithm for small kernels.
+     *
+     * @param src input image.
+     * @param dst output image of the same size and the same number of channels as src.
+     * @param ddepth desired depth of the destination image, see REF: filter_depths "combinations"
+     * @param kernel convolution kernel (or rather a correlation kernel), a single-channel floating point
+     * matrix; if you want to apply different kernels to different channels, split the image into
+     * separate color planes using split and process them individually.
+     * @param anchor anchor of the kernel that indicates the relative position of a filtered point within
+     * the kernel; the anchor should lie within the kernel; default value (-1,-1) means that the anchor
+     * is at the kernel center.
+     * SEE:  sepFilter2D, dft, matchTemplate
+     */
+    public static void filter2D(Mat src, Mat dst, int ddepth, Mat kernel, Point anchor) {
+        filter2D_2(src.nativeObj, dst.nativeObj, ddepth, kernel.nativeObj, anchor.x, anchor.y);
+    }
+
+    /**
+     * Convolves an image with the kernel.
+     *
+     * The function applies an arbitrary linear filter to an image. In-place operation is supported. When
+     * the aperture is partially outside the image, the function interpolates outlier pixel values
+     * according to the specified border mode.
+     *
+     * The function does actually compute correlation, not the convolution:
+     *
+     * \(\texttt{dst} (x,y) =  \sum _{ \substack{0\leq x' &lt; \texttt{kernel.cols}\\{0\leq y' &lt; \texttt{kernel.rows}}}}  \texttt{kernel} (x',y')* \texttt{src} (x+x'- \texttt{anchor.x} ,y+y'- \texttt{anchor.y} )\)
+     *
+     * That is, the kernel is not mirrored around the anchor point. If you need a real convolution, flip
+     * the kernel using #flip and set the new anchor to `(kernel.cols - anchor.x - 1, kernel.rows -
+     * anchor.y - 1)`.
+     *
+     * The function uses the DFT-based algorithm in case of sufficiently large kernels (~{@code 11 x 11} or
+     * larger) and the direct algorithm for small kernels.
+     *
+     * @param src input image.
+     * @param dst output image of the same size and the same number of channels as src.
+     * @param ddepth desired depth of the destination image, see REF: filter_depths "combinations"
+     * @param kernel convolution kernel (or rather a correlation kernel), a single-channel floating point
+     * matrix; if you want to apply different kernels to different channels, split the image into
+     * separate color planes using split and process them individually.
+     * the kernel; the anchor should lie within the kernel; default value (-1,-1) means that the anchor
+     * is at the kernel center.
+     * SEE:  sepFilter2D, dft, matchTemplate
+     */
+    public static void filter2D(Mat src, Mat dst, int ddepth, Mat kernel) {
+        filter2D_3(src.nativeObj, dst.nativeObj, ddepth, kernel.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::findContours(Mat& image, vector_vector_Point& contours, Mat& hierarchy, int mode, int method, Point offset = Point())
+    //
+
+    /**
+     * Finds contours in a binary image.
+     *
+     * The function retrieves contours from the binary image using the algorithm CITE: Suzuki85 . The contours
+     * are a useful tool for shape analysis and object detection and recognition. See squares.cpp in the
+     * OpenCV sample directory.
+     * <b>Note:</b> Since opencv 3.2 source image is not modified by this function.
+     *
+     * @param image Source, an 8-bit single-channel image. Non-zero pixels are treated as 1's. Zero
+     * pixels remain 0's, so the image is treated as binary . You can use #compare, #inRange, #threshold ,
+     * #adaptiveThreshold, #Canny, and others to create a binary image out of a grayscale or color one.
+     * If mode equals to #RETR_CCOMP or #RETR_FLOODFILL, the input can also be a 32-bit integer image of labels (CV_32SC1).
+     * @param contours Detected contours. Each contour is stored as a vector of points (e.g.
+     * std::vector&lt;std::vector&lt;cv::Point&gt; &gt;).
+     * @param hierarchy Optional output vector (e.g. std::vector&lt;cv::Vec4i&gt;), containing information about the image topology. It has
+     * as many elements as the number of contours. For each i-th contour contours[i], the elements
+     * hierarchy[i][0] , hierarchy[i][1] , hierarchy[i][2] , and hierarchy[i][3] are set to 0-based indices
+     * in contours of the next and previous contours at the same hierarchical level, the first child
+     * contour and the parent contour, respectively. If for the contour i there are no next, previous,
+     * parent, or nested contours, the corresponding elements of hierarchy[i] will be negative.
+     * @param mode Contour retrieval mode, see #RetrievalModes
+     * @param method Contour approximation method, see #ContourApproximationModes
+     * @param offset Optional offset by which every contour point is shifted. This is useful if the
+     * contours are extracted from the image ROI and then they should be analyzed in the whole image
+     * context.
+     */
+    public static void findContours(Mat image, List<MatOfPoint> contours, Mat hierarchy, int mode, int method, Point offset) {
+        Mat contours_mat = new Mat();
+        findContours_0(image.nativeObj, contours_mat.nativeObj, hierarchy.nativeObj, mode, method, offset.x, offset.y);
+        Converters.Mat_to_vector_vector_Point(contours_mat, contours);
+        contours_mat.release();
+    }
+
+    /**
+     * Finds contours in a binary image.
+     *
+     * The function retrieves contours from the binary image using the algorithm CITE: Suzuki85 . The contours
+     * are a useful tool for shape analysis and object detection and recognition. See squares.cpp in the
+     * OpenCV sample directory.
+     * <b>Note:</b> Since opencv 3.2 source image is not modified by this function.
+     *
+     * @param image Source, an 8-bit single-channel image. Non-zero pixels are treated as 1's. Zero
+     * pixels remain 0's, so the image is treated as binary . You can use #compare, #inRange, #threshold ,
+     * #adaptiveThreshold, #Canny, and others to create a binary image out of a grayscale or color one.
+     * If mode equals to #RETR_CCOMP or #RETR_FLOODFILL, the input can also be a 32-bit integer image of labels (CV_32SC1).
+     * @param contours Detected contours. Each contour is stored as a vector of points (e.g.
+     * std::vector&lt;std::vector&lt;cv::Point&gt; &gt;).
+     * @param hierarchy Optional output vector (e.g. std::vector&lt;cv::Vec4i&gt;), containing information about the image topology. It has
+     * as many elements as the number of contours. For each i-th contour contours[i], the elements
+     * hierarchy[i][0] , hierarchy[i][1] , hierarchy[i][2] , and hierarchy[i][3] are set to 0-based indices
+     * in contours of the next and previous contours at the same hierarchical level, the first child
+     * contour and the parent contour, respectively. If for the contour i there are no next, previous,
+     * parent, or nested contours, the corresponding elements of hierarchy[i] will be negative.
+     * @param mode Contour retrieval mode, see #RetrievalModes
+     * @param method Contour approximation method, see #ContourApproximationModes
+     * contours are extracted from the image ROI and then they should be analyzed in the whole image
+     * context.
+     */
+    public static void findContours(Mat image, List<MatOfPoint> contours, Mat hierarchy, int mode, int method) {
+        Mat contours_mat = new Mat();
+        findContours_1(image.nativeObj, contours_mat.nativeObj, hierarchy.nativeObj, mode, method);
+        Converters.Mat_to_vector_vector_Point(contours_mat, contours);
+        contours_mat.release();
+    }
+
+
+    //
+    // C++:  void cv::fitLine(Mat points, Mat& line, int distType, double param, double reps, double aeps)
+    //
+
+    /**
+     * Fits a line to a 2D or 3D point set.
+     *
+     * The function fitLine fits a line to a 2D or 3D point set by minimizing \(\sum_i \rho(r_i)\) where
+     * \(r_i\) is a distance between the \(i^{th}\) point, the line and \(\rho(r)\) is a distance function, one
+     * of the following:
+     * <ul>
+     *   <li>
+     *   DIST_L2
+     * \(\rho (r) = r^2/2  \quad \text{(the simplest and the fastest least-squares method)}\)
+     *   </li>
+     *   <li>
+     *  DIST_L1
+     * \(\rho (r) = r\)
+     *   </li>
+     *   <li>
+     *  DIST_L12
+     * \(\rho (r) = 2  \cdot ( \sqrt{1 + \frac{r^2}{2}} - 1)\)
+     *   </li>
+     *   <li>
+     *  DIST_FAIR
+     * \(\rho \left (r \right ) = C^2  \cdot \left (  \frac{r}{C} -  \log{\left(1 + \frac{r}{C}\right)} \right )  \quad \text{where} \quad C=1.3998\)
+     *   </li>
+     *   <li>
+     *  DIST_WELSCH
+     * \(\rho \left (r \right ) =  \frac{C^2}{2} \cdot \left ( 1 -  \exp{\left(-\left(\frac{r}{C}\right)^2\right)} \right )  \quad \text{where} \quad C=2.9846\)
+     *   </li>
+     *   <li>
+     *  DIST_HUBER
+     * \(\rho (r) =  \fork{r^2/2}{if \(r &lt; C\)}{C \cdot (r-C/2)}{otherwise} \quad \text{where} \quad C=1.345\)
+     *   </li>
+     * </ul>
+     *
+     * The algorithm is based on the M-estimator ( &lt;http://en.wikipedia.org/wiki/M-estimator&gt; ) technique
+     * that iteratively fits the line using the weighted least-squares algorithm. After each iteration the
+     * weights \(w_i\) are adjusted to be inversely proportional to \(\rho(r_i)\) .
+     *
+     * @param points Input vector of 2D or 3D points, stored in std::vector&lt;&gt; or Mat.
+     * @param line Output line parameters. In case of 2D fitting, it should be a vector of 4 elements
+     * (like Vec4f) - (vx, vy, x0, y0), where (vx, vy) is a normalized vector collinear to the line and
+     * (x0, y0) is a point on the line. In case of 3D fitting, it should be a vector of 6 elements (like
+     * Vec6f) - (vx, vy, vz, x0, y0, z0), where (vx, vy, vz) is a normalized vector collinear to the line
+     * and (x0, y0, z0) is a point on the line.
+     * @param distType Distance used by the M-estimator, see #DistanceTypes
+     * @param param Numerical parameter ( C ) for some types of distances. If it is 0, an optimal value
+     * is chosen.
+     * @param reps Sufficient accuracy for the radius (distance between the coordinate origin and the line).
+     * @param aeps Sufficient accuracy for the angle. 0.01 would be a good default value for reps and aeps.
+     */
+    public static void fitLine(Mat points, Mat line, int distType, double param, double reps, double aeps) {
+        fitLine_0(points.nativeObj, line.nativeObj, distType, param, reps, aeps);
+    }
+
+
+    //
+    // C++:  void cv::getDerivKernels(Mat& kx, Mat& ky, int dx, int dy, int ksize, bool normalize = false, int ktype = CV_32F)
+    //
+
+    /**
+     * Returns filter coefficients for computing spatial image derivatives.
+     *
+     * The function computes and returns the filter coefficients for spatial image derivatives. When
+     * {@code ksize=CV_SCHARR}, the Scharr \(3 \times 3\) kernels are generated (see #Scharr). Otherwise, Sobel
+     * kernels are generated (see #Sobel). The filters are normally passed to #sepFilter2D or to
+     *
+     * @param kx Output matrix of row filter coefficients. It has the type ktype .
+     * @param ky Output matrix of column filter coefficients. It has the type ktype .
+     * @param dx Derivative order in respect of x.
+     * @param dy Derivative order in respect of y.
+     * @param ksize Aperture size. It can be CV_SCHARR, 1, 3, 5, or 7.
+     * @param normalize Flag indicating whether to normalize (scale down) the filter coefficients or not.
+     * Theoretically, the coefficients should have the denominator \(=2^{ksize*2-dx-dy-2}\). If you are
+     * going to filter floating-point images, you are likely to use the normalized kernels. But if you
+     * compute derivatives of an 8-bit image, store the results in a 16-bit image, and wish to preserve
+     * all the fractional bits, you may want to set normalize=false .
+     * @param ktype Type of filter coefficients. It can be CV_32f or CV_64F .
+     */
+    public static void getDerivKernels(Mat kx, Mat ky, int dx, int dy, int ksize, boolean normalize, int ktype) {
+        getDerivKernels_0(kx.nativeObj, ky.nativeObj, dx, dy, ksize, normalize, ktype);
+    }
+
+    /**
+     * Returns filter coefficients for computing spatial image derivatives.
+     *
+     * The function computes and returns the filter coefficients for spatial image derivatives. When
+     * {@code ksize=CV_SCHARR}, the Scharr \(3 \times 3\) kernels are generated (see #Scharr). Otherwise, Sobel
+     * kernels are generated (see #Sobel). The filters are normally passed to #sepFilter2D or to
+     *
+     * @param kx Output matrix of row filter coefficients. It has the type ktype .
+     * @param ky Output matrix of column filter coefficients. It has the type ktype .
+     * @param dx Derivative order in respect of x.
+     * @param dy Derivative order in respect of y.
+     * @param ksize Aperture size. It can be CV_SCHARR, 1, 3, 5, or 7.
+     * @param normalize Flag indicating whether to normalize (scale down) the filter coefficients or not.
+     * Theoretically, the coefficients should have the denominator \(=2^{ksize*2-dx-dy-2}\). If you are
+     * going to filter floating-point images, you are likely to use the normalized kernels. But if you
+     * compute derivatives of an 8-bit image, store the results in a 16-bit image, and wish to preserve
+     * all the fractional bits, you may want to set normalize=false .
+     */
+    public static void getDerivKernels(Mat kx, Mat ky, int dx, int dy, int ksize, boolean normalize) {
+        getDerivKernels_1(kx.nativeObj, ky.nativeObj, dx, dy, ksize, normalize);
+    }
+
+    /**
+     * Returns filter coefficients for computing spatial image derivatives.
+     *
+     * The function computes and returns the filter coefficients for spatial image derivatives. When
+     * {@code ksize=CV_SCHARR}, the Scharr \(3 \times 3\) kernels are generated (see #Scharr). Otherwise, Sobel
+     * kernels are generated (see #Sobel). The filters are normally passed to #sepFilter2D or to
+     *
+     * @param kx Output matrix of row filter coefficients. It has the type ktype .
+     * @param ky Output matrix of column filter coefficients. It has the type ktype .
+     * @param dx Derivative order in respect of x.
+     * @param dy Derivative order in respect of y.
+     * @param ksize Aperture size. It can be CV_SCHARR, 1, 3, 5, or 7.
+     * Theoretically, the coefficients should have the denominator \(=2^{ksize*2-dx-dy-2}\). If you are
+     * going to filter floating-point images, you are likely to use the normalized kernels. But if you
+     * compute derivatives of an 8-bit image, store the results in a 16-bit image, and wish to preserve
+     * all the fractional bits, you may want to set normalize=false .
+     */
+    public static void getDerivKernels(Mat kx, Mat ky, int dx, int dy, int ksize) {
+        getDerivKernels_2(kx.nativeObj, ky.nativeObj, dx, dy, ksize);
+    }
+
+
+    //
+    // C++:  void cv::getRectSubPix(Mat image, Size patchSize, Point2f center, Mat& patch, int patchType = -1)
+    //
+
+    /**
+     * Retrieves a pixel rectangle from an image with sub-pixel accuracy.
+     *
+     * The function getRectSubPix extracts pixels from src:
+     *
+     * \(patch(x, y) = src(x +  \texttt{center.x} - ( \texttt{dst.cols} -1)*0.5, y +  \texttt{center.y} - ( \texttt{dst.rows} -1)*0.5)\)
+     *
+     * where the values of the pixels at non-integer coordinates are retrieved using bilinear
+     * interpolation. Every channel of multi-channel images is processed independently. Also
+     * the image should be a single channel or three channel image. While the center of the
+     * rectangle must be inside the image, parts of the rectangle may be outside.
+     *
+     * @param image Source image.
+     * @param patchSize Size of the extracted patch.
+     * @param center Floating point coordinates of the center of the extracted rectangle within the
+     * source image. The center must be inside the image.
+     * @param patch Extracted patch that has the size patchSize and the same number of channels as src .
+     * @param patchType Depth of the extracted pixels. By default, they have the same depth as src .
+     *
+     * SEE:  warpAffine, warpPerspective
+     */
+    public static void getRectSubPix(Mat image, Size patchSize, Point center, Mat patch, int patchType) {
+        getRectSubPix_0(image.nativeObj, patchSize.width, patchSize.height, center.x, center.y, patch.nativeObj, patchType);
+    }
+
+    /**
+     * Retrieves a pixel rectangle from an image with sub-pixel accuracy.
+     *
+     * The function getRectSubPix extracts pixels from src:
+     *
+     * \(patch(x, y) = src(x +  \texttt{center.x} - ( \texttt{dst.cols} -1)*0.5, y +  \texttt{center.y} - ( \texttt{dst.rows} -1)*0.5)\)
+     *
+     * where the values of the pixels at non-integer coordinates are retrieved using bilinear
+     * interpolation. Every channel of multi-channel images is processed independently. Also
+     * the image should be a single channel or three channel image. While the center of the
+     * rectangle must be inside the image, parts of the rectangle may be outside.
+     *
+     * @param image Source image.
+     * @param patchSize Size of the extracted patch.
+     * @param center Floating point coordinates of the center of the extracted rectangle within the
+     * source image. The center must be inside the image.
+     * @param patch Extracted patch that has the size patchSize and the same number of channels as src .
+     *
+     * SEE:  warpAffine, warpPerspective
+     */
+    public static void getRectSubPix(Mat image, Size patchSize, Point center, Mat patch) {
+        getRectSubPix_1(image.nativeObj, patchSize.width, patchSize.height, center.x, center.y, patch.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::goodFeaturesToTrack(Mat image, vector_Point& corners, int maxCorners, double qualityLevel, double minDistance, Mat mask, int blockSize, int gradientSize, bool useHarrisDetector = false, double k = 0.04)
+    //
+
+    public static void goodFeaturesToTrack(Mat image, MatOfPoint corners, int maxCorners, double qualityLevel, double minDistance, Mat mask, int blockSize, int gradientSize, boolean useHarrisDetector, double k) {
+        Mat corners_mat = corners;
+        goodFeaturesToTrack_0(image.nativeObj, corners_mat.nativeObj, maxCorners, qualityLevel, minDistance, mask.nativeObj, blockSize, gradientSize, useHarrisDetector, k);
+    }
+
+    public static void goodFeaturesToTrack(Mat image, MatOfPoint corners, int maxCorners, double qualityLevel, double minDistance, Mat mask, int blockSize, int gradientSize, boolean useHarrisDetector) {
+        Mat corners_mat = corners;
+        goodFeaturesToTrack_1(image.nativeObj, corners_mat.nativeObj, maxCorners, qualityLevel, minDistance, mask.nativeObj, blockSize, gradientSize, useHarrisDetector);
+    }
+
+    public static void goodFeaturesToTrack(Mat image, MatOfPoint corners, int maxCorners, double qualityLevel, double minDistance, Mat mask, int blockSize, int gradientSize) {
+        Mat corners_mat = corners;
+        goodFeaturesToTrack_2(image.nativeObj, corners_mat.nativeObj, maxCorners, qualityLevel, minDistance, mask.nativeObj, blockSize, gradientSize);
+    }
+
+
+    //
+    // C++:  void cv::goodFeaturesToTrack(Mat image, vector_Point& corners, int maxCorners, double qualityLevel, double minDistance, Mat mask = Mat(), int blockSize = 3, bool useHarrisDetector = false, double k = 0.04)
+    //
+
+    /**
+     * Determines strong corners on an image.
+     *
+     * The function finds the most prominent corners in the image or in the specified image region, as
+     * described in CITE: Shi94
+     *
+     * <ul>
+     *   <li>
+     *    Function calculates the corner quality measure at every source image pixel using the
+     *     #cornerMinEigenVal or #cornerHarris .
+     *   </li>
+     *   <li>
+     *    Function performs a non-maximum suppression (the local maximums in *3 x 3* neighborhood are
+     *     retained).
+     *   </li>
+     *   <li>
+     *    The corners with the minimal eigenvalue less than
+     *     \(\texttt{qualityLevel} \cdot \max_{x,y} qualityMeasureMap(x,y)\) are rejected.
+     *   </li>
+     *   <li>
+     *    The remaining corners are sorted by the quality measure in the descending order.
+     *   </li>
+     *   <li>
+     *    Function throws away each corner for which there is a stronger corner at a distance less than
+     *     maxDistance.
+     *   </li>
+     * </ul>
+     *
+     * The function can be used to initialize a point-based tracker of an object.
+     *
+     * <b>Note:</b> If the function is called with different values A and B of the parameter qualityLevel , and
+     * A &gt; B, the vector of returned corners with qualityLevel=A will be the prefix of the output vector
+     * with qualityLevel=B .
+     *
+     * @param image Input 8-bit or floating-point 32-bit, single-channel image.
+     * @param corners Output vector of detected corners.
+     * @param maxCorners Maximum number of corners to return. If there are more corners than are found,
+     * the strongest of them is returned. {@code maxCorners &lt;= 0} implies that no limit on the maximum is set
+     * and all detected corners are returned.
+     * @param qualityLevel Parameter characterizing the minimal accepted quality of image corners. The
+     * parameter value is multiplied by the best corner quality measure, which is the minimal eigenvalue
+     * (see #cornerMinEigenVal ) or the Harris function response (see #cornerHarris ). The corners with the
+     * quality measure less than the product are rejected. For example, if the best corner has the
+     * quality measure = 1500, and the qualityLevel=0.01 , then all the corners with the quality measure
+     * less than 15 are rejected.
+     * @param minDistance Minimum possible Euclidean distance between the returned corners.
+     * @param mask Optional region of interest. If the image is not empty (it needs to have the type
+     * CV_8UC1 and the same size as image ), it specifies the region in which the corners are detected.
+     * @param blockSize Size of an average block for computing a derivative covariation matrix over each
+     * pixel neighborhood. See cornerEigenValsAndVecs .
+     * @param useHarrisDetector Parameter indicating whether to use a Harris detector (see #cornerHarris)
+     * or #cornerMinEigenVal.
+     * @param k Free parameter of the Harris detector.
+     *
+     * SEE:  cornerMinEigenVal, cornerHarris, calcOpticalFlowPyrLK, estimateRigidTransform,
+     */
+    public static void goodFeaturesToTrack(Mat image, MatOfPoint corners, int maxCorners, double qualityLevel, double minDistance, Mat mask, int blockSize, boolean useHarrisDetector, double k) {
+        Mat corners_mat = corners;
+        goodFeaturesToTrack_3(image.nativeObj, corners_mat.nativeObj, maxCorners, qualityLevel, minDistance, mask.nativeObj, blockSize, useHarrisDetector, k);
+    }
+
+    /**
+     * Determines strong corners on an image.
+     *
+     * The function finds the most prominent corners in the image or in the specified image region, as
+     * described in CITE: Shi94
+     *
+     * <ul>
+     *   <li>
+     *    Function calculates the corner quality measure at every source image pixel using the
+     *     #cornerMinEigenVal or #cornerHarris .
+     *   </li>
+     *   <li>
+     *    Function performs a non-maximum suppression (the local maximums in *3 x 3* neighborhood are
+     *     retained).
+     *   </li>
+     *   <li>
+     *    The corners with the minimal eigenvalue less than
+     *     \(\texttt{qualityLevel} \cdot \max_{x,y} qualityMeasureMap(x,y)\) are rejected.
+     *   </li>
+     *   <li>
+     *    The remaining corners are sorted by the quality measure in the descending order.
+     *   </li>
+     *   <li>
+     *    Function throws away each corner for which there is a stronger corner at a distance less than
+     *     maxDistance.
+     *   </li>
+     * </ul>
+     *
+     * The function can be used to initialize a point-based tracker of an object.
+     *
+     * <b>Note:</b> If the function is called with different values A and B of the parameter qualityLevel , and
+     * A &gt; B, the vector of returned corners with qualityLevel=A will be the prefix of the output vector
+     * with qualityLevel=B .
+     *
+     * @param image Input 8-bit or floating-point 32-bit, single-channel image.
+     * @param corners Output vector of detected corners.
+     * @param maxCorners Maximum number of corners to return. If there are more corners than are found,
+     * the strongest of them is returned. {@code maxCorners &lt;= 0} implies that no limit on the maximum is set
+     * and all detected corners are returned.
+     * @param qualityLevel Parameter characterizing the minimal accepted quality of image corners. The
+     * parameter value is multiplied by the best corner quality measure, which is the minimal eigenvalue
+     * (see #cornerMinEigenVal ) or the Harris function response (see #cornerHarris ). The corners with the
+     * quality measure less than the product are rejected. For example, if the best corner has the
+     * quality measure = 1500, and the qualityLevel=0.01 , then all the corners with the quality measure
+     * less than 15 are rejected.
+     * @param minDistance Minimum possible Euclidean distance between the returned corners.
+     * @param mask Optional region of interest. If the image is not empty (it needs to have the type
+     * CV_8UC1 and the same size as image ), it specifies the region in which the corners are detected.
+     * @param blockSize Size of an average block for computing a derivative covariation matrix over each
+     * pixel neighborhood. See cornerEigenValsAndVecs .
+     * @param useHarrisDetector Parameter indicating whether to use a Harris detector (see #cornerHarris)
+     * or #cornerMinEigenVal.
+     *
+     * SEE:  cornerMinEigenVal, cornerHarris, calcOpticalFlowPyrLK, estimateRigidTransform,
+     */
+    public static void goodFeaturesToTrack(Mat image, MatOfPoint corners, int maxCorners, double qualityLevel, double minDistance, Mat mask, int blockSize, boolean useHarrisDetector) {
+        Mat corners_mat = corners;
+        goodFeaturesToTrack_4(image.nativeObj, corners_mat.nativeObj, maxCorners, qualityLevel, minDistance, mask.nativeObj, blockSize, useHarrisDetector);
+    }
+
+    /**
+     * Determines strong corners on an image.
+     *
+     * The function finds the most prominent corners in the image or in the specified image region, as
+     * described in CITE: Shi94
+     *
+     * <ul>
+     *   <li>
+     *    Function calculates the corner quality measure at every source image pixel using the
+     *     #cornerMinEigenVal or #cornerHarris .
+     *   </li>
+     *   <li>
+     *    Function performs a non-maximum suppression (the local maximums in *3 x 3* neighborhood are
+     *     retained).
+     *   </li>
+     *   <li>
+     *    The corners with the minimal eigenvalue less than
+     *     \(\texttt{qualityLevel} \cdot \max_{x,y} qualityMeasureMap(x,y)\) are rejected.
+     *   </li>
+     *   <li>
+     *    The remaining corners are sorted by the quality measure in the descending order.
+     *   </li>
+     *   <li>
+     *    Function throws away each corner for which there is a stronger corner at a distance less than
+     *     maxDistance.
+     *   </li>
+     * </ul>
+     *
+     * The function can be used to initialize a point-based tracker of an object.
+     *
+     * <b>Note:</b> If the function is called with different values A and B of the parameter qualityLevel , and
+     * A &gt; B, the vector of returned corners with qualityLevel=A will be the prefix of the output vector
+     * with qualityLevel=B .
+     *
+     * @param image Input 8-bit or floating-point 32-bit, single-channel image.
+     * @param corners Output vector of detected corners.
+     * @param maxCorners Maximum number of corners to return. If there are more corners than are found,
+     * the strongest of them is returned. {@code maxCorners &lt;= 0} implies that no limit on the maximum is set
+     * and all detected corners are returned.
+     * @param qualityLevel Parameter characterizing the minimal accepted quality of image corners. The
+     * parameter value is multiplied by the best corner quality measure, which is the minimal eigenvalue
+     * (see #cornerMinEigenVal ) or the Harris function response (see #cornerHarris ). The corners with the
+     * quality measure less than the product are rejected. For example, if the best corner has the
+     * quality measure = 1500, and the qualityLevel=0.01 , then all the corners with the quality measure
+     * less than 15 are rejected.
+     * @param minDistance Minimum possible Euclidean distance between the returned corners.
+     * @param mask Optional region of interest. If the image is not empty (it needs to have the type
+     * CV_8UC1 and the same size as image ), it specifies the region in which the corners are detected.
+     * @param blockSize Size of an average block for computing a derivative covariation matrix over each
+     * pixel neighborhood. See cornerEigenValsAndVecs .
+     * or #cornerMinEigenVal.
+     *
+     * SEE:  cornerMinEigenVal, cornerHarris, calcOpticalFlowPyrLK, estimateRigidTransform,
+     */
+    public static void goodFeaturesToTrack(Mat image, MatOfPoint corners, int maxCorners, double qualityLevel, double minDistance, Mat mask, int blockSize) {
+        Mat corners_mat = corners;
+        goodFeaturesToTrack_5(image.nativeObj, corners_mat.nativeObj, maxCorners, qualityLevel, minDistance, mask.nativeObj, blockSize);
+    }
+
+    /**
+     * Determines strong corners on an image.
+     *
+     * The function finds the most prominent corners in the image or in the specified image region, as
+     * described in CITE: Shi94
+     *
+     * <ul>
+     *   <li>
+     *    Function calculates the corner quality measure at every source image pixel using the
+     *     #cornerMinEigenVal or #cornerHarris .
+     *   </li>
+     *   <li>
+     *    Function performs a non-maximum suppression (the local maximums in *3 x 3* neighborhood are
+     *     retained).
+     *   </li>
+     *   <li>
+     *    The corners with the minimal eigenvalue less than
+     *     \(\texttt{qualityLevel} \cdot \max_{x,y} qualityMeasureMap(x,y)\) are rejected.
+     *   </li>
+     *   <li>
+     *    The remaining corners are sorted by the quality measure in the descending order.
+     *   </li>
+     *   <li>
+     *    Function throws away each corner for which there is a stronger corner at a distance less than
+     *     maxDistance.
+     *   </li>
+     * </ul>
+     *
+     * The function can be used to initialize a point-based tracker of an object.
+     *
+     * <b>Note:</b> If the function is called with different values A and B of the parameter qualityLevel , and
+     * A &gt; B, the vector of returned corners with qualityLevel=A will be the prefix of the output vector
+     * with qualityLevel=B .
+     *
+     * @param image Input 8-bit or floating-point 32-bit, single-channel image.
+     * @param corners Output vector of detected corners.
+     * @param maxCorners Maximum number of corners to return. If there are more corners than are found,
+     * the strongest of them is returned. {@code maxCorners &lt;= 0} implies that no limit on the maximum is set
+     * and all detected corners are returned.
+     * @param qualityLevel Parameter characterizing the minimal accepted quality of image corners. The
+     * parameter value is multiplied by the best corner quality measure, which is the minimal eigenvalue
+     * (see #cornerMinEigenVal ) or the Harris function response (see #cornerHarris ). The corners with the
+     * quality measure less than the product are rejected. For example, if the best corner has the
+     * quality measure = 1500, and the qualityLevel=0.01 , then all the corners with the quality measure
+     * less than 15 are rejected.
+     * @param minDistance Minimum possible Euclidean distance between the returned corners.
+     * @param mask Optional region of interest. If the image is not empty (it needs to have the type
+     * CV_8UC1 and the same size as image ), it specifies the region in which the corners are detected.
+     * pixel neighborhood. See cornerEigenValsAndVecs .
+     * or #cornerMinEigenVal.
+     *
+     * SEE:  cornerMinEigenVal, cornerHarris, calcOpticalFlowPyrLK, estimateRigidTransform,
+     */
+    public static void goodFeaturesToTrack(Mat image, MatOfPoint corners, int maxCorners, double qualityLevel, double minDistance, Mat mask) {
+        Mat corners_mat = corners;
+        goodFeaturesToTrack_6(image.nativeObj, corners_mat.nativeObj, maxCorners, qualityLevel, minDistance, mask.nativeObj);
+    }
+
+    /**
+     * Determines strong corners on an image.
+     *
+     * The function finds the most prominent corners in the image or in the specified image region, as
+     * described in CITE: Shi94
+     *
+     * <ul>
+     *   <li>
+     *    Function calculates the corner quality measure at every source image pixel using the
+     *     #cornerMinEigenVal or #cornerHarris .
+     *   </li>
+     *   <li>
+     *    Function performs a non-maximum suppression (the local maximums in *3 x 3* neighborhood are
+     *     retained).
+     *   </li>
+     *   <li>
+     *    The corners with the minimal eigenvalue less than
+     *     \(\texttt{qualityLevel} \cdot \max_{x,y} qualityMeasureMap(x,y)\) are rejected.
+     *   </li>
+     *   <li>
+     *    The remaining corners are sorted by the quality measure in the descending order.
+     *   </li>
+     *   <li>
+     *    Function throws away each corner for which there is a stronger corner at a distance less than
+     *     maxDistance.
+     *   </li>
+     * </ul>
+     *
+     * The function can be used to initialize a point-based tracker of an object.
+     *
+     * <b>Note:</b> If the function is called with different values A and B of the parameter qualityLevel , and
+     * A &gt; B, the vector of returned corners with qualityLevel=A will be the prefix of the output vector
+     * with qualityLevel=B .
+     *
+     * @param image Input 8-bit or floating-point 32-bit, single-channel image.
+     * @param corners Output vector of detected corners.
+     * @param maxCorners Maximum number of corners to return. If there are more corners than are found,
+     * the strongest of them is returned. {@code maxCorners &lt;= 0} implies that no limit on the maximum is set
+     * and all detected corners are returned.
+     * @param qualityLevel Parameter characterizing the minimal accepted quality of image corners. The
+     * parameter value is multiplied by the best corner quality measure, which is the minimal eigenvalue
+     * (see #cornerMinEigenVal ) or the Harris function response (see #cornerHarris ). The corners with the
+     * quality measure less than the product are rejected. For example, if the best corner has the
+     * quality measure = 1500, and the qualityLevel=0.01 , then all the corners with the quality measure
+     * less than 15 are rejected.
+     * @param minDistance Minimum possible Euclidean distance between the returned corners.
+     * CV_8UC1 and the same size as image ), it specifies the region in which the corners are detected.
+     * pixel neighborhood. See cornerEigenValsAndVecs .
+     * or #cornerMinEigenVal.
+     *
+     * SEE:  cornerMinEigenVal, cornerHarris, calcOpticalFlowPyrLK, estimateRigidTransform,
+     */
+    public static void goodFeaturesToTrack(Mat image, MatOfPoint corners, int maxCorners, double qualityLevel, double minDistance) {
+        Mat corners_mat = corners;
+        goodFeaturesToTrack_7(image.nativeObj, corners_mat.nativeObj, maxCorners, qualityLevel, minDistance);
+    }
+
+
+    //
+    // C++:  void cv::grabCut(Mat img, Mat& mask, Rect rect, Mat& bgdModel, Mat& fgdModel, int iterCount, int mode = GC_EVAL)
+    //
+
+    /**
+     * Runs the GrabCut algorithm.
+     *
+     * The function implements the [GrabCut image segmentation algorithm](http://en.wikipedia.org/wiki/GrabCut).
+     *
+     * @param img Input 8-bit 3-channel image.
+     * @param mask Input/output 8-bit single-channel mask. The mask is initialized by the function when
+     * mode is set to #GC_INIT_WITH_RECT. Its elements may have one of the #GrabCutClasses.
+     * @param rect ROI containing a segmented object. The pixels outside of the ROI are marked as
+     * "obvious background". The parameter is only used when mode==#GC_INIT_WITH_RECT .
+     * @param bgdModel Temporary array for the background model. Do not modify it while you are
+     * processing the same image.
+     * @param fgdModel Temporary arrays for the foreground model. Do not modify it while you are
+     * processing the same image.
+     * @param iterCount Number of iterations the algorithm should make before returning the result. Note
+     * that the result can be refined with further calls with mode==#GC_INIT_WITH_MASK or
+     * mode==GC_EVAL .
+     * @param mode Operation mode that could be one of the #GrabCutModes
+     */
+    public static void grabCut(Mat img, Mat mask, Rect rect, Mat bgdModel, Mat fgdModel, int iterCount, int mode) {
+        grabCut_0(img.nativeObj, mask.nativeObj, rect.x, rect.y, rect.width, rect.height, bgdModel.nativeObj, fgdModel.nativeObj, iterCount, mode);
+    }
+
+    /**
+     * Runs the GrabCut algorithm.
+     *
+     * The function implements the [GrabCut image segmentation algorithm](http://en.wikipedia.org/wiki/GrabCut).
+     *
+     * @param img Input 8-bit 3-channel image.
+     * @param mask Input/output 8-bit single-channel mask. The mask is initialized by the function when
+     * mode is set to #GC_INIT_WITH_RECT. Its elements may have one of the #GrabCutClasses.
+     * @param rect ROI containing a segmented object. The pixels outside of the ROI are marked as
+     * "obvious background". The parameter is only used when mode==#GC_INIT_WITH_RECT .
+     * @param bgdModel Temporary array for the background model. Do not modify it while you are
+     * processing the same image.
+     * @param fgdModel Temporary arrays for the foreground model. Do not modify it while you are
+     * processing the same image.
+     * @param iterCount Number of iterations the algorithm should make before returning the result. Note
+     * that the result can be refined with further calls with mode==#GC_INIT_WITH_MASK or
+     * mode==GC_EVAL .
+     */
+    public static void grabCut(Mat img, Mat mask, Rect rect, Mat bgdModel, Mat fgdModel, int iterCount) {
+        grabCut_1(img.nativeObj, mask.nativeObj, rect.x, rect.y, rect.width, rect.height, bgdModel.nativeObj, fgdModel.nativeObj, iterCount);
+    }
+
+
+    //
+    // C++:  void cv::initUndistortRectifyMap(Mat cameraMatrix, Mat distCoeffs, Mat R, Mat newCameraMatrix, Size size, int m1type, Mat& map1, Mat& map2)
+    //
+
+    /**
+     * Computes the undistortion and rectification transformation map.
+     *
+     * The function computes the joint undistortion and rectification transformation and represents the
+     * result in the form of maps for remap. The undistorted image looks like original, as if it is
+     * captured with a camera using the camera matrix =newCameraMatrix and zero distortion. In case of a
+     * monocular camera, newCameraMatrix is usually equal to cameraMatrix, or it can be computed by
+     * #getOptimalNewCameraMatrix for a better control over scaling. In case of a stereo camera,
+     * newCameraMatrix is normally set to P1 or P2 computed by #stereoRectify .
+     *
+     * Also, this new camera is oriented differently in the coordinate space, according to R. That, for
+     * example, helps to align two heads of a stereo camera so that the epipolar lines on both images
+     * become horizontal and have the same y- coordinate (in case of a horizontally aligned stereo camera).
+     *
+     * The function actually builds the maps for the inverse mapping algorithm that is used by remap. That
+     * is, for each pixel \((u, v)\) in the destination (corrected and rectified) image, the function
+     * computes the corresponding coordinates in the source image (that is, in the original image from
+     * camera). The following process is applied:
+     * \(
+     * \begin{array}{l}
+     * x  \leftarrow (u - {c'}_x)/{f'}_x  \\
+     * y  \leftarrow (v - {c'}_y)/{f'}_y  \\
+     * {[X\,Y\,W]} ^T  \leftarrow R^{-1}*[x \, y \, 1]^T  \\
+     * x'  \leftarrow X/W  \\
+     * y'  \leftarrow Y/W  \\
+     * r^2  \leftarrow x'^2 + y'^2 \\
+     * x''  \leftarrow x' \frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6}
+     * + 2p_1 x' y' + p_2(r^2 + 2 x'^2)  + s_1 r^2 + s_2 r^4\\
+     * y''  \leftarrow y' \frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6}
+     * + p_1 (r^2 + 2 y'^2) + 2 p_2 x' y' + s_3 r^2 + s_4 r^4 \\
+     * s\vecthree{x'''}{y'''}{1} =
+     * \vecthreethree{R_{33}(\tau_x, \tau_y)}{0}{-R_{13}((\tau_x, \tau_y)}
+     * {0}{R_{33}(\tau_x, \tau_y)}{-R_{23}(\tau_x, \tau_y)}
+     * {0}{0}{1} R(\tau_x, \tau_y) \vecthree{x''}{y''}{1}\\
+     * map_x(u,v)  \leftarrow x''' f_x + c_x  \\
+     * map_y(u,v)  \leftarrow y''' f_y + c_y
+     * \end{array}
+     * \)
+     * where \((k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6[, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\)
+     * are the distortion coefficients.
+     *
+     * In case of a stereo camera, this function is called twice: once for each camera head, after
+     * stereoRectify, which in its turn is called after #stereoCalibrate. But if the stereo camera
+     * was not calibrated, it is still possible to compute the rectification transformations directly from
+     * the fundamental matrix using #stereoRectifyUncalibrated. For each camera, the function computes
+     * homography H as the rectification transformation in a pixel domain, not a rotation matrix R in 3D
+     * space. R can be computed from H as
+     * \(\texttt{R} = \texttt{cameraMatrix} ^{-1} \cdot \texttt{H} \cdot \texttt{cameraMatrix}\)
+     * where cameraMatrix can be chosen arbitrarily.
+     *
+     * @param cameraMatrix Input camera matrix \(A=\vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}\) .
+     * @param distCoeffs Input vector of distortion coefficients
+     * \((k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6[, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\)
+     * of 4, 5, 8, 12 or 14 elements. If the vector is NULL/empty, the zero distortion coefficients are assumed.
+     * @param R Optional rectification transformation in the object space (3x3 matrix). R1 or R2 ,
+     * computed by #stereoRectify can be passed here. If the matrix is empty, the identity transformation
+     * is assumed. In cvInitUndistortMap R assumed to be an identity matrix.
+     * @param newCameraMatrix New camera matrix \(A'=\vecthreethree{f_x'}{0}{c_x'}{0}{f_y'}{c_y'}{0}{0}{1}\).
+     * @param size Undistorted image size.
+     * @param m1type Type of the first output map that can be CV_32FC1, CV_32FC2 or CV_16SC2, see #convertMaps
+     * @param map1 The first output map.
+     * @param map2 The second output map.
+     */
+    public static void initUndistortRectifyMap(Mat cameraMatrix, Mat distCoeffs, Mat R, Mat newCameraMatrix, Size size, int m1type, Mat map1, Mat map2) {
+        initUndistortRectifyMap_0(cameraMatrix.nativeObj, distCoeffs.nativeObj, R.nativeObj, newCameraMatrix.nativeObj, size.width, size.height, m1type, map1.nativeObj, map2.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::integral(Mat src, Mat& sum, Mat& sqsum, Mat& tilted, int sdepth = -1, int sqdepth = -1)
+    //
+
+    /**
+     * Calculates the integral of an image.
+     *
+     * The function calculates one or more integral images for the source image as follows:
+     *
+     * \(\texttt{sum} (X,Y) =  \sum _{x&lt;X,y&lt;Y}  \texttt{image} (x,y)\)
+     *
+     * \(\texttt{sqsum} (X,Y) =  \sum _{x&lt;X,y&lt;Y}  \texttt{image} (x,y)^2\)
+     *
+     * \(\texttt{tilted} (X,Y) =  \sum _{y&lt;Y,abs(x-X+1) \leq Y-y-1}  \texttt{image} (x,y)\)
+     *
+     * Using these integral images, you can calculate sum, mean, and standard deviation over a specific
+     * up-right or rotated rectangular region of the image in a constant time, for example:
+     *
+     * \(\sum _{x_1 \leq x &lt; x_2,  \, y_1  \leq y &lt; y_2}  \texttt{image} (x,y) =  \texttt{sum} (x_2,y_2)- \texttt{sum} (x_1,y_2)- \texttt{sum} (x_2,y_1)+ \texttt{sum} (x_1,y_1)\)
+     *
+     * It makes possible to do a fast blurring or fast block correlation with a variable window size, for
+     * example. In case of multi-channel images, sums for each channel are accumulated independently.
+     *
+     * As a practical example, the next figure shows the calculation of the integral of a straight
+     * rectangle Rect(3,3,3,2) and of a tilted rectangle Rect(5,1,2,3) . The selected pixels in the
+     * original image are shown, as well as the relative pixels in the integral images sum and tilted .
+     *
+     * ![integral calculation example](pics/integral.png)
+     *
+     * @param src input image as \(W \times H\), 8-bit or floating-point (32f or 64f).
+     * @param sum integral image as \((W+1)\times (H+1)\) , 32-bit integer or floating-point (32f or 64f).
+     * @param sqsum integral image for squared pixel values; it is \((W+1)\times (H+1)\), double-precision
+     * floating-point (64f) array.
+     * @param tilted integral for the image rotated by 45 degrees; it is \((W+1)\times (H+1)\) array with
+     * the same data type as sum.
+     * @param sdepth desired depth of the integral and the tilted integral images, CV_32S, CV_32F, or
+     * CV_64F.
+     * @param sqdepth desired depth of the integral image of squared pixel values, CV_32F or CV_64F.
+     */
+    public static void integral3(Mat src, Mat sum, Mat sqsum, Mat tilted, int sdepth, int sqdepth) {
+        integral3_0(src.nativeObj, sum.nativeObj, sqsum.nativeObj, tilted.nativeObj, sdepth, sqdepth);
+    }
+
+    /**
+     * Calculates the integral of an image.
+     *
+     * The function calculates one or more integral images for the source image as follows:
+     *
+     * \(\texttt{sum} (X,Y) =  \sum _{x&lt;X,y&lt;Y}  \texttt{image} (x,y)\)
+     *
+     * \(\texttt{sqsum} (X,Y) =  \sum _{x&lt;X,y&lt;Y}  \texttt{image} (x,y)^2\)
+     *
+     * \(\texttt{tilted} (X,Y) =  \sum _{y&lt;Y,abs(x-X+1) \leq Y-y-1}  \texttt{image} (x,y)\)
+     *
+     * Using these integral images, you can calculate sum, mean, and standard deviation over a specific
+     * up-right or rotated rectangular region of the image in a constant time, for example:
+     *
+     * \(\sum _{x_1 \leq x &lt; x_2,  \, y_1  \leq y &lt; y_2}  \texttt{image} (x,y) =  \texttt{sum} (x_2,y_2)- \texttt{sum} (x_1,y_2)- \texttt{sum} (x_2,y_1)+ \texttt{sum} (x_1,y_1)\)
+     *
+     * It makes possible to do a fast blurring or fast block correlation with a variable window size, for
+     * example. In case of multi-channel images, sums for each channel are accumulated independently.
+     *
+     * As a practical example, the next figure shows the calculation of the integral of a straight
+     * rectangle Rect(3,3,3,2) and of a tilted rectangle Rect(5,1,2,3) . The selected pixels in the
+     * original image are shown, as well as the relative pixels in the integral images sum and tilted .
+     *
+     * ![integral calculation example](pics/integral.png)
+     *
+     * @param src input image as \(W \times H\), 8-bit or floating-point (32f or 64f).
+     * @param sum integral image as \((W+1)\times (H+1)\) , 32-bit integer or floating-point (32f or 64f).
+     * @param sqsum integral image for squared pixel values; it is \((W+1)\times (H+1)\), double-precision
+     * floating-point (64f) array.
+     * @param tilted integral for the image rotated by 45 degrees; it is \((W+1)\times (H+1)\) array with
+     * the same data type as sum.
+     * @param sdepth desired depth of the integral and the tilted integral images, CV_32S, CV_32F, or
+     * CV_64F.
+     */
+    public static void integral3(Mat src, Mat sum, Mat sqsum, Mat tilted, int sdepth) {
+        integral3_1(src.nativeObj, sum.nativeObj, sqsum.nativeObj, tilted.nativeObj, sdepth);
+    }
+
+    /**
+     * Calculates the integral of an image.
+     *
+     * The function calculates one or more integral images for the source image as follows:
+     *
+     * \(\texttt{sum} (X,Y) =  \sum _{x&lt;X,y&lt;Y}  \texttt{image} (x,y)\)
+     *
+     * \(\texttt{sqsum} (X,Y) =  \sum _{x&lt;X,y&lt;Y}  \texttt{image} (x,y)^2\)
+     *
+     * \(\texttt{tilted} (X,Y) =  \sum _{y&lt;Y,abs(x-X+1) \leq Y-y-1}  \texttt{image} (x,y)\)
+     *
+     * Using these integral images, you can calculate sum, mean, and standard deviation over a specific
+     * up-right or rotated rectangular region of the image in a constant time, for example:
+     *
+     * \(\sum _{x_1 \leq x &lt; x_2,  \, y_1  \leq y &lt; y_2}  \texttt{image} (x,y) =  \texttt{sum} (x_2,y_2)- \texttt{sum} (x_1,y_2)- \texttt{sum} (x_2,y_1)+ \texttt{sum} (x_1,y_1)\)
+     *
+     * It makes possible to do a fast blurring or fast block correlation with a variable window size, for
+     * example. In case of multi-channel images, sums for each channel are accumulated independently.
+     *
+     * As a practical example, the next figure shows the calculation of the integral of a straight
+     * rectangle Rect(3,3,3,2) and of a tilted rectangle Rect(5,1,2,3) . The selected pixels in the
+     * original image are shown, as well as the relative pixels in the integral images sum and tilted .
+     *
+     * ![integral calculation example](pics/integral.png)
+     *
+     * @param src input image as \(W \times H\), 8-bit or floating-point (32f or 64f).
+     * @param sum integral image as \((W+1)\times (H+1)\) , 32-bit integer or floating-point (32f or 64f).
+     * @param sqsum integral image for squared pixel values; it is \((W+1)\times (H+1)\), double-precision
+     * floating-point (64f) array.
+     * @param tilted integral for the image rotated by 45 degrees; it is \((W+1)\times (H+1)\) array with
+     * the same data type as sum.
+     * CV_64F.
+     */
+    public static void integral3(Mat src, Mat sum, Mat sqsum, Mat tilted) {
+        integral3_2(src.nativeObj, sum.nativeObj, sqsum.nativeObj, tilted.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::integral(Mat src, Mat& sum, Mat& sqsum, int sdepth = -1, int sqdepth = -1)
+    //
+
+    public static void integral2(Mat src, Mat sum, Mat sqsum, int sdepth, int sqdepth) {
+        integral2_0(src.nativeObj, sum.nativeObj, sqsum.nativeObj, sdepth, sqdepth);
+    }
+
+    public static void integral2(Mat src, Mat sum, Mat sqsum, int sdepth) {
+        integral2_1(src.nativeObj, sum.nativeObj, sqsum.nativeObj, sdepth);
+    }
+
+    public static void integral2(Mat src, Mat sum, Mat sqsum) {
+        integral2_2(src.nativeObj, sum.nativeObj, sqsum.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::integral(Mat src, Mat& sum, int sdepth = -1)
+    //
+
+    public static void integral(Mat src, Mat sum, int sdepth) {
+        integral_0(src.nativeObj, sum.nativeObj, sdepth);
+    }
+
+    public static void integral(Mat src, Mat sum) {
+        integral_1(src.nativeObj, sum.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::invertAffineTransform(Mat M, Mat& iM)
+    //
+
+    /**
+     * Inverts an affine transformation.
+     *
+     * The function computes an inverse affine transformation represented by \(2 \times 3\) matrix M:
+     *
+     * \(\begin{bmatrix} a_{11} &amp; a_{12} &amp; b_1  \\ a_{21} &amp; a_{22} &amp; b_2 \end{bmatrix}\)
+     *
+     * The result is also a \(2 \times 3\) matrix of the same type as M.
+     *
+     * @param M Original affine transformation.
+     * @param iM Output reverse affine transformation.
+     */
+    public static void invertAffineTransform(Mat M, Mat iM) {
+        invertAffineTransform_0(M.nativeObj, iM.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::line(Mat& img, Point pt1, Point pt2, Scalar color, int thickness = 1, int lineType = LINE_8, int shift = 0)
+    //
+
+    /**
+     * Draws a line segment connecting two points.
+     *
+     * The function line draws the line segment between pt1 and pt2 points in the image. The line is
+     * clipped by the image boundaries. For non-antialiased lines with integer coordinates, the 8-connected
+     * or 4-connected Bresenham algorithm is used. Thick lines are drawn with rounding endings. Antialiased
+     * lines are drawn using Gaussian filtering.
+     *
+     * @param img Image.
+     * @param pt1 First point of the line segment.
+     * @param pt2 Second point of the line segment.
+     * @param color Line color.
+     * @param thickness Line thickness.
+     * @param lineType Type of the line. See #LineTypes.
+     * @param shift Number of fractional bits in the point coordinates.
+     */
+    public static void line(Mat img, Point pt1, Point pt2, Scalar color, int thickness, int lineType, int shift) {
+        line_0(img.nativeObj, pt1.x, pt1.y, pt2.x, pt2.y, color.val[0], color.val[1], color.val[2], color.val[3], thickness, lineType, shift);
+    }
+
+    /**
+     * Draws a line segment connecting two points.
+     *
+     * The function line draws the line segment between pt1 and pt2 points in the image. The line is
+     * clipped by the image boundaries. For non-antialiased lines with integer coordinates, the 8-connected
+     * or 4-connected Bresenham algorithm is used. Thick lines are drawn with rounding endings. Antialiased
+     * lines are drawn using Gaussian filtering.
+     *
+     * @param img Image.
+     * @param pt1 First point of the line segment.
+     * @param pt2 Second point of the line segment.
+     * @param color Line color.
+     * @param thickness Line thickness.
+     * @param lineType Type of the line. See #LineTypes.
+     */
+    public static void line(Mat img, Point pt1, Point pt2, Scalar color, int thickness, int lineType) {
+        line_1(img.nativeObj, pt1.x, pt1.y, pt2.x, pt2.y, color.val[0], color.val[1], color.val[2], color.val[3], thickness, lineType);
+    }
+
+    /**
+     * Draws a line segment connecting two points.
+     *
+     * The function line draws the line segment between pt1 and pt2 points in the image. The line is
+     * clipped by the image boundaries. For non-antialiased lines with integer coordinates, the 8-connected
+     * or 4-connected Bresenham algorithm is used. Thick lines are drawn with rounding endings. Antialiased
+     * lines are drawn using Gaussian filtering.
+     *
+     * @param img Image.
+     * @param pt1 First point of the line segment.
+     * @param pt2 Second point of the line segment.
+     * @param color Line color.
+     * @param thickness Line thickness.
+     */
+    public static void line(Mat img, Point pt1, Point pt2, Scalar color, int thickness) {
+        line_2(img.nativeObj, pt1.x, pt1.y, pt2.x, pt2.y, color.val[0], color.val[1], color.val[2], color.val[3], thickness);
+    }
+
+    /**
+     * Draws a line segment connecting two points.
+     *
+     * The function line draws the line segment between pt1 and pt2 points in the image. The line is
+     * clipped by the image boundaries. For non-antialiased lines with integer coordinates, the 8-connected
+     * or 4-connected Bresenham algorithm is used. Thick lines are drawn with rounding endings. Antialiased
+     * lines are drawn using Gaussian filtering.
+     *
+     * @param img Image.
+     * @param pt1 First point of the line segment.
+     * @param pt2 Second point of the line segment.
+     * @param color Line color.
+     */
+    public static void line(Mat img, Point pt1, Point pt2, Scalar color) {
+        line_3(img.nativeObj, pt1.x, pt1.y, pt2.x, pt2.y, color.val[0], color.val[1], color.val[2], color.val[3]);
+    }
+
+
+    //
+    // C++:  void cv::linearPolar(Mat src, Mat& dst, Point2f center, double maxRadius, int flags)
+    //
+
+    /**
+     * Remaps an image to polar coordinates space.
+     *
+     * @deprecated This function produces same result as cv::warpPolar(src, dst, src.size(), center, maxRadius, flags)
+     *
+     *
+     * Transform the source image using the following transformation (See REF: polar_remaps_reference_image "Polar remaps reference image c)"):
+     * \(\begin{array}{l}
+     *   dst( \rho , \phi ) = src(x,y) \\
+     *   dst.size() \leftarrow src.size()
+     * \end{array}\)
+     *
+     * where
+     * \(\begin{array}{l}
+     *   I = (dx,dy) = (x - center.x,y - center.y) \\
+     *   \rho = Kmag \cdot \texttt{magnitude} (I) ,\\
+     *   \phi = angle \cdot \texttt{angle} (I)
+     * \end{array}\)
+     *
+     * and
+     * \(\begin{array}{l}
+     *   Kx = src.cols / maxRadius \\
+     *   Ky = src.rows / 2\Pi
+     * \end{array}\)
+     *
+     *
+     * @param src Source image
+     * @param dst Destination image. It will have same size and type as src.
+     * @param center The transformation center;
+     * @param maxRadius The radius of the bounding circle to transform. It determines the inverse magnitude scale parameter too.
+     * @param flags A combination of interpolation methods, see #InterpolationFlags
+     *
+     * <b>Note:</b>
+     * <ul>
+     *   <li>
+     *    The function can not operate in-place.
+     *   </li>
+     *   <li>
+     *    To calculate magnitude and angle in degrees #cartToPolar is used internally thus angles are measured from 0 to 360 with accuracy about 0.3 degrees.
+     *   </li>
+     * </ul>
+     *
+     * SEE: cv::logPolar
+     */
+    @Deprecated
+    public static void linearPolar(Mat src, Mat dst, Point center, double maxRadius, int flags) {
+        linearPolar_0(src.nativeObj, dst.nativeObj, center.x, center.y, maxRadius, flags);
+    }
+
+
+    //
+    // C++:  void cv::logPolar(Mat src, Mat& dst, Point2f center, double M, int flags)
+    //
+
+    /**
+     * Remaps an image to semilog-polar coordinates space.
+     *
+     * @deprecated This function produces same result as cv::warpPolar(src, dst, src.size(), center, maxRadius, flags+WARP_POLAR_LOG);
+     *
+     *
+     * Transform the source image using the following transformation (See REF: polar_remaps_reference_image "Polar remaps reference image d)"):
+     * \(\begin{array}{l}
+     *   dst( \rho , \phi ) = src(x,y) \\
+     *   dst.size() \leftarrow src.size()
+     * \end{array}\)
+     *
+     * where
+     * \(\begin{array}{l}
+     *   I = (dx,dy) = (x - center.x,y - center.y) \\
+     *   \rho = M \cdot log_e(\texttt{magnitude} (I)) ,\\
+     *   \phi = Kangle \cdot \texttt{angle} (I) \\
+     * \end{array}\)
+     *
+     * and
+     * \(\begin{array}{l}
+     *   M = src.cols / log_e(maxRadius) \\
+     *   Kangle = src.rows / 2\Pi \\
+     * \end{array}\)
+     *
+     * The function emulates the human "foveal" vision and can be used for fast scale and
+     * rotation-invariant template matching, for object tracking and so forth.
+     * @param src Source image
+     * @param dst Destination image. It will have same size and type as src.
+     * @param center The transformation center; where the output precision is maximal
+     * @param M Magnitude scale parameter. It determines the radius of the bounding circle to transform too.
+     * @param flags A combination of interpolation methods, see #InterpolationFlags
+     *
+     * <b>Note:</b>
+     * <ul>
+     *   <li>
+     *    The function can not operate in-place.
+     *   </li>
+     *   <li>
+     *    To calculate magnitude and angle in degrees #cartToPolar is used internally thus angles are measured from 0 to 360 with accuracy about 0.3 degrees.
+     *   </li>
+     * </ul>
+     *
+     * SEE: cv::linearPolar
+     */
+    @Deprecated
+    public static void logPolar(Mat src, Mat dst, Point center, double M, int flags) {
+        logPolar_0(src.nativeObj, dst.nativeObj, center.x, center.y, M, flags);
+    }
+
+
+    //
+    // C++:  void cv::matchTemplate(Mat image, Mat templ, Mat& result, int method, Mat mask = Mat())
+    //
+
+    /**
+     * Compares a template against overlapped image regions.
+     *
+     * The function slides through image , compares the overlapped patches of size \(w \times h\) against
+     * templ using the specified method and stores the comparison results in result . Here are the formulae
+     * for the available comparison methods ( \(I\) denotes image, \(T\) template, \(R\) result ). The summation
+     * is done over template and/or the image patch: \(x' = 0...w-1, y' = 0...h-1\)
+     *
+     * After the function finishes the comparison, the best matches can be found as global minimums (when
+     * #TM_SQDIFF was used) or maximums (when #TM_CCORR or #TM_CCOEFF was used) using the
+     * #minMaxLoc function. In case of a color image, template summation in the numerator and each sum in
+     * the denominator is done over all of the channels and separate mean values are used for each channel.
+     * That is, the function can take a color template and a color image. The result will still be a
+     * single-channel image, which is easier to analyze.
+     *
+     * @param image Image where the search is running. It must be 8-bit or 32-bit floating-point.
+     * @param templ Searched template. It must be not greater than the source image and have the same
+     * data type.
+     * @param result Map of comparison results. It must be single-channel 32-bit floating-point. If image
+     * is \(W \times H\) and templ is \(w \times h\) , then result is \((W-w+1) \times (H-h+1)\) .
+     * @param method Parameter specifying the comparison method, see #TemplateMatchModes
+     * @param mask Mask of searched template. It must have the same datatype and size with templ. It is
+     * not set by default. Currently, only the #TM_SQDIFF and #TM_CCORR_NORMED methods are supported.
+     */
+    public static void matchTemplate(Mat image, Mat templ, Mat result, int method, Mat mask) {
+        matchTemplate_0(image.nativeObj, templ.nativeObj, result.nativeObj, method, mask.nativeObj);
+    }
+
+    /**
+     * Compares a template against overlapped image regions.
+     *
+     * The function slides through image , compares the overlapped patches of size \(w \times h\) against
+     * templ using the specified method and stores the comparison results in result . Here are the formulae
+     * for the available comparison methods ( \(I\) denotes image, \(T\) template, \(R\) result ). The summation
+     * is done over template and/or the image patch: \(x' = 0...w-1, y' = 0...h-1\)
+     *
+     * After the function finishes the comparison, the best matches can be found as global minimums (when
+     * #TM_SQDIFF was used) or maximums (when #TM_CCORR or #TM_CCOEFF was used) using the
+     * #minMaxLoc function. In case of a color image, template summation in the numerator and each sum in
+     * the denominator is done over all of the channels and separate mean values are used for each channel.
+     * That is, the function can take a color template and a color image. The result will still be a
+     * single-channel image, which is easier to analyze.
+     *
+     * @param image Image where the search is running. It must be 8-bit or 32-bit floating-point.
+     * @param templ Searched template. It must be not greater than the source image and have the same
+     * data type.
+     * @param result Map of comparison results. It must be single-channel 32-bit floating-point. If image
+     * is \(W \times H\) and templ is \(w \times h\) , then result is \((W-w+1) \times (H-h+1)\) .
+     * @param method Parameter specifying the comparison method, see #TemplateMatchModes
+     * not set by default. Currently, only the #TM_SQDIFF and #TM_CCORR_NORMED methods are supported.
+     */
+    public static void matchTemplate(Mat image, Mat templ, Mat result, int method) {
+        matchTemplate_1(image.nativeObj, templ.nativeObj, result.nativeObj, method);
+    }
+
+
+    //
+    // C++:  void cv::medianBlur(Mat src, Mat& dst, int ksize)
+    //
+
+    /**
+     * Blurs an image using the median filter.
+     *
+     * The function smoothes an image using the median filter with the \(\texttt{ksize} \times
+     * \texttt{ksize}\) aperture. Each channel of a multi-channel image is processed independently.
+     * In-place operation is supported.
+     *
+     * <b>Note:</b> The median filter uses #BORDER_REPLICATE internally to cope with border pixels, see #BorderTypes
+     *
+     * @param src input 1-, 3-, or 4-channel image; when ksize is 3 or 5, the image depth should be
+     * CV_8U, CV_16U, or CV_32F, for larger aperture sizes, it can only be CV_8U.
+     * @param dst destination array of the same size and type as src.
+     * @param ksize aperture linear size; it must be odd and greater than 1, for example: 3, 5, 7 ...
+     * SEE:  bilateralFilter, blur, boxFilter, GaussianBlur
+     */
+    public static void medianBlur(Mat src, Mat dst, int ksize) {
+        medianBlur_0(src.nativeObj, dst.nativeObj, ksize);
+    }
+
+
+    //
+    // C++:  void cv::minEnclosingCircle(vector_Point2f points, Point2f& center, float& radius)
+    //
+
+    /**
+     * Finds a circle of the minimum area enclosing a 2D point set.
+     *
+     * The function finds the minimal enclosing circle of a 2D point set using an iterative algorithm.
+     *
+     * @param points Input vector of 2D points, stored in std::vector&lt;&gt; or Mat
+     * @param center Output center of the circle.
+     * @param radius Output radius of the circle.
+     */
+    public static void minEnclosingCircle(MatOfPoint2f points, Point center, float[] radius) {
+        Mat points_mat = points;
+        double[] center_out = new double[2];
+        double[] radius_out = new double[1];
+        minEnclosingCircle_0(points_mat.nativeObj, center_out, radius_out);
+        if(center!=null){ center.x = center_out[0]; center.y = center_out[1]; } 
+        if(radius!=null) radius[0] = (float)radius_out[0];
+    }
+
+
+    //
+    // C++:  void cv::morphologyEx(Mat src, Mat& dst, int op, Mat kernel, Point anchor = Point(-1,-1), int iterations = 1, int borderType = BORDER_CONSTANT, Scalar borderValue = morphologyDefaultBorderValue())
+    //
+
+    /**
+     * Performs advanced morphological transformations.
+     *
+     * The function cv::morphologyEx can perform advanced morphological transformations using an erosion and dilation as
+     * basic operations.
+     *
+     * Any of the operations can be done in-place. In case of multi-channel images, each channel is
+     * processed independently.
+     *
+     * @param src Source image. The number of channels can be arbitrary. The depth should be one of
+     * CV_8U, CV_16U, CV_16S, CV_32F or CV_64F.
+     * @param dst Destination image of the same size and type as source image.
+     * @param op Type of a morphological operation, see #MorphTypes
+     * @param kernel Structuring element. It can be created using #getStructuringElement.
+     * @param anchor Anchor position with the kernel. Negative values mean that the anchor is at the
+     * kernel center.
+     * @param iterations Number of times erosion and dilation are applied.
+     * @param borderType Pixel extrapolation method, see #BorderTypes. #BORDER_WRAP is not supported.
+     * @param borderValue Border value in case of a constant border. The default value has a special
+     * meaning.
+     * SEE:  dilate, erode, getStructuringElement
+     * <b>Note:</b> The number of iterations is the number of times erosion or dilatation operation will be applied.
+     * For instance, an opening operation (#MORPH_OPEN) with two iterations is equivalent to apply
+     * successively: erode -&gt; erode -&gt; dilate -&gt; dilate (and not erode -&gt; dilate -&gt; erode -&gt; dilate).
+     */
+    public static void morphologyEx(Mat src, Mat dst, int op, Mat kernel, Point anchor, int iterations, int borderType, Scalar borderValue) {
+        morphologyEx_0(src.nativeObj, dst.nativeObj, op, kernel.nativeObj, anchor.x, anchor.y, iterations, borderType, borderValue.val[0], borderValue.val[1], borderValue.val[2], borderValue.val[3]);
+    }
+
+    /**
+     * Performs advanced morphological transformations.
+     *
+     * The function cv::morphologyEx can perform advanced morphological transformations using an erosion and dilation as
+     * basic operations.
+     *
+     * Any of the operations can be done in-place. In case of multi-channel images, each channel is
+     * processed independently.
+     *
+     * @param src Source image. The number of channels can be arbitrary. The depth should be one of
+     * CV_8U, CV_16U, CV_16S, CV_32F or CV_64F.
+     * @param dst Destination image of the same size and type as source image.
+     * @param op Type of a morphological operation, see #MorphTypes
+     * @param kernel Structuring element. It can be created using #getStructuringElement.
+     * @param anchor Anchor position with the kernel. Negative values mean that the anchor is at the
+     * kernel center.
+     * @param iterations Number of times erosion and dilation are applied.
+     * @param borderType Pixel extrapolation method, see #BorderTypes. #BORDER_WRAP is not supported.
+     * meaning.
+     * SEE:  dilate, erode, getStructuringElement
+     * <b>Note:</b> The number of iterations is the number of times erosion or dilatation operation will be applied.
+     * For instance, an opening operation (#MORPH_OPEN) with two iterations is equivalent to apply
+     * successively: erode -&gt; erode -&gt; dilate -&gt; dilate (and not erode -&gt; dilate -&gt; erode -&gt; dilate).
+     */
+    public static void morphologyEx(Mat src, Mat dst, int op, Mat kernel, Point anchor, int iterations, int borderType) {
+        morphologyEx_1(src.nativeObj, dst.nativeObj, op, kernel.nativeObj, anchor.x, anchor.y, iterations, borderType);
+    }
+
+    /**
+     * Performs advanced morphological transformations.
+     *
+     * The function cv::morphologyEx can perform advanced morphological transformations using an erosion and dilation as
+     * basic operations.
+     *
+     * Any of the operations can be done in-place. In case of multi-channel images, each channel is
+     * processed independently.
+     *
+     * @param src Source image. The number of channels can be arbitrary. The depth should be one of
+     * CV_8U, CV_16U, CV_16S, CV_32F or CV_64F.
+     * @param dst Destination image of the same size and type as source image.
+     * @param op Type of a morphological operation, see #MorphTypes
+     * @param kernel Structuring element. It can be created using #getStructuringElement.
+     * @param anchor Anchor position with the kernel. Negative values mean that the anchor is at the
+     * kernel center.
+     * @param iterations Number of times erosion and dilation are applied.
+     * meaning.
+     * SEE:  dilate, erode, getStructuringElement
+     * <b>Note:</b> The number of iterations is the number of times erosion or dilatation operation will be applied.
+     * For instance, an opening operation (#MORPH_OPEN) with two iterations is equivalent to apply
+     * successively: erode -&gt; erode -&gt; dilate -&gt; dilate (and not erode -&gt; dilate -&gt; erode -&gt; dilate).
+     */
+    public static void morphologyEx(Mat src, Mat dst, int op, Mat kernel, Point anchor, int iterations) {
+        morphologyEx_2(src.nativeObj, dst.nativeObj, op, kernel.nativeObj, anchor.x, anchor.y, iterations);
+    }
+
+    /**
+     * Performs advanced morphological transformations.
+     *
+     * The function cv::morphologyEx can perform advanced morphological transformations using an erosion and dilation as
+     * basic operations.
+     *
+     * Any of the operations can be done in-place. In case of multi-channel images, each channel is
+     * processed independently.
+     *
+     * @param src Source image. The number of channels can be arbitrary. The depth should be one of
+     * CV_8U, CV_16U, CV_16S, CV_32F or CV_64F.
+     * @param dst Destination image of the same size and type as source image.
+     * @param op Type of a morphological operation, see #MorphTypes
+     * @param kernel Structuring element. It can be created using #getStructuringElement.
+     * @param anchor Anchor position with the kernel. Negative values mean that the anchor is at the
+     * kernel center.
+     * meaning.
+     * SEE:  dilate, erode, getStructuringElement
+     * <b>Note:</b> The number of iterations is the number of times erosion or dilatation operation will be applied.
+     * For instance, an opening operation (#MORPH_OPEN) with two iterations is equivalent to apply
+     * successively: erode -&gt; erode -&gt; dilate -&gt; dilate (and not erode -&gt; dilate -&gt; erode -&gt; dilate).
+     */
+    public static void morphologyEx(Mat src, Mat dst, int op, Mat kernel, Point anchor) {
+        morphologyEx_3(src.nativeObj, dst.nativeObj, op, kernel.nativeObj, anchor.x, anchor.y);
+    }
+
+    /**
+     * Performs advanced morphological transformations.
+     *
+     * The function cv::morphologyEx can perform advanced morphological transformations using an erosion and dilation as
+     * basic operations.
+     *
+     * Any of the operations can be done in-place. In case of multi-channel images, each channel is
+     * processed independently.
+     *
+     * @param src Source image. The number of channels can be arbitrary. The depth should be one of
+     * CV_8U, CV_16U, CV_16S, CV_32F or CV_64F.
+     * @param dst Destination image of the same size and type as source image.
+     * @param op Type of a morphological operation, see #MorphTypes
+     * @param kernel Structuring element. It can be created using #getStructuringElement.
+     * kernel center.
+     * meaning.
+     * SEE:  dilate, erode, getStructuringElement
+     * <b>Note:</b> The number of iterations is the number of times erosion or dilatation operation will be applied.
+     * For instance, an opening operation (#MORPH_OPEN) with two iterations is equivalent to apply
+     * successively: erode -&gt; erode -&gt; dilate -&gt; dilate (and not erode -&gt; dilate -&gt; erode -&gt; dilate).
+     */
+    public static void morphologyEx(Mat src, Mat dst, int op, Mat kernel) {
+        morphologyEx_4(src.nativeObj, dst.nativeObj, op, kernel.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::polylines(Mat& img, vector_vector_Point pts, bool isClosed, Scalar color, int thickness = 1, int lineType = LINE_8, int shift = 0)
+    //
+
+    /**
+     * Draws several polygonal curves.
+     *
+     * @param img Image.
+     * @param pts Array of polygonal curves.
+     * @param isClosed Flag indicating whether the drawn polylines are closed or not. If they are closed,
+     * the function draws a line from the last vertex of each curve to its first vertex.
+     * @param color Polyline color.
+     * @param thickness Thickness of the polyline edges.
+     * @param lineType Type of the line segments. See #LineTypes
+     * @param shift Number of fractional bits in the vertex coordinates.
+     *
+     * The function cv::polylines draws one or more polygonal curves.
+     */
+    public static void polylines(Mat img, List<MatOfPoint> pts, boolean isClosed, Scalar color, int thickness, int lineType, int shift) {
+        List<Mat> pts_tmplm = new ArrayList<Mat>((pts != null) ? pts.size() : 0);
+        Mat pts_mat = Converters.vector_vector_Point_to_Mat(pts, pts_tmplm);
+        polylines_0(img.nativeObj, pts_mat.nativeObj, isClosed, color.val[0], color.val[1], color.val[2], color.val[3], thickness, lineType, shift);
+    }
+
+    /**
+     * Draws several polygonal curves.
+     *
+     * @param img Image.
+     * @param pts Array of polygonal curves.
+     * @param isClosed Flag indicating whether the drawn polylines are closed or not. If they are closed,
+     * the function draws a line from the last vertex of each curve to its first vertex.
+     * @param color Polyline color.
+     * @param thickness Thickness of the polyline edges.
+     * @param lineType Type of the line segments. See #LineTypes
+     *
+     * The function cv::polylines draws one or more polygonal curves.
+     */
+    public static void polylines(Mat img, List<MatOfPoint> pts, boolean isClosed, Scalar color, int thickness, int lineType) {
+        List<Mat> pts_tmplm = new ArrayList<Mat>((pts != null) ? pts.size() : 0);
+        Mat pts_mat = Converters.vector_vector_Point_to_Mat(pts, pts_tmplm);
+        polylines_1(img.nativeObj, pts_mat.nativeObj, isClosed, color.val[0], color.val[1], color.val[2], color.val[3], thickness, lineType);
+    }
+
+    /**
+     * Draws several polygonal curves.
+     *
+     * @param img Image.
+     * @param pts Array of polygonal curves.
+     * @param isClosed Flag indicating whether the drawn polylines are closed or not. If they are closed,
+     * the function draws a line from the last vertex of each curve to its first vertex.
+     * @param color Polyline color.
+     * @param thickness Thickness of the polyline edges.
+     *
+     * The function cv::polylines draws one or more polygonal curves.
+     */
+    public static void polylines(Mat img, List<MatOfPoint> pts, boolean isClosed, Scalar color, int thickness) {
+        List<Mat> pts_tmplm = new ArrayList<Mat>((pts != null) ? pts.size() : 0);
+        Mat pts_mat = Converters.vector_vector_Point_to_Mat(pts, pts_tmplm);
+        polylines_2(img.nativeObj, pts_mat.nativeObj, isClosed, color.val[0], color.val[1], color.val[2], color.val[3], thickness);
+    }
+
+    /**
+     * Draws several polygonal curves.
+     *
+     * @param img Image.
+     * @param pts Array of polygonal curves.
+     * @param isClosed Flag indicating whether the drawn polylines are closed or not. If they are closed,
+     * the function draws a line from the last vertex of each curve to its first vertex.
+     * @param color Polyline color.
+     *
+     * The function cv::polylines draws one or more polygonal curves.
+     */
+    public static void polylines(Mat img, List<MatOfPoint> pts, boolean isClosed, Scalar color) {
+        List<Mat> pts_tmplm = new ArrayList<Mat>((pts != null) ? pts.size() : 0);
+        Mat pts_mat = Converters.vector_vector_Point_to_Mat(pts, pts_tmplm);
+        polylines_3(img.nativeObj, pts_mat.nativeObj, isClosed, color.val[0], color.val[1], color.val[2], color.val[3]);
+    }
+
+
+    //
+    // C++:  void cv::preCornerDetect(Mat src, Mat& dst, int ksize, int borderType = BORDER_DEFAULT)
+    //
+
+    /**
+     * Calculates a feature map for corner detection.
+     *
+     * The function calculates the complex spatial derivative-based function of the source image
+     *
+     * \(\texttt{dst} = (D_x  \texttt{src} )^2  \cdot D_{yy}  \texttt{src} + (D_y  \texttt{src} )^2  \cdot D_{xx}  \texttt{src} - 2 D_x  \texttt{src} \cdot D_y  \texttt{src} \cdot D_{xy}  \texttt{src}\)
+     *
+     * where \(D_x\),\(D_y\) are the first image derivatives, \(D_{xx}\),\(D_{yy}\) are the second image
+     * derivatives, and \(D_{xy}\) is the mixed derivative.
+     *
+     * The corners can be found as local maximums of the functions, as shown below:
+     * <code>
+     *     Mat corners, dilated_corners;
+     *     preCornerDetect(image, corners, 3);
+     *     // dilation with 3x3 rectangular structuring element
+     *     dilate(corners, dilated_corners, Mat(), 1);
+     *     Mat corner_mask = corners == dilated_corners;
+     * </code>
+     *
+     * @param src Source single-channel 8-bit of floating-point image.
+     * @param dst Output image that has the type CV_32F and the same size as src .
+     * @param ksize %Aperture size of the Sobel .
+     * @param borderType Pixel extrapolation method. See #BorderTypes. #BORDER_WRAP is not supported.
+     */
+    public static void preCornerDetect(Mat src, Mat dst, int ksize, int borderType) {
+        preCornerDetect_0(src.nativeObj, dst.nativeObj, ksize, borderType);
+    }
+
+    /**
+     * Calculates a feature map for corner detection.
+     *
+     * The function calculates the complex spatial derivative-based function of the source image
+     *
+     * \(\texttt{dst} = (D_x  \texttt{src} )^2  \cdot D_{yy}  \texttt{src} + (D_y  \texttt{src} )^2  \cdot D_{xx}  \texttt{src} - 2 D_x  \texttt{src} \cdot D_y  \texttt{src} \cdot D_{xy}  \texttt{src}\)
+     *
+     * where \(D_x\),\(D_y\) are the first image derivatives, \(D_{xx}\),\(D_{yy}\) are the second image
+     * derivatives, and \(D_{xy}\) is the mixed derivative.
+     *
+     * The corners can be found as local maximums of the functions, as shown below:
+     * <code>
+     *     Mat corners, dilated_corners;
+     *     preCornerDetect(image, corners, 3);
+     *     // dilation with 3x3 rectangular structuring element
+     *     dilate(corners, dilated_corners, Mat(), 1);
+     *     Mat corner_mask = corners == dilated_corners;
+     * </code>
+     *
+     * @param src Source single-channel 8-bit of floating-point image.
+     * @param dst Output image that has the type CV_32F and the same size as src .
+     * @param ksize %Aperture size of the Sobel .
+     */
+    public static void preCornerDetect(Mat src, Mat dst, int ksize) {
+        preCornerDetect_1(src.nativeObj, dst.nativeObj, ksize);
+    }
+
+
+    //
+    // C++:  void cv::putText(Mat& img, String text, Point org, int fontFace, double fontScale, Scalar color, int thickness = 1, int lineType = LINE_8, bool bottomLeftOrigin = false)
+    //
+
+    /**
+     * Draws a text string.
+     *
+     * The function cv::putText renders the specified text string in the image. Symbols that cannot be rendered
+     * using the specified font are replaced by question marks. See #getTextSize for a text rendering code
+     * example.
+     *
+     * @param img Image.
+     * @param text Text string to be drawn.
+     * @param org Bottom-left corner of the text string in the image.
+     * @param fontFace Font type, see #HersheyFonts.
+     * @param fontScale Font scale factor that is multiplied by the font-specific base size.
+     * @param color Text color.
+     * @param thickness Thickness of the lines used to draw a text.
+     * @param lineType Line type. See #LineTypes
+     * @param bottomLeftOrigin When true, the image data origin is at the bottom-left corner. Otherwise,
+     * it is at the top-left corner.
+     */
+    public static void putText(Mat img, String text, Point org, int fontFace, double fontScale, Scalar color, int thickness, int lineType, boolean bottomLeftOrigin) {
+        putText_0(img.nativeObj, text, org.x, org.y, fontFace, fontScale, color.val[0], color.val[1], color.val[2], color.val[3], thickness, lineType, bottomLeftOrigin);
+    }
+
+    /**
+     * Draws a text string.
+     *
+     * The function cv::putText renders the specified text string in the image. Symbols that cannot be rendered
+     * using the specified font are replaced by question marks. See #getTextSize for a text rendering code
+     * example.
+     *
+     * @param img Image.
+     * @param text Text string to be drawn.
+     * @param org Bottom-left corner of the text string in the image.
+     * @param fontFace Font type, see #HersheyFonts.
+     * @param fontScale Font scale factor that is multiplied by the font-specific base size.
+     * @param color Text color.
+     * @param thickness Thickness of the lines used to draw a text.
+     * @param lineType Line type. See #LineTypes
+     * it is at the top-left corner.
+     */
+    public static void putText(Mat img, String text, Point org, int fontFace, double fontScale, Scalar color, int thickness, int lineType) {
+        putText_1(img.nativeObj, text, org.x, org.y, fontFace, fontScale, color.val[0], color.val[1], color.val[2], color.val[3], thickness, lineType);
+    }
+
+    /**
+     * Draws a text string.
+     *
+     * The function cv::putText renders the specified text string in the image. Symbols that cannot be rendered
+     * using the specified font are replaced by question marks. See #getTextSize for a text rendering code
+     * example.
+     *
+     * @param img Image.
+     * @param text Text string to be drawn.
+     * @param org Bottom-left corner of the text string in the image.
+     * @param fontFace Font type, see #HersheyFonts.
+     * @param fontScale Font scale factor that is multiplied by the font-specific base size.
+     * @param color Text color.
+     * @param thickness Thickness of the lines used to draw a text.
+     * it is at the top-left corner.
+     */
+    public static void putText(Mat img, String text, Point org, int fontFace, double fontScale, Scalar color, int thickness) {
+        putText_2(img.nativeObj, text, org.x, org.y, fontFace, fontScale, color.val[0], color.val[1], color.val[2], color.val[3], thickness);
+    }
+
+    /**
+     * Draws a text string.
+     *
+     * The function cv::putText renders the specified text string in the image. Symbols that cannot be rendered
+     * using the specified font are replaced by question marks. See #getTextSize for a text rendering code
+     * example.
+     *
+     * @param img Image.
+     * @param text Text string to be drawn.
+     * @param org Bottom-left corner of the text string in the image.
+     * @param fontFace Font type, see #HersheyFonts.
+     * @param fontScale Font scale factor that is multiplied by the font-specific base size.
+     * @param color Text color.
+     * it is at the top-left corner.
+     */
+    public static void putText(Mat img, String text, Point org, int fontFace, double fontScale, Scalar color) {
+        putText_3(img.nativeObj, text, org.x, org.y, fontFace, fontScale, color.val[0], color.val[1], color.val[2], color.val[3]);
+    }
+
+
+    //
+    // C++:  void cv::pyrDown(Mat src, Mat& dst, Size dstsize = Size(), int borderType = BORDER_DEFAULT)
+    //
+
+    /**
+     * Blurs an image and downsamples it.
+     *
+     * By default, size of the output image is computed as {@code Size((src.cols+1)/2, (src.rows+1)/2)}, but in
+     * any case, the following conditions should be satisfied:
+     *
+     * \(\begin{array}{l} | \texttt{dstsize.width} *2-src.cols| \leq 2 \\ | \texttt{dstsize.height} *2-src.rows| \leq 2 \end{array}\)
+     *
+     * The function performs the downsampling step of the Gaussian pyramid construction. First, it
+     * convolves the source image with the kernel:
+     *
+     * \(\frac{1}{256} \begin{bmatrix} 1 &amp; 4 &amp; 6 &amp; 4 &amp; 1  \\ 4 &amp; 16 &amp; 24 &amp; 16 &amp; 4  \\ 6 &amp; 24 &amp; 36 &amp; 24 &amp; 6  \\ 4 &amp; 16 &amp; 24 &amp; 16 &amp; 4  \\ 1 &amp; 4 &amp; 6 &amp; 4 &amp; 1 \end{bmatrix}\)
+     *
+     * Then, it downsamples the image by rejecting even rows and columns.
+     *
+     * @param src input image.
+     * @param dst output image; it has the specified size and the same type as src.
+     * @param dstsize size of the output image.
+     * @param borderType Pixel extrapolation method, see #BorderTypes (#BORDER_CONSTANT isn't supported)
+     */
+    public static void pyrDown(Mat src, Mat dst, Size dstsize, int borderType) {
+        pyrDown_0(src.nativeObj, dst.nativeObj, dstsize.width, dstsize.height, borderType);
+    }
+
+    /**
+     * Blurs an image and downsamples it.
+     *
+     * By default, size of the output image is computed as {@code Size((src.cols+1)/2, (src.rows+1)/2)}, but in
+     * any case, the following conditions should be satisfied:
+     *
+     * \(\begin{array}{l} | \texttt{dstsize.width} *2-src.cols| \leq 2 \\ | \texttt{dstsize.height} *2-src.rows| \leq 2 \end{array}\)
+     *
+     * The function performs the downsampling step of the Gaussian pyramid construction. First, it
+     * convolves the source image with the kernel:
+     *
+     * \(\frac{1}{256} \begin{bmatrix} 1 &amp; 4 &amp; 6 &amp; 4 &amp; 1  \\ 4 &amp; 16 &amp; 24 &amp; 16 &amp; 4  \\ 6 &amp; 24 &amp; 36 &amp; 24 &amp; 6  \\ 4 &amp; 16 &amp; 24 &amp; 16 &amp; 4  \\ 1 &amp; 4 &amp; 6 &amp; 4 &amp; 1 \end{bmatrix}\)
+     *
+     * Then, it downsamples the image by rejecting even rows and columns.
+     *
+     * @param src input image.
+     * @param dst output image; it has the specified size and the same type as src.
+     * @param dstsize size of the output image.
+     */
+    public static void pyrDown(Mat src, Mat dst, Size dstsize) {
+        pyrDown_1(src.nativeObj, dst.nativeObj, dstsize.width, dstsize.height);
+    }
+
+    /**
+     * Blurs an image and downsamples it.
+     *
+     * By default, size of the output image is computed as {@code Size((src.cols+1)/2, (src.rows+1)/2)}, but in
+     * any case, the following conditions should be satisfied:
+     *
+     * \(\begin{array}{l} | \texttt{dstsize.width} *2-src.cols| \leq 2 \\ | \texttt{dstsize.height} *2-src.rows| \leq 2 \end{array}\)
+     *
+     * The function performs the downsampling step of the Gaussian pyramid construction. First, it
+     * convolves the source image with the kernel:
+     *
+     * \(\frac{1}{256} \begin{bmatrix} 1 &amp; 4 &amp; 6 &amp; 4 &amp; 1  \\ 4 &amp; 16 &amp; 24 &amp; 16 &amp; 4  \\ 6 &amp; 24 &amp; 36 &amp; 24 &amp; 6  \\ 4 &amp; 16 &amp; 24 &amp; 16 &amp; 4  \\ 1 &amp; 4 &amp; 6 &amp; 4 &amp; 1 \end{bmatrix}\)
+     *
+     * Then, it downsamples the image by rejecting even rows and columns.
+     *
+     * @param src input image.
+     * @param dst output image; it has the specified size and the same type as src.
+     */
+    public static void pyrDown(Mat src, Mat dst) {
+        pyrDown_2(src.nativeObj, dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::pyrMeanShiftFiltering(Mat src, Mat& dst, double sp, double sr, int maxLevel = 1, TermCriteria termcrit = TermCriteria(TermCriteria::MAX_ITER+TermCriteria::EPS,5,1))
+    //
+
+    /**
+     * Performs initial step of meanshift segmentation of an image.
+     *
+     * The function implements the filtering stage of meanshift segmentation, that is, the output of the
+     * function is the filtered "posterized" image with color gradients and fine-grain texture flattened.
+     * At every pixel (X,Y) of the input image (or down-sized input image, see below) the function executes
+     * meanshift iterations, that is, the pixel (X,Y) neighborhood in the joint space-color hyperspace is
+     * considered:
+     *
+     * \((x,y): X- \texttt{sp} \le x  \le X+ \texttt{sp} , Y- \texttt{sp} \le y  \le Y+ \texttt{sp} , ||(R,G,B)-(r,g,b)||   \le \texttt{sr}\)
+     *
+     * where (R,G,B) and (r,g,b) are the vectors of color components at (X,Y) and (x,y), respectively
+     * (though, the algorithm does not depend on the color space used, so any 3-component color space can
+     * be used instead). Over the neighborhood the average spatial value (X',Y') and average color vector
+     * (R',G',B') are found and they act as the neighborhood center on the next iteration:
+     *
+     * \((X,Y)~(X',Y'), (R,G,B)~(R',G',B').\)
+     *
+     * After the iterations over, the color components of the initial pixel (that is, the pixel from where
+     * the iterations started) are set to the final value (average color at the last iteration):
+     *
+     * \(I(X,Y) &lt;- (R*,G*,B*)\)
+     *
+     * When maxLevel &gt; 0, the gaussian pyramid of maxLevel+1 levels is built, and the above procedure is
+     * run on the smallest layer first. After that, the results are propagated to the larger layer and the
+     * iterations are run again only on those pixels where the layer colors differ by more than sr from the
+     * lower-resolution layer of the pyramid. That makes boundaries of color regions sharper. Note that the
+     * results will be actually different from the ones obtained by running the meanshift procedure on the
+     * whole original image (i.e. when maxLevel==0).
+     *
+     * @param src The source 8-bit, 3-channel image.
+     * @param dst The destination image of the same format and the same size as the source.
+     * @param sp The spatial window radius.
+     * @param sr The color window radius.
+     * @param maxLevel Maximum level of the pyramid for the segmentation.
+     * @param termcrit Termination criteria: when to stop meanshift iterations.
+     */
+    public static void pyrMeanShiftFiltering(Mat src, Mat dst, double sp, double sr, int maxLevel, TermCriteria termcrit) {
+        pyrMeanShiftFiltering_0(src.nativeObj, dst.nativeObj, sp, sr, maxLevel, termcrit.type, termcrit.maxCount, termcrit.epsilon);
+    }
+
+    /**
+     * Performs initial step of meanshift segmentation of an image.
+     *
+     * The function implements the filtering stage of meanshift segmentation, that is, the output of the
+     * function is the filtered "posterized" image with color gradients and fine-grain texture flattened.
+     * At every pixel (X,Y) of the input image (or down-sized input image, see below) the function executes
+     * meanshift iterations, that is, the pixel (X,Y) neighborhood in the joint space-color hyperspace is
+     * considered:
+     *
+     * \((x,y): X- \texttt{sp} \le x  \le X+ \texttt{sp} , Y- \texttt{sp} \le y  \le Y+ \texttt{sp} , ||(R,G,B)-(r,g,b)||   \le \texttt{sr}\)
+     *
+     * where (R,G,B) and (r,g,b) are the vectors of color components at (X,Y) and (x,y), respectively
+     * (though, the algorithm does not depend on the color space used, so any 3-component color space can
+     * be used instead). Over the neighborhood the average spatial value (X',Y') and average color vector
+     * (R',G',B') are found and they act as the neighborhood center on the next iteration:
+     *
+     * \((X,Y)~(X',Y'), (R,G,B)~(R',G',B').\)
+     *
+     * After the iterations over, the color components of the initial pixel (that is, the pixel from where
+     * the iterations started) are set to the final value (average color at the last iteration):
+     *
+     * \(I(X,Y) &lt;- (R*,G*,B*)\)
+     *
+     * When maxLevel &gt; 0, the gaussian pyramid of maxLevel+1 levels is built, and the above procedure is
+     * run on the smallest layer first. After that, the results are propagated to the larger layer and the
+     * iterations are run again only on those pixels where the layer colors differ by more than sr from the
+     * lower-resolution layer of the pyramid. That makes boundaries of color regions sharper. Note that the
+     * results will be actually different from the ones obtained by running the meanshift procedure on the
+     * whole original image (i.e. when maxLevel==0).
+     *
+     * @param src The source 8-bit, 3-channel image.
+     * @param dst The destination image of the same format and the same size as the source.
+     * @param sp The spatial window radius.
+     * @param sr The color window radius.
+     * @param maxLevel Maximum level of the pyramid for the segmentation.
+     */
+    public static void pyrMeanShiftFiltering(Mat src, Mat dst, double sp, double sr, int maxLevel) {
+        pyrMeanShiftFiltering_1(src.nativeObj, dst.nativeObj, sp, sr, maxLevel);
+    }
+
+    /**
+     * Performs initial step of meanshift segmentation of an image.
+     *
+     * The function implements the filtering stage of meanshift segmentation, that is, the output of the
+     * function is the filtered "posterized" image with color gradients and fine-grain texture flattened.
+     * At every pixel (X,Y) of the input image (or down-sized input image, see below) the function executes
+     * meanshift iterations, that is, the pixel (X,Y) neighborhood in the joint space-color hyperspace is
+     * considered:
+     *
+     * \((x,y): X- \texttt{sp} \le x  \le X+ \texttt{sp} , Y- \texttt{sp} \le y  \le Y+ \texttt{sp} , ||(R,G,B)-(r,g,b)||   \le \texttt{sr}\)
+     *
+     * where (R,G,B) and (r,g,b) are the vectors of color components at (X,Y) and (x,y), respectively
+     * (though, the algorithm does not depend on the color space used, so any 3-component color space can
+     * be used instead). Over the neighborhood the average spatial value (X',Y') and average color vector
+     * (R',G',B') are found and they act as the neighborhood center on the next iteration:
+     *
+     * \((X,Y)~(X',Y'), (R,G,B)~(R',G',B').\)
+     *
+     * After the iterations over, the color components of the initial pixel (that is, the pixel from where
+     * the iterations started) are set to the final value (average color at the last iteration):
+     *
+     * \(I(X,Y) &lt;- (R*,G*,B*)\)
+     *
+     * When maxLevel &gt; 0, the gaussian pyramid of maxLevel+1 levels is built, and the above procedure is
+     * run on the smallest layer first. After that, the results are propagated to the larger layer and the
+     * iterations are run again only on those pixels where the layer colors differ by more than sr from the
+     * lower-resolution layer of the pyramid. That makes boundaries of color regions sharper. Note that the
+     * results will be actually different from the ones obtained by running the meanshift procedure on the
+     * whole original image (i.e. when maxLevel==0).
+     *
+     * @param src The source 8-bit, 3-channel image.
+     * @param dst The destination image of the same format and the same size as the source.
+     * @param sp The spatial window radius.
+     * @param sr The color window radius.
+     */
+    public static void pyrMeanShiftFiltering(Mat src, Mat dst, double sp, double sr) {
+        pyrMeanShiftFiltering_2(src.nativeObj, dst.nativeObj, sp, sr);
+    }
+
+
+    //
+    // C++:  void cv::pyrUp(Mat src, Mat& dst, Size dstsize = Size(), int borderType = BORDER_DEFAULT)
+    //
+
+    /**
+     * Upsamples an image and then blurs it.
+     *
+     * By default, size of the output image is computed as {@code Size(src.cols\*2, (src.rows\*2)}, but in any
+     * case, the following conditions should be satisfied:
+     *
+     * \(\begin{array}{l} | \texttt{dstsize.width} -src.cols*2| \leq  ( \texttt{dstsize.width}   \mod  2)  \\ | \texttt{dstsize.height} -src.rows*2| \leq  ( \texttt{dstsize.height}   \mod  2) \end{array}\)
+     *
+     * The function performs the upsampling step of the Gaussian pyramid construction, though it can
+     * actually be used to construct the Laplacian pyramid. First, it upsamples the source image by
+     * injecting even zero rows and columns and then convolves the result with the same kernel as in
+     * pyrDown multiplied by 4.
+     *
+     * @param src input image.
+     * @param dst output image. It has the specified size and the same type as src .
+     * @param dstsize size of the output image.
+     * @param borderType Pixel extrapolation method, see #BorderTypes (only #BORDER_DEFAULT is supported)
+     */
+    public static void pyrUp(Mat src, Mat dst, Size dstsize, int borderType) {
+        pyrUp_0(src.nativeObj, dst.nativeObj, dstsize.width, dstsize.height, borderType);
+    }
+
+    /**
+     * Upsamples an image and then blurs it.
+     *
+     * By default, size of the output image is computed as {@code Size(src.cols\*2, (src.rows\*2)}, but in any
+     * case, the following conditions should be satisfied:
+     *
+     * \(\begin{array}{l} | \texttt{dstsize.width} -src.cols*2| \leq  ( \texttt{dstsize.width}   \mod  2)  \\ | \texttt{dstsize.height} -src.rows*2| \leq  ( \texttt{dstsize.height}   \mod  2) \end{array}\)
+     *
+     * The function performs the upsampling step of the Gaussian pyramid construction, though it can
+     * actually be used to construct the Laplacian pyramid. First, it upsamples the source image by
+     * injecting even zero rows and columns and then convolves the result with the same kernel as in
+     * pyrDown multiplied by 4.
+     *
+     * @param src input image.
+     * @param dst output image. It has the specified size and the same type as src .
+     * @param dstsize size of the output image.
+     */
+    public static void pyrUp(Mat src, Mat dst, Size dstsize) {
+        pyrUp_1(src.nativeObj, dst.nativeObj, dstsize.width, dstsize.height);
+    }
+
+    /**
+     * Upsamples an image and then blurs it.
+     *
+     * By default, size of the output image is computed as {@code Size(src.cols\*2, (src.rows\*2)}, but in any
+     * case, the following conditions should be satisfied:
+     *
+     * \(\begin{array}{l} | \texttt{dstsize.width} -src.cols*2| \leq  ( \texttt{dstsize.width}   \mod  2)  \\ | \texttt{dstsize.height} -src.rows*2| \leq  ( \texttt{dstsize.height}   \mod  2) \end{array}\)
+     *
+     * The function performs the upsampling step of the Gaussian pyramid construction, though it can
+     * actually be used to construct the Laplacian pyramid. First, it upsamples the source image by
+     * injecting even zero rows and columns and then convolves the result with the same kernel as in
+     * pyrDown multiplied by 4.
+     *
+     * @param src input image.
+     * @param dst output image. It has the specified size and the same type as src .
+     */
+    public static void pyrUp(Mat src, Mat dst) {
+        pyrUp_2(src.nativeObj, dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::rectangle(Mat& img, Point pt1, Point pt2, Scalar color, int thickness = 1, int lineType = LINE_8, int shift = 0)
+    //
+
+    /**
+     * Draws a simple, thick, or filled up-right rectangle.
+     *
+     * The function cv::rectangle draws a rectangle outline or a filled rectangle whose two opposite corners
+     * are pt1 and pt2.
+     *
+     * @param img Image.
+     * @param pt1 Vertex of the rectangle.
+     * @param pt2 Vertex of the rectangle opposite to pt1 .
+     * @param color Rectangle color or brightness (grayscale image).
+     * @param thickness Thickness of lines that make up the rectangle. Negative values, like #FILLED,
+     * mean that the function has to draw a filled rectangle.
+     * @param lineType Type of the line. See #LineTypes
+     * @param shift Number of fractional bits in the point coordinates.
+     */
+    public static void rectangle(Mat img, Point pt1, Point pt2, Scalar color, int thickness, int lineType, int shift) {
+        rectangle_0(img.nativeObj, pt1.x, pt1.y, pt2.x, pt2.y, color.val[0], color.val[1], color.val[2], color.val[3], thickness, lineType, shift);
+    }
+
+    /**
+     * Draws a simple, thick, or filled up-right rectangle.
+     *
+     * The function cv::rectangle draws a rectangle outline or a filled rectangle whose two opposite corners
+     * are pt1 and pt2.
+     *
+     * @param img Image.
+     * @param pt1 Vertex of the rectangle.
+     * @param pt2 Vertex of the rectangle opposite to pt1 .
+     * @param color Rectangle color or brightness (grayscale image).
+     * @param thickness Thickness of lines that make up the rectangle. Negative values, like #FILLED,
+     * mean that the function has to draw a filled rectangle.
+     * @param lineType Type of the line. See #LineTypes
+     */
+    public static void rectangle(Mat img, Point pt1, Point pt2, Scalar color, int thickness, int lineType) {
+        rectangle_1(img.nativeObj, pt1.x, pt1.y, pt2.x, pt2.y, color.val[0], color.val[1], color.val[2], color.val[3], thickness, lineType);
+    }
+
+    /**
+     * Draws a simple, thick, or filled up-right rectangle.
+     *
+     * The function cv::rectangle draws a rectangle outline or a filled rectangle whose two opposite corners
+     * are pt1 and pt2.
+     *
+     * @param img Image.
+     * @param pt1 Vertex of the rectangle.
+     * @param pt2 Vertex of the rectangle opposite to pt1 .
+     * @param color Rectangle color or brightness (grayscale image).
+     * @param thickness Thickness of lines that make up the rectangle. Negative values, like #FILLED,
+     * mean that the function has to draw a filled rectangle.
+     */
+    public static void rectangle(Mat img, Point pt1, Point pt2, Scalar color, int thickness) {
+        rectangle_2(img.nativeObj, pt1.x, pt1.y, pt2.x, pt2.y, color.val[0], color.val[1], color.val[2], color.val[3], thickness);
+    }
+
+    /**
+     * Draws a simple, thick, or filled up-right rectangle.
+     *
+     * The function cv::rectangle draws a rectangle outline or a filled rectangle whose two opposite corners
+     * are pt1 and pt2.
+     *
+     * @param img Image.
+     * @param pt1 Vertex of the rectangle.
+     * @param pt2 Vertex of the rectangle opposite to pt1 .
+     * @param color Rectangle color or brightness (grayscale image).
+     * mean that the function has to draw a filled rectangle.
+     */
+    public static void rectangle(Mat img, Point pt1, Point pt2, Scalar color) {
+        rectangle_3(img.nativeObj, pt1.x, pt1.y, pt2.x, pt2.y, color.val[0], color.val[1], color.val[2], color.val[3]);
+    }
+
+
+    //
+    // C++:  void cv::remap(Mat src, Mat& dst, Mat map1, Mat map2, int interpolation, int borderMode = BORDER_CONSTANT, Scalar borderValue = Scalar())
+    //
+
+    /**
+     * Applies a generic geometrical transformation to an image.
+     *
+     * The function remap transforms the source image using the specified map:
+     *
+     * \(\texttt{dst} (x,y) =  \texttt{src} (map_x(x,y),map_y(x,y))\)
+     *
+     * where values of pixels with non-integer coordinates are computed using one of available
+     * interpolation methods. \(map_x\) and \(map_y\) can be encoded as separate floating-point maps
+     * in \(map_1\) and \(map_2\) respectively, or interleaved floating-point maps of \((x,y)\) in
+     * \(map_1\), or fixed-point maps created by using convertMaps. The reason you might want to
+     * convert from floating to fixed-point representations of a map is that they can yield much faster
+     * (\~2x) remapping operations. In the converted case, \(map_1\) contains pairs (cvFloor(x),
+     * cvFloor(y)) and \(map_2\) contains indices in a table of interpolation coefficients.
+     *
+     * This function cannot operate in-place.
+     *
+     * @param src Source image.
+     * @param dst Destination image. It has the same size as map1 and the same type as src .
+     * @param map1 The first map of either (x,y) points or just x values having the type CV_16SC2 ,
+     * CV_32FC1, or CV_32FC2. See convertMaps for details on converting a floating point
+     * representation to fixed-point for speed.
+     * @param map2 The second map of y values having the type CV_16UC1, CV_32FC1, or none (empty map
+     * if map1 is (x,y) points), respectively.
+     * @param interpolation Interpolation method (see #InterpolationFlags). The method #INTER_AREA is
+     * not supported by this function.
+     * @param borderMode Pixel extrapolation method (see #BorderTypes). When
+     * borderMode=#BORDER_TRANSPARENT, it means that the pixels in the destination image that
+     * corresponds to the "outliers" in the source image are not modified by the function.
+     * @param borderValue Value used in case of a constant border. By default, it is 0.
+     * <b>Note:</b>
+     * Due to current implementation limitations the size of an input and output images should be less than 32767x32767.
+     */
+    public static void remap(Mat src, Mat dst, Mat map1, Mat map2, int interpolation, int borderMode, Scalar borderValue) {
+        remap_0(src.nativeObj, dst.nativeObj, map1.nativeObj, map2.nativeObj, interpolation, borderMode, borderValue.val[0], borderValue.val[1], borderValue.val[2], borderValue.val[3]);
+    }
+
+    /**
+     * Applies a generic geometrical transformation to an image.
+     *
+     * The function remap transforms the source image using the specified map:
+     *
+     * \(\texttt{dst} (x,y) =  \texttt{src} (map_x(x,y),map_y(x,y))\)
+     *
+     * where values of pixels with non-integer coordinates are computed using one of available
+     * interpolation methods. \(map_x\) and \(map_y\) can be encoded as separate floating-point maps
+     * in \(map_1\) and \(map_2\) respectively, or interleaved floating-point maps of \((x,y)\) in
+     * \(map_1\), or fixed-point maps created by using convertMaps. The reason you might want to
+     * convert from floating to fixed-point representations of a map is that they can yield much faster
+     * (\~2x) remapping operations. In the converted case, \(map_1\) contains pairs (cvFloor(x),
+     * cvFloor(y)) and \(map_2\) contains indices in a table of interpolation coefficients.
+     *
+     * This function cannot operate in-place.
+     *
+     * @param src Source image.
+     * @param dst Destination image. It has the same size as map1 and the same type as src .
+     * @param map1 The first map of either (x,y) points or just x values having the type CV_16SC2 ,
+     * CV_32FC1, or CV_32FC2. See convertMaps for details on converting a floating point
+     * representation to fixed-point for speed.
+     * @param map2 The second map of y values having the type CV_16UC1, CV_32FC1, or none (empty map
+     * if map1 is (x,y) points), respectively.
+     * @param interpolation Interpolation method (see #InterpolationFlags). The method #INTER_AREA is
+     * not supported by this function.
+     * @param borderMode Pixel extrapolation method (see #BorderTypes). When
+     * borderMode=#BORDER_TRANSPARENT, it means that the pixels in the destination image that
+     * corresponds to the "outliers" in the source image are not modified by the function.
+     * <b>Note:</b>
+     * Due to current implementation limitations the size of an input and output images should be less than 32767x32767.
+     */
+    public static void remap(Mat src, Mat dst, Mat map1, Mat map2, int interpolation, int borderMode) {
+        remap_1(src.nativeObj, dst.nativeObj, map1.nativeObj, map2.nativeObj, interpolation, borderMode);
+    }
+
+    /**
+     * Applies a generic geometrical transformation to an image.
+     *
+     * The function remap transforms the source image using the specified map:
+     *
+     * \(\texttt{dst} (x,y) =  \texttt{src} (map_x(x,y),map_y(x,y))\)
+     *
+     * where values of pixels with non-integer coordinates are computed using one of available
+     * interpolation methods. \(map_x\) and \(map_y\) can be encoded as separate floating-point maps
+     * in \(map_1\) and \(map_2\) respectively, or interleaved floating-point maps of \((x,y)\) in
+     * \(map_1\), or fixed-point maps created by using convertMaps. The reason you might want to
+     * convert from floating to fixed-point representations of a map is that they can yield much faster
+     * (\~2x) remapping operations. In the converted case, \(map_1\) contains pairs (cvFloor(x),
+     * cvFloor(y)) and \(map_2\) contains indices in a table of interpolation coefficients.
+     *
+     * This function cannot operate in-place.
+     *
+     * @param src Source image.
+     * @param dst Destination image. It has the same size as map1 and the same type as src .
+     * @param map1 The first map of either (x,y) points or just x values having the type CV_16SC2 ,
+     * CV_32FC1, or CV_32FC2. See convertMaps for details on converting a floating point
+     * representation to fixed-point for speed.
+     * @param map2 The second map of y values having the type CV_16UC1, CV_32FC1, or none (empty map
+     * if map1 is (x,y) points), respectively.
+     * @param interpolation Interpolation method (see #InterpolationFlags). The method #INTER_AREA is
+     * not supported by this function.
+     * borderMode=#BORDER_TRANSPARENT, it means that the pixels in the destination image that
+     * corresponds to the "outliers" in the source image are not modified by the function.
+     * <b>Note:</b>
+     * Due to current implementation limitations the size of an input and output images should be less than 32767x32767.
+     */
+    public static void remap(Mat src, Mat dst, Mat map1, Mat map2, int interpolation) {
+        remap_2(src.nativeObj, dst.nativeObj, map1.nativeObj, map2.nativeObj, interpolation);
+    }
+
+
+    //
+    // C++:  void cv::resize(Mat src, Mat& dst, Size dsize, double fx = 0, double fy = 0, int interpolation = INTER_LINEAR)
+    //
+
+    /**
+     * Resizes an image.
+     *
+     * The function resize resizes the image src down to or up to the specified size. Note that the
+     * initial dst type or size are not taken into account. Instead, the size and type are derived from
+     * the {@code src},{@code dsize},{@code fx}, and {@code fy}. If you want to resize src so that it fits the pre-created dst,
+     * you may call the function as follows:
+     * <code>
+     *     // explicitly specify dsize=dst.size(); fx and fy will be computed from that.
+     *     resize(src, dst, dst.size(), 0, 0, interpolation);
+     * </code>
+     * If you want to decimate the image by factor of 2 in each direction, you can call the function this
+     * way:
+     * <code>
+     *     // specify fx and fy and let the function compute the destination image size.
+     *     resize(src, dst, Size(), 0.5, 0.5, interpolation);
+     * </code>
+     * To shrink an image, it will generally look best with #INTER_AREA interpolation, whereas to
+     * enlarge an image, it will generally look best with c#INTER_CUBIC (slow) or #INTER_LINEAR
+     * (faster but still looks OK).
+     *
+     * @param src input image.
+     * @param dst output image; it has the size dsize (when it is non-zero) or the size computed from
+     * src.size(), fx, and fy; the type of dst is the same as of src.
+     * @param dsize output image size; if it equals zero, it is computed as:
+     *  \(\texttt{dsize = Size(round(fx*src.cols), round(fy*src.rows))}\)
+     *  Either dsize or both fx and fy must be non-zero.
+     * @param fx scale factor along the horizontal axis; when it equals 0, it is computed as
+     * \(\texttt{(double)dsize.width/src.cols}\)
+     * @param fy scale factor along the vertical axis; when it equals 0, it is computed as
+     * \(\texttt{(double)dsize.height/src.rows}\)
+     * @param interpolation interpolation method, see #InterpolationFlags
+     *
+     * SEE:  warpAffine, warpPerspective, remap
+     */
+    public static void resize(Mat src, Mat dst, Size dsize, double fx, double fy, int interpolation) {
+        resize_0(src.nativeObj, dst.nativeObj, dsize.width, dsize.height, fx, fy, interpolation);
+    }
+
+    /**
+     * Resizes an image.
+     *
+     * The function resize resizes the image src down to or up to the specified size. Note that the
+     * initial dst type or size are not taken into account. Instead, the size and type are derived from
+     * the {@code src},{@code dsize},{@code fx}, and {@code fy}. If you want to resize src so that it fits the pre-created dst,
+     * you may call the function as follows:
+     * <code>
+     *     // explicitly specify dsize=dst.size(); fx and fy will be computed from that.
+     *     resize(src, dst, dst.size(), 0, 0, interpolation);
+     * </code>
+     * If you want to decimate the image by factor of 2 in each direction, you can call the function this
+     * way:
+     * <code>
+     *     // specify fx and fy and let the function compute the destination image size.
+     *     resize(src, dst, Size(), 0.5, 0.5, interpolation);
+     * </code>
+     * To shrink an image, it will generally look best with #INTER_AREA interpolation, whereas to
+     * enlarge an image, it will generally look best with c#INTER_CUBIC (slow) or #INTER_LINEAR
+     * (faster but still looks OK).
+     *
+     * @param src input image.
+     * @param dst output image; it has the size dsize (when it is non-zero) or the size computed from
+     * src.size(), fx, and fy; the type of dst is the same as of src.
+     * @param dsize output image size; if it equals zero, it is computed as:
+     *  \(\texttt{dsize = Size(round(fx*src.cols), round(fy*src.rows))}\)
+     *  Either dsize or both fx and fy must be non-zero.
+     * @param fx scale factor along the horizontal axis; when it equals 0, it is computed as
+     * \(\texttt{(double)dsize.width/src.cols}\)
+     * @param fy scale factor along the vertical axis; when it equals 0, it is computed as
+     * \(\texttt{(double)dsize.height/src.rows}\)
+     *
+     * SEE:  warpAffine, warpPerspective, remap
+     */
+    public static void resize(Mat src, Mat dst, Size dsize, double fx, double fy) {
+        resize_1(src.nativeObj, dst.nativeObj, dsize.width, dsize.height, fx, fy);
+    }
+
+    /**
+     * Resizes an image.
+     *
+     * The function resize resizes the image src down to or up to the specified size. Note that the
+     * initial dst type or size are not taken into account. Instead, the size and type are derived from
+     * the {@code src},{@code dsize},{@code fx}, and {@code fy}. If you want to resize src so that it fits the pre-created dst,
+     * you may call the function as follows:
+     * <code>
+     *     // explicitly specify dsize=dst.size(); fx and fy will be computed from that.
+     *     resize(src, dst, dst.size(), 0, 0, interpolation);
+     * </code>
+     * If you want to decimate the image by factor of 2 in each direction, you can call the function this
+     * way:
+     * <code>
+     *     // specify fx and fy and let the function compute the destination image size.
+     *     resize(src, dst, Size(), 0.5, 0.5, interpolation);
+     * </code>
+     * To shrink an image, it will generally look best with #INTER_AREA interpolation, whereas to
+     * enlarge an image, it will generally look best with c#INTER_CUBIC (slow) or #INTER_LINEAR
+     * (faster but still looks OK).
+     *
+     * @param src input image.
+     * @param dst output image; it has the size dsize (when it is non-zero) or the size computed from
+     * src.size(), fx, and fy; the type of dst is the same as of src.
+     * @param dsize output image size; if it equals zero, it is computed as:
+     *  \(\texttt{dsize = Size(round(fx*src.cols), round(fy*src.rows))}\)
+     *  Either dsize or both fx and fy must be non-zero.
+     * @param fx scale factor along the horizontal axis; when it equals 0, it is computed as
+     * \(\texttt{(double)dsize.width/src.cols}\)
+     * \(\texttt{(double)dsize.height/src.rows}\)
+     *
+     * SEE:  warpAffine, warpPerspective, remap
+     */
+    public static void resize(Mat src, Mat dst, Size dsize, double fx) {
+        resize_2(src.nativeObj, dst.nativeObj, dsize.width, dsize.height, fx);
+    }
+
+    /**
+     * Resizes an image.
+     *
+     * The function resize resizes the image src down to or up to the specified size. Note that the
+     * initial dst type or size are not taken into account. Instead, the size and type are derived from
+     * the {@code src},{@code dsize},{@code fx}, and {@code fy}. If you want to resize src so that it fits the pre-created dst,
+     * you may call the function as follows:
+     * <code>
+     *     // explicitly specify dsize=dst.size(); fx and fy will be computed from that.
+     *     resize(src, dst, dst.size(), 0, 0, interpolation);
+     * </code>
+     * If you want to decimate the image by factor of 2 in each direction, you can call the function this
+     * way:
+     * <code>
+     *     // specify fx and fy and let the function compute the destination image size.
+     *     resize(src, dst, Size(), 0.5, 0.5, interpolation);
+     * </code>
+     * To shrink an image, it will generally look best with #INTER_AREA interpolation, whereas to
+     * enlarge an image, it will generally look best with c#INTER_CUBIC (slow) or #INTER_LINEAR
+     * (faster but still looks OK).
+     *
+     * @param src input image.
+     * @param dst output image; it has the size dsize (when it is non-zero) or the size computed from
+     * src.size(), fx, and fy; the type of dst is the same as of src.
+     * @param dsize output image size; if it equals zero, it is computed as:
+     *  \(\texttt{dsize = Size(round(fx*src.cols), round(fy*src.rows))}\)
+     *  Either dsize or both fx and fy must be non-zero.
+     * \(\texttt{(double)dsize.width/src.cols}\)
+     * \(\texttt{(double)dsize.height/src.rows}\)
+     *
+     * SEE:  warpAffine, warpPerspective, remap
+     */
+    public static void resize(Mat src, Mat dst, Size dsize) {
+        resize_3(src.nativeObj, dst.nativeObj, dsize.width, dsize.height);
+    }
+
+
+    //
+    // C++:  void cv::sepFilter2D(Mat src, Mat& dst, int ddepth, Mat kernelX, Mat kernelY, Point anchor = Point(-1,-1), double delta = 0, int borderType = BORDER_DEFAULT)
+    //
+
+    /**
+     * Applies a separable linear filter to an image.
+     *
+     * The function applies a separable linear filter to the image. That is, first, every row of src is
+     * filtered with the 1D kernel kernelX. Then, every column of the result is filtered with the 1D
+     * kernel kernelY. The final result shifted by delta is stored in dst .
+     *
+     * @param src Source image.
+     * @param dst Destination image of the same size and the same number of channels as src .
+     * @param ddepth Destination image depth, see REF: filter_depths "combinations"
+     * @param kernelX Coefficients for filtering each row.
+     * @param kernelY Coefficients for filtering each column.
+     * @param anchor Anchor position within the kernel. The default value \((-1,-1)\) means that the anchor
+     * is at the kernel center.
+     * @param delta Value added to the filtered results before storing them.
+     * @param borderType Pixel extrapolation method, see #BorderTypes. #BORDER_WRAP is not supported.
+     * SEE:  filter2D, Sobel, GaussianBlur, boxFilter, blur
+     */
+    public static void sepFilter2D(Mat src, Mat dst, int ddepth, Mat kernelX, Mat kernelY, Point anchor, double delta, int borderType) {
+        sepFilter2D_0(src.nativeObj, dst.nativeObj, ddepth, kernelX.nativeObj, kernelY.nativeObj, anchor.x, anchor.y, delta, borderType);
+    }
+
+    /**
+     * Applies a separable linear filter to an image.
+     *
+     * The function applies a separable linear filter to the image. That is, first, every row of src is
+     * filtered with the 1D kernel kernelX. Then, every column of the result is filtered with the 1D
+     * kernel kernelY. The final result shifted by delta is stored in dst .
+     *
+     * @param src Source image.
+     * @param dst Destination image of the same size and the same number of channels as src .
+     * @param ddepth Destination image depth, see REF: filter_depths "combinations"
+     * @param kernelX Coefficients for filtering each row.
+     * @param kernelY Coefficients for filtering each column.
+     * @param anchor Anchor position within the kernel. The default value \((-1,-1)\) means that the anchor
+     * is at the kernel center.
+     * @param delta Value added to the filtered results before storing them.
+     * SEE:  filter2D, Sobel, GaussianBlur, boxFilter, blur
+     */
+    public static void sepFilter2D(Mat src, Mat dst, int ddepth, Mat kernelX, Mat kernelY, Point anchor, double delta) {
+        sepFilter2D_1(src.nativeObj, dst.nativeObj, ddepth, kernelX.nativeObj, kernelY.nativeObj, anchor.x, anchor.y, delta);
+    }
+
+    /**
+     * Applies a separable linear filter to an image.
+     *
+     * The function applies a separable linear filter to the image. That is, first, every row of src is
+     * filtered with the 1D kernel kernelX. Then, every column of the result is filtered with the 1D
+     * kernel kernelY. The final result shifted by delta is stored in dst .
+     *
+     * @param src Source image.
+     * @param dst Destination image of the same size and the same number of channels as src .
+     * @param ddepth Destination image depth, see REF: filter_depths "combinations"
+     * @param kernelX Coefficients for filtering each row.
+     * @param kernelY Coefficients for filtering each column.
+     * @param anchor Anchor position within the kernel. The default value \((-1,-1)\) means that the anchor
+     * is at the kernel center.
+     * SEE:  filter2D, Sobel, GaussianBlur, boxFilter, blur
+     */
+    public static void sepFilter2D(Mat src, Mat dst, int ddepth, Mat kernelX, Mat kernelY, Point anchor) {
+        sepFilter2D_2(src.nativeObj, dst.nativeObj, ddepth, kernelX.nativeObj, kernelY.nativeObj, anchor.x, anchor.y);
+    }
+
+    /**
+     * Applies a separable linear filter to an image.
+     *
+     * The function applies a separable linear filter to the image. That is, first, every row of src is
+     * filtered with the 1D kernel kernelX. Then, every column of the result is filtered with the 1D
+     * kernel kernelY. The final result shifted by delta is stored in dst .
+     *
+     * @param src Source image.
+     * @param dst Destination image of the same size and the same number of channels as src .
+     * @param ddepth Destination image depth, see REF: filter_depths "combinations"
+     * @param kernelX Coefficients for filtering each row.
+     * @param kernelY Coefficients for filtering each column.
+     * is at the kernel center.
+     * SEE:  filter2D, Sobel, GaussianBlur, boxFilter, blur
+     */
+    public static void sepFilter2D(Mat src, Mat dst, int ddepth, Mat kernelX, Mat kernelY) {
+        sepFilter2D_3(src.nativeObj, dst.nativeObj, ddepth, kernelX.nativeObj, kernelY.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::spatialGradient(Mat src, Mat& dx, Mat& dy, int ksize = 3, int borderType = BORDER_DEFAULT)
+    //
+
+    /**
+     * Calculates the first order image derivative in both x and y using a Sobel operator
+     *
+     * Equivalent to calling:
+     *
+     * <code>
+     * Sobel( src, dx, CV_16SC1, 1, 0, 3 );
+     * Sobel( src, dy, CV_16SC1, 0, 1, 3 );
+     * </code>
+     *
+     * @param src input image.
+     * @param dx output image with first-order derivative in x.
+     * @param dy output image with first-order derivative in y.
+     * @param ksize size of Sobel kernel. It must be 3.
+     * @param borderType pixel extrapolation method, see #BorderTypes.
+     *                   Only #BORDER_DEFAULT=#BORDER_REFLECT_101 and #BORDER_REPLICATE are supported.
+     *
+     * SEE: Sobel
+     */
+    public static void spatialGradient(Mat src, Mat dx, Mat dy, int ksize, int borderType) {
+        spatialGradient_0(src.nativeObj, dx.nativeObj, dy.nativeObj, ksize, borderType);
+    }
+
+    /**
+     * Calculates the first order image derivative in both x and y using a Sobel operator
+     *
+     * Equivalent to calling:
+     *
+     * <code>
+     * Sobel( src, dx, CV_16SC1, 1, 0, 3 );
+     * Sobel( src, dy, CV_16SC1, 0, 1, 3 );
+     * </code>
+     *
+     * @param src input image.
+     * @param dx output image with first-order derivative in x.
+     * @param dy output image with first-order derivative in y.
+     * @param ksize size of Sobel kernel. It must be 3.
+     *                   Only #BORDER_DEFAULT=#BORDER_REFLECT_101 and #BORDER_REPLICATE are supported.
+     *
+     * SEE: Sobel
+     */
+    public static void spatialGradient(Mat src, Mat dx, Mat dy, int ksize) {
+        spatialGradient_1(src.nativeObj, dx.nativeObj, dy.nativeObj, ksize);
+    }
+
+    /**
+     * Calculates the first order image derivative in both x and y using a Sobel operator
+     *
+     * Equivalent to calling:
+     *
+     * <code>
+     * Sobel( src, dx, CV_16SC1, 1, 0, 3 );
+     * Sobel( src, dy, CV_16SC1, 0, 1, 3 );
+     * </code>
+     *
+     * @param src input image.
+     * @param dx output image with first-order derivative in x.
+     * @param dy output image with first-order derivative in y.
+     *                   Only #BORDER_DEFAULT=#BORDER_REFLECT_101 and #BORDER_REPLICATE are supported.
+     *
+     * SEE: Sobel
+     */
+    public static void spatialGradient(Mat src, Mat dx, Mat dy) {
+        spatialGradient_2(src.nativeObj, dx.nativeObj, dy.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::sqrBoxFilter(Mat src, Mat& dst, int ddepth, Size ksize, Point anchor = Point(-1, -1), bool normalize = true, int borderType = BORDER_DEFAULT)
+    //
+
+    /**
+     * Calculates the normalized sum of squares of the pixel values overlapping the filter.
+     *
+     * For every pixel \( (x, y) \) in the source image, the function calculates the sum of squares of those neighboring
+     * pixel values which overlap the filter placed over the pixel \( (x, y) \).
+     *
+     * The unnormalized square box filter can be useful in computing local image statistics such as the the local
+     * variance and standard deviation around the neighborhood of a pixel.
+     *
+     * @param src input image
+     * @param dst output image of the same size and type as _src
+     * @param ddepth the output image depth (-1 to use src.depth())
+     * @param ksize kernel size
+     * @param anchor kernel anchor point. The default value of Point(-1, -1) denotes that the anchor is at the kernel
+     * center.
+     * @param normalize flag, specifying whether the kernel is to be normalized by it's area or not.
+     * @param borderType border mode used to extrapolate pixels outside of the image, see #BorderTypes. #BORDER_WRAP is not supported.
+     * SEE: boxFilter
+     */
+    public static void sqrBoxFilter(Mat src, Mat dst, int ddepth, Size ksize, Point anchor, boolean normalize, int borderType) {
+        sqrBoxFilter_0(src.nativeObj, dst.nativeObj, ddepth, ksize.width, ksize.height, anchor.x, anchor.y, normalize, borderType);
+    }
+
+    /**
+     * Calculates the normalized sum of squares of the pixel values overlapping the filter.
+     *
+     * For every pixel \( (x, y) \) in the source image, the function calculates the sum of squares of those neighboring
+     * pixel values which overlap the filter placed over the pixel \( (x, y) \).
+     *
+     * The unnormalized square box filter can be useful in computing local image statistics such as the the local
+     * variance and standard deviation around the neighborhood of a pixel.
+     *
+     * @param src input image
+     * @param dst output image of the same size and type as _src
+     * @param ddepth the output image depth (-1 to use src.depth())
+     * @param ksize kernel size
+     * @param anchor kernel anchor point. The default value of Point(-1, -1) denotes that the anchor is at the kernel
+     * center.
+     * @param normalize flag, specifying whether the kernel is to be normalized by it's area or not.
+     * SEE: boxFilter
+     */
+    public static void sqrBoxFilter(Mat src, Mat dst, int ddepth, Size ksize, Point anchor, boolean normalize) {
+        sqrBoxFilter_1(src.nativeObj, dst.nativeObj, ddepth, ksize.width, ksize.height, anchor.x, anchor.y, normalize);
+    }
+
+    /**
+     * Calculates the normalized sum of squares of the pixel values overlapping the filter.
+     *
+     * For every pixel \( (x, y) \) in the source image, the function calculates the sum of squares of those neighboring
+     * pixel values which overlap the filter placed over the pixel \( (x, y) \).
+     *
+     * The unnormalized square box filter can be useful in computing local image statistics such as the the local
+     * variance and standard deviation around the neighborhood of a pixel.
+     *
+     * @param src input image
+     * @param dst output image of the same size and type as _src
+     * @param ddepth the output image depth (-1 to use src.depth())
+     * @param ksize kernel size
+     * @param anchor kernel anchor point. The default value of Point(-1, -1) denotes that the anchor is at the kernel
+     * center.
+     * SEE: boxFilter
+     */
+    public static void sqrBoxFilter(Mat src, Mat dst, int ddepth, Size ksize, Point anchor) {
+        sqrBoxFilter_2(src.nativeObj, dst.nativeObj, ddepth, ksize.width, ksize.height, anchor.x, anchor.y);
+    }
+
+    /**
+     * Calculates the normalized sum of squares of the pixel values overlapping the filter.
+     *
+     * For every pixel \( (x, y) \) in the source image, the function calculates the sum of squares of those neighboring
+     * pixel values which overlap the filter placed over the pixel \( (x, y) \).
+     *
+     * The unnormalized square box filter can be useful in computing local image statistics such as the the local
+     * variance and standard deviation around the neighborhood of a pixel.
+     *
+     * @param src input image
+     * @param dst output image of the same size and type as _src
+     * @param ddepth the output image depth (-1 to use src.depth())
+     * @param ksize kernel size
+     * center.
+     * SEE: boxFilter
+     */
+    public static void sqrBoxFilter(Mat src, Mat dst, int ddepth, Size ksize) {
+        sqrBoxFilter_3(src.nativeObj, dst.nativeObj, ddepth, ksize.width, ksize.height);
+    }
+
+
+    //
+    // C++:  void cv::undistort(Mat src, Mat& dst, Mat cameraMatrix, Mat distCoeffs, Mat newCameraMatrix = Mat())
+    //
+
+    /**
+     * Transforms an image to compensate for lens distortion.
+     *
+     * The function transforms an image to compensate radial and tangential lens distortion.
+     *
+     * The function is simply a combination of #initUndistortRectifyMap (with unity R ) and #remap
+     * (with bilinear interpolation). See the former function for details of the transformation being
+     * performed.
+     *
+     * Those pixels in the destination image, for which there is no correspondent pixels in the source
+     * image, are filled with zeros (black color).
+     *
+     * A particular subset of the source image that will be visible in the corrected image can be regulated
+     * by newCameraMatrix. You can use #getOptimalNewCameraMatrix to compute the appropriate
+     * newCameraMatrix depending on your requirements.
+     *
+     * The camera matrix and the distortion parameters can be determined using #calibrateCamera. If
+     * the resolution of images is different from the resolution used at the calibration stage, \(f_x,
+     * f_y, c_x\) and \(c_y\) need to be scaled accordingly, while the distortion coefficients remain
+     * the same.
+     *
+     * @param src Input (distorted) image.
+     * @param dst Output (corrected) image that has the same size and type as src .
+     * @param cameraMatrix Input camera matrix \(A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}\) .
+     * @param distCoeffs Input vector of distortion coefficients
+     * \((k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6[, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\)
+     * of 4, 5, 8, 12 or 14 elements. If the vector is NULL/empty, the zero distortion coefficients are assumed.
+     * @param newCameraMatrix Camera matrix of the distorted image. By default, it is the same as
+     * cameraMatrix but you may additionally scale and shift the result by using a different matrix.
+     */
+    public static void undistort(Mat src, Mat dst, Mat cameraMatrix, Mat distCoeffs, Mat newCameraMatrix) {
+        undistort_0(src.nativeObj, dst.nativeObj, cameraMatrix.nativeObj, distCoeffs.nativeObj, newCameraMatrix.nativeObj);
+    }
+
+    /**
+     * Transforms an image to compensate for lens distortion.
+     *
+     * The function transforms an image to compensate radial and tangential lens distortion.
+     *
+     * The function is simply a combination of #initUndistortRectifyMap (with unity R ) and #remap
+     * (with bilinear interpolation). See the former function for details of the transformation being
+     * performed.
+     *
+     * Those pixels in the destination image, for which there is no correspondent pixels in the source
+     * image, are filled with zeros (black color).
+     *
+     * A particular subset of the source image that will be visible in the corrected image can be regulated
+     * by newCameraMatrix. You can use #getOptimalNewCameraMatrix to compute the appropriate
+     * newCameraMatrix depending on your requirements.
+     *
+     * The camera matrix and the distortion parameters can be determined using #calibrateCamera. If
+     * the resolution of images is different from the resolution used at the calibration stage, \(f_x,
+     * f_y, c_x\) and \(c_y\) need to be scaled accordingly, while the distortion coefficients remain
+     * the same.
+     *
+     * @param src Input (distorted) image.
+     * @param dst Output (corrected) image that has the same size and type as src .
+     * @param cameraMatrix Input camera matrix \(A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}\) .
+     * @param distCoeffs Input vector of distortion coefficients
+     * \((k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6[, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\)
+     * of 4, 5, 8, 12 or 14 elements. If the vector is NULL/empty, the zero distortion coefficients are assumed.
+     * cameraMatrix but you may additionally scale and shift the result by using a different matrix.
+     */
+    public static void undistort(Mat src, Mat dst, Mat cameraMatrix, Mat distCoeffs) {
+        undistort_1(src.nativeObj, dst.nativeObj, cameraMatrix.nativeObj, distCoeffs.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::undistortPoints(Mat src, Mat& dst, Mat cameraMatrix, Mat distCoeffs, Mat R, Mat P, TermCriteria criteria)
+    //
+
+    /**
+     *
+     *     <b>Note:</b> Default version of #undistortPoints does 5 iterations to compute undistorted points.
+     * @param src automatically generated
+     * @param dst automatically generated
+     * @param cameraMatrix automatically generated
+     * @param distCoeffs automatically generated
+     * @param R automatically generated
+     * @param P automatically generated
+     * @param criteria automatically generated
+     */
+    public static void undistortPointsIter(Mat src, Mat dst, Mat cameraMatrix, Mat distCoeffs, Mat R, Mat P, TermCriteria criteria) {
+        undistortPointsIter_0(src.nativeObj, dst.nativeObj, cameraMatrix.nativeObj, distCoeffs.nativeObj, R.nativeObj, P.nativeObj, criteria.type, criteria.maxCount, criteria.epsilon);
+    }
+
+
+    //
+    // C++:  void cv::undistortPoints(Mat src, Mat& dst, Mat cameraMatrix, Mat distCoeffs, Mat R = Mat(), Mat P = Mat())
+    //
+
+    /**
+     * Computes the ideal point coordinates from the observed point coordinates.
+     *
+     * The function is similar to #undistort and #initUndistortRectifyMap but it operates on a
+     * sparse set of points instead of a raster image. Also the function performs a reverse transformation
+     * to projectPoints. In case of a 3D object, it does not reconstruct its 3D coordinates, but for a
+     * planar object, it does, up to a translation vector, if the proper R is specified.
+     *
+     * For each observed point coordinate \((u, v)\) the function computes:
+     * \(
+     * \begin{array}{l}
+     * x^{"}  \leftarrow (u - c_x)/f_x  \\
+     * y^{"}  \leftarrow (v - c_y)/f_y  \\
+     * (x',y') = undistort(x^{"},y^{"}, \texttt{distCoeffs}) \\
+     * {[X\,Y\,W]} ^T  \leftarrow R*[x' \, y' \, 1]^T  \\
+     * x  \leftarrow X/W  \\
+     * y  \leftarrow Y/W  \\
+     * \text{only performed if P is specified:} \\
+     * u'  \leftarrow x {f'}_x + {c'}_x  \\
+     * v'  \leftarrow y {f'}_y + {c'}_y
+     * \end{array}
+     * \)
+     *
+     * where *undistort* is an approximate iterative algorithm that estimates the normalized original
+     * point coordinates out of the normalized distorted point coordinates ("normalized" means that the
+     * coordinates do not depend on the camera matrix).
+     *
+     * The function can be used for both a stereo camera head or a monocular camera (when R is empty).
+     * @param src Observed point coordinates, 2xN/Nx2 1-channel or 1xN/Nx1 2-channel (CV_32FC2 or CV_64FC2) (or
+     * vector&lt;Point2f&gt; ).
+     * @param dst Output ideal point coordinates (1xN/Nx1 2-channel or vector&lt;Point2f&gt; ) after undistortion and reverse perspective
+     * transformation. If matrix P is identity or omitted, dst will contain normalized point coordinates.
+     * @param cameraMatrix Camera matrix \(\vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}\) .
+     * @param distCoeffs Input vector of distortion coefficients
+     * \((k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6[, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\)
+     * of 4, 5, 8, 12 or 14 elements. If the vector is NULL/empty, the zero distortion coefficients are assumed.
+     * @param R Rectification transformation in the object space (3x3 matrix). R1 or R2 computed by
+     * #stereoRectify can be passed here. If the matrix is empty, the identity transformation is used.
+     * @param P New camera matrix (3x3) or new projection matrix (3x4) \(\begin{bmatrix} {f'}_x &amp; 0 &amp; {c'}_x &amp; t_x \\ 0 &amp; {f'}_y &amp; {c'}_y &amp; t_y \\ 0 &amp; 0 &amp; 1 &amp; t_z \end{bmatrix}\). P1 or P2 computed by
+     * #stereoRectify can be passed here. If the matrix is empty, the identity new camera matrix is used.
+     */
+    public static void undistortPoints(Mat src, Mat dst, Mat cameraMatrix, Mat distCoeffs, Mat R, Mat P) {
+        undistortPoints_0(src.nativeObj, dst.nativeObj, cameraMatrix.nativeObj, distCoeffs.nativeObj, R.nativeObj, P.nativeObj);
+    }
+
+    /**
+     * Computes the ideal point coordinates from the observed point coordinates.
+     *
+     * The function is similar to #undistort and #initUndistortRectifyMap but it operates on a
+     * sparse set of points instead of a raster image. Also the function performs a reverse transformation
+     * to projectPoints. In case of a 3D object, it does not reconstruct its 3D coordinates, but for a
+     * planar object, it does, up to a translation vector, if the proper R is specified.
+     *
+     * For each observed point coordinate \((u, v)\) the function computes:
+     * \(
+     * \begin{array}{l}
+     * x^{"}  \leftarrow (u - c_x)/f_x  \\
+     * y^{"}  \leftarrow (v - c_y)/f_y  \\
+     * (x',y') = undistort(x^{"},y^{"}, \texttt{distCoeffs}) \\
+     * {[X\,Y\,W]} ^T  \leftarrow R*[x' \, y' \, 1]^T  \\
+     * x  \leftarrow X/W  \\
+     * y  \leftarrow Y/W  \\
+     * \text{only performed if P is specified:} \\
+     * u'  \leftarrow x {f'}_x + {c'}_x  \\
+     * v'  \leftarrow y {f'}_y + {c'}_y
+     * \end{array}
+     * \)
+     *
+     * where *undistort* is an approximate iterative algorithm that estimates the normalized original
+     * point coordinates out of the normalized distorted point coordinates ("normalized" means that the
+     * coordinates do not depend on the camera matrix).
+     *
+     * The function can be used for both a stereo camera head or a monocular camera (when R is empty).
+     * @param src Observed point coordinates, 2xN/Nx2 1-channel or 1xN/Nx1 2-channel (CV_32FC2 or CV_64FC2) (or
+     * vector&lt;Point2f&gt; ).
+     * @param dst Output ideal point coordinates (1xN/Nx1 2-channel or vector&lt;Point2f&gt; ) after undistortion and reverse perspective
+     * transformation. If matrix P is identity or omitted, dst will contain normalized point coordinates.
+     * @param cameraMatrix Camera matrix \(\vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}\) .
+     * @param distCoeffs Input vector of distortion coefficients
+     * \((k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6[, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\)
+     * of 4, 5, 8, 12 or 14 elements. If the vector is NULL/empty, the zero distortion coefficients are assumed.
+     * @param R Rectification transformation in the object space (3x3 matrix). R1 or R2 computed by
+     * #stereoRectify can be passed here. If the matrix is empty, the identity transformation is used.
+     * #stereoRectify can be passed here. If the matrix is empty, the identity new camera matrix is used.
+     */
+    public static void undistortPoints(Mat src, Mat dst, Mat cameraMatrix, Mat distCoeffs, Mat R) {
+        undistortPoints_1(src.nativeObj, dst.nativeObj, cameraMatrix.nativeObj, distCoeffs.nativeObj, R.nativeObj);
+    }
+
+    /**
+     * Computes the ideal point coordinates from the observed point coordinates.
+     *
+     * The function is similar to #undistort and #initUndistortRectifyMap but it operates on a
+     * sparse set of points instead of a raster image. Also the function performs a reverse transformation
+     * to projectPoints. In case of a 3D object, it does not reconstruct its 3D coordinates, but for a
+     * planar object, it does, up to a translation vector, if the proper R is specified.
+     *
+     * For each observed point coordinate \((u, v)\) the function computes:
+     * \(
+     * \begin{array}{l}
+     * x^{"}  \leftarrow (u - c_x)/f_x  \\
+     * y^{"}  \leftarrow (v - c_y)/f_y  \\
+     * (x',y') = undistort(x^{"},y^{"}, \texttt{distCoeffs}) \\
+     * {[X\,Y\,W]} ^T  \leftarrow R*[x' \, y' \, 1]^T  \\
+     * x  \leftarrow X/W  \\
+     * y  \leftarrow Y/W  \\
+     * \text{only performed if P is specified:} \\
+     * u'  \leftarrow x {f'}_x + {c'}_x  \\
+     * v'  \leftarrow y {f'}_y + {c'}_y
+     * \end{array}
+     * \)
+     *
+     * where *undistort* is an approximate iterative algorithm that estimates the normalized original
+     * point coordinates out of the normalized distorted point coordinates ("normalized" means that the
+     * coordinates do not depend on the camera matrix).
+     *
+     * The function can be used for both a stereo camera head or a monocular camera (when R is empty).
+     * @param src Observed point coordinates, 2xN/Nx2 1-channel or 1xN/Nx1 2-channel (CV_32FC2 or CV_64FC2) (or
+     * vector&lt;Point2f&gt; ).
+     * @param dst Output ideal point coordinates (1xN/Nx1 2-channel or vector&lt;Point2f&gt; ) after undistortion and reverse perspective
+     * transformation. If matrix P is identity or omitted, dst will contain normalized point coordinates.
+     * @param cameraMatrix Camera matrix \(\vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}\) .
+     * @param distCoeffs Input vector of distortion coefficients
+     * \((k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6[, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\)
+     * of 4, 5, 8, 12 or 14 elements. If the vector is NULL/empty, the zero distortion coefficients are assumed.
+     * #stereoRectify can be passed here. If the matrix is empty, the identity transformation is used.
+     * #stereoRectify can be passed here. If the matrix is empty, the identity new camera matrix is used.
+     */
+    public static void undistortPoints(Mat src, Mat dst, Mat cameraMatrix, Mat distCoeffs) {
+        undistortPoints_2(src.nativeObj, dst.nativeObj, cameraMatrix.nativeObj, distCoeffs.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::warpAffine(Mat src, Mat& dst, Mat M, Size dsize, int flags = INTER_LINEAR, int borderMode = BORDER_CONSTANT, Scalar borderValue = Scalar())
+    //
+
+    /**
+     * Applies an affine transformation to an image.
+     *
+     * The function warpAffine transforms the source image using the specified matrix:
+     *
+     * \(\texttt{dst} (x,y) =  \texttt{src} ( \texttt{M} _{11} x +  \texttt{M} _{12} y +  \texttt{M} _{13}, \texttt{M} _{21} x +  \texttt{M} _{22} y +  \texttt{M} _{23})\)
+     *
+     * when the flag #WARP_INVERSE_MAP is set. Otherwise, the transformation is first inverted
+     * with #invertAffineTransform and then put in the formula above instead of M. The function cannot
+     * operate in-place.
+     *
+     * @param src input image.
+     * @param dst output image that has the size dsize and the same type as src .
+     * @param M \(2\times 3\) transformation matrix.
+     * @param dsize size of the output image.
+     * @param flags combination of interpolation methods (see #InterpolationFlags) and the optional
+     * flag #WARP_INVERSE_MAP that means that M is the inverse transformation (
+     * \(\texttt{dst}\rightarrow\texttt{src}\) ).
+     * @param borderMode pixel extrapolation method (see #BorderTypes); when
+     * borderMode=#BORDER_TRANSPARENT, it means that the pixels in the destination image corresponding to
+     * the "outliers" in the source image are not modified by the function.
+     * @param borderValue value used in case of a constant border; by default, it is 0.
+     *
+     * SEE:  warpPerspective, resize, remap, getRectSubPix, transform
+     */
+    public static void warpAffine(Mat src, Mat dst, Mat M, Size dsize, int flags, int borderMode, Scalar borderValue) {
+        warpAffine_0(src.nativeObj, dst.nativeObj, M.nativeObj, dsize.width, dsize.height, flags, borderMode, borderValue.val[0], borderValue.val[1], borderValue.val[2], borderValue.val[3]);
+    }
+
+    /**
+     * Applies an affine transformation to an image.
+     *
+     * The function warpAffine transforms the source image using the specified matrix:
+     *
+     * \(\texttt{dst} (x,y) =  \texttt{src} ( \texttt{M} _{11} x +  \texttt{M} _{12} y +  \texttt{M} _{13}, \texttt{M} _{21} x +  \texttt{M} _{22} y +  \texttt{M} _{23})\)
+     *
+     * when the flag #WARP_INVERSE_MAP is set. Otherwise, the transformation is first inverted
+     * with #invertAffineTransform and then put in the formula above instead of M. The function cannot
+     * operate in-place.
+     *
+     * @param src input image.
+     * @param dst output image that has the size dsize and the same type as src .
+     * @param M \(2\times 3\) transformation matrix.
+     * @param dsize size of the output image.
+     * @param flags combination of interpolation methods (see #InterpolationFlags) and the optional
+     * flag #WARP_INVERSE_MAP that means that M is the inverse transformation (
+     * \(\texttt{dst}\rightarrow\texttt{src}\) ).
+     * @param borderMode pixel extrapolation method (see #BorderTypes); when
+     * borderMode=#BORDER_TRANSPARENT, it means that the pixels in the destination image corresponding to
+     * the "outliers" in the source image are not modified by the function.
+     *
+     * SEE:  warpPerspective, resize, remap, getRectSubPix, transform
+     */
+    public static void warpAffine(Mat src, Mat dst, Mat M, Size dsize, int flags, int borderMode) {
+        warpAffine_1(src.nativeObj, dst.nativeObj, M.nativeObj, dsize.width, dsize.height, flags, borderMode);
+    }
+
+    /**
+     * Applies an affine transformation to an image.
+     *
+     * The function warpAffine transforms the source image using the specified matrix:
+     *
+     * \(\texttt{dst} (x,y) =  \texttt{src} ( \texttt{M} _{11} x +  \texttt{M} _{12} y +  \texttt{M} _{13}, \texttt{M} _{21} x +  \texttt{M} _{22} y +  \texttt{M} _{23})\)
+     *
+     * when the flag #WARP_INVERSE_MAP is set. Otherwise, the transformation is first inverted
+     * with #invertAffineTransform and then put in the formula above instead of M. The function cannot
+     * operate in-place.
+     *
+     * @param src input image.
+     * @param dst output image that has the size dsize and the same type as src .
+     * @param M \(2\times 3\) transformation matrix.
+     * @param dsize size of the output image.
+     * @param flags combination of interpolation methods (see #InterpolationFlags) and the optional
+     * flag #WARP_INVERSE_MAP that means that M is the inverse transformation (
+     * \(\texttt{dst}\rightarrow\texttt{src}\) ).
+     * borderMode=#BORDER_TRANSPARENT, it means that the pixels in the destination image corresponding to
+     * the "outliers" in the source image are not modified by the function.
+     *
+     * SEE:  warpPerspective, resize, remap, getRectSubPix, transform
+     */
+    public static void warpAffine(Mat src, Mat dst, Mat M, Size dsize, int flags) {
+        warpAffine_2(src.nativeObj, dst.nativeObj, M.nativeObj, dsize.width, dsize.height, flags);
+    }
+
+    /**
+     * Applies an affine transformation to an image.
+     *
+     * The function warpAffine transforms the source image using the specified matrix:
+     *
+     * \(\texttt{dst} (x,y) =  \texttt{src} ( \texttt{M} _{11} x +  \texttt{M} _{12} y +  \texttt{M} _{13}, \texttt{M} _{21} x +  \texttt{M} _{22} y +  \texttt{M} _{23})\)
+     *
+     * when the flag #WARP_INVERSE_MAP is set. Otherwise, the transformation is first inverted
+     * with #invertAffineTransform and then put in the formula above instead of M. The function cannot
+     * operate in-place.
+     *
+     * @param src input image.
+     * @param dst output image that has the size dsize and the same type as src .
+     * @param M \(2\times 3\) transformation matrix.
+     * @param dsize size of the output image.
+     * flag #WARP_INVERSE_MAP that means that M is the inverse transformation (
+     * \(\texttt{dst}\rightarrow\texttt{src}\) ).
+     * borderMode=#BORDER_TRANSPARENT, it means that the pixels in the destination image corresponding to
+     * the "outliers" in the source image are not modified by the function.
+     *
+     * SEE:  warpPerspective, resize, remap, getRectSubPix, transform
+     */
+    public static void warpAffine(Mat src, Mat dst, Mat M, Size dsize) {
+        warpAffine_3(src.nativeObj, dst.nativeObj, M.nativeObj, dsize.width, dsize.height);
+    }
+
+
+    //
+    // C++:  void cv::warpPerspective(Mat src, Mat& dst, Mat M, Size dsize, int flags = INTER_LINEAR, int borderMode = BORDER_CONSTANT, Scalar borderValue = Scalar())
+    //
+
+    /**
+     * Applies a perspective transformation to an image.
+     *
+     * The function warpPerspective transforms the source image using the specified matrix:
+     *
+     * \(\texttt{dst} (x,y) =  \texttt{src} \left ( \frac{M_{11} x + M_{12} y + M_{13}}{M_{31} x + M_{32} y + M_{33}} ,
+     *      \frac{M_{21} x + M_{22} y + M_{23}}{M_{31} x + M_{32} y + M_{33}} \right )\)
+     *
+     * when the flag #WARP_INVERSE_MAP is set. Otherwise, the transformation is first inverted with invert
+     * and then put in the formula above instead of M. The function cannot operate in-place.
+     *
+     * @param src input image.
+     * @param dst output image that has the size dsize and the same type as src .
+     * @param M \(3\times 3\) transformation matrix.
+     * @param dsize size of the output image.
+     * @param flags combination of interpolation methods (#INTER_LINEAR or #INTER_NEAREST) and the
+     * optional flag #WARP_INVERSE_MAP, that sets M as the inverse transformation (
+     * \(\texttt{dst}\rightarrow\texttt{src}\) ).
+     * @param borderMode pixel extrapolation method (#BORDER_CONSTANT or #BORDER_REPLICATE).
+     * @param borderValue value used in case of a constant border; by default, it equals 0.
+     *
+     * SEE:  warpAffine, resize, remap, getRectSubPix, perspectiveTransform
+     */
+    public static void warpPerspective(Mat src, Mat dst, Mat M, Size dsize, int flags, int borderMode, Scalar borderValue) {
+        warpPerspective_0(src.nativeObj, dst.nativeObj, M.nativeObj, dsize.width, dsize.height, flags, borderMode, borderValue.val[0], borderValue.val[1], borderValue.val[2], borderValue.val[3]);
+    }
+
+    /**
+     * Applies a perspective transformation to an image.
+     *
+     * The function warpPerspective transforms the source image using the specified matrix:
+     *
+     * \(\texttt{dst} (x,y) =  \texttt{src} \left ( \frac{M_{11} x + M_{12} y + M_{13}}{M_{31} x + M_{32} y + M_{33}} ,
+     *      \frac{M_{21} x + M_{22} y + M_{23}}{M_{31} x + M_{32} y + M_{33}} \right )\)
+     *
+     * when the flag #WARP_INVERSE_MAP is set. Otherwise, the transformation is first inverted with invert
+     * and then put in the formula above instead of M. The function cannot operate in-place.
+     *
+     * @param src input image.
+     * @param dst output image that has the size dsize and the same type as src .
+     * @param M \(3\times 3\) transformation matrix.
+     * @param dsize size of the output image.
+     * @param flags combination of interpolation methods (#INTER_LINEAR or #INTER_NEAREST) and the
+     * optional flag #WARP_INVERSE_MAP, that sets M as the inverse transformation (
+     * \(\texttt{dst}\rightarrow\texttt{src}\) ).
+     * @param borderMode pixel extrapolation method (#BORDER_CONSTANT or #BORDER_REPLICATE).
+     *
+     * SEE:  warpAffine, resize, remap, getRectSubPix, perspectiveTransform
+     */
+    public static void warpPerspective(Mat src, Mat dst, Mat M, Size dsize, int flags, int borderMode) {
+        warpPerspective_1(src.nativeObj, dst.nativeObj, M.nativeObj, dsize.width, dsize.height, flags, borderMode);
+    }
+
+    /**
+     * Applies a perspective transformation to an image.
+     *
+     * The function warpPerspective transforms the source image using the specified matrix:
+     *
+     * \(\texttt{dst} (x,y) =  \texttt{src} \left ( \frac{M_{11} x + M_{12} y + M_{13}}{M_{31} x + M_{32} y + M_{33}} ,
+     *      \frac{M_{21} x + M_{22} y + M_{23}}{M_{31} x + M_{32} y + M_{33}} \right )\)
+     *
+     * when the flag #WARP_INVERSE_MAP is set. Otherwise, the transformation is first inverted with invert
+     * and then put in the formula above instead of M. The function cannot operate in-place.
+     *
+     * @param src input image.
+     * @param dst output image that has the size dsize and the same type as src .
+     * @param M \(3\times 3\) transformation matrix.
+     * @param dsize size of the output image.
+     * @param flags combination of interpolation methods (#INTER_LINEAR or #INTER_NEAREST) and the
+     * optional flag #WARP_INVERSE_MAP, that sets M as the inverse transformation (
+     * \(\texttt{dst}\rightarrow\texttt{src}\) ).
+     *
+     * SEE:  warpAffine, resize, remap, getRectSubPix, perspectiveTransform
+     */
+    public static void warpPerspective(Mat src, Mat dst, Mat M, Size dsize, int flags) {
+        warpPerspective_2(src.nativeObj, dst.nativeObj, M.nativeObj, dsize.width, dsize.height, flags);
+    }
+
+    /**
+     * Applies a perspective transformation to an image.
+     *
+     * The function warpPerspective transforms the source image using the specified matrix:
+     *
+     * \(\texttt{dst} (x,y) =  \texttt{src} \left ( \frac{M_{11} x + M_{12} y + M_{13}}{M_{31} x + M_{32} y + M_{33}} ,
+     *      \frac{M_{21} x + M_{22} y + M_{23}}{M_{31} x + M_{32} y + M_{33}} \right )\)
+     *
+     * when the flag #WARP_INVERSE_MAP is set. Otherwise, the transformation is first inverted with invert
+     * and then put in the formula above instead of M. The function cannot operate in-place.
+     *
+     * @param src input image.
+     * @param dst output image that has the size dsize and the same type as src .
+     * @param M \(3\times 3\) transformation matrix.
+     * @param dsize size of the output image.
+     * optional flag #WARP_INVERSE_MAP, that sets M as the inverse transformation (
+     * \(\texttt{dst}\rightarrow\texttt{src}\) ).
+     *
+     * SEE:  warpAffine, resize, remap, getRectSubPix, perspectiveTransform
+     */
+    public static void warpPerspective(Mat src, Mat dst, Mat M, Size dsize) {
+        warpPerspective_3(src.nativeObj, dst.nativeObj, M.nativeObj, dsize.width, dsize.height);
+    }
+
+
+    //
+    // C++:  void cv::warpPolar(Mat src, Mat& dst, Size dsize, Point2f center, double maxRadius, int flags)
+    //
+
+    /**
+     * Remaps an image to polar or semilog-polar coordinates space
+     *
+     *  polar_remaps_reference_image
+     * ![Polar remaps reference](pics/polar_remap_doc.png)
+     *
+     * Transform the source image using the following transformation:
+     * \(
+     * dst(\rho , \phi ) = src(x,y)
+     * \)
+     *
+     * where
+     * \(
+     * \begin{array}{l}
+     * \vec{I} = (x - center.x, \;y - center.y) \\
+     * \phi = Kangle \cdot \texttt{angle} (\vec{I}) \\
+     * \rho = \left\{\begin{matrix}
+     * Klin \cdot \texttt{magnitude} (\vec{I}) &amp; default \\
+     * Klog \cdot log_e(\texttt{magnitude} (\vec{I})) &amp; if \; semilog \\
+     * \end{matrix}\right.
+     * \end{array}
+     * \)
+     *
+     * and
+     * \(
+     * \begin{array}{l}
+     * Kangle = dsize.height / 2\Pi \\
+     * Klin = dsize.width / maxRadius \\
+     * Klog = dsize.width / log_e(maxRadius) \\
+     * \end{array}
+     * \)
+     *
+     *
+     * \par Linear vs semilog mapping
+     *
+     * Polar mapping can be linear or semi-log. Add one of #WarpPolarMode to {@code flags} to specify the polar mapping mode.
+     *
+     * Linear is the default mode.
+     *
+     * The semilog mapping emulates the human "foveal" vision that permit very high acuity on the line of sight (central vision)
+     * in contrast to peripheral vision where acuity is minor.
+     *
+     * \par Option on {@code dsize}:
+     *
+     * <ul>
+     *   <li>
+     *  if both values in {@code dsize &lt;=0 } (default),
+     * the destination image will have (almost) same area of source bounding circle:
+     * \(\begin{array}{l}
+     * dsize.area  \leftarrow (maxRadius^2 \cdot \Pi) \\
+     * dsize.width = \texttt{cvRound}(maxRadius) \\
+     * dsize.height = \texttt{cvRound}(maxRadius \cdot \Pi) \\
+     * \end{array}\)
+     *   </li>
+     * </ul>
+     *
+     *
+     * <ul>
+     *   <li>
+     *  if only {@code dsize.height &lt;= 0},
+     * the destination image area will be proportional to the bounding circle area but scaled by {@code Kx * Kx}:
+     * \(\begin{array}{l}
+     * dsize.height = \texttt{cvRound}(dsize.width \cdot \Pi) \\
+     * \end{array}
+     * \)
+     *   </li>
+     * </ul>
+     *
+     * <ul>
+     *   <li>
+     *  if both values in {@code dsize &gt; 0 },
+     * the destination image will have the given size therefore the area of the bounding circle will be scaled to {@code dsize}.
+     *   </li>
+     * </ul>
+     *
+     *
+     * \par Reverse mapping
+     *
+     * You can get reverse mapping adding #WARP_INVERSE_MAP to {@code flags}
+     * \snippet polar_transforms.cpp InverseMap
+     *
+     * In addiction, to calculate the original coordinate from a polar mapped coordinate \((rho, phi)-&gt;(x, y)\):
+     * \snippet polar_transforms.cpp InverseCoordinate
+     *
+     * @param src Source image.
+     * @param dst Destination image. It will have same type as src.
+     * @param dsize The destination image size (see description for valid options).
+     * @param center The transformation center.
+     * @param maxRadius The radius of the bounding circle to transform. It determines the inverse magnitude scale parameter too.
+     * @param flags A combination of interpolation methods, #InterpolationFlags + #WarpPolarMode.
+     * <ul>
+     *   <li>
+     *              Add #WARP_POLAR_LINEAR to select linear polar mapping (default)
+     *   </li>
+     *   <li>
+     *              Add #WARP_POLAR_LOG to select semilog polar mapping
+     *   </li>
+     *   <li>
+     *              Add #WARP_INVERSE_MAP for reverse mapping.
+     *   </li>
+     * </ul>
+     * <b>Note:</b>
+     * <ul>
+     *   <li>
+     *   The function can not operate in-place.
+     *   </li>
+     *   <li>
+     *   To calculate magnitude and angle in degrees #cartToPolar is used internally thus angles are measured from 0 to 360 with accuracy about 0.3 degrees.
+     *   </li>
+     *   <li>
+     *   This function uses #remap. Due to current implementation limitations the size of an input and output images should be less than 32767x32767.
+     *   </li>
+     * </ul>
+     *
+     * SEE: cv::remap
+     */
+    public static void warpPolar(Mat src, Mat dst, Size dsize, Point center, double maxRadius, int flags) {
+        warpPolar_0(src.nativeObj, dst.nativeObj, dsize.width, dsize.height, center.x, center.y, maxRadius, flags);
+    }
+
+
+    //
+    // C++:  void cv::watershed(Mat image, Mat& markers)
+    //
+
+    /**
+     * Performs a marker-based image segmentation using the watershed algorithm.
+     *
+     * The function implements one of the variants of watershed, non-parametric marker-based segmentation
+     * algorithm, described in CITE: Meyer92 .
+     *
+     * Before passing the image to the function, you have to roughly outline the desired regions in the
+     * image markers with positive (&gt;0) indices. So, every region is represented as one or more connected
+     * components with the pixel values 1, 2, 3, and so on. Such markers can be retrieved from a binary
+     * mask using #findContours and #drawContours (see the watershed.cpp demo). The markers are "seeds" of
+     * the future image regions. All the other pixels in markers , whose relation to the outlined regions
+     * is not known and should be defined by the algorithm, should be set to 0's. In the function output,
+     * each pixel in markers is set to a value of the "seed" components or to -1 at boundaries between the
+     * regions.
+     *
+     * <b>Note:</b> Any two neighbor connected components are not necessarily separated by a watershed boundary
+     * (-1's pixels); for example, they can touch each other in the initial marker image passed to the
+     * function.
+     *
+     * @param image Input 8-bit 3-channel image.
+     * @param markers Input/output 32-bit single-channel image (map) of markers. It should have the same
+     * size as image .
+     *
+     * SEE: findContours
+     *
+     *  imgproc_misc
+     */
+    public static void watershed(Mat image, Mat markers) {
+        watershed_0(image.nativeObj, markers.nativeObj);
+    }
+
+
+
+// C++: Size getTextSize(const String& text, int fontFace, double fontScale, int thickness, int* baseLine);
+//javadoc:getTextSize(text, fontFace, fontScale, thickness, baseLine)
+public static Size getTextSize(String text, int fontFace, double fontScale, int thickness, int[] baseLine) {
+    if(baseLine != null && baseLine.length != 1)
+        throw new java.lang.IllegalArgumentException("'baseLine' must be 'int[1]' or 'null'.");
+    Size retVal = new Size(n_getTextSize(text, fontFace, fontScale, thickness, baseLine));
+    return retVal;
+}
+
+
+
+
+    // C++:  Mat cv::getAffineTransform(vector_Point2f src, vector_Point2f dst)
+    private static native long getAffineTransform_0(long src_mat_nativeObj, long dst_mat_nativeObj);
+
+    // C++:  Mat cv::getDefaultNewCameraMatrix(Mat cameraMatrix, Size imgsize = Size(), bool centerPrincipalPoint = false)
+    private static native long getDefaultNewCameraMatrix_0(long cameraMatrix_nativeObj, double imgsize_width, double imgsize_height, boolean centerPrincipalPoint);
+    private static native long getDefaultNewCameraMatrix_1(long cameraMatrix_nativeObj, double imgsize_width, double imgsize_height);
+    private static native long getDefaultNewCameraMatrix_2(long cameraMatrix_nativeObj);
+
+    // C++:  Mat cv::getGaborKernel(Size ksize, double sigma, double theta, double lambd, double gamma, double psi = CV_PI*0.5, int ktype = CV_64F)
+    private static native long getGaborKernel_0(double ksize_width, double ksize_height, double sigma, double theta, double lambd, double gamma, double psi, int ktype);
+    private static native long getGaborKernel_1(double ksize_width, double ksize_height, double sigma, double theta, double lambd, double gamma, double psi);
+    private static native long getGaborKernel_2(double ksize_width, double ksize_height, double sigma, double theta, double lambd, double gamma);
+
+    // C++:  Mat cv::getGaussianKernel(int ksize, double sigma, int ktype = CV_64F)
+    private static native long getGaussianKernel_0(int ksize, double sigma, int ktype);
+    private static native long getGaussianKernel_1(int ksize, double sigma);
+
+    // C++:  Mat cv::getPerspectiveTransform(Mat src, Mat dst)
+    private static native long getPerspectiveTransform_0(long src_nativeObj, long dst_nativeObj);
+
+    // C++:  Mat cv::getRotationMatrix2D(Point2f center, double angle, double scale)
+    private static native long getRotationMatrix2D_0(double center_x, double center_y, double angle, double scale);
+
+    // C++:  Mat cv::getStructuringElement(int shape, Size ksize, Point anchor = Point(-1,-1))
+    private static native long getStructuringElement_0(int shape, double ksize_width, double ksize_height, double anchor_x, double anchor_y);
+    private static native long getStructuringElement_1(int shape, double ksize_width, double ksize_height);
+
+    // C++:  Moments cv::moments(Mat array, bool binaryImage = false)
+    private static native double[] moments_0(long array_nativeObj, boolean binaryImage);
+    private static native double[] moments_1(long array_nativeObj);
+
+    // C++:  Point2d cv::phaseCorrelate(Mat src1, Mat src2, Mat window = Mat(), double* response = 0)
+    private static native double[] phaseCorrelate_0(long src1_nativeObj, long src2_nativeObj, long window_nativeObj, double[] response_out);
+    private static native double[] phaseCorrelate_1(long src1_nativeObj, long src2_nativeObj, long window_nativeObj);
+    private static native double[] phaseCorrelate_2(long src1_nativeObj, long src2_nativeObj);
+
+    // C++:  Ptr_CLAHE cv::createCLAHE(double clipLimit = 40.0, Size tileGridSize = Size(8, 8))
+    private static native long createCLAHE_0(double clipLimit, double tileGridSize_width, double tileGridSize_height);
+    private static native long createCLAHE_1(double clipLimit);
+    private static native long createCLAHE_2();
+
+    // C++:  Ptr_GeneralizedHoughBallard cv::createGeneralizedHoughBallard()
+    private static native long createGeneralizedHoughBallard_0();
+
+    // C++:  Ptr_GeneralizedHoughGuil cv::createGeneralizedHoughGuil()
+    private static native long createGeneralizedHoughGuil_0();
+
+    // C++:  Ptr_LineSegmentDetector cv::createLineSegmentDetector(int _refine = LSD_REFINE_STD, double _scale = 0.8, double _sigma_scale = 0.6, double _quant = 2.0, double _ang_th = 22.5, double _log_eps = 0, double _density_th = 0.7, int _n_bins = 1024)
+    private static native long createLineSegmentDetector_0(int _refine, double _scale, double _sigma_scale, double _quant, double _ang_th, double _log_eps, double _density_th, int _n_bins);
+    private static native long createLineSegmentDetector_1(int _refine, double _scale, double _sigma_scale, double _quant, double _ang_th, double _log_eps, double _density_th);
+    private static native long createLineSegmentDetector_2(int _refine, double _scale, double _sigma_scale, double _quant, double _ang_th, double _log_eps);
+    private static native long createLineSegmentDetector_3(int _refine, double _scale, double _sigma_scale, double _quant, double _ang_th);
+    private static native long createLineSegmentDetector_4(int _refine, double _scale, double _sigma_scale, double _quant);
+    private static native long createLineSegmentDetector_5(int _refine, double _scale, double _sigma_scale);
+    private static native long createLineSegmentDetector_6(int _refine, double _scale);
+    private static native long createLineSegmentDetector_7(int _refine);
+    private static native long createLineSegmentDetector_8();
+
+    // C++:  Rect cv::boundingRect(Mat array)
+    private static native double[] boundingRect_0(long array_nativeObj);
+
+    // C++:  RotatedRect cv::fitEllipse(vector_Point2f points)
+    private static native double[] fitEllipse_0(long points_mat_nativeObj);
+
+    // C++:  RotatedRect cv::fitEllipseAMS(Mat points)
+    private static native double[] fitEllipseAMS_0(long points_nativeObj);
+
+    // C++:  RotatedRect cv::fitEllipseDirect(Mat points)
+    private static native double[] fitEllipseDirect_0(long points_nativeObj);
+
+    // C++:  RotatedRect cv::minAreaRect(vector_Point2f points)
+    private static native double[] minAreaRect_0(long points_mat_nativeObj);
+
+    // C++:  bool cv::clipLine(Rect imgRect, Point& pt1, Point& pt2)
+    private static native boolean clipLine_0(int imgRect_x, int imgRect_y, int imgRect_width, int imgRect_height, double pt1_x, double pt1_y, double[] pt1_out, double pt2_x, double pt2_y, double[] pt2_out);
+
+    // C++:  bool cv::isContourConvex(vector_Point contour)
+    private static native boolean isContourConvex_0(long contour_mat_nativeObj);
+
+    // C++:  double cv::arcLength(vector_Point2f curve, bool closed)
+    private static native double arcLength_0(long curve_mat_nativeObj, boolean closed);
+
+    // C++:  double cv::compareHist(Mat H1, Mat H2, int method)
+    private static native double compareHist_0(long H1_nativeObj, long H2_nativeObj, int method);
+
+    // C++:  double cv::contourArea(Mat contour, bool oriented = false)
+    private static native double contourArea_0(long contour_nativeObj, boolean oriented);
+    private static native double contourArea_1(long contour_nativeObj);
+
+    // C++:  double cv::getFontScaleFromHeight(int fontFace, int pixelHeight, int thickness = 1)
+    private static native double getFontScaleFromHeight_0(int fontFace, int pixelHeight, int thickness);
+    private static native double getFontScaleFromHeight_1(int fontFace, int pixelHeight);
+
+    // C++:  double cv::matchShapes(Mat contour1, Mat contour2, int method, double parameter)
+    private static native double matchShapes_0(long contour1_nativeObj, long contour2_nativeObj, int method, double parameter);
+
+    // C++:  double cv::minEnclosingTriangle(Mat points, Mat& triangle)
+    private static native double minEnclosingTriangle_0(long points_nativeObj, long triangle_nativeObj);
+
+    // C++:  double cv::pointPolygonTest(vector_Point2f contour, Point2f pt, bool measureDist)
+    private static native double pointPolygonTest_0(long contour_mat_nativeObj, double pt_x, double pt_y, boolean measureDist);
+
+    // C++:  double cv::threshold(Mat src, Mat& dst, double thresh, double maxval, int type)
+    private static native double threshold_0(long src_nativeObj, long dst_nativeObj, double thresh, double maxval, int type);
+
+    // C++:  float cv::initWideAngleProjMap(Mat cameraMatrix, Mat distCoeffs, Size imageSize, int destImageWidth, int m1type, Mat& map1, Mat& map2, int projType = PROJ_SPHERICAL_EQRECT, double alpha = 0)
+    private static native float initWideAngleProjMap_0(long cameraMatrix_nativeObj, long distCoeffs_nativeObj, double imageSize_width, double imageSize_height, int destImageWidth, int m1type, long map1_nativeObj, long map2_nativeObj, int projType, double alpha);
+    private static native float initWideAngleProjMap_1(long cameraMatrix_nativeObj, long distCoeffs_nativeObj, double imageSize_width, double imageSize_height, int destImageWidth, int m1type, long map1_nativeObj, long map2_nativeObj, int projType);
+    private static native float initWideAngleProjMap_2(long cameraMatrix_nativeObj, long distCoeffs_nativeObj, double imageSize_width, double imageSize_height, int destImageWidth, int m1type, long map1_nativeObj, long map2_nativeObj);
+
+    // C++:  float cv::intersectConvexConvex(Mat _p1, Mat _p2, Mat& _p12, bool handleNested = true)
+    private static native float intersectConvexConvex_0(long _p1_nativeObj, long _p2_nativeObj, long _p12_nativeObj, boolean handleNested);
+    private static native float intersectConvexConvex_1(long _p1_nativeObj, long _p2_nativeObj, long _p12_nativeObj);
+
+    // C++:  float cv::wrapperEMD(Mat signature1, Mat signature2, int distType, Mat cost = Mat(), Ptr_float& lowerBound = Ptr<float>(), Mat& flow = Mat())
+    private static native float EMD_0(long signature1_nativeObj, long signature2_nativeObj, int distType, long cost_nativeObj, long flow_nativeObj);
+    private static native float EMD_1(long signature1_nativeObj, long signature2_nativeObj, int distType, long cost_nativeObj);
+    private static native float EMD_3(long signature1_nativeObj, long signature2_nativeObj, int distType);
+
+    // C++:  int cv::connectedComponents(Mat image, Mat& labels, int connectivity, int ltype, int ccltype)
+    private static native int connectedComponentsWithAlgorithm_0(long image_nativeObj, long labels_nativeObj, int connectivity, int ltype, int ccltype);
+
+    // C++:  int cv::connectedComponents(Mat image, Mat& labels, int connectivity = 8, int ltype = CV_32S)
+    private static native int connectedComponents_0(long image_nativeObj, long labels_nativeObj, int connectivity, int ltype);
+    private static native int connectedComponents_1(long image_nativeObj, long labels_nativeObj, int connectivity);
+    private static native int connectedComponents_2(long image_nativeObj, long labels_nativeObj);
+
+    // C++:  int cv::connectedComponentsWithStats(Mat image, Mat& labels, Mat& stats, Mat& centroids, int connectivity, int ltype, int ccltype)
+    private static native int connectedComponentsWithStatsWithAlgorithm_0(long image_nativeObj, long labels_nativeObj, long stats_nativeObj, long centroids_nativeObj, int connectivity, int ltype, int ccltype);
+
+    // C++:  int cv::connectedComponentsWithStats(Mat image, Mat& labels, Mat& stats, Mat& centroids, int connectivity = 8, int ltype = CV_32S)
+    private static native int connectedComponentsWithStats_0(long image_nativeObj, long labels_nativeObj, long stats_nativeObj, long centroids_nativeObj, int connectivity, int ltype);
+    private static native int connectedComponentsWithStats_1(long image_nativeObj, long labels_nativeObj, long stats_nativeObj, long centroids_nativeObj, int connectivity);
+    private static native int connectedComponentsWithStats_2(long image_nativeObj, long labels_nativeObj, long stats_nativeObj, long centroids_nativeObj);
+
+    // C++:  int cv::floodFill(Mat& image, Mat& mask, Point seedPoint, Scalar newVal, Rect* rect = 0, Scalar loDiff = Scalar(), Scalar upDiff = Scalar(), int flags = 4)
+    private static native int floodFill_0(long image_nativeObj, long mask_nativeObj, double seedPoint_x, double seedPoint_y, double newVal_val0, double newVal_val1, double newVal_val2, double newVal_val3, double[] rect_out, double loDiff_val0, double loDiff_val1, double loDiff_val2, double loDiff_val3, double upDiff_val0, double upDiff_val1, double upDiff_val2, double upDiff_val3, int flags);
+    private static native int floodFill_1(long image_nativeObj, long mask_nativeObj, double seedPoint_x, double seedPoint_y, double newVal_val0, double newVal_val1, double newVal_val2, double newVal_val3, double[] rect_out, double loDiff_val0, double loDiff_val1, double loDiff_val2, double loDiff_val3, double upDiff_val0, double upDiff_val1, double upDiff_val2, double upDiff_val3);
+    private static native int floodFill_2(long image_nativeObj, long mask_nativeObj, double seedPoint_x, double seedPoint_y, double newVal_val0, double newVal_val1, double newVal_val2, double newVal_val3, double[] rect_out, double loDiff_val0, double loDiff_val1, double loDiff_val2, double loDiff_val3);
+    private static native int floodFill_3(long image_nativeObj, long mask_nativeObj, double seedPoint_x, double seedPoint_y, double newVal_val0, double newVal_val1, double newVal_val2, double newVal_val3, double[] rect_out);
+    private static native int floodFill_4(long image_nativeObj, long mask_nativeObj, double seedPoint_x, double seedPoint_y, double newVal_val0, double newVal_val1, double newVal_val2, double newVal_val3);
+
+    // C++:  int cv::rotatedRectangleIntersection(RotatedRect rect1, RotatedRect rect2, Mat& intersectingRegion)
+    private static native int rotatedRectangleIntersection_0(double rect1_center_x, double rect1_center_y, double rect1_size_width, double rect1_size_height, double rect1_angle, double rect2_center_x, double rect2_center_y, double rect2_size_width, double rect2_size_height, double rect2_angle, long intersectingRegion_nativeObj);
+
+    // C++:  void cv::Canny(Mat dx, Mat dy, Mat& edges, double threshold1, double threshold2, bool L2gradient = false)
+    private static native void Canny_0(long dx_nativeObj, long dy_nativeObj, long edges_nativeObj, double threshold1, double threshold2, boolean L2gradient);
+    private static native void Canny_1(long dx_nativeObj, long dy_nativeObj, long edges_nativeObj, double threshold1, double threshold2);
+
+    // C++:  void cv::Canny(Mat image, Mat& edges, double threshold1, double threshold2, int apertureSize = 3, bool L2gradient = false)
+    private static native void Canny_2(long image_nativeObj, long edges_nativeObj, double threshold1, double threshold2, int apertureSize, boolean L2gradient);
+    private static native void Canny_3(long image_nativeObj, long edges_nativeObj, double threshold1, double threshold2, int apertureSize);
+    private static native void Canny_4(long image_nativeObj, long edges_nativeObj, double threshold1, double threshold2);
+
+    // C++:  void cv::GaussianBlur(Mat src, Mat& dst, Size ksize, double sigmaX, double sigmaY = 0, int borderType = BORDER_DEFAULT)
+    private static native void GaussianBlur_0(long src_nativeObj, long dst_nativeObj, double ksize_width, double ksize_height, double sigmaX, double sigmaY, int borderType);
+    private static native void GaussianBlur_1(long src_nativeObj, long dst_nativeObj, double ksize_width, double ksize_height, double sigmaX, double sigmaY);
+    private static native void GaussianBlur_2(long src_nativeObj, long dst_nativeObj, double ksize_width, double ksize_height, double sigmaX);
+
+    // C++:  void cv::HoughCircles(Mat image, Mat& circles, int method, double dp, double minDist, double param1 = 100, double param2 = 100, int minRadius = 0, int maxRadius = 0)
+    private static native void HoughCircles_0(long image_nativeObj, long circles_nativeObj, int method, double dp, double minDist, double param1, double param2, int minRadius, int maxRadius);
+    private static native void HoughCircles_1(long image_nativeObj, long circles_nativeObj, int method, double dp, double minDist, double param1, double param2, int minRadius);
+    private static native void HoughCircles_2(long image_nativeObj, long circles_nativeObj, int method, double dp, double minDist, double param1, double param2);
+    private static native void HoughCircles_3(long image_nativeObj, long circles_nativeObj, int method, double dp, double minDist, double param1);
+    private static native void HoughCircles_4(long image_nativeObj, long circles_nativeObj, int method, double dp, double minDist);
+
+    // C++:  void cv::HoughLines(Mat image, Mat& lines, double rho, double theta, int threshold, double srn = 0, double stn = 0, double min_theta = 0, double max_theta = CV_PI)
+    private static native void HoughLines_0(long image_nativeObj, long lines_nativeObj, double rho, double theta, int threshold, double srn, double stn, double min_theta, double max_theta);
+    private static native void HoughLines_1(long image_nativeObj, long lines_nativeObj, double rho, double theta, int threshold, double srn, double stn, double min_theta);
+    private static native void HoughLines_2(long image_nativeObj, long lines_nativeObj, double rho, double theta, int threshold, double srn, double stn);
+    private static native void HoughLines_3(long image_nativeObj, long lines_nativeObj, double rho, double theta, int threshold, double srn);
+    private static native void HoughLines_4(long image_nativeObj, long lines_nativeObj, double rho, double theta, int threshold);
+
+    // C++:  void cv::HoughLinesP(Mat image, Mat& lines, double rho, double theta, int threshold, double minLineLength = 0, double maxLineGap = 0)
+    private static native void HoughLinesP_0(long image_nativeObj, long lines_nativeObj, double rho, double theta, int threshold, double minLineLength, double maxLineGap);
+    private static native void HoughLinesP_1(long image_nativeObj, long lines_nativeObj, double rho, double theta, int threshold, double minLineLength);
+    private static native void HoughLinesP_2(long image_nativeObj, long lines_nativeObj, double rho, double theta, int threshold);
+
+    // C++:  void cv::HoughLinesPointSet(Mat _point, Mat& _lines, int lines_max, int threshold, double min_rho, double max_rho, double rho_step, double min_theta, double max_theta, double theta_step)
+    private static native void HoughLinesPointSet_0(long _point_nativeObj, long _lines_nativeObj, int lines_max, int threshold, double min_rho, double max_rho, double rho_step, double min_theta, double max_theta, double theta_step);
+
+    // C++:  void cv::HuMoments(Moments m, Mat& hu)
+    private static native void HuMoments_0(double m_m00, double m_m10, double m_m01, double m_m20, double m_m11, double m_m02, double m_m30, double m_m21, double m_m12, double m_m03, long hu_nativeObj);
+
+    // C++:  void cv::Laplacian(Mat src, Mat& dst, int ddepth, int ksize = 1, double scale = 1, double delta = 0, int borderType = BORDER_DEFAULT)
+    private static native void Laplacian_0(long src_nativeObj, long dst_nativeObj, int ddepth, int ksize, double scale, double delta, int borderType);
+    private static native void Laplacian_1(long src_nativeObj, long dst_nativeObj, int ddepth, int ksize, double scale, double delta);
+    private static native void Laplacian_2(long src_nativeObj, long dst_nativeObj, int ddepth, int ksize, double scale);
+    private static native void Laplacian_3(long src_nativeObj, long dst_nativeObj, int ddepth, int ksize);
+    private static native void Laplacian_4(long src_nativeObj, long dst_nativeObj, int ddepth);
+
+    // C++:  void cv::Scharr(Mat src, Mat& dst, int ddepth, int dx, int dy, double scale = 1, double delta = 0, int borderType = BORDER_DEFAULT)
+    private static native void Scharr_0(long src_nativeObj, long dst_nativeObj, int ddepth, int dx, int dy, double scale, double delta, int borderType);
+    private static native void Scharr_1(long src_nativeObj, long dst_nativeObj, int ddepth, int dx, int dy, double scale, double delta);
+    private static native void Scharr_2(long src_nativeObj, long dst_nativeObj, int ddepth, int dx, int dy, double scale);
+    private static native void Scharr_3(long src_nativeObj, long dst_nativeObj, int ddepth, int dx, int dy);
+
+    // C++:  void cv::Sobel(Mat src, Mat& dst, int ddepth, int dx, int dy, int ksize = 3, double scale = 1, double delta = 0, int borderType = BORDER_DEFAULT)
+    private static native void Sobel_0(long src_nativeObj, long dst_nativeObj, int ddepth, int dx, int dy, int ksize, double scale, double delta, int borderType);
+    private static native void Sobel_1(long src_nativeObj, long dst_nativeObj, int ddepth, int dx, int dy, int ksize, double scale, double delta);
+    private static native void Sobel_2(long src_nativeObj, long dst_nativeObj, int ddepth, int dx, int dy, int ksize, double scale);
+    private static native void Sobel_3(long src_nativeObj, long dst_nativeObj, int ddepth, int dx, int dy, int ksize);
+    private static native void Sobel_4(long src_nativeObj, long dst_nativeObj, int ddepth, int dx, int dy);
+
+    // C++:  void cv::accumulate(Mat src, Mat& dst, Mat mask = Mat())
+    private static native void accumulate_0(long src_nativeObj, long dst_nativeObj, long mask_nativeObj);
+    private static native void accumulate_1(long src_nativeObj, long dst_nativeObj);
+
+    // C++:  void cv::accumulateProduct(Mat src1, Mat src2, Mat& dst, Mat mask = Mat())
+    private static native void accumulateProduct_0(long src1_nativeObj, long src2_nativeObj, long dst_nativeObj, long mask_nativeObj);
+    private static native void accumulateProduct_1(long src1_nativeObj, long src2_nativeObj, long dst_nativeObj);
+
+    // C++:  void cv::accumulateSquare(Mat src, Mat& dst, Mat mask = Mat())
+    private static native void accumulateSquare_0(long src_nativeObj, long dst_nativeObj, long mask_nativeObj);
+    private static native void accumulateSquare_1(long src_nativeObj, long dst_nativeObj);
+
+    // C++:  void cv::accumulateWeighted(Mat src, Mat& dst, double alpha, Mat mask = Mat())
+    private static native void accumulateWeighted_0(long src_nativeObj, long dst_nativeObj, double alpha, long mask_nativeObj);
+    private static native void accumulateWeighted_1(long src_nativeObj, long dst_nativeObj, double alpha);
+
+    // C++:  void cv::adaptiveThreshold(Mat src, Mat& dst, double maxValue, int adaptiveMethod, int thresholdType, int blockSize, double C)
+    private static native void adaptiveThreshold_0(long src_nativeObj, long dst_nativeObj, double maxValue, int adaptiveMethod, int thresholdType, int blockSize, double C);
+
+    // C++:  void cv::applyColorMap(Mat src, Mat& dst, Mat userColor)
+    private static native void applyColorMap_0(long src_nativeObj, long dst_nativeObj, long userColor_nativeObj);
+
+    // C++:  void cv::applyColorMap(Mat src, Mat& dst, int colormap)
+    private static native void applyColorMap_1(long src_nativeObj, long dst_nativeObj, int colormap);
+
+    // C++:  void cv::approxPolyDP(vector_Point2f curve, vector_Point2f& approxCurve, double epsilon, bool closed)
+    private static native void approxPolyDP_0(long curve_mat_nativeObj, long approxCurve_mat_nativeObj, double epsilon, boolean closed);
+
+    // C++:  void cv::arrowedLine(Mat& img, Point pt1, Point pt2, Scalar color, int thickness = 1, int line_type = 8, int shift = 0, double tipLength = 0.1)
+    private static native void arrowedLine_0(long img_nativeObj, double pt1_x, double pt1_y, double pt2_x, double pt2_y, double color_val0, double color_val1, double color_val2, double color_val3, int thickness, int line_type, int shift, double tipLength);
+    private static native void arrowedLine_1(long img_nativeObj, double pt1_x, double pt1_y, double pt2_x, double pt2_y, double color_val0, double color_val1, double color_val2, double color_val3, int thickness, int line_type, int shift);
+    private static native void arrowedLine_2(long img_nativeObj, double pt1_x, double pt1_y, double pt2_x, double pt2_y, double color_val0, double color_val1, double color_val2, double color_val3, int thickness, int line_type);
+    private static native void arrowedLine_3(long img_nativeObj, double pt1_x, double pt1_y, double pt2_x, double pt2_y, double color_val0, double color_val1, double color_val2, double color_val3, int thickness);
+    private static native void arrowedLine_4(long img_nativeObj, double pt1_x, double pt1_y, double pt2_x, double pt2_y, double color_val0, double color_val1, double color_val2, double color_val3);
+
+    // C++:  void cv::bilateralFilter(Mat src, Mat& dst, int d, double sigmaColor, double sigmaSpace, int borderType = BORDER_DEFAULT)
+    private static native void bilateralFilter_0(long src_nativeObj, long dst_nativeObj, int d, double sigmaColor, double sigmaSpace, int borderType);
+    private static native void bilateralFilter_1(long src_nativeObj, long dst_nativeObj, int d, double sigmaColor, double sigmaSpace);
+
+    // C++:  void cv::blur(Mat src, Mat& dst, Size ksize, Point anchor = Point(-1,-1), int borderType = BORDER_DEFAULT)
+    private static native void blur_0(long src_nativeObj, long dst_nativeObj, double ksize_width, double ksize_height, double anchor_x, double anchor_y, int borderType);
+    private static native void blur_1(long src_nativeObj, long dst_nativeObj, double ksize_width, double ksize_height, double anchor_x, double anchor_y);
+    private static native void blur_2(long src_nativeObj, long dst_nativeObj, double ksize_width, double ksize_height);
+
+    // C++:  void cv::boxFilter(Mat src, Mat& dst, int ddepth, Size ksize, Point anchor = Point(-1,-1), bool normalize = true, int borderType = BORDER_DEFAULT)
+    private static native void boxFilter_0(long src_nativeObj, long dst_nativeObj, int ddepth, double ksize_width, double ksize_height, double anchor_x, double anchor_y, boolean normalize, int borderType);
+    private static native void boxFilter_1(long src_nativeObj, long dst_nativeObj, int ddepth, double ksize_width, double ksize_height, double anchor_x, double anchor_y, boolean normalize);
+    private static native void boxFilter_2(long src_nativeObj, long dst_nativeObj, int ddepth, double ksize_width, double ksize_height, double anchor_x, double anchor_y);
+    private static native void boxFilter_3(long src_nativeObj, long dst_nativeObj, int ddepth, double ksize_width, double ksize_height);
+
+    // C++:  void cv::boxPoints(RotatedRect box, Mat& points)
+    private static native void boxPoints_0(double box_center_x, double box_center_y, double box_size_width, double box_size_height, double box_angle, long points_nativeObj);
+
+    // C++:  void cv::calcBackProject(vector_Mat images, vector_int channels, Mat hist, Mat& dst, vector_float ranges, double scale)
+    private static native void calcBackProject_0(long images_mat_nativeObj, long channels_mat_nativeObj, long hist_nativeObj, long dst_nativeObj, long ranges_mat_nativeObj, double scale);
+
+    // C++:  void cv::calcHist(vector_Mat images, vector_int channels, Mat mask, Mat& hist, vector_int histSize, vector_float ranges, bool accumulate = false)
+    private static native void calcHist_0(long images_mat_nativeObj, long channels_mat_nativeObj, long mask_nativeObj, long hist_nativeObj, long histSize_mat_nativeObj, long ranges_mat_nativeObj, boolean accumulate);
+    private static native void calcHist_1(long images_mat_nativeObj, long channels_mat_nativeObj, long mask_nativeObj, long hist_nativeObj, long histSize_mat_nativeObj, long ranges_mat_nativeObj);
+
+    // C++:  void cv::circle(Mat& img, Point center, int radius, Scalar color, int thickness = 1, int lineType = LINE_8, int shift = 0)
+    private static native void circle_0(long img_nativeObj, double center_x, double center_y, int radius, double color_val0, double color_val1, double color_val2, double color_val3, int thickness, int lineType, int shift);
+    private static native void circle_1(long img_nativeObj, double center_x, double center_y, int radius, double color_val0, double color_val1, double color_val2, double color_val3, int thickness, int lineType);
+    private static native void circle_2(long img_nativeObj, double center_x, double center_y, int radius, double color_val0, double color_val1, double color_val2, double color_val3, int thickness);
+    private static native void circle_3(long img_nativeObj, double center_x, double center_y, int radius, double color_val0, double color_val1, double color_val2, double color_val3);
+
+    // C++:  void cv::convertMaps(Mat map1, Mat map2, Mat& dstmap1, Mat& dstmap2, int dstmap1type, bool nninterpolation = false)
+    private static native void convertMaps_0(long map1_nativeObj, long map2_nativeObj, long dstmap1_nativeObj, long dstmap2_nativeObj, int dstmap1type, boolean nninterpolation);
+    private static native void convertMaps_1(long map1_nativeObj, long map2_nativeObj, long dstmap1_nativeObj, long dstmap2_nativeObj, int dstmap1type);
+
+    // C++:  void cv::convexHull(vector_Point points, vector_int& hull, bool clockwise = false,  _hidden_  returnPoints = true)
+    private static native void convexHull_0(long points_mat_nativeObj, long hull_mat_nativeObj, boolean clockwise);
+    private static native void convexHull_2(long points_mat_nativeObj, long hull_mat_nativeObj);
+
+    // C++:  void cv::convexityDefects(vector_Point contour, vector_int convexhull, vector_Vec4i& convexityDefects)
+    private static native void convexityDefects_0(long contour_mat_nativeObj, long convexhull_mat_nativeObj, long convexityDefects_mat_nativeObj);
+
+    // C++:  void cv::cornerEigenValsAndVecs(Mat src, Mat& dst, int blockSize, int ksize, int borderType = BORDER_DEFAULT)
+    private static native void cornerEigenValsAndVecs_0(long src_nativeObj, long dst_nativeObj, int blockSize, int ksize, int borderType);
+    private static native void cornerEigenValsAndVecs_1(long src_nativeObj, long dst_nativeObj, int blockSize, int ksize);
+
+    // C++:  void cv::cornerHarris(Mat src, Mat& dst, int blockSize, int ksize, double k, int borderType = BORDER_DEFAULT)
+    private static native void cornerHarris_0(long src_nativeObj, long dst_nativeObj, int blockSize, int ksize, double k, int borderType);
+    private static native void cornerHarris_1(long src_nativeObj, long dst_nativeObj, int blockSize, int ksize, double k);
+
+    // C++:  void cv::cornerMinEigenVal(Mat src, Mat& dst, int blockSize, int ksize = 3, int borderType = BORDER_DEFAULT)
+    private static native void cornerMinEigenVal_0(long src_nativeObj, long dst_nativeObj, int blockSize, int ksize, int borderType);
+    private static native void cornerMinEigenVal_1(long src_nativeObj, long dst_nativeObj, int blockSize, int ksize);
+    private static native void cornerMinEigenVal_2(long src_nativeObj, long dst_nativeObj, int blockSize);
+
+    // C++:  void cv::cornerSubPix(Mat image, Mat& corners, Size winSize, Size zeroZone, TermCriteria criteria)
+    private static native void cornerSubPix_0(long image_nativeObj, long corners_nativeObj, double winSize_width, double winSize_height, double zeroZone_width, double zeroZone_height, int criteria_type, int criteria_maxCount, double criteria_epsilon);
+
+    // C++:  void cv::createHanningWindow(Mat& dst, Size winSize, int type)
+    private static native void createHanningWindow_0(long dst_nativeObj, double winSize_width, double winSize_height, int type);
+
+    // C++:  void cv::cvtColor(Mat src, Mat& dst, int code, int dstCn = 0)
+    private static native void cvtColor_0(long src_nativeObj, long dst_nativeObj, int code, int dstCn);
+    private static native void cvtColor_1(long src_nativeObj, long dst_nativeObj, int code);
+
+    // C++:  void cv::cvtColorTwoPlane(Mat src1, Mat src2, Mat& dst, int code)
+    private static native void cvtColorTwoPlane_0(long src1_nativeObj, long src2_nativeObj, long dst_nativeObj, int code);
+
+    // C++:  void cv::demosaicing(Mat src, Mat& dst, int code, int dstCn = 0)
+    private static native void demosaicing_0(long src_nativeObj, long dst_nativeObj, int code, int dstCn);
+    private static native void demosaicing_1(long src_nativeObj, long dst_nativeObj, int code);
+
+    // C++:  void cv::dilate(Mat src, Mat& dst, Mat kernel, Point anchor = Point(-1,-1), int iterations = 1, int borderType = BORDER_CONSTANT, Scalar borderValue = morphologyDefaultBorderValue())
+    private static native void dilate_0(long src_nativeObj, long dst_nativeObj, long kernel_nativeObj, double anchor_x, double anchor_y, int iterations, int borderType, double borderValue_val0, double borderValue_val1, double borderValue_val2, double borderValue_val3);
+    private static native void dilate_1(long src_nativeObj, long dst_nativeObj, long kernel_nativeObj, double anchor_x, double anchor_y, int iterations, int borderType);
+    private static native void dilate_2(long src_nativeObj, long dst_nativeObj, long kernel_nativeObj, double anchor_x, double anchor_y, int iterations);
+    private static native void dilate_3(long src_nativeObj, long dst_nativeObj, long kernel_nativeObj, double anchor_x, double anchor_y);
+    private static native void dilate_4(long src_nativeObj, long dst_nativeObj, long kernel_nativeObj);
+
+    // C++:  void cv::distanceTransform(Mat src, Mat& dst, Mat& labels, int distanceType, int maskSize, int labelType = DIST_LABEL_CCOMP)
+    private static native void distanceTransformWithLabels_0(long src_nativeObj, long dst_nativeObj, long labels_nativeObj, int distanceType, int maskSize, int labelType);
+    private static native void distanceTransformWithLabels_1(long src_nativeObj, long dst_nativeObj, long labels_nativeObj, int distanceType, int maskSize);
+
+    // C++:  void cv::distanceTransform(Mat src, Mat& dst, int distanceType, int maskSize, int dstType = CV_32F)
+    private static native void distanceTransform_0(long src_nativeObj, long dst_nativeObj, int distanceType, int maskSize, int dstType);
+    private static native void distanceTransform_1(long src_nativeObj, long dst_nativeObj, int distanceType, int maskSize);
+
+    // C++:  void cv::drawContours(Mat& image, vector_vector_Point contours, int contourIdx, Scalar color, int thickness = 1, int lineType = LINE_8, Mat hierarchy = Mat(), int maxLevel = INT_MAX, Point offset = Point())
+    private static native void drawContours_0(long image_nativeObj, long contours_mat_nativeObj, int contourIdx, double color_val0, double color_val1, double color_val2, double color_val3, int thickness, int lineType, long hierarchy_nativeObj, int maxLevel, double offset_x, double offset_y);
+    private static native void drawContours_1(long image_nativeObj, long contours_mat_nativeObj, int contourIdx, double color_val0, double color_val1, double color_val2, double color_val3, int thickness, int lineType, long hierarchy_nativeObj, int maxLevel);
+    private static native void drawContours_2(long image_nativeObj, long contours_mat_nativeObj, int contourIdx, double color_val0, double color_val1, double color_val2, double color_val3, int thickness, int lineType, long hierarchy_nativeObj);
+    private static native void drawContours_3(long image_nativeObj, long contours_mat_nativeObj, int contourIdx, double color_val0, double color_val1, double color_val2, double color_val3, int thickness, int lineType);
+    private static native void drawContours_4(long image_nativeObj, long contours_mat_nativeObj, int contourIdx, double color_val0, double color_val1, double color_val2, double color_val3, int thickness);
+    private static native void drawContours_5(long image_nativeObj, long contours_mat_nativeObj, int contourIdx, double color_val0, double color_val1, double color_val2, double color_val3);
+
+    // C++:  void cv::drawMarker(Mat& img, Point position, Scalar color, int markerType = MARKER_CROSS, int markerSize = 20, int thickness = 1, int line_type = 8)
+    private static native void drawMarker_0(long img_nativeObj, double position_x, double position_y, double color_val0, double color_val1, double color_val2, double color_val3, int markerType, int markerSize, int thickness, int line_type);
+    private static native void drawMarker_1(long img_nativeObj, double position_x, double position_y, double color_val0, double color_val1, double color_val2, double color_val3, int markerType, int markerSize, int thickness);
+    private static native void drawMarker_2(long img_nativeObj, double position_x, double position_y, double color_val0, double color_val1, double color_val2, double color_val3, int markerType, int markerSize);
+    private static native void drawMarker_3(long img_nativeObj, double position_x, double position_y, double color_val0, double color_val1, double color_val2, double color_val3, int markerType);
+    private static native void drawMarker_4(long img_nativeObj, double position_x, double position_y, double color_val0, double color_val1, double color_val2, double color_val3);
+
+    // C++:  void cv::ellipse(Mat& img, Point center, Size axes, double angle, double startAngle, double endAngle, Scalar color, int thickness = 1, int lineType = LINE_8, int shift = 0)
+    private static native void ellipse_0(long img_nativeObj, double center_x, double center_y, double axes_width, double axes_height, double angle, double startAngle, double endAngle, double color_val0, double color_val1, double color_val2, double color_val3, int thickness, int lineType, int shift);
+    private static native void ellipse_1(long img_nativeObj, double center_x, double center_y, double axes_width, double axes_height, double angle, double startAngle, double endAngle, double color_val0, double color_val1, double color_val2, double color_val3, int thickness, int lineType);
+    private static native void ellipse_2(long img_nativeObj, double center_x, double center_y, double axes_width, double axes_height, double angle, double startAngle, double endAngle, double color_val0, double color_val1, double color_val2, double color_val3, int thickness);
+    private static native void ellipse_3(long img_nativeObj, double center_x, double center_y, double axes_width, double axes_height, double angle, double startAngle, double endAngle, double color_val0, double color_val1, double color_val2, double color_val3);
+
+    // C++:  void cv::ellipse(Mat& img, RotatedRect box, Scalar color, int thickness = 1, int lineType = LINE_8)
+    private static native void ellipse_4(long img_nativeObj, double box_center_x, double box_center_y, double box_size_width, double box_size_height, double box_angle, double color_val0, double color_val1, double color_val2, double color_val3, int thickness, int lineType);
+    private static native void ellipse_5(long img_nativeObj, double box_center_x, double box_center_y, double box_size_width, double box_size_height, double box_angle, double color_val0, double color_val1, double color_val2, double color_val3, int thickness);
+    private static native void ellipse_6(long img_nativeObj, double box_center_x, double box_center_y, double box_size_width, double box_size_height, double box_angle, double color_val0, double color_val1, double color_val2, double color_val3);
+
+    // C++:  void cv::ellipse2Poly(Point center, Size axes, int angle, int arcStart, int arcEnd, int delta, vector_Point& pts)
+    private static native void ellipse2Poly_0(double center_x, double center_y, double axes_width, double axes_height, int angle, int arcStart, int arcEnd, int delta, long pts_mat_nativeObj);
+
+    // C++:  void cv::equalizeHist(Mat src, Mat& dst)
+    private static native void equalizeHist_0(long src_nativeObj, long dst_nativeObj);
+
+    // C++:  void cv::erode(Mat src, Mat& dst, Mat kernel, Point anchor = Point(-1,-1), int iterations = 1, int borderType = BORDER_CONSTANT, Scalar borderValue = morphologyDefaultBorderValue())
+    private static native void erode_0(long src_nativeObj, long dst_nativeObj, long kernel_nativeObj, double anchor_x, double anchor_y, int iterations, int borderType, double borderValue_val0, double borderValue_val1, double borderValue_val2, double borderValue_val3);
+    private static native void erode_1(long src_nativeObj, long dst_nativeObj, long kernel_nativeObj, double anchor_x, double anchor_y, int iterations, int borderType);
+    private static native void erode_2(long src_nativeObj, long dst_nativeObj, long kernel_nativeObj, double anchor_x, double anchor_y, int iterations);
+    private static native void erode_3(long src_nativeObj, long dst_nativeObj, long kernel_nativeObj, double anchor_x, double anchor_y);
+    private static native void erode_4(long src_nativeObj, long dst_nativeObj, long kernel_nativeObj);
+
+    // C++:  void cv::fillConvexPoly(Mat& img, vector_Point points, Scalar color, int lineType = LINE_8, int shift = 0)
+    private static native void fillConvexPoly_0(long img_nativeObj, long points_mat_nativeObj, double color_val0, double color_val1, double color_val2, double color_val3, int lineType, int shift);
+    private static native void fillConvexPoly_1(long img_nativeObj, long points_mat_nativeObj, double color_val0, double color_val1, double color_val2, double color_val3, int lineType);
+    private static native void fillConvexPoly_2(long img_nativeObj, long points_mat_nativeObj, double color_val0, double color_val1, double color_val2, double color_val3);
+
+    // C++:  void cv::fillPoly(Mat& img, vector_vector_Point pts, Scalar color, int lineType = LINE_8, int shift = 0, Point offset = Point())
+    private static native void fillPoly_0(long img_nativeObj, long pts_mat_nativeObj, double color_val0, double color_val1, double color_val2, double color_val3, int lineType, int shift, double offset_x, double offset_y);
+    private static native void fillPoly_1(long img_nativeObj, long pts_mat_nativeObj, double color_val0, double color_val1, double color_val2, double color_val3, int lineType, int shift);
+    private static native void fillPoly_2(long img_nativeObj, long pts_mat_nativeObj, double color_val0, double color_val1, double color_val2, double color_val3, int lineType);
+    private static native void fillPoly_3(long img_nativeObj, long pts_mat_nativeObj, double color_val0, double color_val1, double color_val2, double color_val3);
+
+    // C++:  void cv::filter2D(Mat src, Mat& dst, int ddepth, Mat kernel, Point anchor = Point(-1,-1), double delta = 0, int borderType = BORDER_DEFAULT)
+    private static native void filter2D_0(long src_nativeObj, long dst_nativeObj, int ddepth, long kernel_nativeObj, double anchor_x, double anchor_y, double delta, int borderType);
+    private static native void filter2D_1(long src_nativeObj, long dst_nativeObj, int ddepth, long kernel_nativeObj, double anchor_x, double anchor_y, double delta);
+    private static native void filter2D_2(long src_nativeObj, long dst_nativeObj, int ddepth, long kernel_nativeObj, double anchor_x, double anchor_y);
+    private static native void filter2D_3(long src_nativeObj, long dst_nativeObj, int ddepth, long kernel_nativeObj);
+
+    // C++:  void cv::findContours(Mat& image, vector_vector_Point& contours, Mat& hierarchy, int mode, int method, Point offset = Point())
+    private static native void findContours_0(long image_nativeObj, long contours_mat_nativeObj, long hierarchy_nativeObj, int mode, int method, double offset_x, double offset_y);
+    private static native void findContours_1(long image_nativeObj, long contours_mat_nativeObj, long hierarchy_nativeObj, int mode, int method);
+
+    // C++:  void cv::fitLine(Mat points, Mat& line, int distType, double param, double reps, double aeps)
+    private static native void fitLine_0(long points_nativeObj, long line_nativeObj, int distType, double param, double reps, double aeps);
+
+    // C++:  void cv::getDerivKernels(Mat& kx, Mat& ky, int dx, int dy, int ksize, bool normalize = false, int ktype = CV_32F)
+    private static native void getDerivKernels_0(long kx_nativeObj, long ky_nativeObj, int dx, int dy, int ksize, boolean normalize, int ktype);
+    private static native void getDerivKernels_1(long kx_nativeObj, long ky_nativeObj, int dx, int dy, int ksize, boolean normalize);
+    private static native void getDerivKernels_2(long kx_nativeObj, long ky_nativeObj, int dx, int dy, int ksize);
+
+    // C++:  void cv::getRectSubPix(Mat image, Size patchSize, Point2f center, Mat& patch, int patchType = -1)
+    private static native void getRectSubPix_0(long image_nativeObj, double patchSize_width, double patchSize_height, double center_x, double center_y, long patch_nativeObj, int patchType);
+    private static native void getRectSubPix_1(long image_nativeObj, double patchSize_width, double patchSize_height, double center_x, double center_y, long patch_nativeObj);
+
+    // C++:  void cv::goodFeaturesToTrack(Mat image, vector_Point& corners, int maxCorners, double qualityLevel, double minDistance, Mat mask, int blockSize, int gradientSize, bool useHarrisDetector = false, double k = 0.04)
+    private static native void goodFeaturesToTrack_0(long image_nativeObj, long corners_mat_nativeObj, int maxCorners, double qualityLevel, double minDistance, long mask_nativeObj, int blockSize, int gradientSize, boolean useHarrisDetector, double k);
+    private static native void goodFeaturesToTrack_1(long image_nativeObj, long corners_mat_nativeObj, int maxCorners, double qualityLevel, double minDistance, long mask_nativeObj, int blockSize, int gradientSize, boolean useHarrisDetector);
+    private static native void goodFeaturesToTrack_2(long image_nativeObj, long corners_mat_nativeObj, int maxCorners, double qualityLevel, double minDistance, long mask_nativeObj, int blockSize, int gradientSize);
+
+    // C++:  void cv::goodFeaturesToTrack(Mat image, vector_Point& corners, int maxCorners, double qualityLevel, double minDistance, Mat mask = Mat(), int blockSize = 3, bool useHarrisDetector = false, double k = 0.04)
+    private static native void goodFeaturesToTrack_3(long image_nativeObj, long corners_mat_nativeObj, int maxCorners, double qualityLevel, double minDistance, long mask_nativeObj, int blockSize, boolean useHarrisDetector, double k);
+    private static native void goodFeaturesToTrack_4(long image_nativeObj, long corners_mat_nativeObj, int maxCorners, double qualityLevel, double minDistance, long mask_nativeObj, int blockSize, boolean useHarrisDetector);
+    private static native void goodFeaturesToTrack_5(long image_nativeObj, long corners_mat_nativeObj, int maxCorners, double qualityLevel, double minDistance, long mask_nativeObj, int blockSize);
+    private static native void goodFeaturesToTrack_6(long image_nativeObj, long corners_mat_nativeObj, int maxCorners, double qualityLevel, double minDistance, long mask_nativeObj);
+    private static native void goodFeaturesToTrack_7(long image_nativeObj, long corners_mat_nativeObj, int maxCorners, double qualityLevel, double minDistance);
+
+    // C++:  void cv::grabCut(Mat img, Mat& mask, Rect rect, Mat& bgdModel, Mat& fgdModel, int iterCount, int mode = GC_EVAL)
+    private static native void grabCut_0(long img_nativeObj, long mask_nativeObj, int rect_x, int rect_y, int rect_width, int rect_height, long bgdModel_nativeObj, long fgdModel_nativeObj, int iterCount, int mode);
+    private static native void grabCut_1(long img_nativeObj, long mask_nativeObj, int rect_x, int rect_y, int rect_width, int rect_height, long bgdModel_nativeObj, long fgdModel_nativeObj, int iterCount);
+
+    // C++:  void cv::initUndistortRectifyMap(Mat cameraMatrix, Mat distCoeffs, Mat R, Mat newCameraMatrix, Size size, int m1type, Mat& map1, Mat& map2)
+    private static native void initUndistortRectifyMap_0(long cameraMatrix_nativeObj, long distCoeffs_nativeObj, long R_nativeObj, long newCameraMatrix_nativeObj, double size_width, double size_height, int m1type, long map1_nativeObj, long map2_nativeObj);
+
+    // C++:  void cv::integral(Mat src, Mat& sum, Mat& sqsum, Mat& tilted, int sdepth = -1, int sqdepth = -1)
+    private static native void integral3_0(long src_nativeObj, long sum_nativeObj, long sqsum_nativeObj, long tilted_nativeObj, int sdepth, int sqdepth);
+    private static native void integral3_1(long src_nativeObj, long sum_nativeObj, long sqsum_nativeObj, long tilted_nativeObj, int sdepth);
+    private static native void integral3_2(long src_nativeObj, long sum_nativeObj, long sqsum_nativeObj, long tilted_nativeObj);
+
+    // C++:  void cv::integral(Mat src, Mat& sum, Mat& sqsum, int sdepth = -1, int sqdepth = -1)
+    private static native void integral2_0(long src_nativeObj, long sum_nativeObj, long sqsum_nativeObj, int sdepth, int sqdepth);
+    private static native void integral2_1(long src_nativeObj, long sum_nativeObj, long sqsum_nativeObj, int sdepth);
+    private static native void integral2_2(long src_nativeObj, long sum_nativeObj, long sqsum_nativeObj);
+
+    // C++:  void cv::integral(Mat src, Mat& sum, int sdepth = -1)
+    private static native void integral_0(long src_nativeObj, long sum_nativeObj, int sdepth);
+    private static native void integral_1(long src_nativeObj, long sum_nativeObj);
+
+    // C++:  void cv::invertAffineTransform(Mat M, Mat& iM)
+    private static native void invertAffineTransform_0(long M_nativeObj, long iM_nativeObj);
+
+    // C++:  void cv::line(Mat& img, Point pt1, Point pt2, Scalar color, int thickness = 1, int lineType = LINE_8, int shift = 0)
+    private static native void line_0(long img_nativeObj, double pt1_x, double pt1_y, double pt2_x, double pt2_y, double color_val0, double color_val1, double color_val2, double color_val3, int thickness, int lineType, int shift);
+    private static native void line_1(long img_nativeObj, double pt1_x, double pt1_y, double pt2_x, double pt2_y, double color_val0, double color_val1, double color_val2, double color_val3, int thickness, int lineType);
+    private static native void line_2(long img_nativeObj, double pt1_x, double pt1_y, double pt2_x, double pt2_y, double color_val0, double color_val1, double color_val2, double color_val3, int thickness);
+    private static native void line_3(long img_nativeObj, double pt1_x, double pt1_y, double pt2_x, double pt2_y, double color_val0, double color_val1, double color_val2, double color_val3);
+
+    // C++:  void cv::linearPolar(Mat src, Mat& dst, Point2f center, double maxRadius, int flags)
+    private static native void linearPolar_0(long src_nativeObj, long dst_nativeObj, double center_x, double center_y, double maxRadius, int flags);
+
+    // C++:  void cv::logPolar(Mat src, Mat& dst, Point2f center, double M, int flags)
+    private static native void logPolar_0(long src_nativeObj, long dst_nativeObj, double center_x, double center_y, double M, int flags);
+
+    // C++:  void cv::matchTemplate(Mat image, Mat templ, Mat& result, int method, Mat mask = Mat())
+    private static native void matchTemplate_0(long image_nativeObj, long templ_nativeObj, long result_nativeObj, int method, long mask_nativeObj);
+    private static native void matchTemplate_1(long image_nativeObj, long templ_nativeObj, long result_nativeObj, int method);
+
+    // C++:  void cv::medianBlur(Mat src, Mat& dst, int ksize)
+    private static native void medianBlur_0(long src_nativeObj, long dst_nativeObj, int ksize);
+
+    // C++:  void cv::minEnclosingCircle(vector_Point2f points, Point2f& center, float& radius)
+    private static native void minEnclosingCircle_0(long points_mat_nativeObj, double[] center_out, double[] radius_out);
+
+    // C++:  void cv::morphologyEx(Mat src, Mat& dst, int op, Mat kernel, Point anchor = Point(-1,-1), int iterations = 1, int borderType = BORDER_CONSTANT, Scalar borderValue = morphologyDefaultBorderValue())
+    private static native void morphologyEx_0(long src_nativeObj, long dst_nativeObj, int op, long kernel_nativeObj, double anchor_x, double anchor_y, int iterations, int borderType, double borderValue_val0, double borderValue_val1, double borderValue_val2, double borderValue_val3);
+    private static native void morphologyEx_1(long src_nativeObj, long dst_nativeObj, int op, long kernel_nativeObj, double anchor_x, double anchor_y, int iterations, int borderType);
+    private static native void morphologyEx_2(long src_nativeObj, long dst_nativeObj, int op, long kernel_nativeObj, double anchor_x, double anchor_y, int iterations);
+    private static native void morphologyEx_3(long src_nativeObj, long dst_nativeObj, int op, long kernel_nativeObj, double anchor_x, double anchor_y);
+    private static native void morphologyEx_4(long src_nativeObj, long dst_nativeObj, int op, long kernel_nativeObj);
+
+    // C++:  void cv::polylines(Mat& img, vector_vector_Point pts, bool isClosed, Scalar color, int thickness = 1, int lineType = LINE_8, int shift = 0)
+    private static native void polylines_0(long img_nativeObj, long pts_mat_nativeObj, boolean isClosed, double color_val0, double color_val1, double color_val2, double color_val3, int thickness, int lineType, int shift);
+    private static native void polylines_1(long img_nativeObj, long pts_mat_nativeObj, boolean isClosed, double color_val0, double color_val1, double color_val2, double color_val3, int thickness, int lineType);
+    private static native void polylines_2(long img_nativeObj, long pts_mat_nativeObj, boolean isClosed, double color_val0, double color_val1, double color_val2, double color_val3, int thickness);
+    private static native void polylines_3(long img_nativeObj, long pts_mat_nativeObj, boolean isClosed, double color_val0, double color_val1, double color_val2, double color_val3);
+
+    // C++:  void cv::preCornerDetect(Mat src, Mat& dst, int ksize, int borderType = BORDER_DEFAULT)
+    private static native void preCornerDetect_0(long src_nativeObj, long dst_nativeObj, int ksize, int borderType);
+    private static native void preCornerDetect_1(long src_nativeObj, long dst_nativeObj, int ksize);
+
+    // C++:  void cv::putText(Mat& img, String text, Point org, int fontFace, double fontScale, Scalar color, int thickness = 1, int lineType = LINE_8, bool bottomLeftOrigin = false)
+    private static native void putText_0(long img_nativeObj, String text, double org_x, double org_y, int fontFace, double fontScale, double color_val0, double color_val1, double color_val2, double color_val3, int thickness, int lineType, boolean bottomLeftOrigin);
+    private static native void putText_1(long img_nativeObj, String text, double org_x, double org_y, int fontFace, double fontScale, double color_val0, double color_val1, double color_val2, double color_val3, int thickness, int lineType);
+    private static native void putText_2(long img_nativeObj, String text, double org_x, double org_y, int fontFace, double fontScale, double color_val0, double color_val1, double color_val2, double color_val3, int thickness);
+    private static native void putText_3(long img_nativeObj, String text, double org_x, double org_y, int fontFace, double fontScale, double color_val0, double color_val1, double color_val2, double color_val3);
+
+    // C++:  void cv::pyrDown(Mat src, Mat& dst, Size dstsize = Size(), int borderType = BORDER_DEFAULT)
+    private static native void pyrDown_0(long src_nativeObj, long dst_nativeObj, double dstsize_width, double dstsize_height, int borderType);
+    private static native void pyrDown_1(long src_nativeObj, long dst_nativeObj, double dstsize_width, double dstsize_height);
+    private static native void pyrDown_2(long src_nativeObj, long dst_nativeObj);
+
+    // C++:  void cv::pyrMeanShiftFiltering(Mat src, Mat& dst, double sp, double sr, int maxLevel = 1, TermCriteria termcrit = TermCriteria(TermCriteria::MAX_ITER+TermCriteria::EPS,5,1))
+    private static native void pyrMeanShiftFiltering_0(long src_nativeObj, long dst_nativeObj, double sp, double sr, int maxLevel, int termcrit_type, int termcrit_maxCount, double termcrit_epsilon);
+    private static native void pyrMeanShiftFiltering_1(long src_nativeObj, long dst_nativeObj, double sp, double sr, int maxLevel);
+    private static native void pyrMeanShiftFiltering_2(long src_nativeObj, long dst_nativeObj, double sp, double sr);
+
+    // C++:  void cv::pyrUp(Mat src, Mat& dst, Size dstsize = Size(), int borderType = BORDER_DEFAULT)
+    private static native void pyrUp_0(long src_nativeObj, long dst_nativeObj, double dstsize_width, double dstsize_height, int borderType);
+    private static native void pyrUp_1(long src_nativeObj, long dst_nativeObj, double dstsize_width, double dstsize_height);
+    private static native void pyrUp_2(long src_nativeObj, long dst_nativeObj);
+
+    // C++:  void cv::rectangle(Mat& img, Point pt1, Point pt2, Scalar color, int thickness = 1, int lineType = LINE_8, int shift = 0)
+    private static native void rectangle_0(long img_nativeObj, double pt1_x, double pt1_y, double pt2_x, double pt2_y, double color_val0, double color_val1, double color_val2, double color_val3, int thickness, int lineType, int shift);
+    private static native void rectangle_1(long img_nativeObj, double pt1_x, double pt1_y, double pt2_x, double pt2_y, double color_val0, double color_val1, double color_val2, double color_val3, int thickness, int lineType);
+    private static native void rectangle_2(long img_nativeObj, double pt1_x, double pt1_y, double pt2_x, double pt2_y, double color_val0, double color_val1, double color_val2, double color_val3, int thickness);
+    private static native void rectangle_3(long img_nativeObj, double pt1_x, double pt1_y, double pt2_x, double pt2_y, double color_val0, double color_val1, double color_val2, double color_val3);
+
+    // C++:  void cv::remap(Mat src, Mat& dst, Mat map1, Mat map2, int interpolation, int borderMode = BORDER_CONSTANT, Scalar borderValue = Scalar())
+    private static native void remap_0(long src_nativeObj, long dst_nativeObj, long map1_nativeObj, long map2_nativeObj, int interpolation, int borderMode, double borderValue_val0, double borderValue_val1, double borderValue_val2, double borderValue_val3);
+    private static native void remap_1(long src_nativeObj, long dst_nativeObj, long map1_nativeObj, long map2_nativeObj, int interpolation, int borderMode);
+    private static native void remap_2(long src_nativeObj, long dst_nativeObj, long map1_nativeObj, long map2_nativeObj, int interpolation);
+
+    // C++:  void cv::resize(Mat src, Mat& dst, Size dsize, double fx = 0, double fy = 0, int interpolation = INTER_LINEAR)
+    private static native void resize_0(long src_nativeObj, long dst_nativeObj, double dsize_width, double dsize_height, double fx, double fy, int interpolation);
+    private static native void resize_1(long src_nativeObj, long dst_nativeObj, double dsize_width, double dsize_height, double fx, double fy);
+    private static native void resize_2(long src_nativeObj, long dst_nativeObj, double dsize_width, double dsize_height, double fx);
+    private static native void resize_3(long src_nativeObj, long dst_nativeObj, double dsize_width, double dsize_height);
+
+    // C++:  void cv::sepFilter2D(Mat src, Mat& dst, int ddepth, Mat kernelX, Mat kernelY, Point anchor = Point(-1,-1), double delta = 0, int borderType = BORDER_DEFAULT)
+    private static native void sepFilter2D_0(long src_nativeObj, long dst_nativeObj, int ddepth, long kernelX_nativeObj, long kernelY_nativeObj, double anchor_x, double anchor_y, double delta, int borderType);
+    private static native void sepFilter2D_1(long src_nativeObj, long dst_nativeObj, int ddepth, long kernelX_nativeObj, long kernelY_nativeObj, double anchor_x, double anchor_y, double delta);
+    private static native void sepFilter2D_2(long src_nativeObj, long dst_nativeObj, int ddepth, long kernelX_nativeObj, long kernelY_nativeObj, double anchor_x, double anchor_y);
+    private static native void sepFilter2D_3(long src_nativeObj, long dst_nativeObj, int ddepth, long kernelX_nativeObj, long kernelY_nativeObj);
+
+    // C++:  void cv::spatialGradient(Mat src, Mat& dx, Mat& dy, int ksize = 3, int borderType = BORDER_DEFAULT)
+    private static native void spatialGradient_0(long src_nativeObj, long dx_nativeObj, long dy_nativeObj, int ksize, int borderType);
+    private static native void spatialGradient_1(long src_nativeObj, long dx_nativeObj, long dy_nativeObj, int ksize);
+    private static native void spatialGradient_2(long src_nativeObj, long dx_nativeObj, long dy_nativeObj);
+
+    // C++:  void cv::sqrBoxFilter(Mat src, Mat& dst, int ddepth, Size ksize, Point anchor = Point(-1, -1), bool normalize = true, int borderType = BORDER_DEFAULT)
+    private static native void sqrBoxFilter_0(long src_nativeObj, long dst_nativeObj, int ddepth, double ksize_width, double ksize_height, double anchor_x, double anchor_y, boolean normalize, int borderType);
+    private static native void sqrBoxFilter_1(long src_nativeObj, long dst_nativeObj, int ddepth, double ksize_width, double ksize_height, double anchor_x, double anchor_y, boolean normalize);
+    private static native void sqrBoxFilter_2(long src_nativeObj, long dst_nativeObj, int ddepth, double ksize_width, double ksize_height, double anchor_x, double anchor_y);
+    private static native void sqrBoxFilter_3(long src_nativeObj, long dst_nativeObj, int ddepth, double ksize_width, double ksize_height);
+
+    // C++:  void cv::undistort(Mat src, Mat& dst, Mat cameraMatrix, Mat distCoeffs, Mat newCameraMatrix = Mat())
+    private static native void undistort_0(long src_nativeObj, long dst_nativeObj, long cameraMatrix_nativeObj, long distCoeffs_nativeObj, long newCameraMatrix_nativeObj);
+    private static native void undistort_1(long src_nativeObj, long dst_nativeObj, long cameraMatrix_nativeObj, long distCoeffs_nativeObj);
+
+    // C++:  void cv::undistortPoints(Mat src, Mat& dst, Mat cameraMatrix, Mat distCoeffs, Mat R, Mat P, TermCriteria criteria)
+    private static native void undistortPointsIter_0(long src_nativeObj, long dst_nativeObj, long cameraMatrix_nativeObj, long distCoeffs_nativeObj, long R_nativeObj, long P_nativeObj, int criteria_type, int criteria_maxCount, double criteria_epsilon);
+
+    // C++:  void cv::undistortPoints(Mat src, Mat& dst, Mat cameraMatrix, Mat distCoeffs, Mat R = Mat(), Mat P = Mat())
+    private static native void undistortPoints_0(long src_nativeObj, long dst_nativeObj, long cameraMatrix_nativeObj, long distCoeffs_nativeObj, long R_nativeObj, long P_nativeObj);
+    private static native void undistortPoints_1(long src_nativeObj, long dst_nativeObj, long cameraMatrix_nativeObj, long distCoeffs_nativeObj, long R_nativeObj);
+    private static native void undistortPoints_2(long src_nativeObj, long dst_nativeObj, long cameraMatrix_nativeObj, long distCoeffs_nativeObj);
+
+    // C++:  void cv::warpAffine(Mat src, Mat& dst, Mat M, Size dsize, int flags = INTER_LINEAR, int borderMode = BORDER_CONSTANT, Scalar borderValue = Scalar())
+    private static native void warpAffine_0(long src_nativeObj, long dst_nativeObj, long M_nativeObj, double dsize_width, double dsize_height, int flags, int borderMode, double borderValue_val0, double borderValue_val1, double borderValue_val2, double borderValue_val3);
+    private static native void warpAffine_1(long src_nativeObj, long dst_nativeObj, long M_nativeObj, double dsize_width, double dsize_height, int flags, int borderMode);
+    private static native void warpAffine_2(long src_nativeObj, long dst_nativeObj, long M_nativeObj, double dsize_width, double dsize_height, int flags);
+    private static native void warpAffine_3(long src_nativeObj, long dst_nativeObj, long M_nativeObj, double dsize_width, double dsize_height);
+
+    // C++:  void cv::warpPerspective(Mat src, Mat& dst, Mat M, Size dsize, int flags = INTER_LINEAR, int borderMode = BORDER_CONSTANT, Scalar borderValue = Scalar())
+    private static native void warpPerspective_0(long src_nativeObj, long dst_nativeObj, long M_nativeObj, double dsize_width, double dsize_height, int flags, int borderMode, double borderValue_val0, double borderValue_val1, double borderValue_val2, double borderValue_val3);
+    private static native void warpPerspective_1(long src_nativeObj, long dst_nativeObj, long M_nativeObj, double dsize_width, double dsize_height, int flags, int borderMode);
+    private static native void warpPerspective_2(long src_nativeObj, long dst_nativeObj, long M_nativeObj, double dsize_width, double dsize_height, int flags);
+    private static native void warpPerspective_3(long src_nativeObj, long dst_nativeObj, long M_nativeObj, double dsize_width, double dsize_height);
+
+    // C++:  void cv::warpPolar(Mat src, Mat& dst, Size dsize, Point2f center, double maxRadius, int flags)
+    private static native void warpPolar_0(long src_nativeObj, long dst_nativeObj, double dsize_width, double dsize_height, double center_x, double center_y, double maxRadius, int flags);
+
+    // C++:  void cv::watershed(Mat image, Mat& markers)
+    private static native void watershed_0(long image_nativeObj, long markers_nativeObj);
+private static native double[] n_getTextSize(String text, int fontFace, double fontScale, int thickness, int[] baseLine);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/imgproc/LineSegmentDetector.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/imgproc/LineSegmentDetector.java	(date 1605830247767)
+++ openCVLibrary3411/src/main/java/org/opencv/imgproc/LineSegmentDetector.java	(date 1605830247767)
@@ -0,0 +1,226 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.imgproc;
+
+import org.opencv.core.Algorithm;
+import org.opencv.core.Mat;
+import org.opencv.core.Size;
+
+// C++: class LineSegmentDetector
+/**
+ * Line segment detector class
+ *
+ * following the algorithm described at CITE: Rafael12 .
+ *
+ * <b>Note:</b> Implementation has been removed due original code license conflict
+ */
+public class LineSegmentDetector extends Algorithm {
+
+    protected LineSegmentDetector(long addr) { super(addr); }
+
+    // internal usage only
+    public static LineSegmentDetector __fromPtr__(long addr) { return new LineSegmentDetector(addr); }
+
+    //
+    // C++:  int cv::LineSegmentDetector::compareSegments(Size size, Mat lines1, Mat lines2, Mat& _image = Mat())
+    //
+
+    /**
+     * Draws two groups of lines in blue and red, counting the non overlapping (mismatching) pixels.
+     *
+     *     @param size The size of the image, where lines1 and lines2 were found.
+     *     @param lines1 The first group of lines that needs to be drawn. It is visualized in blue color.
+     *     @param lines2 The second group of lines. They visualized in red color.
+     *     @param _image Optional image, where the lines will be drawn. The image should be color(3-channel)
+     *     in order for lines1 and lines2 to be drawn in the above mentioned colors.
+     * @return automatically generated
+     */
+    public int compareSegments(Size size, Mat lines1, Mat lines2, Mat _image) {
+        return compareSegments_0(nativeObj, size.width, size.height, lines1.nativeObj, lines2.nativeObj, _image.nativeObj);
+    }
+
+    /**
+     * Draws two groups of lines in blue and red, counting the non overlapping (mismatching) pixels.
+     *
+     *     @param size The size of the image, where lines1 and lines2 were found.
+     *     @param lines1 The first group of lines that needs to be drawn. It is visualized in blue color.
+     *     @param lines2 The second group of lines. They visualized in red color.
+     *     in order for lines1 and lines2 to be drawn in the above mentioned colors.
+     * @return automatically generated
+     */
+    public int compareSegments(Size size, Mat lines1, Mat lines2) {
+        return compareSegments_1(nativeObj, size.width, size.height, lines1.nativeObj, lines2.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::LineSegmentDetector::detect(Mat _image, Mat& _lines, Mat& width = Mat(), Mat& prec = Mat(), Mat& nfa = Mat())
+    //
+
+    /**
+     * Finds lines in the input image.
+     *
+     *     This is the output of the default parameters of the algorithm on the above shown image.
+     *
+     *     ![image](pics/building_lsd.png)
+     *
+     *     @param _image A grayscale (CV_8UC1) input image. If only a roi needs to be selected, use:
+     *     {@code lsd_ptr-&gt;detect(image(roi), lines, ...); lines += Scalar(roi.x, roi.y, roi.x, roi.y);}
+     *     @param _lines A vector of Vec4i or Vec4f elements specifying the beginning and ending point of a line. Where
+     *     Vec4i/Vec4f is (x1, y1, x2, y2), point 1 is the start, point 2 - end. Returned lines are strictly
+     *     oriented depending on the gradient.
+     *     @param width Vector of widths of the regions, where the lines are found. E.g. Width of line.
+     *     @param prec Vector of precisions with which the lines are found.
+     *     @param nfa Vector containing number of false alarms in the line region, with precision of 10%. The
+     *     bigger the value, logarithmically better the detection.
+     * <ul>
+     *   <li>
+     *      -1 corresponds to 10 mean false alarms
+     *   </li>
+     *   <li>
+     *      0 corresponds to 1 mean false alarm
+     *   </li>
+     *   <li>
+     *      1 corresponds to 0.1 mean false alarms
+     *     This vector will be calculated only when the objects type is #LSD_REFINE_ADV.
+     *   </li>
+     * </ul>
+     */
+    public void detect(Mat _image, Mat _lines, Mat width, Mat prec, Mat nfa) {
+        detect_0(nativeObj, _image.nativeObj, _lines.nativeObj, width.nativeObj, prec.nativeObj, nfa.nativeObj);
+    }
+
+    /**
+     * Finds lines in the input image.
+     *
+     *     This is the output of the default parameters of the algorithm on the above shown image.
+     *
+     *     ![image](pics/building_lsd.png)
+     *
+     *     @param _image A grayscale (CV_8UC1) input image. If only a roi needs to be selected, use:
+     *     {@code lsd_ptr-&gt;detect(image(roi), lines, ...); lines += Scalar(roi.x, roi.y, roi.x, roi.y);}
+     *     @param _lines A vector of Vec4i or Vec4f elements specifying the beginning and ending point of a line. Where
+     *     Vec4i/Vec4f is (x1, y1, x2, y2), point 1 is the start, point 2 - end. Returned lines are strictly
+     *     oriented depending on the gradient.
+     *     @param width Vector of widths of the regions, where the lines are found. E.g. Width of line.
+     *     @param prec Vector of precisions with which the lines are found.
+     *     bigger the value, logarithmically better the detection.
+     * <ul>
+     *   <li>
+     *      -1 corresponds to 10 mean false alarms
+     *   </li>
+     *   <li>
+     *      0 corresponds to 1 mean false alarm
+     *   </li>
+     *   <li>
+     *      1 corresponds to 0.1 mean false alarms
+     *     This vector will be calculated only when the objects type is #LSD_REFINE_ADV.
+     *   </li>
+     * </ul>
+     */
+    public void detect(Mat _image, Mat _lines, Mat width, Mat prec) {
+        detect_1(nativeObj, _image.nativeObj, _lines.nativeObj, width.nativeObj, prec.nativeObj);
+    }
+
+    /**
+     * Finds lines in the input image.
+     *
+     *     This is the output of the default parameters of the algorithm on the above shown image.
+     *
+     *     ![image](pics/building_lsd.png)
+     *
+     *     @param _image A grayscale (CV_8UC1) input image. If only a roi needs to be selected, use:
+     *     {@code lsd_ptr-&gt;detect(image(roi), lines, ...); lines += Scalar(roi.x, roi.y, roi.x, roi.y);}
+     *     @param _lines A vector of Vec4i or Vec4f elements specifying the beginning and ending point of a line. Where
+     *     Vec4i/Vec4f is (x1, y1, x2, y2), point 1 is the start, point 2 - end. Returned lines are strictly
+     *     oriented depending on the gradient.
+     *     @param width Vector of widths of the regions, where the lines are found. E.g. Width of line.
+     *     bigger the value, logarithmically better the detection.
+     * <ul>
+     *   <li>
+     *      -1 corresponds to 10 mean false alarms
+     *   </li>
+     *   <li>
+     *      0 corresponds to 1 mean false alarm
+     *   </li>
+     *   <li>
+     *      1 corresponds to 0.1 mean false alarms
+     *     This vector will be calculated only when the objects type is #LSD_REFINE_ADV.
+     *   </li>
+     * </ul>
+     */
+    public void detect(Mat _image, Mat _lines, Mat width) {
+        detect_2(nativeObj, _image.nativeObj, _lines.nativeObj, width.nativeObj);
+    }
+
+    /**
+     * Finds lines in the input image.
+     *
+     *     This is the output of the default parameters of the algorithm on the above shown image.
+     *
+     *     ![image](pics/building_lsd.png)
+     *
+     *     @param _image A grayscale (CV_8UC1) input image. If only a roi needs to be selected, use:
+     *     {@code lsd_ptr-&gt;detect(image(roi), lines, ...); lines += Scalar(roi.x, roi.y, roi.x, roi.y);}
+     *     @param _lines A vector of Vec4i or Vec4f elements specifying the beginning and ending point of a line. Where
+     *     Vec4i/Vec4f is (x1, y1, x2, y2), point 1 is the start, point 2 - end. Returned lines are strictly
+     *     oriented depending on the gradient.
+     *     bigger the value, logarithmically better the detection.
+     * <ul>
+     *   <li>
+     *      -1 corresponds to 10 mean false alarms
+     *   </li>
+     *   <li>
+     *      0 corresponds to 1 mean false alarm
+     *   </li>
+     *   <li>
+     *      1 corresponds to 0.1 mean false alarms
+     *     This vector will be calculated only when the objects type is #LSD_REFINE_ADV.
+     *   </li>
+     * </ul>
+     */
+    public void detect(Mat _image, Mat _lines) {
+        detect_3(nativeObj, _image.nativeObj, _lines.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::LineSegmentDetector::drawSegments(Mat& _image, Mat lines)
+    //
+
+    /**
+     * Draws the line segments on a given image.
+     *     @param _image The image, where the lines will be drawn. Should be bigger or equal to the image,
+     *     where the lines were found.
+     *     @param lines A vector of the lines that needed to be drawn.
+     */
+    public void drawSegments(Mat _image, Mat lines) {
+        drawSegments_0(nativeObj, _image.nativeObj, lines.nativeObj);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++:  int cv::LineSegmentDetector::compareSegments(Size size, Mat lines1, Mat lines2, Mat& _image = Mat())
+    private static native int compareSegments_0(long nativeObj, double size_width, double size_height, long lines1_nativeObj, long lines2_nativeObj, long _image_nativeObj);
+    private static native int compareSegments_1(long nativeObj, double size_width, double size_height, long lines1_nativeObj, long lines2_nativeObj);
+
+    // C++:  void cv::LineSegmentDetector::detect(Mat _image, Mat& _lines, Mat& width = Mat(), Mat& prec = Mat(), Mat& nfa = Mat())
+    private static native void detect_0(long nativeObj, long _image_nativeObj, long _lines_nativeObj, long width_nativeObj, long prec_nativeObj, long nfa_nativeObj);
+    private static native void detect_1(long nativeObj, long _image_nativeObj, long _lines_nativeObj, long width_nativeObj, long prec_nativeObj);
+    private static native void detect_2(long nativeObj, long _image_nativeObj, long _lines_nativeObj, long width_nativeObj);
+    private static native void detect_3(long nativeObj, long _image_nativeObj, long _lines_nativeObj);
+
+    // C++:  void cv::LineSegmentDetector::drawSegments(Mat& _image, Mat lines)
+    private static native void drawSegments_0(long nativeObj, long _image_nativeObj, long lines_nativeObj);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/imgproc/Moments.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/imgproc/Moments.java	(date 1605830247776)
+++ openCVLibrary3411/src/main/java/org/opencv/imgproc/Moments.java	(date 1605830247776)
@@ -0,0 +1,242 @@
+package org.opencv.imgproc;
+
+//javadoc:Moments
+public class Moments {
+
+    public double m00;
+    public double m10;
+    public double m01;
+    public double m20;
+    public double m11;
+    public double m02;
+    public double m30;
+    public double m21;
+    public double m12;
+    public double m03;
+
+    public double mu20;
+    public double mu11;
+    public double mu02;
+    public double mu30;
+    public double mu21;
+    public double mu12;
+    public double mu03;
+
+    public double nu20;
+    public double nu11;
+    public double nu02;
+    public double nu30;
+    public double nu21;
+    public double nu12;
+    public double nu03;
+
+    public Moments(
+        double m00,
+        double m10,
+        double m01,
+        double m20,
+        double m11,
+        double m02,
+        double m30,
+        double m21,
+        double m12,
+        double m03)
+    {
+        this.m00 = m00;
+        this.m10 = m10;
+        this.m01 = m01;
+        this.m20 = m20;
+        this.m11 = m11;
+        this.m02 = m02;
+        this.m30 = m30;
+        this.m21 = m21;
+        this.m12 = m12;
+        this.m03 = m03;
+        this.completeState();
+    }
+
+    public Moments() {
+        this(0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
+    }
+
+    public Moments(double[] vals) {
+        set(vals);
+    }
+
+    public void set(double[] vals) {
+        if (vals != null) {
+            m00 = vals.length > 0 ? vals[0] : 0;
+            m10 = vals.length > 1 ? vals[1] : 0;
+            m01 = vals.length > 2 ? vals[2] : 0;
+            m20 = vals.length > 3 ? vals[3] : 0;
+            m11 = vals.length > 4 ? vals[4] : 0;
+            m02 = vals.length > 5 ? vals[5] : 0;
+            m30 = vals.length > 6 ? vals[6] : 0;
+            m21 = vals.length > 7 ? vals[7] : 0;
+            m12 = vals.length > 8 ? vals[8] : 0;
+            m03 = vals.length > 9 ? vals[9] : 0;
+            this.completeState();
+        } else {
+            m00 = 0;
+            m10 = 0;
+            m01 = 0;
+            m20 = 0;
+            m11 = 0;
+            m02 = 0;
+            m30 = 0;
+            m21 = 0;
+            m12 = 0;
+            m03 = 0;
+            mu20 = 0;
+            mu11 = 0;
+            mu02 = 0;
+            mu30 = 0;
+            mu21 = 0;
+            mu12 = 0;
+            mu03 = 0;
+            nu20 = 0;
+            nu11 = 0;
+            nu02 = 0;
+            nu30 = 0;
+            nu21 = 0;
+            nu12 = 0;
+            nu03 = 0;
+        }
+    }
+
+    @Override
+    public String toString() {
+        return "Moments [ " +
+            "\n" +
+            "m00=" + m00 + ", " +
+            "\n" +
+            "m10=" + m10 + ", " +
+            "m01=" + m01 + ", " +
+            "\n" +
+            "m20=" + m20 + ", " +
+            "m11=" + m11 + ", " +
+            "m02=" + m02 + ", " +
+            "\n" +
+            "m30=" + m30 + ", " +
+            "m21=" + m21 + ", " +
+            "m12=" + m12 + ", " +
+            "m03=" + m03 + ", " +
+            "\n" +
+            "mu20=" + mu20 + ", " +
+            "mu11=" + mu11 + ", " +
+            "mu02=" + mu02 + ", " +
+            "\n" +
+            "mu30=" + mu30 + ", " +
+            "mu21=" + mu21 + ", " +
+            "mu12=" + mu12 + ", " +
+            "mu03=" + mu03 + ", " +
+            "\n" +
+            "nu20=" + nu20 + ", " +
+            "nu11=" + nu11 + ", " +
+            "nu02=" + nu02 + ", " +
+            "\n" +
+            "nu30=" + nu30 + ", " +
+            "nu21=" + nu21 + ", " +
+            "nu12=" + nu12 + ", " +
+            "nu03=" + nu03 + ", " +
+            "\n]";
+    }
+
+    protected void completeState()
+    {
+        double cx = 0, cy = 0;
+        double mu20, mu11, mu02;
+        double inv_m00 = 0.0;
+
+        if( Math.abs(this.m00) > 0.00000001 )
+        {
+            inv_m00 = 1. / this.m00;
+            cx = this.m10 * inv_m00;
+            cy = this.m01 * inv_m00;
+        }
+
+        // mu20 = m20 - m10*cx
+        mu20 = this.m20 - this.m10 * cx;
+        // mu11 = m11 - m10*cy
+        mu11 = this.m11 - this.m10 * cy;
+        // mu02 = m02 - m01*cy
+        mu02 = this.m02 - this.m01 * cy;
+
+        this.mu20 = mu20;
+        this.mu11 = mu11;
+        this.mu02 = mu02;
+
+        // mu30 = m30 - cx*(3*mu20 + cx*m10)
+        this.mu30 = this.m30 - cx * (3 * mu20 + cx * this.m10);
+        mu11 += mu11;
+        // mu21 = m21 - cx*(2*mu11 + cx*m01) - cy*mu20
+        this.mu21 = this.m21 - cx * (mu11 + cx * this.m01) - cy * mu20;
+        // mu12 = m12 - cy*(2*mu11 + cy*m10) - cx*mu02
+        this.mu12 = this.m12 - cy * (mu11 + cy * this.m10) - cx * mu02;
+        // mu03 = m03 - cy*(3*mu02 + cy*m01)
+        this.mu03 = this.m03 - cy * (3 * mu02 + cy * this.m01);
+
+
+        double inv_sqrt_m00 = Math.sqrt(Math.abs(inv_m00));
+        double s2 = inv_m00*inv_m00, s3 = s2*inv_sqrt_m00;
+
+        this.nu20 = this.mu20*s2;
+        this.nu11 = this.mu11*s2;
+        this.nu02 = this.mu02*s2;
+        this.nu30 = this.mu30*s3;
+        this.nu21 = this.mu21*s3;
+        this.nu12 = this.mu12*s3;
+        this.nu03 = this.mu03*s3;
+
+    }
+
+    public double get_m00() { return this.m00; }
+    public double get_m10() { return this.m10; }
+    public double get_m01() { return this.m01; }
+    public double get_m20() { return this.m20; }
+    public double get_m11() { return this.m11; }
+    public double get_m02() { return this.m02; }
+    public double get_m30() { return this.m30; }
+    public double get_m21() { return this.m21; }
+    public double get_m12() { return this.m12; }
+    public double get_m03() { return this.m03; }
+    public double get_mu20() { return this.mu20; }
+    public double get_mu11() { return this.mu11; }
+    public double get_mu02() { return this.mu02; }
+    public double get_mu30() { return this.mu30; }
+    public double get_mu21() { return this.mu21; }
+    public double get_mu12() { return this.mu12; }
+    public double get_mu03() { return this.mu03; }
+    public double get_nu20() { return this.nu20; }
+    public double get_nu11() { return this.nu11; }
+    public double get_nu02() { return this.nu02; }
+    public double get_nu30() { return this.nu30; }
+    public double get_nu21() { return this.nu21; }
+    public double get_nu12() { return this.nu12; }
+    public double get_nu03() { return this.nu03; }
+
+    public void set_m00(double m00) { this.m00 = m00; }
+    public void set_m10(double m10) { this.m10 = m10; }
+    public void set_m01(double m01) { this.m01 = m01; }
+    public void set_m20(double m20) { this.m20 = m20; }
+    public void set_m11(double m11) { this.m11 = m11; }
+    public void set_m02(double m02) { this.m02 = m02; }
+    public void set_m30(double m30) { this.m30 = m30; }
+    public void set_m21(double m21) { this.m21 = m21; }
+    public void set_m12(double m12) { this.m12 = m12; }
+    public void set_m03(double m03) { this.m03 = m03; }
+    public void set_mu20(double mu20) { this.mu20 = mu20; }
+    public void set_mu11(double mu11) { this.mu11 = mu11; }
+    public void set_mu02(double mu02) { this.mu02 = mu02; }
+    public void set_mu30(double mu30) { this.mu30 = mu30; }
+    public void set_mu21(double mu21) { this.mu21 = mu21; }
+    public void set_mu12(double mu12) { this.mu12 = mu12; }
+    public void set_mu03(double mu03) { this.mu03 = mu03; }
+    public void set_nu20(double nu20) { this.nu20 = nu20; }
+    public void set_nu11(double nu11) { this.nu11 = nu11; }
+    public void set_nu02(double nu02) { this.nu02 = nu02; }
+    public void set_nu30(double nu30) { this.nu30 = nu30; }
+    public void set_nu21(double nu21) { this.nu21 = nu21; }
+    public void set_nu12(double nu12) { this.nu12 = nu12; }
+    public void set_nu03(double nu03) { this.nu03 = nu03; }
+}
Index: openCVLibrary3411/src/main/java/org/opencv/imgproc/Subdiv2D.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/imgproc/Subdiv2D.java	(date 1605830247778)
+++ openCVLibrary3411/src/main/java/org/opencv/imgproc/Subdiv2D.java	(date 1605830247778)
@@ -0,0 +1,554 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.imgproc;
+
+import java.util.ArrayList;
+import java.util.List;
+import org.opencv.core.Mat;
+import org.opencv.core.MatOfFloat4;
+import org.opencv.core.MatOfFloat6;
+import org.opencv.core.MatOfInt;
+import org.opencv.core.MatOfPoint2f;
+import org.opencv.core.Point;
+import org.opencv.core.Rect;
+import org.opencv.utils.Converters;
+
+// C++: class Subdiv2D
+
+public class Subdiv2D {
+
+    protected final long nativeObj;
+    protected Subdiv2D(long addr) { nativeObj = addr; }
+
+    public long getNativeObjAddr() { return nativeObj; }
+
+    // internal usage only
+    public static Subdiv2D __fromPtr__(long addr) { return new Subdiv2D(addr); }
+
+    // C++: enum <unnamed>
+    public static final int
+            PTLOC_ERROR = -2,
+            PTLOC_OUTSIDE_RECT = -1,
+            PTLOC_INSIDE = 0,
+            PTLOC_VERTEX = 1,
+            PTLOC_ON_EDGE = 2,
+            NEXT_AROUND_ORG = 0x00,
+            NEXT_AROUND_DST = 0x22,
+            PREV_AROUND_ORG = 0x11,
+            PREV_AROUND_DST = 0x33,
+            NEXT_AROUND_LEFT = 0x13,
+            NEXT_AROUND_RIGHT = 0x31,
+            PREV_AROUND_LEFT = 0x20,
+            PREV_AROUND_RIGHT = 0x02;
+
+
+    //
+    // C++:   cv::Subdiv2D::Subdiv2D(Rect rect)
+    //
+
+    /**
+     *
+     *
+     *     @param rect Rectangle that includes all of the 2D points that are to be added to the subdivision.
+     *
+     *     The function creates an empty Delaunay subdivision where 2D points can be added using the function
+     *     insert() . All of the points to be added must be within the specified rectangle, otherwise a runtime
+     *     error is raised.
+     */
+    public Subdiv2D(Rect rect) {
+        nativeObj = Subdiv2D_0(rect.x, rect.y, rect.width, rect.height);
+    }
+
+
+    //
+    // C++:   cv::Subdiv2D::Subdiv2D()
+    //
+
+    /**
+     * creates an empty Subdiv2D object.
+     *     To create a new empty Delaunay subdivision you need to use the #initDelaunay function.
+     */
+    public Subdiv2D() {
+        nativeObj = Subdiv2D_1();
+    }
+
+
+    //
+    // C++:  Point2f cv::Subdiv2D::getVertex(int vertex, int* firstEdge = 0)
+    //
+
+    /**
+     * Returns vertex location from vertex ID.
+     *
+     *     @param vertex vertex ID.
+     *     @param firstEdge Optional. The first edge ID which is connected to the vertex.
+     *     @return vertex (x,y)
+     */
+    public Point getVertex(int vertex, int[] firstEdge) {
+        double[] firstEdge_out = new double[1];
+        Point retVal = new Point(getVertex_0(nativeObj, vertex, firstEdge_out));
+        if(firstEdge!=null) firstEdge[0] = (int)firstEdge_out[0];
+        return retVal;
+    }
+
+    /**
+     * Returns vertex location from vertex ID.
+     *
+     *     @param vertex vertex ID.
+     *     @return vertex (x,y)
+     */
+    public Point getVertex(int vertex) {
+        return new Point(getVertex_1(nativeObj, vertex));
+    }
+
+
+    //
+    // C++:  int cv::Subdiv2D::edgeDst(int edge, Point2f* dstpt = 0)
+    //
+
+    /**
+     * Returns the edge destination.
+     *
+     *     @param edge Subdivision edge ID.
+     *     @param dstpt Output vertex location.
+     *
+     *     @return vertex ID.
+     */
+    public int edgeDst(int edge, Point dstpt) {
+        double[] dstpt_out = new double[2];
+        int retVal = edgeDst_0(nativeObj, edge, dstpt_out);
+        if(dstpt!=null){ dstpt.x = dstpt_out[0]; dstpt.y = dstpt_out[1]; } 
+        return retVal;
+    }
+
+    /**
+     * Returns the edge destination.
+     *
+     *     @param edge Subdivision edge ID.
+     *
+     *     @return vertex ID.
+     */
+    public int edgeDst(int edge) {
+        return edgeDst_1(nativeObj, edge);
+    }
+
+
+    //
+    // C++:  int cv::Subdiv2D::edgeOrg(int edge, Point2f* orgpt = 0)
+    //
+
+    /**
+     * Returns the edge origin.
+     *
+     *     @param edge Subdivision edge ID.
+     *     @param orgpt Output vertex location.
+     *
+     *     @return vertex ID.
+     */
+    public int edgeOrg(int edge, Point orgpt) {
+        double[] orgpt_out = new double[2];
+        int retVal = edgeOrg_0(nativeObj, edge, orgpt_out);
+        if(orgpt!=null){ orgpt.x = orgpt_out[0]; orgpt.y = orgpt_out[1]; } 
+        return retVal;
+    }
+
+    /**
+     * Returns the edge origin.
+     *
+     *     @param edge Subdivision edge ID.
+     *
+     *     @return vertex ID.
+     */
+    public int edgeOrg(int edge) {
+        return edgeOrg_1(nativeObj, edge);
+    }
+
+
+    //
+    // C++:  int cv::Subdiv2D::findNearest(Point2f pt, Point2f* nearestPt = 0)
+    //
+
+    /**
+     * Finds the subdivision vertex closest to the given point.
+     *
+     *     @param pt Input point.
+     *     @param nearestPt Output subdivision vertex point.
+     *
+     *     The function is another function that locates the input point within the subdivision. It finds the
+     *     subdivision vertex that is the closest to the input point. It is not necessarily one of vertices
+     *     of the facet containing the input point, though the facet (located using locate() ) is used as a
+     *     starting point.
+     *
+     *     @return vertex ID.
+     */
+    public int findNearest(Point pt, Point nearestPt) {
+        double[] nearestPt_out = new double[2];
+        int retVal = findNearest_0(nativeObj, pt.x, pt.y, nearestPt_out);
+        if(nearestPt!=null){ nearestPt.x = nearestPt_out[0]; nearestPt.y = nearestPt_out[1]; } 
+        return retVal;
+    }
+
+    /**
+     * Finds the subdivision vertex closest to the given point.
+     *
+     *     @param pt Input point.
+     *
+     *     The function is another function that locates the input point within the subdivision. It finds the
+     *     subdivision vertex that is the closest to the input point. It is not necessarily one of vertices
+     *     of the facet containing the input point, though the facet (located using locate() ) is used as a
+     *     starting point.
+     *
+     *     @return vertex ID.
+     */
+    public int findNearest(Point pt) {
+        return findNearest_1(nativeObj, pt.x, pt.y);
+    }
+
+
+    //
+    // C++:  int cv::Subdiv2D::getEdge(int edge, int nextEdgeType)
+    //
+
+    /**
+     * Returns one of the edges related to the given edge.
+     *
+     *     @param edge Subdivision edge ID.
+     *     @param nextEdgeType Parameter specifying which of the related edges to return.
+     *     The following values are possible:
+     * <ul>
+     *   <li>
+     *        NEXT_AROUND_ORG next around the edge origin ( eOnext on the picture below if e is the input edge)
+     *   </li>
+     *   <li>
+     *        NEXT_AROUND_DST next around the edge vertex ( eDnext )
+     *   </li>
+     *   <li>
+     *        PREV_AROUND_ORG previous around the edge origin (reversed eRnext )
+     *   </li>
+     *   <li>
+     *        PREV_AROUND_DST previous around the edge destination (reversed eLnext )
+     *   </li>
+     *   <li>
+     *        NEXT_AROUND_LEFT next around the left facet ( eLnext )
+     *   </li>
+     *   <li>
+     *        NEXT_AROUND_RIGHT next around the right facet ( eRnext )
+     *   </li>
+     *   <li>
+     *        PREV_AROUND_LEFT previous around the left facet (reversed eOnext )
+     *   </li>
+     *   <li>
+     *        PREV_AROUND_RIGHT previous around the right facet (reversed eDnext )
+     *   </li>
+     * </ul>
+     *
+     *     ![sample output](pics/quadedge.png)
+     *
+     *     @return edge ID related to the input edge.
+     */
+    public int getEdge(int edge, int nextEdgeType) {
+        return getEdge_0(nativeObj, edge, nextEdgeType);
+    }
+
+
+    //
+    // C++:  int cv::Subdiv2D::insert(Point2f pt)
+    //
+
+    /**
+     * Insert a single point into a Delaunay triangulation.
+     *
+     *     @param pt Point to insert.
+     *
+     *     The function inserts a single point into a subdivision and modifies the subdivision topology
+     *     appropriately. If a point with the same coordinates exists already, no new point is added.
+     *     @return the ID of the point.
+     *
+     *     <b>Note:</b> If the point is outside of the triangulation specified rect a runtime error is raised.
+     */
+    public int insert(Point pt) {
+        return insert_0(nativeObj, pt.x, pt.y);
+    }
+
+
+    //
+    // C++:  int cv::Subdiv2D::locate(Point2f pt, int& edge, int& vertex)
+    //
+
+    /**
+     * Returns the location of a point within a Delaunay triangulation.
+     *
+     *     @param pt Point to locate.
+     *     @param edge Output edge that the point belongs to or is located to the right of it.
+     *     @param vertex Optional output vertex the input point coincides with.
+     *
+     *     The function locates the input point within the subdivision and gives one of the triangle edges
+     *     or vertices.
+     *
+     *     @return an integer which specify one of the following five cases for point location:
+     * <ul>
+     *   <li>
+     *       The point falls into some facet. The function returns #PTLOC_INSIDE and edge will contain one of
+     *        edges of the facet.
+     *   </li>
+     *   <li>
+     *       The point falls onto the edge. The function returns #PTLOC_ON_EDGE and edge will contain this edge.
+     *   </li>
+     *   <li>
+     *       The point coincides with one of the subdivision vertices. The function returns #PTLOC_VERTEX and
+     *        vertex will contain a pointer to the vertex.
+     *   </li>
+     *   <li>
+     *       The point is outside the subdivision reference rectangle. The function returns #PTLOC_OUTSIDE_RECT
+     *        and no pointers are filled.
+     *   </li>
+     *   <li>
+     *       One of input arguments is invalid. A runtime error is raised or, if silent or "parent" error
+     *        processing mode is selected, #PTLOC_ERROR is returned.
+     *   </li>
+     * </ul>
+     */
+    public int locate(Point pt, int[] edge, int[] vertex) {
+        double[] edge_out = new double[1];
+        double[] vertex_out = new double[1];
+        int retVal = locate_0(nativeObj, pt.x, pt.y, edge_out, vertex_out);
+        if(edge!=null) edge[0] = (int)edge_out[0];
+        if(vertex!=null) vertex[0] = (int)vertex_out[0];
+        return retVal;
+    }
+
+
+    //
+    // C++:  int cv::Subdiv2D::nextEdge(int edge)
+    //
+
+    /**
+     * Returns next edge around the edge origin.
+     *
+     *     @param edge Subdivision edge ID.
+     *
+     *     @return an integer which is next edge ID around the edge origin: eOnext on the
+     *     picture above if e is the input edge).
+     */
+    public int nextEdge(int edge) {
+        return nextEdge_0(nativeObj, edge);
+    }
+
+
+    //
+    // C++:  int cv::Subdiv2D::rotateEdge(int edge, int rotate)
+    //
+
+    /**
+     * Returns another edge of the same quad-edge.
+     *
+     *     @param edge Subdivision edge ID.
+     *     @param rotate Parameter specifying which of the edges of the same quad-edge as the input
+     *     one to return. The following values are possible:
+     * <ul>
+     *   <li>
+     *        0 - the input edge ( e on the picture below if e is the input edge)
+     *   </li>
+     *   <li>
+     *        1 - the rotated edge ( eRot )
+     *   </li>
+     *   <li>
+     *        2 - the reversed edge (reversed e (in green))
+     *   </li>
+     *   <li>
+     *        3 - the reversed rotated edge (reversed eRot (in green))
+     *   </li>
+     * </ul>
+     *
+     *     @return one of the edges ID of the same quad-edge as the input edge.
+     */
+    public int rotateEdge(int edge, int rotate) {
+        return rotateEdge_0(nativeObj, edge, rotate);
+    }
+
+
+    //
+    // C++:  int cv::Subdiv2D::symEdge(int edge)
+    //
+
+    public int symEdge(int edge) {
+        return symEdge_0(nativeObj, edge);
+    }
+
+
+    //
+    // C++:  void cv::Subdiv2D::getEdgeList(vector_Vec4f& edgeList)
+    //
+
+    /**
+     * Returns a list of all edges.
+     *
+     *     @param edgeList Output vector.
+     *
+     *     The function gives each edge as a 4 numbers vector, where each two are one of the edge
+     *     vertices. i.e. org_x = v[0], org_y = v[1], dst_x = v[2], dst_y = v[3].
+     */
+    public void getEdgeList(MatOfFloat4 edgeList) {
+        Mat edgeList_mat = edgeList;
+        getEdgeList_0(nativeObj, edgeList_mat.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::Subdiv2D::getLeadingEdgeList(vector_int& leadingEdgeList)
+    //
+
+    /**
+     * Returns a list of the leading edge ID connected to each triangle.
+     *
+     *     @param leadingEdgeList Output vector.
+     *
+     *     The function gives one edge ID for each triangle.
+     */
+    public void getLeadingEdgeList(MatOfInt leadingEdgeList) {
+        Mat leadingEdgeList_mat = leadingEdgeList;
+        getLeadingEdgeList_0(nativeObj, leadingEdgeList_mat.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::Subdiv2D::getTriangleList(vector_Vec6f& triangleList)
+    //
+
+    /**
+     * Returns a list of all triangles.
+     *
+     *     @param triangleList Output vector.
+     *
+     *     The function gives each triangle as a 6 numbers vector, where each two are one of the triangle
+     *     vertices. i.e. p1_x = v[0], p1_y = v[1], p2_x = v[2], p2_y = v[3], p3_x = v[4], p3_y = v[5].
+     */
+    public void getTriangleList(MatOfFloat6 triangleList) {
+        Mat triangleList_mat = triangleList;
+        getTriangleList_0(nativeObj, triangleList_mat.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::Subdiv2D::getVoronoiFacetList(vector_int idx, vector_vector_Point2f& facetList, vector_Point2f& facetCenters)
+    //
+
+    /**
+     * Returns a list of all Voronoi facets.
+     *
+     *     @param idx Vector of vertices IDs to consider. For all vertices you can pass empty vector.
+     *     @param facetList Output vector of the Voronoi facets.
+     *     @param facetCenters Output vector of the Voronoi facets center points.
+     */
+    public void getVoronoiFacetList(MatOfInt idx, List<MatOfPoint2f> facetList, MatOfPoint2f facetCenters) {
+        Mat idx_mat = idx;
+        Mat facetList_mat = new Mat();
+        Mat facetCenters_mat = facetCenters;
+        getVoronoiFacetList_0(nativeObj, idx_mat.nativeObj, facetList_mat.nativeObj, facetCenters_mat.nativeObj);
+        Converters.Mat_to_vector_vector_Point2f(facetList_mat, facetList);
+        facetList_mat.release();
+    }
+
+
+    //
+    // C++:  void cv::Subdiv2D::initDelaunay(Rect rect)
+    //
+
+    /**
+     * Creates a new empty Delaunay subdivision
+     *
+     *     @param rect Rectangle that includes all of the 2D points that are to be added to the subdivision.
+     */
+    public void initDelaunay(Rect rect) {
+        initDelaunay_0(nativeObj, rect.x, rect.y, rect.width, rect.height);
+    }
+
+
+    //
+    // C++:  void cv::Subdiv2D::insert(vector_Point2f ptvec)
+    //
+
+    /**
+     * Insert multiple points into a Delaunay triangulation.
+     *
+     *     @param ptvec Points to insert.
+     *
+     *     The function inserts a vector of points into a subdivision and modifies the subdivision topology
+     *     appropriately.
+     */
+    public void insert(MatOfPoint2f ptvec) {
+        Mat ptvec_mat = ptvec;
+        insert_1(nativeObj, ptvec_mat.nativeObj);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++:   cv::Subdiv2D::Subdiv2D(Rect rect)
+    private static native long Subdiv2D_0(int rect_x, int rect_y, int rect_width, int rect_height);
+
+    // C++:   cv::Subdiv2D::Subdiv2D()
+    private static native long Subdiv2D_1();
+
+    // C++:  Point2f cv::Subdiv2D::getVertex(int vertex, int* firstEdge = 0)
+    private static native double[] getVertex_0(long nativeObj, int vertex, double[] firstEdge_out);
+    private static native double[] getVertex_1(long nativeObj, int vertex);
+
+    // C++:  int cv::Subdiv2D::edgeDst(int edge, Point2f* dstpt = 0)
+    private static native int edgeDst_0(long nativeObj, int edge, double[] dstpt_out);
+    private static native int edgeDst_1(long nativeObj, int edge);
+
+    // C++:  int cv::Subdiv2D::edgeOrg(int edge, Point2f* orgpt = 0)
+    private static native int edgeOrg_0(long nativeObj, int edge, double[] orgpt_out);
+    private static native int edgeOrg_1(long nativeObj, int edge);
+
+    // C++:  int cv::Subdiv2D::findNearest(Point2f pt, Point2f* nearestPt = 0)
+    private static native int findNearest_0(long nativeObj, double pt_x, double pt_y, double[] nearestPt_out);
+    private static native int findNearest_1(long nativeObj, double pt_x, double pt_y);
+
+    // C++:  int cv::Subdiv2D::getEdge(int edge, int nextEdgeType)
+    private static native int getEdge_0(long nativeObj, int edge, int nextEdgeType);
+
+    // C++:  int cv::Subdiv2D::insert(Point2f pt)
+    private static native int insert_0(long nativeObj, double pt_x, double pt_y);
+
+    // C++:  int cv::Subdiv2D::locate(Point2f pt, int& edge, int& vertex)
+    private static native int locate_0(long nativeObj, double pt_x, double pt_y, double[] edge_out, double[] vertex_out);
+
+    // C++:  int cv::Subdiv2D::nextEdge(int edge)
+    private static native int nextEdge_0(long nativeObj, int edge);
+
+    // C++:  int cv::Subdiv2D::rotateEdge(int edge, int rotate)
+    private static native int rotateEdge_0(long nativeObj, int edge, int rotate);
+
+    // C++:  int cv::Subdiv2D::symEdge(int edge)
+    private static native int symEdge_0(long nativeObj, int edge);
+
+    // C++:  void cv::Subdiv2D::getEdgeList(vector_Vec4f& edgeList)
+    private static native void getEdgeList_0(long nativeObj, long edgeList_mat_nativeObj);
+
+    // C++:  void cv::Subdiv2D::getLeadingEdgeList(vector_int& leadingEdgeList)
+    private static native void getLeadingEdgeList_0(long nativeObj, long leadingEdgeList_mat_nativeObj);
+
+    // C++:  void cv::Subdiv2D::getTriangleList(vector_Vec6f& triangleList)
+    private static native void getTriangleList_0(long nativeObj, long triangleList_mat_nativeObj);
+
+    // C++:  void cv::Subdiv2D::getVoronoiFacetList(vector_int idx, vector_vector_Point2f& facetList, vector_Point2f& facetCenters)
+    private static native void getVoronoiFacetList_0(long nativeObj, long idx_mat_nativeObj, long facetList_mat_nativeObj, long facetCenters_mat_nativeObj);
+
+    // C++:  void cv::Subdiv2D::initDelaunay(Rect rect)
+    private static native void initDelaunay_0(long nativeObj, int rect_x, int rect_y, int rect_width, int rect_height);
+
+    // C++:  void cv::Subdiv2D::insert(vector_Point2f ptvec)
+    private static native void insert_1(long nativeObj, long ptvec_mat_nativeObj);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/imgproc/CLAHE.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/imgproc/CLAHE.java	(date 1605830247725)
+++ openCVLibrary3411/src/main/java/org/opencv/imgproc/CLAHE.java	(date 1605830247725)
@@ -0,0 +1,120 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.imgproc;
+
+import org.opencv.core.Algorithm;
+import org.opencv.core.Mat;
+import org.opencv.core.Size;
+
+// C++: class CLAHE
+/**
+ * Base class for Contrast Limited Adaptive Histogram Equalization.
+ */
+public class CLAHE extends Algorithm {
+
+    protected CLAHE(long addr) { super(addr); }
+
+    // internal usage only
+    public static CLAHE __fromPtr__(long addr) { return new CLAHE(addr); }
+
+    //
+    // C++:  Size cv::CLAHE::getTilesGridSize()
+    //
+
+    public Size getTilesGridSize() {
+        return new Size(getTilesGridSize_0(nativeObj));
+    }
+
+
+    //
+    // C++:  double cv::CLAHE::getClipLimit()
+    //
+
+    public double getClipLimit() {
+        return getClipLimit_0(nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::CLAHE::apply(Mat src, Mat& dst)
+    //
+
+    /**
+     * Equalizes the histogram of a grayscale image using Contrast Limited Adaptive Histogram Equalization.
+     *
+     *     @param src Source image of type CV_8UC1 or CV_16UC1.
+     *     @param dst Destination image.
+     */
+    public void apply(Mat src, Mat dst) {
+        apply_0(nativeObj, src.nativeObj, dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::CLAHE::collectGarbage()
+    //
+
+    public void collectGarbage() {
+        collectGarbage_0(nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::CLAHE::setClipLimit(double clipLimit)
+    //
+
+    /**
+     * Sets threshold for contrast limiting.
+     *
+     *     @param clipLimit threshold value.
+     */
+    public void setClipLimit(double clipLimit) {
+        setClipLimit_0(nativeObj, clipLimit);
+    }
+
+
+    //
+    // C++:  void cv::CLAHE::setTilesGridSize(Size tileGridSize)
+    //
+
+    /**
+     * Sets size of grid for histogram equalization. Input image will be divided into
+     *     equally sized rectangular tiles.
+     *
+     *     @param tileGridSize defines the number of tiles in row and column.
+     */
+    public void setTilesGridSize(Size tileGridSize) {
+        setTilesGridSize_0(nativeObj, tileGridSize.width, tileGridSize.height);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++:  Size cv::CLAHE::getTilesGridSize()
+    private static native double[] getTilesGridSize_0(long nativeObj);
+
+    // C++:  double cv::CLAHE::getClipLimit()
+    private static native double getClipLimit_0(long nativeObj);
+
+    // C++:  void cv::CLAHE::apply(Mat src, Mat& dst)
+    private static native void apply_0(long nativeObj, long src_nativeObj, long dst_nativeObj);
+
+    // C++:  void cv::CLAHE::collectGarbage()
+    private static native void collectGarbage_0(long nativeObj);
+
+    // C++:  void cv::CLAHE::setClipLimit(double clipLimit)
+    private static native void setClipLimit_0(long nativeObj, double clipLimit);
+
+    // C++:  void cv::CLAHE::setTilesGridSize(Size tileGridSize)
+    private static native void setTilesGridSize_0(long nativeObj, double tileGridSize_width, double tileGridSize_height);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/imgproc/GeneralizedHough.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/imgproc/GeneralizedHough.java	(date 1605830247730)
+++ openCVLibrary3411/src/main/java/org/opencv/imgproc/GeneralizedHough.java	(date 1605830247730)
@@ -0,0 +1,219 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.imgproc;
+
+import org.opencv.core.Algorithm;
+import org.opencv.core.Mat;
+import org.opencv.core.Point;
+
+// C++: class GeneralizedHough
+/**
+ * finds arbitrary template in the grayscale image using Generalized Hough Transform
+ */
+public class GeneralizedHough extends Algorithm {
+
+    protected GeneralizedHough(long addr) { super(addr); }
+
+    // internal usage only
+    public static GeneralizedHough __fromPtr__(long addr) { return new GeneralizedHough(addr); }
+
+    //
+    // C++:  double cv::GeneralizedHough::getDp()
+    //
+
+    public double getDp() {
+        return getDp_0(nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::GeneralizedHough::getMinDist()
+    //
+
+    public double getMinDist() {
+        return getMinDist_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::GeneralizedHough::getCannyHighThresh()
+    //
+
+    public int getCannyHighThresh() {
+        return getCannyHighThresh_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::GeneralizedHough::getCannyLowThresh()
+    //
+
+    public int getCannyLowThresh() {
+        return getCannyLowThresh_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::GeneralizedHough::getMaxBufferSize()
+    //
+
+    public int getMaxBufferSize() {
+        return getMaxBufferSize_0(nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::GeneralizedHough::detect(Mat edges, Mat dx, Mat dy, Mat& positions, Mat& votes = Mat())
+    //
+
+    public void detect(Mat edges, Mat dx, Mat dy, Mat positions, Mat votes) {
+        detect_0(nativeObj, edges.nativeObj, dx.nativeObj, dy.nativeObj, positions.nativeObj, votes.nativeObj);
+    }
+
+    public void detect(Mat edges, Mat dx, Mat dy, Mat positions) {
+        detect_1(nativeObj, edges.nativeObj, dx.nativeObj, dy.nativeObj, positions.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::GeneralizedHough::detect(Mat image, Mat& positions, Mat& votes = Mat())
+    //
+
+    public void detect(Mat image, Mat positions, Mat votes) {
+        detect_2(nativeObj, image.nativeObj, positions.nativeObj, votes.nativeObj);
+    }
+
+    public void detect(Mat image, Mat positions) {
+        detect_3(nativeObj, image.nativeObj, positions.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::GeneralizedHough::setCannyHighThresh(int cannyHighThresh)
+    //
+
+    public void setCannyHighThresh(int cannyHighThresh) {
+        setCannyHighThresh_0(nativeObj, cannyHighThresh);
+    }
+
+
+    //
+    // C++:  void cv::GeneralizedHough::setCannyLowThresh(int cannyLowThresh)
+    //
+
+    public void setCannyLowThresh(int cannyLowThresh) {
+        setCannyLowThresh_0(nativeObj, cannyLowThresh);
+    }
+
+
+    //
+    // C++:  void cv::GeneralizedHough::setDp(double dp)
+    //
+
+    public void setDp(double dp) {
+        setDp_0(nativeObj, dp);
+    }
+
+
+    //
+    // C++:  void cv::GeneralizedHough::setMaxBufferSize(int maxBufferSize)
+    //
+
+    public void setMaxBufferSize(int maxBufferSize) {
+        setMaxBufferSize_0(nativeObj, maxBufferSize);
+    }
+
+
+    //
+    // C++:  void cv::GeneralizedHough::setMinDist(double minDist)
+    //
+
+    public void setMinDist(double minDist) {
+        setMinDist_0(nativeObj, minDist);
+    }
+
+
+    //
+    // C++:  void cv::GeneralizedHough::setTemplate(Mat edges, Mat dx, Mat dy, Point templCenter = Point(-1, -1))
+    //
+
+    public void setTemplate(Mat edges, Mat dx, Mat dy, Point templCenter) {
+        setTemplate_0(nativeObj, edges.nativeObj, dx.nativeObj, dy.nativeObj, templCenter.x, templCenter.y);
+    }
+
+    public void setTemplate(Mat edges, Mat dx, Mat dy) {
+        setTemplate_1(nativeObj, edges.nativeObj, dx.nativeObj, dy.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::GeneralizedHough::setTemplate(Mat templ, Point templCenter = Point(-1, -1))
+    //
+
+    public void setTemplate(Mat templ, Point templCenter) {
+        setTemplate_2(nativeObj, templ.nativeObj, templCenter.x, templCenter.y);
+    }
+
+    public void setTemplate(Mat templ) {
+        setTemplate_3(nativeObj, templ.nativeObj);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++:  double cv::GeneralizedHough::getDp()
+    private static native double getDp_0(long nativeObj);
+
+    // C++:  double cv::GeneralizedHough::getMinDist()
+    private static native double getMinDist_0(long nativeObj);
+
+    // C++:  int cv::GeneralizedHough::getCannyHighThresh()
+    private static native int getCannyHighThresh_0(long nativeObj);
+
+    // C++:  int cv::GeneralizedHough::getCannyLowThresh()
+    private static native int getCannyLowThresh_0(long nativeObj);
+
+    // C++:  int cv::GeneralizedHough::getMaxBufferSize()
+    private static native int getMaxBufferSize_0(long nativeObj);
+
+    // C++:  void cv::GeneralizedHough::detect(Mat edges, Mat dx, Mat dy, Mat& positions, Mat& votes = Mat())
+    private static native void detect_0(long nativeObj, long edges_nativeObj, long dx_nativeObj, long dy_nativeObj, long positions_nativeObj, long votes_nativeObj);
+    private static native void detect_1(long nativeObj, long edges_nativeObj, long dx_nativeObj, long dy_nativeObj, long positions_nativeObj);
+
+    // C++:  void cv::GeneralizedHough::detect(Mat image, Mat& positions, Mat& votes = Mat())
+    private static native void detect_2(long nativeObj, long image_nativeObj, long positions_nativeObj, long votes_nativeObj);
+    private static native void detect_3(long nativeObj, long image_nativeObj, long positions_nativeObj);
+
+    // C++:  void cv::GeneralizedHough::setCannyHighThresh(int cannyHighThresh)
+    private static native void setCannyHighThresh_0(long nativeObj, int cannyHighThresh);
+
+    // C++:  void cv::GeneralizedHough::setCannyLowThresh(int cannyLowThresh)
+    private static native void setCannyLowThresh_0(long nativeObj, int cannyLowThresh);
+
+    // C++:  void cv::GeneralizedHough::setDp(double dp)
+    private static native void setDp_0(long nativeObj, double dp);
+
+    // C++:  void cv::GeneralizedHough::setMaxBufferSize(int maxBufferSize)
+    private static native void setMaxBufferSize_0(long nativeObj, int maxBufferSize);
+
+    // C++:  void cv::GeneralizedHough::setMinDist(double minDist)
+    private static native void setMinDist_0(long nativeObj, double minDist);
+
+    // C++:  void cv::GeneralizedHough::setTemplate(Mat edges, Mat dx, Mat dy, Point templCenter = Point(-1, -1))
+    private static native void setTemplate_0(long nativeObj, long edges_nativeObj, long dx_nativeObj, long dy_nativeObj, double templCenter_x, double templCenter_y);
+    private static native void setTemplate_1(long nativeObj, long edges_nativeObj, long dx_nativeObj, long dy_nativeObj);
+
+    // C++:  void cv::GeneralizedHough::setTemplate(Mat templ, Point templCenter = Point(-1, -1))
+    private static native void setTemplate_2(long nativeObj, long templ_nativeObj, double templCenter_x, double templCenter_y);
+    private static native void setTemplate_3(long nativeObj, long templ_nativeObj);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/imgproc/GeneralizedHoughBallard.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/imgproc/GeneralizedHoughBallard.java	(date 1605830247743)
+++ openCVLibrary3411/src/main/java/org/opencv/imgproc/GeneralizedHoughBallard.java	(date 1605830247743)
@@ -0,0 +1,79 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.imgproc;
+
+import org.opencv.imgproc.GeneralizedHough;
+
+// C++: class GeneralizedHoughBallard
+/**
+ * finds arbitrary template in the grayscale image using Generalized Hough Transform
+ *
+ * Detects position only without translation and rotation CITE: Ballard1981 .
+ */
+public class GeneralizedHoughBallard extends GeneralizedHough {
+
+    protected GeneralizedHoughBallard(long addr) { super(addr); }
+
+    // internal usage only
+    public static GeneralizedHoughBallard __fromPtr__(long addr) { return new GeneralizedHoughBallard(addr); }
+
+    //
+    // C++:  int cv::GeneralizedHoughBallard::getLevels()
+    //
+
+    public int getLevels() {
+        return getLevels_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::GeneralizedHoughBallard::getVotesThreshold()
+    //
+
+    public int getVotesThreshold() {
+        return getVotesThreshold_0(nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::GeneralizedHoughBallard::setLevels(int levels)
+    //
+
+    public void setLevels(int levels) {
+        setLevels_0(nativeObj, levels);
+    }
+
+
+    //
+    // C++:  void cv::GeneralizedHoughBallard::setVotesThreshold(int votesThreshold)
+    //
+
+    public void setVotesThreshold(int votesThreshold) {
+        setVotesThreshold_0(nativeObj, votesThreshold);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++:  int cv::GeneralizedHoughBallard::getLevels()
+    private static native int getLevels_0(long nativeObj);
+
+    // C++:  int cv::GeneralizedHoughBallard::getVotesThreshold()
+    private static native int getVotesThreshold_0(long nativeObj);
+
+    // C++:  void cv::GeneralizedHoughBallard::setLevels(int levels)
+    private static native void setLevels_0(long nativeObj, int levels);
+
+    // C++:  void cv::GeneralizedHoughBallard::setVotesThreshold(int votesThreshold)
+    private static native void setVotesThreshold_0(long nativeObj, int votesThreshold);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/imgproc/GeneralizedHoughGuil.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/imgproc/GeneralizedHoughGuil.java	(date 1605830247745)
+++ openCVLibrary3411/src/main/java/org/opencv/imgproc/GeneralizedHoughGuil.java	(date 1605830247745)
@@ -0,0 +1,319 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.imgproc;
+
+import org.opencv.imgproc.GeneralizedHough;
+
+// C++: class GeneralizedHoughGuil
+/**
+ * finds arbitrary template in the grayscale image using Generalized Hough Transform
+ *
+ * Detects position, translation and rotation CITE: Guil1999 .
+ */
+public class GeneralizedHoughGuil extends GeneralizedHough {
+
+    protected GeneralizedHoughGuil(long addr) { super(addr); }
+
+    // internal usage only
+    public static GeneralizedHoughGuil __fromPtr__(long addr) { return new GeneralizedHoughGuil(addr); }
+
+    //
+    // C++:  double cv::GeneralizedHoughGuil::getAngleEpsilon()
+    //
+
+    public double getAngleEpsilon() {
+        return getAngleEpsilon_0(nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::GeneralizedHoughGuil::getAngleStep()
+    //
+
+    public double getAngleStep() {
+        return getAngleStep_0(nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::GeneralizedHoughGuil::getMaxAngle()
+    //
+
+    public double getMaxAngle() {
+        return getMaxAngle_0(nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::GeneralizedHoughGuil::getMaxScale()
+    //
+
+    public double getMaxScale() {
+        return getMaxScale_0(nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::GeneralizedHoughGuil::getMinAngle()
+    //
+
+    public double getMinAngle() {
+        return getMinAngle_0(nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::GeneralizedHoughGuil::getMinScale()
+    //
+
+    public double getMinScale() {
+        return getMinScale_0(nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::GeneralizedHoughGuil::getScaleStep()
+    //
+
+    public double getScaleStep() {
+        return getScaleStep_0(nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::GeneralizedHoughGuil::getXi()
+    //
+
+    public double getXi() {
+        return getXi_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::GeneralizedHoughGuil::getAngleThresh()
+    //
+
+    public int getAngleThresh() {
+        return getAngleThresh_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::GeneralizedHoughGuil::getLevels()
+    //
+
+    public int getLevels() {
+        return getLevels_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::GeneralizedHoughGuil::getPosThresh()
+    //
+
+    public int getPosThresh() {
+        return getPosThresh_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::GeneralizedHoughGuil::getScaleThresh()
+    //
+
+    public int getScaleThresh() {
+        return getScaleThresh_0(nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::GeneralizedHoughGuil::setAngleEpsilon(double angleEpsilon)
+    //
+
+    public void setAngleEpsilon(double angleEpsilon) {
+        setAngleEpsilon_0(nativeObj, angleEpsilon);
+    }
+
+
+    //
+    // C++:  void cv::GeneralizedHoughGuil::setAngleStep(double angleStep)
+    //
+
+    public void setAngleStep(double angleStep) {
+        setAngleStep_0(nativeObj, angleStep);
+    }
+
+
+    //
+    // C++:  void cv::GeneralizedHoughGuil::setAngleThresh(int angleThresh)
+    //
+
+    public void setAngleThresh(int angleThresh) {
+        setAngleThresh_0(nativeObj, angleThresh);
+    }
+
+
+    //
+    // C++:  void cv::GeneralizedHoughGuil::setLevels(int levels)
+    //
+
+    public void setLevels(int levels) {
+        setLevels_0(nativeObj, levels);
+    }
+
+
+    //
+    // C++:  void cv::GeneralizedHoughGuil::setMaxAngle(double maxAngle)
+    //
+
+    public void setMaxAngle(double maxAngle) {
+        setMaxAngle_0(nativeObj, maxAngle);
+    }
+
+
+    //
+    // C++:  void cv::GeneralizedHoughGuil::setMaxScale(double maxScale)
+    //
+
+    public void setMaxScale(double maxScale) {
+        setMaxScale_0(nativeObj, maxScale);
+    }
+
+
+    //
+    // C++:  void cv::GeneralizedHoughGuil::setMinAngle(double minAngle)
+    //
+
+    public void setMinAngle(double minAngle) {
+        setMinAngle_0(nativeObj, minAngle);
+    }
+
+
+    //
+    // C++:  void cv::GeneralizedHoughGuil::setMinScale(double minScale)
+    //
+
+    public void setMinScale(double minScale) {
+        setMinScale_0(nativeObj, minScale);
+    }
+
+
+    //
+    // C++:  void cv::GeneralizedHoughGuil::setPosThresh(int posThresh)
+    //
+
+    public void setPosThresh(int posThresh) {
+        setPosThresh_0(nativeObj, posThresh);
+    }
+
+
+    //
+    // C++:  void cv::GeneralizedHoughGuil::setScaleStep(double scaleStep)
+    //
+
+    public void setScaleStep(double scaleStep) {
+        setScaleStep_0(nativeObj, scaleStep);
+    }
+
+
+    //
+    // C++:  void cv::GeneralizedHoughGuil::setScaleThresh(int scaleThresh)
+    //
+
+    public void setScaleThresh(int scaleThresh) {
+        setScaleThresh_0(nativeObj, scaleThresh);
+    }
+
+
+    //
+    // C++:  void cv::GeneralizedHoughGuil::setXi(double xi)
+    //
+
+    public void setXi(double xi) {
+        setXi_0(nativeObj, xi);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++:  double cv::GeneralizedHoughGuil::getAngleEpsilon()
+    private static native double getAngleEpsilon_0(long nativeObj);
+
+    // C++:  double cv::GeneralizedHoughGuil::getAngleStep()
+    private static native double getAngleStep_0(long nativeObj);
+
+    // C++:  double cv::GeneralizedHoughGuil::getMaxAngle()
+    private static native double getMaxAngle_0(long nativeObj);
+
+    // C++:  double cv::GeneralizedHoughGuil::getMaxScale()
+    private static native double getMaxScale_0(long nativeObj);
+
+    // C++:  double cv::GeneralizedHoughGuil::getMinAngle()
+    private static native double getMinAngle_0(long nativeObj);
+
+    // C++:  double cv::GeneralizedHoughGuil::getMinScale()
+    private static native double getMinScale_0(long nativeObj);
+
+    // C++:  double cv::GeneralizedHoughGuil::getScaleStep()
+    private static native double getScaleStep_0(long nativeObj);
+
+    // C++:  double cv::GeneralizedHoughGuil::getXi()
+    private static native double getXi_0(long nativeObj);
+
+    // C++:  int cv::GeneralizedHoughGuil::getAngleThresh()
+    private static native int getAngleThresh_0(long nativeObj);
+
+    // C++:  int cv::GeneralizedHoughGuil::getLevels()
+    private static native int getLevels_0(long nativeObj);
+
+    // C++:  int cv::GeneralizedHoughGuil::getPosThresh()
+    private static native int getPosThresh_0(long nativeObj);
+
+    // C++:  int cv::GeneralizedHoughGuil::getScaleThresh()
+    private static native int getScaleThresh_0(long nativeObj);
+
+    // C++:  void cv::GeneralizedHoughGuil::setAngleEpsilon(double angleEpsilon)
+    private static native void setAngleEpsilon_0(long nativeObj, double angleEpsilon);
+
+    // C++:  void cv::GeneralizedHoughGuil::setAngleStep(double angleStep)
+    private static native void setAngleStep_0(long nativeObj, double angleStep);
+
+    // C++:  void cv::GeneralizedHoughGuil::setAngleThresh(int angleThresh)
+    private static native void setAngleThresh_0(long nativeObj, int angleThresh);
+
+    // C++:  void cv::GeneralizedHoughGuil::setLevels(int levels)
+    private static native void setLevels_0(long nativeObj, int levels);
+
+    // C++:  void cv::GeneralizedHoughGuil::setMaxAngle(double maxAngle)
+    private static native void setMaxAngle_0(long nativeObj, double maxAngle);
+
+    // C++:  void cv::GeneralizedHoughGuil::setMaxScale(double maxScale)
+    private static native void setMaxScale_0(long nativeObj, double maxScale);
+
+    // C++:  void cv::GeneralizedHoughGuil::setMinAngle(double minAngle)
+    private static native void setMinAngle_0(long nativeObj, double minAngle);
+
+    // C++:  void cv::GeneralizedHoughGuil::setMinScale(double minScale)
+    private static native void setMinScale_0(long nativeObj, double minScale);
+
+    // C++:  void cv::GeneralizedHoughGuil::setPosThresh(int posThresh)
+    private static native void setPosThresh_0(long nativeObj, int posThresh);
+
+    // C++:  void cv::GeneralizedHoughGuil::setScaleStep(double scaleStep)
+    private static native void setScaleStep_0(long nativeObj, double scaleStep);
+
+    // C++:  void cv::GeneralizedHoughGuil::setScaleThresh(int scaleThresh)
+    private static native void setScaleThresh_0(long nativeObj, int scaleThresh);
+
+    // C++:  void cv::GeneralizedHoughGuil::setXi(double xi)
+    private static native void setXi_0(long nativeObj, double xi);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/calib3d/Calib3d.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/calib3d/Calib3d.java	(date 1605830247210)
+++ openCVLibrary3411/src/main/java/org/opencv/calib3d/Calib3d.java	(date 1605830247210)
@@ -0,0 +1,10765 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.calib3d;
+
+import java.util.ArrayList;
+import java.util.List;
+import org.opencv.core.Mat;
+import org.opencv.core.MatOfDouble;
+import org.opencv.core.MatOfPoint2f;
+import org.opencv.core.MatOfPoint3f;
+import org.opencv.core.Point;
+import org.opencv.core.Rect;
+import org.opencv.core.Size;
+import org.opencv.core.TermCriteria;
+import org.opencv.utils.Converters;
+
+// C++: class Calib3d
+
+public class Calib3d {
+
+    // C++: enum <unnamed>
+    public static final int
+            CV_ITERATIVE = 0,
+            CV_EPNP = 1,
+            CV_P3P = 2,
+            CV_DLS = 3,
+            CvLevMarq_DONE = 0,
+            CvLevMarq_STARTED = 1,
+            CvLevMarq_CALC_J = 2,
+            CvLevMarq_CHECK_ERR = 3,
+            LMEDS = 4,
+            RANSAC = 8,
+            RHO = 16,
+            CALIB_CB_ADAPTIVE_THRESH = 1,
+            CALIB_CB_NORMALIZE_IMAGE = 2,
+            CALIB_CB_FILTER_QUADS = 4,
+            CALIB_CB_FAST_CHECK = 8,
+            CALIB_CB_SYMMETRIC_GRID = 1,
+            CALIB_CB_ASYMMETRIC_GRID = 2,
+            CALIB_CB_CLUSTERING = 4,
+            CALIB_USE_INTRINSIC_GUESS = 0x00001,
+            CALIB_FIX_ASPECT_RATIO = 0x00002,
+            CALIB_FIX_PRINCIPAL_POINT = 0x00004,
+            CALIB_ZERO_TANGENT_DIST = 0x00008,
+            CALIB_FIX_FOCAL_LENGTH = 0x00010,
+            CALIB_FIX_K1 = 0x00020,
+            CALIB_FIX_K2 = 0x00040,
+            CALIB_FIX_K3 = 0x00080,
+            CALIB_FIX_K4 = 0x00800,
+            CALIB_FIX_K5 = 0x01000,
+            CALIB_FIX_K6 = 0x02000,
+            CALIB_RATIONAL_MODEL = 0x04000,
+            CALIB_THIN_PRISM_MODEL = 0x08000,
+            CALIB_FIX_S1_S2_S3_S4 = 0x10000,
+            CALIB_TILTED_MODEL = 0x40000,
+            CALIB_FIX_TAUX_TAUY = 0x80000,
+            CALIB_USE_QR = 0x100000,
+            CALIB_FIX_TANGENT_DIST = 0x200000,
+            CALIB_FIX_INTRINSIC = 0x00100,
+            CALIB_SAME_FOCAL_LENGTH = 0x00200,
+            CALIB_ZERO_DISPARITY = 0x00400,
+            CALIB_USE_LU = (1 << 17),
+            CALIB_USE_EXTRINSIC_GUESS = (1 << 22),
+            FM_7POINT = 1,
+            FM_8POINT = 2,
+            FM_LMEDS = 4,
+            FM_RANSAC = 8,
+            fisheye_CALIB_USE_INTRINSIC_GUESS = 1 << 0,
+            fisheye_CALIB_RECOMPUTE_EXTRINSIC = 1 << 1,
+            fisheye_CALIB_CHECK_COND = 1 << 2,
+            fisheye_CALIB_FIX_SKEW = 1 << 3,
+            fisheye_CALIB_FIX_K1 = 1 << 4,
+            fisheye_CALIB_FIX_K2 = 1 << 5,
+            fisheye_CALIB_FIX_K3 = 1 << 6,
+            fisheye_CALIB_FIX_K4 = 1 << 7,
+            fisheye_CALIB_FIX_INTRINSIC = 1 << 8,
+            fisheye_CALIB_FIX_PRINCIPAL_POINT = 1 << 9;
+
+
+    // C++: enum SolvePnPMethod
+    public static final int
+            SOLVEPNP_ITERATIVE = 0,
+            SOLVEPNP_EPNP = 1,
+            SOLVEPNP_P3P = 2,
+            SOLVEPNP_DLS = 3,
+            SOLVEPNP_UPNP = 4,
+            SOLVEPNP_AP3P = 5,
+            SOLVEPNP_IPPE = 6,
+            SOLVEPNP_IPPE_SQUARE = 7,
+            SOLVEPNP_MAX_COUNT = 7+1;
+
+
+    // C++: enum HandEyeCalibrationMethod
+    public static final int
+            CALIB_HAND_EYE_TSAI = 0,
+            CALIB_HAND_EYE_PARK = 1,
+            CALIB_HAND_EYE_HORAUD = 2,
+            CALIB_HAND_EYE_ANDREFF = 3,
+            CALIB_HAND_EYE_DANIILIDIS = 4;
+
+
+    // C++: enum GridType
+    public static final int
+            CirclesGridFinderParameters_SYMMETRIC_GRID = 0,
+            CirclesGridFinderParameters_ASYMMETRIC_GRID = 1;
+
+
+    //
+    // C++:  Mat cv::estimateAffine2D(Mat from, Mat to, Mat& inliers = Mat(), int method = RANSAC, double ransacReprojThreshold = 3, size_t maxIters = 2000, double confidence = 0.99, size_t refineIters = 10)
+    //
+
+    /**
+     * Computes an optimal affine transformation between two 2D point sets.
+     *
+     * It computes
+     * \(
+     * \begin{bmatrix}
+     * x\\
+     * y\\
+     * \end{bmatrix}
+     * =
+     * \begin{bmatrix}
+     * a_{11} &amp; a_{12}\\
+     * a_{21} &amp; a_{22}\\
+     * \end{bmatrix}
+     * \begin{bmatrix}
+     * X\\
+     * Y\\
+     * \end{bmatrix}
+     * +
+     * \begin{bmatrix}
+     * b_1\\
+     * b_2\\
+     * \end{bmatrix}
+     * \)
+     *
+     * @param from First input 2D point set containing \((X,Y)\).
+     * @param to Second input 2D point set containing \((x,y)\).
+     * @param inliers Output vector indicating which points are inliers (1-inlier, 0-outlier).
+     * @param method Robust method used to compute transformation. The following methods are possible:
+     * <ul>
+     *   <li>
+     *    cv::RANSAC - RANSAC-based robust method
+     *   </li>
+     *   <li>
+     *    cv::LMEDS - Least-Median robust method
+     * RANSAC is the default method.
+     * @param ransacReprojThreshold Maximum reprojection error in the RANSAC algorithm to consider
+     * a point as an inlier. Applies only to RANSAC.
+     * @param maxIters The maximum number of robust method iterations.
+     * @param confidence Confidence level, between 0 and 1, for the estimated transformation. Anything
+     * between 0.95 and 0.99 is usually good enough. Values too close to 1 can slow down the estimation
+     * significantly. Values lower than 0.8-0.9 can result in an incorrectly estimated transformation.
+     * @param refineIters Maximum number of iterations of refining algorithm (Levenberg-Marquardt).
+     * Passing 0 will disable refining, so the output matrix will be output of robust method.
+     *   </li>
+     * </ul>
+     *
+     * @return Output 2D affine transformation matrix \(2 \times 3\) or empty matrix if transformation
+     * could not be estimated. The returned matrix has the following form:
+     * \(
+     * \begin{bmatrix}
+     * a_{11} &amp; a_{12} &amp; b_1\\
+     * a_{21} &amp; a_{22} &amp; b_2\\
+     * \end{bmatrix}
+     * \)
+     *
+     * The function estimates an optimal 2D affine transformation between two 2D point sets using the
+     * selected robust algorithm.
+     *
+     * The computed transformation is then refined further (using only inliers) with the
+     * Levenberg-Marquardt method to reduce the re-projection error even more.
+     *
+     * <b>Note:</b>
+     * The RANSAC method can handle practically any ratio of outliers but needs a threshold to
+     * distinguish inliers from outliers. The method LMeDS does not need any threshold but it works
+     * correctly only when there are more than 50% of inliers.
+     *
+     * SEE: estimateAffinePartial2D, getAffineTransform
+     */
+    public static Mat estimateAffine2D(Mat from, Mat to, Mat inliers, int method, double ransacReprojThreshold, long maxIters, double confidence, long refineIters) {
+        return new Mat(estimateAffine2D_0(from.nativeObj, to.nativeObj, inliers.nativeObj, method, ransacReprojThreshold, maxIters, confidence, refineIters));
+    }
+
+    /**
+     * Computes an optimal affine transformation between two 2D point sets.
+     *
+     * It computes
+     * \(
+     * \begin{bmatrix}
+     * x\\
+     * y\\
+     * \end{bmatrix}
+     * =
+     * \begin{bmatrix}
+     * a_{11} &amp; a_{12}\\
+     * a_{21} &amp; a_{22}\\
+     * \end{bmatrix}
+     * \begin{bmatrix}
+     * X\\
+     * Y\\
+     * \end{bmatrix}
+     * +
+     * \begin{bmatrix}
+     * b_1\\
+     * b_2\\
+     * \end{bmatrix}
+     * \)
+     *
+     * @param from First input 2D point set containing \((X,Y)\).
+     * @param to Second input 2D point set containing \((x,y)\).
+     * @param inliers Output vector indicating which points are inliers (1-inlier, 0-outlier).
+     * @param method Robust method used to compute transformation. The following methods are possible:
+     * <ul>
+     *   <li>
+     *    cv::RANSAC - RANSAC-based robust method
+     *   </li>
+     *   <li>
+     *    cv::LMEDS - Least-Median robust method
+     * RANSAC is the default method.
+     * @param ransacReprojThreshold Maximum reprojection error in the RANSAC algorithm to consider
+     * a point as an inlier. Applies only to RANSAC.
+     * @param maxIters The maximum number of robust method iterations.
+     * @param confidence Confidence level, between 0 and 1, for the estimated transformation. Anything
+     * between 0.95 and 0.99 is usually good enough. Values too close to 1 can slow down the estimation
+     * significantly. Values lower than 0.8-0.9 can result in an incorrectly estimated transformation.
+     * Passing 0 will disable refining, so the output matrix will be output of robust method.
+     *   </li>
+     * </ul>
+     *
+     * @return Output 2D affine transformation matrix \(2 \times 3\) or empty matrix if transformation
+     * could not be estimated. The returned matrix has the following form:
+     * \(
+     * \begin{bmatrix}
+     * a_{11} &amp; a_{12} &amp; b_1\\
+     * a_{21} &amp; a_{22} &amp; b_2\\
+     * \end{bmatrix}
+     * \)
+     *
+     * The function estimates an optimal 2D affine transformation between two 2D point sets using the
+     * selected robust algorithm.
+     *
+     * The computed transformation is then refined further (using only inliers) with the
+     * Levenberg-Marquardt method to reduce the re-projection error even more.
+     *
+     * <b>Note:</b>
+     * The RANSAC method can handle practically any ratio of outliers but needs a threshold to
+     * distinguish inliers from outliers. The method LMeDS does not need any threshold but it works
+     * correctly only when there are more than 50% of inliers.
+     *
+     * SEE: estimateAffinePartial2D, getAffineTransform
+     */
+    public static Mat estimateAffine2D(Mat from, Mat to, Mat inliers, int method, double ransacReprojThreshold, long maxIters, double confidence) {
+        return new Mat(estimateAffine2D_1(from.nativeObj, to.nativeObj, inliers.nativeObj, method, ransacReprojThreshold, maxIters, confidence));
+    }
+
+    /**
+     * Computes an optimal affine transformation between two 2D point sets.
+     *
+     * It computes
+     * \(
+     * \begin{bmatrix}
+     * x\\
+     * y\\
+     * \end{bmatrix}
+     * =
+     * \begin{bmatrix}
+     * a_{11} &amp; a_{12}\\
+     * a_{21} &amp; a_{22}\\
+     * \end{bmatrix}
+     * \begin{bmatrix}
+     * X\\
+     * Y\\
+     * \end{bmatrix}
+     * +
+     * \begin{bmatrix}
+     * b_1\\
+     * b_2\\
+     * \end{bmatrix}
+     * \)
+     *
+     * @param from First input 2D point set containing \((X,Y)\).
+     * @param to Second input 2D point set containing \((x,y)\).
+     * @param inliers Output vector indicating which points are inliers (1-inlier, 0-outlier).
+     * @param method Robust method used to compute transformation. The following methods are possible:
+     * <ul>
+     *   <li>
+     *    cv::RANSAC - RANSAC-based robust method
+     *   </li>
+     *   <li>
+     *    cv::LMEDS - Least-Median robust method
+     * RANSAC is the default method.
+     * @param ransacReprojThreshold Maximum reprojection error in the RANSAC algorithm to consider
+     * a point as an inlier. Applies only to RANSAC.
+     * @param maxIters The maximum number of robust method iterations.
+     * between 0.95 and 0.99 is usually good enough. Values too close to 1 can slow down the estimation
+     * significantly. Values lower than 0.8-0.9 can result in an incorrectly estimated transformation.
+     * Passing 0 will disable refining, so the output matrix will be output of robust method.
+     *   </li>
+     * </ul>
+     *
+     * @return Output 2D affine transformation matrix \(2 \times 3\) or empty matrix if transformation
+     * could not be estimated. The returned matrix has the following form:
+     * \(
+     * \begin{bmatrix}
+     * a_{11} &amp; a_{12} &amp; b_1\\
+     * a_{21} &amp; a_{22} &amp; b_2\\
+     * \end{bmatrix}
+     * \)
+     *
+     * The function estimates an optimal 2D affine transformation between two 2D point sets using the
+     * selected robust algorithm.
+     *
+     * The computed transformation is then refined further (using only inliers) with the
+     * Levenberg-Marquardt method to reduce the re-projection error even more.
+     *
+     * <b>Note:</b>
+     * The RANSAC method can handle practically any ratio of outliers but needs a threshold to
+     * distinguish inliers from outliers. The method LMeDS does not need any threshold but it works
+     * correctly only when there are more than 50% of inliers.
+     *
+     * SEE: estimateAffinePartial2D, getAffineTransform
+     */
+    public static Mat estimateAffine2D(Mat from, Mat to, Mat inliers, int method, double ransacReprojThreshold, long maxIters) {
+        return new Mat(estimateAffine2D_2(from.nativeObj, to.nativeObj, inliers.nativeObj, method, ransacReprojThreshold, maxIters));
+    }
+
+    /**
+     * Computes an optimal affine transformation between two 2D point sets.
+     *
+     * It computes
+     * \(
+     * \begin{bmatrix}
+     * x\\
+     * y\\
+     * \end{bmatrix}
+     * =
+     * \begin{bmatrix}
+     * a_{11} &amp; a_{12}\\
+     * a_{21} &amp; a_{22}\\
+     * \end{bmatrix}
+     * \begin{bmatrix}
+     * X\\
+     * Y\\
+     * \end{bmatrix}
+     * +
+     * \begin{bmatrix}
+     * b_1\\
+     * b_2\\
+     * \end{bmatrix}
+     * \)
+     *
+     * @param from First input 2D point set containing \((X,Y)\).
+     * @param to Second input 2D point set containing \((x,y)\).
+     * @param inliers Output vector indicating which points are inliers (1-inlier, 0-outlier).
+     * @param method Robust method used to compute transformation. The following methods are possible:
+     * <ul>
+     *   <li>
+     *    cv::RANSAC - RANSAC-based robust method
+     *   </li>
+     *   <li>
+     *    cv::LMEDS - Least-Median robust method
+     * RANSAC is the default method.
+     * @param ransacReprojThreshold Maximum reprojection error in the RANSAC algorithm to consider
+     * a point as an inlier. Applies only to RANSAC.
+     * between 0.95 and 0.99 is usually good enough. Values too close to 1 can slow down the estimation
+     * significantly. Values lower than 0.8-0.9 can result in an incorrectly estimated transformation.
+     * Passing 0 will disable refining, so the output matrix will be output of robust method.
+     *   </li>
+     * </ul>
+     *
+     * @return Output 2D affine transformation matrix \(2 \times 3\) or empty matrix if transformation
+     * could not be estimated. The returned matrix has the following form:
+     * \(
+     * \begin{bmatrix}
+     * a_{11} &amp; a_{12} &amp; b_1\\
+     * a_{21} &amp; a_{22} &amp; b_2\\
+     * \end{bmatrix}
+     * \)
+     *
+     * The function estimates an optimal 2D affine transformation between two 2D point sets using the
+     * selected robust algorithm.
+     *
+     * The computed transformation is then refined further (using only inliers) with the
+     * Levenberg-Marquardt method to reduce the re-projection error even more.
+     *
+     * <b>Note:</b>
+     * The RANSAC method can handle practically any ratio of outliers but needs a threshold to
+     * distinguish inliers from outliers. The method LMeDS does not need any threshold but it works
+     * correctly only when there are more than 50% of inliers.
+     *
+     * SEE: estimateAffinePartial2D, getAffineTransform
+     */
+    public static Mat estimateAffine2D(Mat from, Mat to, Mat inliers, int method, double ransacReprojThreshold) {
+        return new Mat(estimateAffine2D_3(from.nativeObj, to.nativeObj, inliers.nativeObj, method, ransacReprojThreshold));
+    }
+
+    /**
+     * Computes an optimal affine transformation between two 2D point sets.
+     *
+     * It computes
+     * \(
+     * \begin{bmatrix}
+     * x\\
+     * y\\
+     * \end{bmatrix}
+     * =
+     * \begin{bmatrix}
+     * a_{11} &amp; a_{12}\\
+     * a_{21} &amp; a_{22}\\
+     * \end{bmatrix}
+     * \begin{bmatrix}
+     * X\\
+     * Y\\
+     * \end{bmatrix}
+     * +
+     * \begin{bmatrix}
+     * b_1\\
+     * b_2\\
+     * \end{bmatrix}
+     * \)
+     *
+     * @param from First input 2D point set containing \((X,Y)\).
+     * @param to Second input 2D point set containing \((x,y)\).
+     * @param inliers Output vector indicating which points are inliers (1-inlier, 0-outlier).
+     * @param method Robust method used to compute transformation. The following methods are possible:
+     * <ul>
+     *   <li>
+     *    cv::RANSAC - RANSAC-based robust method
+     *   </li>
+     *   <li>
+     *    cv::LMEDS - Least-Median robust method
+     * RANSAC is the default method.
+     * a point as an inlier. Applies only to RANSAC.
+     * between 0.95 and 0.99 is usually good enough. Values too close to 1 can slow down the estimation
+     * significantly. Values lower than 0.8-0.9 can result in an incorrectly estimated transformation.
+     * Passing 0 will disable refining, so the output matrix will be output of robust method.
+     *   </li>
+     * </ul>
+     *
+     * @return Output 2D affine transformation matrix \(2 \times 3\) or empty matrix if transformation
+     * could not be estimated. The returned matrix has the following form:
+     * \(
+     * \begin{bmatrix}
+     * a_{11} &amp; a_{12} &amp; b_1\\
+     * a_{21} &amp; a_{22} &amp; b_2\\
+     * \end{bmatrix}
+     * \)
+     *
+     * The function estimates an optimal 2D affine transformation between two 2D point sets using the
+     * selected robust algorithm.
+     *
+     * The computed transformation is then refined further (using only inliers) with the
+     * Levenberg-Marquardt method to reduce the re-projection error even more.
+     *
+     * <b>Note:</b>
+     * The RANSAC method can handle practically any ratio of outliers but needs a threshold to
+     * distinguish inliers from outliers. The method LMeDS does not need any threshold but it works
+     * correctly only when there are more than 50% of inliers.
+     *
+     * SEE: estimateAffinePartial2D, getAffineTransform
+     */
+    public static Mat estimateAffine2D(Mat from, Mat to, Mat inliers, int method) {
+        return new Mat(estimateAffine2D_4(from.nativeObj, to.nativeObj, inliers.nativeObj, method));
+    }
+
+    /**
+     * Computes an optimal affine transformation between two 2D point sets.
+     *
+     * It computes
+     * \(
+     * \begin{bmatrix}
+     * x\\
+     * y\\
+     * \end{bmatrix}
+     * =
+     * \begin{bmatrix}
+     * a_{11} &amp; a_{12}\\
+     * a_{21} &amp; a_{22}\\
+     * \end{bmatrix}
+     * \begin{bmatrix}
+     * X\\
+     * Y\\
+     * \end{bmatrix}
+     * +
+     * \begin{bmatrix}
+     * b_1\\
+     * b_2\\
+     * \end{bmatrix}
+     * \)
+     *
+     * @param from First input 2D point set containing \((X,Y)\).
+     * @param to Second input 2D point set containing \((x,y)\).
+     * @param inliers Output vector indicating which points are inliers (1-inlier, 0-outlier).
+     * <ul>
+     *   <li>
+     *    cv::RANSAC - RANSAC-based robust method
+     *   </li>
+     *   <li>
+     *    cv::LMEDS - Least-Median robust method
+     * RANSAC is the default method.
+     * a point as an inlier. Applies only to RANSAC.
+     * between 0.95 and 0.99 is usually good enough. Values too close to 1 can slow down the estimation
+     * significantly. Values lower than 0.8-0.9 can result in an incorrectly estimated transformation.
+     * Passing 0 will disable refining, so the output matrix will be output of robust method.
+     *   </li>
+     * </ul>
+     *
+     * @return Output 2D affine transformation matrix \(2 \times 3\) or empty matrix if transformation
+     * could not be estimated. The returned matrix has the following form:
+     * \(
+     * \begin{bmatrix}
+     * a_{11} &amp; a_{12} &amp; b_1\\
+     * a_{21} &amp; a_{22} &amp; b_2\\
+     * \end{bmatrix}
+     * \)
+     *
+     * The function estimates an optimal 2D affine transformation between two 2D point sets using the
+     * selected robust algorithm.
+     *
+     * The computed transformation is then refined further (using only inliers) with the
+     * Levenberg-Marquardt method to reduce the re-projection error even more.
+     *
+     * <b>Note:</b>
+     * The RANSAC method can handle practically any ratio of outliers but needs a threshold to
+     * distinguish inliers from outliers. The method LMeDS does not need any threshold but it works
+     * correctly only when there are more than 50% of inliers.
+     *
+     * SEE: estimateAffinePartial2D, getAffineTransform
+     */
+    public static Mat estimateAffine2D(Mat from, Mat to, Mat inliers) {
+        return new Mat(estimateAffine2D_5(from.nativeObj, to.nativeObj, inliers.nativeObj));
+    }
+
+    /**
+     * Computes an optimal affine transformation between two 2D point sets.
+     *
+     * It computes
+     * \(
+     * \begin{bmatrix}
+     * x\\
+     * y\\
+     * \end{bmatrix}
+     * =
+     * \begin{bmatrix}
+     * a_{11} &amp; a_{12}\\
+     * a_{21} &amp; a_{22}\\
+     * \end{bmatrix}
+     * \begin{bmatrix}
+     * X\\
+     * Y\\
+     * \end{bmatrix}
+     * +
+     * \begin{bmatrix}
+     * b_1\\
+     * b_2\\
+     * \end{bmatrix}
+     * \)
+     *
+     * @param from First input 2D point set containing \((X,Y)\).
+     * @param to Second input 2D point set containing \((x,y)\).
+     * <ul>
+     *   <li>
+     *    cv::RANSAC - RANSAC-based robust method
+     *   </li>
+     *   <li>
+     *    cv::LMEDS - Least-Median robust method
+     * RANSAC is the default method.
+     * a point as an inlier. Applies only to RANSAC.
+     * between 0.95 and 0.99 is usually good enough. Values too close to 1 can slow down the estimation
+     * significantly. Values lower than 0.8-0.9 can result in an incorrectly estimated transformation.
+     * Passing 0 will disable refining, so the output matrix will be output of robust method.
+     *   </li>
+     * </ul>
+     *
+     * @return Output 2D affine transformation matrix \(2 \times 3\) or empty matrix if transformation
+     * could not be estimated. The returned matrix has the following form:
+     * \(
+     * \begin{bmatrix}
+     * a_{11} &amp; a_{12} &amp; b_1\\
+     * a_{21} &amp; a_{22} &amp; b_2\\
+     * \end{bmatrix}
+     * \)
+     *
+     * The function estimates an optimal 2D affine transformation between two 2D point sets using the
+     * selected robust algorithm.
+     *
+     * The computed transformation is then refined further (using only inliers) with the
+     * Levenberg-Marquardt method to reduce the re-projection error even more.
+     *
+     * <b>Note:</b>
+     * The RANSAC method can handle practically any ratio of outliers but needs a threshold to
+     * distinguish inliers from outliers. The method LMeDS does not need any threshold but it works
+     * correctly only when there are more than 50% of inliers.
+     *
+     * SEE: estimateAffinePartial2D, getAffineTransform
+     */
+    public static Mat estimateAffine2D(Mat from, Mat to) {
+        return new Mat(estimateAffine2D_6(from.nativeObj, to.nativeObj));
+    }
+
+
+    //
+    // C++:  Mat cv::estimateAffinePartial2D(Mat from, Mat to, Mat& inliers = Mat(), int method = RANSAC, double ransacReprojThreshold = 3, size_t maxIters = 2000, double confidence = 0.99, size_t refineIters = 10)
+    //
+
+    /**
+     * Computes an optimal limited affine transformation with 4 degrees of freedom between
+     * two 2D point sets.
+     *
+     * @param from First input 2D point set.
+     * @param to Second input 2D point set.
+     * @param inliers Output vector indicating which points are inliers.
+     * @param method Robust method used to compute transformation. The following methods are possible:
+     * <ul>
+     *   <li>
+     *    cv::RANSAC - RANSAC-based robust method
+     *   </li>
+     *   <li>
+     *    cv::LMEDS - Least-Median robust method
+     * RANSAC is the default method.
+     * @param ransacReprojThreshold Maximum reprojection error in the RANSAC algorithm to consider
+     * a point as an inlier. Applies only to RANSAC.
+     * @param maxIters The maximum number of robust method iterations.
+     * @param confidence Confidence level, between 0 and 1, for the estimated transformation. Anything
+     * between 0.95 and 0.99 is usually good enough. Values too close to 1 can slow down the estimation
+     * significantly. Values lower than 0.8-0.9 can result in an incorrectly estimated transformation.
+     * @param refineIters Maximum number of iterations of refining algorithm (Levenberg-Marquardt).
+     * Passing 0 will disable refining, so the output matrix will be output of robust method.
+     *   </li>
+     * </ul>
+     *
+     * @return Output 2D affine transformation (4 degrees of freedom) matrix \(2 \times 3\) or
+     * empty matrix if transformation could not be estimated.
+     *
+     * The function estimates an optimal 2D affine transformation with 4 degrees of freedom limited to
+     * combinations of translation, rotation, and uniform scaling. Uses the selected algorithm for robust
+     * estimation.
+     *
+     * The computed transformation is then refined further (using only inliers) with the
+     * Levenberg-Marquardt method to reduce the re-projection error even more.
+     *
+     * Estimated transformation matrix is:
+     * \( \begin{bmatrix} \cos(\theta) \cdot s &amp; -\sin(\theta) \cdot s &amp; t_x \\
+     *                 \sin(\theta) \cdot s &amp; \cos(\theta) \cdot s &amp; t_y
+     * \end{bmatrix} \)
+     * Where \( \theta \) is the rotation angle, \( s \) the scaling factor and \( t_x, t_y \) are
+     * translations in \( x, y \) axes respectively.
+     *
+     * <b>Note:</b>
+     * The RANSAC method can handle practically any ratio of outliers but need a threshold to
+     * distinguish inliers from outliers. The method LMeDS does not need any threshold but it works
+     * correctly only when there are more than 50% of inliers.
+     *
+     * SEE: estimateAffine2D, getAffineTransform
+     */
+    public static Mat estimateAffinePartial2D(Mat from, Mat to, Mat inliers, int method, double ransacReprojThreshold, long maxIters, double confidence, long refineIters) {
+        return new Mat(estimateAffinePartial2D_0(from.nativeObj, to.nativeObj, inliers.nativeObj, method, ransacReprojThreshold, maxIters, confidence, refineIters));
+    }
+
+    /**
+     * Computes an optimal limited affine transformation with 4 degrees of freedom between
+     * two 2D point sets.
+     *
+     * @param from First input 2D point set.
+     * @param to Second input 2D point set.
+     * @param inliers Output vector indicating which points are inliers.
+     * @param method Robust method used to compute transformation. The following methods are possible:
+     * <ul>
+     *   <li>
+     *    cv::RANSAC - RANSAC-based robust method
+     *   </li>
+     *   <li>
+     *    cv::LMEDS - Least-Median robust method
+     * RANSAC is the default method.
+     * @param ransacReprojThreshold Maximum reprojection error in the RANSAC algorithm to consider
+     * a point as an inlier. Applies only to RANSAC.
+     * @param maxIters The maximum number of robust method iterations.
+     * @param confidence Confidence level, between 0 and 1, for the estimated transformation. Anything
+     * between 0.95 and 0.99 is usually good enough. Values too close to 1 can slow down the estimation
+     * significantly. Values lower than 0.8-0.9 can result in an incorrectly estimated transformation.
+     * Passing 0 will disable refining, so the output matrix will be output of robust method.
+     *   </li>
+     * </ul>
+     *
+     * @return Output 2D affine transformation (4 degrees of freedom) matrix \(2 \times 3\) or
+     * empty matrix if transformation could not be estimated.
+     *
+     * The function estimates an optimal 2D affine transformation with 4 degrees of freedom limited to
+     * combinations of translation, rotation, and uniform scaling. Uses the selected algorithm for robust
+     * estimation.
+     *
+     * The computed transformation is then refined further (using only inliers) with the
+     * Levenberg-Marquardt method to reduce the re-projection error even more.
+     *
+     * Estimated transformation matrix is:
+     * \( \begin{bmatrix} \cos(\theta) \cdot s &amp; -\sin(\theta) \cdot s &amp; t_x \\
+     *                 \sin(\theta) \cdot s &amp; \cos(\theta) \cdot s &amp; t_y
+     * \end{bmatrix} \)
+     * Where \( \theta \) is the rotation angle, \( s \) the scaling factor and \( t_x, t_y \) are
+     * translations in \( x, y \) axes respectively.
+     *
+     * <b>Note:</b>
+     * The RANSAC method can handle practically any ratio of outliers but need a threshold to
+     * distinguish inliers from outliers. The method LMeDS does not need any threshold but it works
+     * correctly only when there are more than 50% of inliers.
+     *
+     * SEE: estimateAffine2D, getAffineTransform
+     */
+    public static Mat estimateAffinePartial2D(Mat from, Mat to, Mat inliers, int method, double ransacReprojThreshold, long maxIters, double confidence) {
+        return new Mat(estimateAffinePartial2D_1(from.nativeObj, to.nativeObj, inliers.nativeObj, method, ransacReprojThreshold, maxIters, confidence));
+    }
+
+    /**
+     * Computes an optimal limited affine transformation with 4 degrees of freedom between
+     * two 2D point sets.
+     *
+     * @param from First input 2D point set.
+     * @param to Second input 2D point set.
+     * @param inliers Output vector indicating which points are inliers.
+     * @param method Robust method used to compute transformation. The following methods are possible:
+     * <ul>
+     *   <li>
+     *    cv::RANSAC - RANSAC-based robust method
+     *   </li>
+     *   <li>
+     *    cv::LMEDS - Least-Median robust method
+     * RANSAC is the default method.
+     * @param ransacReprojThreshold Maximum reprojection error in the RANSAC algorithm to consider
+     * a point as an inlier. Applies only to RANSAC.
+     * @param maxIters The maximum number of robust method iterations.
+     * between 0.95 and 0.99 is usually good enough. Values too close to 1 can slow down the estimation
+     * significantly. Values lower than 0.8-0.9 can result in an incorrectly estimated transformation.
+     * Passing 0 will disable refining, so the output matrix will be output of robust method.
+     *   </li>
+     * </ul>
+     *
+     * @return Output 2D affine transformation (4 degrees of freedom) matrix \(2 \times 3\) or
+     * empty matrix if transformation could not be estimated.
+     *
+     * The function estimates an optimal 2D affine transformation with 4 degrees of freedom limited to
+     * combinations of translation, rotation, and uniform scaling. Uses the selected algorithm for robust
+     * estimation.
+     *
+     * The computed transformation is then refined further (using only inliers) with the
+     * Levenberg-Marquardt method to reduce the re-projection error even more.
+     *
+     * Estimated transformation matrix is:
+     * \( \begin{bmatrix} \cos(\theta) \cdot s &amp; -\sin(\theta) \cdot s &amp; t_x \\
+     *                 \sin(\theta) \cdot s &amp; \cos(\theta) \cdot s &amp; t_y
+     * \end{bmatrix} \)
+     * Where \( \theta \) is the rotation angle, \( s \) the scaling factor and \( t_x, t_y \) are
+     * translations in \( x, y \) axes respectively.
+     *
+     * <b>Note:</b>
+     * The RANSAC method can handle practically any ratio of outliers but need a threshold to
+     * distinguish inliers from outliers. The method LMeDS does not need any threshold but it works
+     * correctly only when there are more than 50% of inliers.
+     *
+     * SEE: estimateAffine2D, getAffineTransform
+     */
+    public static Mat estimateAffinePartial2D(Mat from, Mat to, Mat inliers, int method, double ransacReprojThreshold, long maxIters) {
+        return new Mat(estimateAffinePartial2D_2(from.nativeObj, to.nativeObj, inliers.nativeObj, method, ransacReprojThreshold, maxIters));
+    }
+
+    /**
+     * Computes an optimal limited affine transformation with 4 degrees of freedom between
+     * two 2D point sets.
+     *
+     * @param from First input 2D point set.
+     * @param to Second input 2D point set.
+     * @param inliers Output vector indicating which points are inliers.
+     * @param method Robust method used to compute transformation. The following methods are possible:
+     * <ul>
+     *   <li>
+     *    cv::RANSAC - RANSAC-based robust method
+     *   </li>
+     *   <li>
+     *    cv::LMEDS - Least-Median robust method
+     * RANSAC is the default method.
+     * @param ransacReprojThreshold Maximum reprojection error in the RANSAC algorithm to consider
+     * a point as an inlier. Applies only to RANSAC.
+     * between 0.95 and 0.99 is usually good enough. Values too close to 1 can slow down the estimation
+     * significantly. Values lower than 0.8-0.9 can result in an incorrectly estimated transformation.
+     * Passing 0 will disable refining, so the output matrix will be output of robust method.
+     *   </li>
+     * </ul>
+     *
+     * @return Output 2D affine transformation (4 degrees of freedom) matrix \(2 \times 3\) or
+     * empty matrix if transformation could not be estimated.
+     *
+     * The function estimates an optimal 2D affine transformation with 4 degrees of freedom limited to
+     * combinations of translation, rotation, and uniform scaling. Uses the selected algorithm for robust
+     * estimation.
+     *
+     * The computed transformation is then refined further (using only inliers) with the
+     * Levenberg-Marquardt method to reduce the re-projection error even more.
+     *
+     * Estimated transformation matrix is:
+     * \( \begin{bmatrix} \cos(\theta) \cdot s &amp; -\sin(\theta) \cdot s &amp; t_x \\
+     *                 \sin(\theta) \cdot s &amp; \cos(\theta) \cdot s &amp; t_y
+     * \end{bmatrix} \)
+     * Where \( \theta \) is the rotation angle, \( s \) the scaling factor and \( t_x, t_y \) are
+     * translations in \( x, y \) axes respectively.
+     *
+     * <b>Note:</b>
+     * The RANSAC method can handle practically any ratio of outliers but need a threshold to
+     * distinguish inliers from outliers. The method LMeDS does not need any threshold but it works
+     * correctly only when there are more than 50% of inliers.
+     *
+     * SEE: estimateAffine2D, getAffineTransform
+     */
+    public static Mat estimateAffinePartial2D(Mat from, Mat to, Mat inliers, int method, double ransacReprojThreshold) {
+        return new Mat(estimateAffinePartial2D_3(from.nativeObj, to.nativeObj, inliers.nativeObj, method, ransacReprojThreshold));
+    }
+
+    /**
+     * Computes an optimal limited affine transformation with 4 degrees of freedom between
+     * two 2D point sets.
+     *
+     * @param from First input 2D point set.
+     * @param to Second input 2D point set.
+     * @param inliers Output vector indicating which points are inliers.
+     * @param method Robust method used to compute transformation. The following methods are possible:
+     * <ul>
+     *   <li>
+     *    cv::RANSAC - RANSAC-based robust method
+     *   </li>
+     *   <li>
+     *    cv::LMEDS - Least-Median robust method
+     * RANSAC is the default method.
+     * a point as an inlier. Applies only to RANSAC.
+     * between 0.95 and 0.99 is usually good enough. Values too close to 1 can slow down the estimation
+     * significantly. Values lower than 0.8-0.9 can result in an incorrectly estimated transformation.
+     * Passing 0 will disable refining, so the output matrix will be output of robust method.
+     *   </li>
+     * </ul>
+     *
+     * @return Output 2D affine transformation (4 degrees of freedom) matrix \(2 \times 3\) or
+     * empty matrix if transformation could not be estimated.
+     *
+     * The function estimates an optimal 2D affine transformation with 4 degrees of freedom limited to
+     * combinations of translation, rotation, and uniform scaling. Uses the selected algorithm for robust
+     * estimation.
+     *
+     * The computed transformation is then refined further (using only inliers) with the
+     * Levenberg-Marquardt method to reduce the re-projection error even more.
+     *
+     * Estimated transformation matrix is:
+     * \( \begin{bmatrix} \cos(\theta) \cdot s &amp; -\sin(\theta) \cdot s &amp; t_x \\
+     *                 \sin(\theta) \cdot s &amp; \cos(\theta) \cdot s &amp; t_y
+     * \end{bmatrix} \)
+     * Where \( \theta \) is the rotation angle, \( s \) the scaling factor and \( t_x, t_y \) are
+     * translations in \( x, y \) axes respectively.
+     *
+     * <b>Note:</b>
+     * The RANSAC method can handle practically any ratio of outliers but need a threshold to
+     * distinguish inliers from outliers. The method LMeDS does not need any threshold but it works
+     * correctly only when there are more than 50% of inliers.
+     *
+     * SEE: estimateAffine2D, getAffineTransform
+     */
+    public static Mat estimateAffinePartial2D(Mat from, Mat to, Mat inliers, int method) {
+        return new Mat(estimateAffinePartial2D_4(from.nativeObj, to.nativeObj, inliers.nativeObj, method));
+    }
+
+    /**
+     * Computes an optimal limited affine transformation with 4 degrees of freedom between
+     * two 2D point sets.
+     *
+     * @param from First input 2D point set.
+     * @param to Second input 2D point set.
+     * @param inliers Output vector indicating which points are inliers.
+     * <ul>
+     *   <li>
+     *    cv::RANSAC - RANSAC-based robust method
+     *   </li>
+     *   <li>
+     *    cv::LMEDS - Least-Median robust method
+     * RANSAC is the default method.
+     * a point as an inlier. Applies only to RANSAC.
+     * between 0.95 and 0.99 is usually good enough. Values too close to 1 can slow down the estimation
+     * significantly. Values lower than 0.8-0.9 can result in an incorrectly estimated transformation.
+     * Passing 0 will disable refining, so the output matrix will be output of robust method.
+     *   </li>
+     * </ul>
+     *
+     * @return Output 2D affine transformation (4 degrees of freedom) matrix \(2 \times 3\) or
+     * empty matrix if transformation could not be estimated.
+     *
+     * The function estimates an optimal 2D affine transformation with 4 degrees of freedom limited to
+     * combinations of translation, rotation, and uniform scaling. Uses the selected algorithm for robust
+     * estimation.
+     *
+     * The computed transformation is then refined further (using only inliers) with the
+     * Levenberg-Marquardt method to reduce the re-projection error even more.
+     *
+     * Estimated transformation matrix is:
+     * \( \begin{bmatrix} \cos(\theta) \cdot s &amp; -\sin(\theta) \cdot s &amp; t_x \\
+     *                 \sin(\theta) \cdot s &amp; \cos(\theta) \cdot s &amp; t_y
+     * \end{bmatrix} \)
+     * Where \( \theta \) is the rotation angle, \( s \) the scaling factor and \( t_x, t_y \) are
+     * translations in \( x, y \) axes respectively.
+     *
+     * <b>Note:</b>
+     * The RANSAC method can handle practically any ratio of outliers but need a threshold to
+     * distinguish inliers from outliers. The method LMeDS does not need any threshold but it works
+     * correctly only when there are more than 50% of inliers.
+     *
+     * SEE: estimateAffine2D, getAffineTransform
+     */
+    public static Mat estimateAffinePartial2D(Mat from, Mat to, Mat inliers) {
+        return new Mat(estimateAffinePartial2D_5(from.nativeObj, to.nativeObj, inliers.nativeObj));
+    }
+
+    /**
+     * Computes an optimal limited affine transformation with 4 degrees of freedom between
+     * two 2D point sets.
+     *
+     * @param from First input 2D point set.
+     * @param to Second input 2D point set.
+     * <ul>
+     *   <li>
+     *    cv::RANSAC - RANSAC-based robust method
+     *   </li>
+     *   <li>
+     *    cv::LMEDS - Least-Median robust method
+     * RANSAC is the default method.
+     * a point as an inlier. Applies only to RANSAC.
+     * between 0.95 and 0.99 is usually good enough. Values too close to 1 can slow down the estimation
+     * significantly. Values lower than 0.8-0.9 can result in an incorrectly estimated transformation.
+     * Passing 0 will disable refining, so the output matrix will be output of robust method.
+     *   </li>
+     * </ul>
+     *
+     * @return Output 2D affine transformation (4 degrees of freedom) matrix \(2 \times 3\) or
+     * empty matrix if transformation could not be estimated.
+     *
+     * The function estimates an optimal 2D affine transformation with 4 degrees of freedom limited to
+     * combinations of translation, rotation, and uniform scaling. Uses the selected algorithm for robust
+     * estimation.
+     *
+     * The computed transformation is then refined further (using only inliers) with the
+     * Levenberg-Marquardt method to reduce the re-projection error even more.
+     *
+     * Estimated transformation matrix is:
+     * \( \begin{bmatrix} \cos(\theta) \cdot s &amp; -\sin(\theta) \cdot s &amp; t_x \\
+     *                 \sin(\theta) \cdot s &amp; \cos(\theta) \cdot s &amp; t_y
+     * \end{bmatrix} \)
+     * Where \( \theta \) is the rotation angle, \( s \) the scaling factor and \( t_x, t_y \) are
+     * translations in \( x, y \) axes respectively.
+     *
+     * <b>Note:</b>
+     * The RANSAC method can handle practically any ratio of outliers but need a threshold to
+     * distinguish inliers from outliers. The method LMeDS does not need any threshold but it works
+     * correctly only when there are more than 50% of inliers.
+     *
+     * SEE: estimateAffine2D, getAffineTransform
+     */
+    public static Mat estimateAffinePartial2D(Mat from, Mat to) {
+        return new Mat(estimateAffinePartial2D_6(from.nativeObj, to.nativeObj));
+    }
+
+
+    //
+    // C++:  Mat cv::findEssentialMat(Mat points1, Mat points2, Mat cameraMatrix, int method = RANSAC, double prob = 0.999, double threshold = 1.0, Mat& mask = Mat())
+    //
+
+    /**
+     * Calculates an essential matrix from the corresponding points in two images.
+     *
+     * @param points1 Array of N (N &gt;= 5) 2D points from the first image. The point coordinates should
+     * be floating-point (single or double precision).
+     * @param points2 Array of the second image points of the same size and format as points1 .
+     * @param cameraMatrix Camera matrix \(K = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}\) .
+     * Note that this function assumes that points1 and points2 are feature points from cameras with the
+     * same camera matrix. If this assumption does not hold for your use case, use
+     * {@code undistortPoints()} with {@code P = cv::NoArray()} for both cameras to transform image points
+     * to normalized image coordinates, which are valid for the identity camera matrix. When
+     * passing these coordinates, pass the identity matrix for this parameter.
+     * @param method Method for computing an essential matrix.
+     * <ul>
+     *   <li>
+     *    <b>RANSAC</b> for the RANSAC algorithm.
+     *   </li>
+     *   <li>
+     *    <b>LMEDS</b> for the LMedS algorithm.
+     * @param prob Parameter used for the RANSAC or LMedS methods only. It specifies a desirable level of
+     * confidence (probability) that the estimated matrix is correct.
+     * @param threshold Parameter used for RANSAC. It is the maximum distance from a point to an epipolar
+     * line in pixels, beyond which the point is considered an outlier and is not used for computing the
+     * final fundamental matrix. It can be set to something like 1-3, depending on the accuracy of the
+     * point localization, image resolution, and the image noise.
+     * @param mask Output array of N elements, every element of which is set to 0 for outliers and to 1
+     * for the other points. The array is computed only in the RANSAC and LMedS methods.
+     *   </li>
+     * </ul>
+     *
+     * This function estimates essential matrix based on the five-point algorithm solver in CITE: Nister03 .
+     * CITE: SteweniusCFS is also a related. The epipolar geometry is described by the following equation:
+     *
+     * \([p_2; 1]^T K^{-T} E K^{-1} [p_1; 1] = 0\)
+     *
+     * where \(E\) is an essential matrix, \(p_1\) and \(p_2\) are corresponding points in the first and the
+     * second images, respectively. The result of this function may be passed further to
+     * decomposeEssentialMat or recoverPose to recover the relative pose between cameras.
+     * @return automatically generated
+     */
+    public static Mat findEssentialMat(Mat points1, Mat points2, Mat cameraMatrix, int method, double prob, double threshold, Mat mask) {
+        return new Mat(findEssentialMat_0(points1.nativeObj, points2.nativeObj, cameraMatrix.nativeObj, method, prob, threshold, mask.nativeObj));
+    }
+
+    /**
+     * Calculates an essential matrix from the corresponding points in two images.
+     *
+     * @param points1 Array of N (N &gt;= 5) 2D points from the first image. The point coordinates should
+     * be floating-point (single or double precision).
+     * @param points2 Array of the second image points of the same size and format as points1 .
+     * @param cameraMatrix Camera matrix \(K = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}\) .
+     * Note that this function assumes that points1 and points2 are feature points from cameras with the
+     * same camera matrix. If this assumption does not hold for your use case, use
+     * {@code undistortPoints()} with {@code P = cv::NoArray()} for both cameras to transform image points
+     * to normalized image coordinates, which are valid for the identity camera matrix. When
+     * passing these coordinates, pass the identity matrix for this parameter.
+     * @param method Method for computing an essential matrix.
+     * <ul>
+     *   <li>
+     *    <b>RANSAC</b> for the RANSAC algorithm.
+     *   </li>
+     *   <li>
+     *    <b>LMEDS</b> for the LMedS algorithm.
+     * @param prob Parameter used for the RANSAC or LMedS methods only. It specifies a desirable level of
+     * confidence (probability) that the estimated matrix is correct.
+     * @param threshold Parameter used for RANSAC. It is the maximum distance from a point to an epipolar
+     * line in pixels, beyond which the point is considered an outlier and is not used for computing the
+     * final fundamental matrix. It can be set to something like 1-3, depending on the accuracy of the
+     * point localization, image resolution, and the image noise.
+     * for the other points. The array is computed only in the RANSAC and LMedS methods.
+     *   </li>
+     * </ul>
+     *
+     * This function estimates essential matrix based on the five-point algorithm solver in CITE: Nister03 .
+     * CITE: SteweniusCFS is also a related. The epipolar geometry is described by the following equation:
+     *
+     * \([p_2; 1]^T K^{-T} E K^{-1} [p_1; 1] = 0\)
+     *
+     * where \(E\) is an essential matrix, \(p_1\) and \(p_2\) are corresponding points in the first and the
+     * second images, respectively. The result of this function may be passed further to
+     * decomposeEssentialMat or recoverPose to recover the relative pose between cameras.
+     * @return automatically generated
+     */
+    public static Mat findEssentialMat(Mat points1, Mat points2, Mat cameraMatrix, int method, double prob, double threshold) {
+        return new Mat(findEssentialMat_1(points1.nativeObj, points2.nativeObj, cameraMatrix.nativeObj, method, prob, threshold));
+    }
+
+    /**
+     * Calculates an essential matrix from the corresponding points in two images.
+     *
+     * @param points1 Array of N (N &gt;= 5) 2D points from the first image. The point coordinates should
+     * be floating-point (single or double precision).
+     * @param points2 Array of the second image points of the same size and format as points1 .
+     * @param cameraMatrix Camera matrix \(K = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}\) .
+     * Note that this function assumes that points1 and points2 are feature points from cameras with the
+     * same camera matrix. If this assumption does not hold for your use case, use
+     * {@code undistortPoints()} with {@code P = cv::NoArray()} for both cameras to transform image points
+     * to normalized image coordinates, which are valid for the identity camera matrix. When
+     * passing these coordinates, pass the identity matrix for this parameter.
+     * @param method Method for computing an essential matrix.
+     * <ul>
+     *   <li>
+     *    <b>RANSAC</b> for the RANSAC algorithm.
+     *   </li>
+     *   <li>
+     *    <b>LMEDS</b> for the LMedS algorithm.
+     * @param prob Parameter used for the RANSAC or LMedS methods only. It specifies a desirable level of
+     * confidence (probability) that the estimated matrix is correct.
+     * line in pixels, beyond which the point is considered an outlier and is not used for computing the
+     * final fundamental matrix. It can be set to something like 1-3, depending on the accuracy of the
+     * point localization, image resolution, and the image noise.
+     * for the other points. The array is computed only in the RANSAC and LMedS methods.
+     *   </li>
+     * </ul>
+     *
+     * This function estimates essential matrix based on the five-point algorithm solver in CITE: Nister03 .
+     * CITE: SteweniusCFS is also a related. The epipolar geometry is described by the following equation:
+     *
+     * \([p_2; 1]^T K^{-T} E K^{-1} [p_1; 1] = 0\)
+     *
+     * where \(E\) is an essential matrix, \(p_1\) and \(p_2\) are corresponding points in the first and the
+     * second images, respectively. The result of this function may be passed further to
+     * decomposeEssentialMat or recoverPose to recover the relative pose between cameras.
+     * @return automatically generated
+     */
+    public static Mat findEssentialMat(Mat points1, Mat points2, Mat cameraMatrix, int method, double prob) {
+        return new Mat(findEssentialMat_2(points1.nativeObj, points2.nativeObj, cameraMatrix.nativeObj, method, prob));
+    }
+
+    /**
+     * Calculates an essential matrix from the corresponding points in two images.
+     *
+     * @param points1 Array of N (N &gt;= 5) 2D points from the first image. The point coordinates should
+     * be floating-point (single or double precision).
+     * @param points2 Array of the second image points of the same size and format as points1 .
+     * @param cameraMatrix Camera matrix \(K = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}\) .
+     * Note that this function assumes that points1 and points2 are feature points from cameras with the
+     * same camera matrix. If this assumption does not hold for your use case, use
+     * {@code undistortPoints()} with {@code P = cv::NoArray()} for both cameras to transform image points
+     * to normalized image coordinates, which are valid for the identity camera matrix. When
+     * passing these coordinates, pass the identity matrix for this parameter.
+     * @param method Method for computing an essential matrix.
+     * <ul>
+     *   <li>
+     *    <b>RANSAC</b> for the RANSAC algorithm.
+     *   </li>
+     *   <li>
+     *    <b>LMEDS</b> for the LMedS algorithm.
+     * confidence (probability) that the estimated matrix is correct.
+     * line in pixels, beyond which the point is considered an outlier and is not used for computing the
+     * final fundamental matrix. It can be set to something like 1-3, depending on the accuracy of the
+     * point localization, image resolution, and the image noise.
+     * for the other points. The array is computed only in the RANSAC and LMedS methods.
+     *   </li>
+     * </ul>
+     *
+     * This function estimates essential matrix based on the five-point algorithm solver in CITE: Nister03 .
+     * CITE: SteweniusCFS is also a related. The epipolar geometry is described by the following equation:
+     *
+     * \([p_2; 1]^T K^{-T} E K^{-1} [p_1; 1] = 0\)
+     *
+     * where \(E\) is an essential matrix, \(p_1\) and \(p_2\) are corresponding points in the first and the
+     * second images, respectively. The result of this function may be passed further to
+     * decomposeEssentialMat or recoverPose to recover the relative pose between cameras.
+     * @return automatically generated
+     */
+    public static Mat findEssentialMat(Mat points1, Mat points2, Mat cameraMatrix, int method) {
+        return new Mat(findEssentialMat_3(points1.nativeObj, points2.nativeObj, cameraMatrix.nativeObj, method));
+    }
+
+    /**
+     * Calculates an essential matrix from the corresponding points in two images.
+     *
+     * @param points1 Array of N (N &gt;= 5) 2D points from the first image. The point coordinates should
+     * be floating-point (single or double precision).
+     * @param points2 Array of the second image points of the same size and format as points1 .
+     * @param cameraMatrix Camera matrix \(K = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}\) .
+     * Note that this function assumes that points1 and points2 are feature points from cameras with the
+     * same camera matrix. If this assumption does not hold for your use case, use
+     * {@code undistortPoints()} with {@code P = cv::NoArray()} for both cameras to transform image points
+     * to normalized image coordinates, which are valid for the identity camera matrix. When
+     * passing these coordinates, pass the identity matrix for this parameter.
+     * <ul>
+     *   <li>
+     *    <b>RANSAC</b> for the RANSAC algorithm.
+     *   </li>
+     *   <li>
+     *    <b>LMEDS</b> for the LMedS algorithm.
+     * confidence (probability) that the estimated matrix is correct.
+     * line in pixels, beyond which the point is considered an outlier and is not used for computing the
+     * final fundamental matrix. It can be set to something like 1-3, depending on the accuracy of the
+     * point localization, image resolution, and the image noise.
+     * for the other points. The array is computed only in the RANSAC and LMedS methods.
+     *   </li>
+     * </ul>
+     *
+     * This function estimates essential matrix based on the five-point algorithm solver in CITE: Nister03 .
+     * CITE: SteweniusCFS is also a related. The epipolar geometry is described by the following equation:
+     *
+     * \([p_2; 1]^T K^{-T} E K^{-1} [p_1; 1] = 0\)
+     *
+     * where \(E\) is an essential matrix, \(p_1\) and \(p_2\) are corresponding points in the first and the
+     * second images, respectively. The result of this function may be passed further to
+     * decomposeEssentialMat or recoverPose to recover the relative pose between cameras.
+     * @return automatically generated
+     */
+    public static Mat findEssentialMat(Mat points1, Mat points2, Mat cameraMatrix) {
+        return new Mat(findEssentialMat_4(points1.nativeObj, points2.nativeObj, cameraMatrix.nativeObj));
+    }
+
+
+    //
+    // C++:  Mat cv::findEssentialMat(Mat points1, Mat points2, double focal = 1.0, Point2d pp = Point2d(0, 0), int method = RANSAC, double prob = 0.999, double threshold = 1.0, Mat& mask = Mat())
+    //
+
+    /**
+     *
+     * @param points1 Array of N (N &gt;= 5) 2D points from the first image. The point coordinates should
+     * be floating-point (single or double precision).
+     * @param points2 Array of the second image points of the same size and format as points1 .
+     * @param focal focal length of the camera. Note that this function assumes that points1 and points2
+     * are feature points from cameras with same focal length and principal point.
+     * @param pp principal point of the camera.
+     * @param method Method for computing a fundamental matrix.
+     * <ul>
+     *   <li>
+     *    <b>RANSAC</b> for the RANSAC algorithm.
+     *   </li>
+     *   <li>
+     *    <b>LMEDS</b> for the LMedS algorithm.
+     * @param threshold Parameter used for RANSAC. It is the maximum distance from a point to an epipolar
+     * line in pixels, beyond which the point is considered an outlier and is not used for computing the
+     * final fundamental matrix. It can be set to something like 1-3, depending on the accuracy of the
+     * point localization, image resolution, and the image noise.
+     * @param prob Parameter used for the RANSAC or LMedS methods only. It specifies a desirable level of
+     * confidence (probability) that the estimated matrix is correct.
+     * @param mask Output array of N elements, every element of which is set to 0 for outliers and to 1
+     * for the other points. The array is computed only in the RANSAC and LMedS methods.
+     *   </li>
+     * </ul>
+     *
+     * This function differs from the one above that it computes camera matrix from focal length and
+     * principal point:
+     *
+     * \(K =
+     * \begin{bmatrix}
+     * f &amp; 0 &amp; x_{pp}  \\
+     * 0 &amp; f &amp; y_{pp}  \\
+     * 0 &amp; 0 &amp; 1
+     * \end{bmatrix}\)
+     * @return automatically generated
+     */
+    public static Mat findEssentialMat(Mat points1, Mat points2, double focal, Point pp, int method, double prob, double threshold, Mat mask) {
+        return new Mat(findEssentialMat_5(points1.nativeObj, points2.nativeObj, focal, pp.x, pp.y, method, prob, threshold, mask.nativeObj));
+    }
+
+    /**
+     *
+     * @param points1 Array of N (N &gt;= 5) 2D points from the first image. The point coordinates should
+     * be floating-point (single or double precision).
+     * @param points2 Array of the second image points of the same size and format as points1 .
+     * @param focal focal length of the camera. Note that this function assumes that points1 and points2
+     * are feature points from cameras with same focal length and principal point.
+     * @param pp principal point of the camera.
+     * @param method Method for computing a fundamental matrix.
+     * <ul>
+     *   <li>
+     *    <b>RANSAC</b> for the RANSAC algorithm.
+     *   </li>
+     *   <li>
+     *    <b>LMEDS</b> for the LMedS algorithm.
+     * @param threshold Parameter used for RANSAC. It is the maximum distance from a point to an epipolar
+     * line in pixels, beyond which the point is considered an outlier and is not used for computing the
+     * final fundamental matrix. It can be set to something like 1-3, depending on the accuracy of the
+     * point localization, image resolution, and the image noise.
+     * @param prob Parameter used for the RANSAC or LMedS methods only. It specifies a desirable level of
+     * confidence (probability) that the estimated matrix is correct.
+     * for the other points. The array is computed only in the RANSAC and LMedS methods.
+     *   </li>
+     * </ul>
+     *
+     * This function differs from the one above that it computes camera matrix from focal length and
+     * principal point:
+     *
+     * \(K =
+     * \begin{bmatrix}
+     * f &amp; 0 &amp; x_{pp}  \\
+     * 0 &amp; f &amp; y_{pp}  \\
+     * 0 &amp; 0 &amp; 1
+     * \end{bmatrix}\)
+     * @return automatically generated
+     */
+    public static Mat findEssentialMat(Mat points1, Mat points2, double focal, Point pp, int method, double prob, double threshold) {
+        return new Mat(findEssentialMat_6(points1.nativeObj, points2.nativeObj, focal, pp.x, pp.y, method, prob, threshold));
+    }
+
+    /**
+     *
+     * @param points1 Array of N (N &gt;= 5) 2D points from the first image. The point coordinates should
+     * be floating-point (single or double precision).
+     * @param points2 Array of the second image points of the same size and format as points1 .
+     * @param focal focal length of the camera. Note that this function assumes that points1 and points2
+     * are feature points from cameras with same focal length and principal point.
+     * @param pp principal point of the camera.
+     * @param method Method for computing a fundamental matrix.
+     * <ul>
+     *   <li>
+     *    <b>RANSAC</b> for the RANSAC algorithm.
+     *   </li>
+     *   <li>
+     *    <b>LMEDS</b> for the LMedS algorithm.
+     * line in pixels, beyond which the point is considered an outlier and is not used for computing the
+     * final fundamental matrix. It can be set to something like 1-3, depending on the accuracy of the
+     * point localization, image resolution, and the image noise.
+     * @param prob Parameter used for the RANSAC or LMedS methods only. It specifies a desirable level of
+     * confidence (probability) that the estimated matrix is correct.
+     * for the other points. The array is computed only in the RANSAC and LMedS methods.
+     *   </li>
+     * </ul>
+     *
+     * This function differs from the one above that it computes camera matrix from focal length and
+     * principal point:
+     *
+     * \(K =
+     * \begin{bmatrix}
+     * f &amp; 0 &amp; x_{pp}  \\
+     * 0 &amp; f &amp; y_{pp}  \\
+     * 0 &amp; 0 &amp; 1
+     * \end{bmatrix}\)
+     * @return automatically generated
+     */
+    public static Mat findEssentialMat(Mat points1, Mat points2, double focal, Point pp, int method, double prob) {
+        return new Mat(findEssentialMat_7(points1.nativeObj, points2.nativeObj, focal, pp.x, pp.y, method, prob));
+    }
+
+    /**
+     *
+     * @param points1 Array of N (N &gt;= 5) 2D points from the first image. The point coordinates should
+     * be floating-point (single or double precision).
+     * @param points2 Array of the second image points of the same size and format as points1 .
+     * @param focal focal length of the camera. Note that this function assumes that points1 and points2
+     * are feature points from cameras with same focal length and principal point.
+     * @param pp principal point of the camera.
+     * @param method Method for computing a fundamental matrix.
+     * <ul>
+     *   <li>
+     *    <b>RANSAC</b> for the RANSAC algorithm.
+     *   </li>
+     *   <li>
+     *    <b>LMEDS</b> for the LMedS algorithm.
+     * line in pixels, beyond which the point is considered an outlier and is not used for computing the
+     * final fundamental matrix. It can be set to something like 1-3, depending on the accuracy of the
+     * point localization, image resolution, and the image noise.
+     * confidence (probability) that the estimated matrix is correct.
+     * for the other points. The array is computed only in the RANSAC and LMedS methods.
+     *   </li>
+     * </ul>
+     *
+     * This function differs from the one above that it computes camera matrix from focal length and
+     * principal point:
+     *
+     * \(K =
+     * \begin{bmatrix}
+     * f &amp; 0 &amp; x_{pp}  \\
+     * 0 &amp; f &amp; y_{pp}  \\
+     * 0 &amp; 0 &amp; 1
+     * \end{bmatrix}\)
+     * @return automatically generated
+     */
+    public static Mat findEssentialMat(Mat points1, Mat points2, double focal, Point pp, int method) {
+        return new Mat(findEssentialMat_8(points1.nativeObj, points2.nativeObj, focal, pp.x, pp.y, method));
+    }
+
+    /**
+     *
+     * @param points1 Array of N (N &gt;= 5) 2D points from the first image. The point coordinates should
+     * be floating-point (single or double precision).
+     * @param points2 Array of the second image points of the same size and format as points1 .
+     * @param focal focal length of the camera. Note that this function assumes that points1 and points2
+     * are feature points from cameras with same focal length and principal point.
+     * @param pp principal point of the camera.
+     * <ul>
+     *   <li>
+     *    <b>RANSAC</b> for the RANSAC algorithm.
+     *   </li>
+     *   <li>
+     *    <b>LMEDS</b> for the LMedS algorithm.
+     * line in pixels, beyond which the point is considered an outlier and is not used for computing the
+     * final fundamental matrix. It can be set to something like 1-3, depending on the accuracy of the
+     * point localization, image resolution, and the image noise.
+     * confidence (probability) that the estimated matrix is correct.
+     * for the other points. The array is computed only in the RANSAC and LMedS methods.
+     *   </li>
+     * </ul>
+     *
+     * This function differs from the one above that it computes camera matrix from focal length and
+     * principal point:
+     *
+     * \(K =
+     * \begin{bmatrix}
+     * f &amp; 0 &amp; x_{pp}  \\
+     * 0 &amp; f &amp; y_{pp}  \\
+     * 0 &amp; 0 &amp; 1
+     * \end{bmatrix}\)
+     * @return automatically generated
+     */
+    public static Mat findEssentialMat(Mat points1, Mat points2, double focal, Point pp) {
+        return new Mat(findEssentialMat_9(points1.nativeObj, points2.nativeObj, focal, pp.x, pp.y));
+    }
+
+    /**
+     *
+     * @param points1 Array of N (N &gt;= 5) 2D points from the first image. The point coordinates should
+     * be floating-point (single or double precision).
+     * @param points2 Array of the second image points of the same size and format as points1 .
+     * @param focal focal length of the camera. Note that this function assumes that points1 and points2
+     * are feature points from cameras with same focal length and principal point.
+     * <ul>
+     *   <li>
+     *    <b>RANSAC</b> for the RANSAC algorithm.
+     *   </li>
+     *   <li>
+     *    <b>LMEDS</b> for the LMedS algorithm.
+     * line in pixels, beyond which the point is considered an outlier and is not used for computing the
+     * final fundamental matrix. It can be set to something like 1-3, depending on the accuracy of the
+     * point localization, image resolution, and the image noise.
+     * confidence (probability) that the estimated matrix is correct.
+     * for the other points. The array is computed only in the RANSAC and LMedS methods.
+     *   </li>
+     * </ul>
+     *
+     * This function differs from the one above that it computes camera matrix from focal length and
+     * principal point:
+     *
+     * \(K =
+     * \begin{bmatrix}
+     * f &amp; 0 &amp; x_{pp}  \\
+     * 0 &amp; f &amp; y_{pp}  \\
+     * 0 &amp; 0 &amp; 1
+     * \end{bmatrix}\)
+     * @return automatically generated
+     */
+    public static Mat findEssentialMat(Mat points1, Mat points2, double focal) {
+        return new Mat(findEssentialMat_10(points1.nativeObj, points2.nativeObj, focal));
+    }
+
+    /**
+     *
+     * @param points1 Array of N (N &gt;= 5) 2D points from the first image. The point coordinates should
+     * be floating-point (single or double precision).
+     * @param points2 Array of the second image points of the same size and format as points1 .
+     * are feature points from cameras with same focal length and principal point.
+     * <ul>
+     *   <li>
+     *    <b>RANSAC</b> for the RANSAC algorithm.
+     *   </li>
+     *   <li>
+     *    <b>LMEDS</b> for the LMedS algorithm.
+     * line in pixels, beyond which the point is considered an outlier and is not used for computing the
+     * final fundamental matrix. It can be set to something like 1-3, depending on the accuracy of the
+     * point localization, image resolution, and the image noise.
+     * confidence (probability) that the estimated matrix is correct.
+     * for the other points. The array is computed only in the RANSAC and LMedS methods.
+     *   </li>
+     * </ul>
+     *
+     * This function differs from the one above that it computes camera matrix from focal length and
+     * principal point:
+     *
+     * \(K =
+     * \begin{bmatrix}
+     * f &amp; 0 &amp; x_{pp}  \\
+     * 0 &amp; f &amp; y_{pp}  \\
+     * 0 &amp; 0 &amp; 1
+     * \end{bmatrix}\)
+     * @return automatically generated
+     */
+    public static Mat findEssentialMat(Mat points1, Mat points2) {
+        return new Mat(findEssentialMat_11(points1.nativeObj, points2.nativeObj));
+    }
+
+
+    //
+    // C++:  Mat cv::findFundamentalMat(vector_Point2f points1, vector_Point2f points2, int method, double ransacReprojThreshold, double confidence, int maxIters, Mat& mask = Mat())
+    //
+
+    /**
+     * Calculates a fundamental matrix from the corresponding points in two images.
+     *
+     * @param points1 Array of N points from the first image. The point coordinates should be
+     * floating-point (single or double precision).
+     * @param points2 Array of the second image points of the same size and format as points1 .
+     * @param method Method for computing a fundamental matrix.
+     * <ul>
+     *   <li>
+     *    <b>CV_FM_7POINT</b> for a 7-point algorithm. \(N = 7\)
+     *   </li>
+     *   <li>
+     *    <b>CV_FM_8POINT</b> for an 8-point algorithm. \(N \ge 8\)
+     *   </li>
+     *   <li>
+     *    <b>CV_FM_RANSAC</b> for the RANSAC algorithm. \(N \ge 8\)
+     *   </li>
+     *   <li>
+     *    <b>CV_FM_LMEDS</b> for the LMedS algorithm. \(N \ge 8\)
+     * @param ransacReprojThreshold Parameter used only for RANSAC. It is the maximum distance from a point to an epipolar
+     * line in pixels, beyond which the point is considered an outlier and is not used for computing the
+     * final fundamental matrix. It can be set to something like 1-3, depending on the accuracy of the
+     * point localization, image resolution, and the image noise.
+     * @param confidence Parameter used for the RANSAC and LMedS methods only. It specifies a desirable level
+     * of confidence (probability) that the estimated matrix is correct.
+     * @param mask
+     * @param maxIters The maximum number of robust method iterations.
+     *   </li>
+     * </ul>
+     *
+     * The epipolar geometry is described by the following equation:
+     *
+     * \([p_2; 1]^T F [p_1; 1] = 0\)
+     *
+     * where \(F\) is a fundamental matrix, \(p_1\) and \(p_2\) are corresponding points in the first and the
+     * second images, respectively.
+     *
+     * The function calculates the fundamental matrix using one of four methods listed above and returns
+     * the found fundamental matrix. Normally just one matrix is found. But in case of the 7-point
+     * algorithm, the function may return up to 3 solutions ( \(9 \times 3\) matrix that stores all 3
+     * matrices sequentially).
+     *
+     * The calculated fundamental matrix may be passed further to computeCorrespondEpilines that finds the
+     * epipolar lines corresponding to the specified points. It can also be passed to
+     * stereoRectifyUncalibrated to compute the rectification transformation. :
+     * <code>
+     *     // Example. Estimation of fundamental matrix using the RANSAC algorithm
+     *     int point_count = 100;
+     *     vector&lt;Point2f&gt; points1(point_count);
+     *     vector&lt;Point2f&gt; points2(point_count);
+     *
+     *     // initialize the points here ...
+     *     for( int i = 0; i &lt; point_count; i++ )
+     *     {
+     *         points1[i] = ...;
+     *         points2[i] = ...;
+     *     }
+     *
+     *     Mat fundamental_matrix =
+     *      findFundamentalMat(points1, points2, FM_RANSAC, 3, 0.99);
+     * </code>
+     * @return automatically generated
+     */
+    public static Mat findFundamentalMat(MatOfPoint2f points1, MatOfPoint2f points2, int method, double ransacReprojThreshold, double confidence, int maxIters, Mat mask) {
+        Mat points1_mat = points1;
+        Mat points2_mat = points2;
+        return new Mat(findFundamentalMat_0(points1_mat.nativeObj, points2_mat.nativeObj, method, ransacReprojThreshold, confidence, maxIters, mask.nativeObj));
+    }
+
+    /**
+     * Calculates a fundamental matrix from the corresponding points in two images.
+     *
+     * @param points1 Array of N points from the first image. The point coordinates should be
+     * floating-point (single or double precision).
+     * @param points2 Array of the second image points of the same size and format as points1 .
+     * @param method Method for computing a fundamental matrix.
+     * <ul>
+     *   <li>
+     *    <b>CV_FM_7POINT</b> for a 7-point algorithm. \(N = 7\)
+     *   </li>
+     *   <li>
+     *    <b>CV_FM_8POINT</b> for an 8-point algorithm. \(N \ge 8\)
+     *   </li>
+     *   <li>
+     *    <b>CV_FM_RANSAC</b> for the RANSAC algorithm. \(N \ge 8\)
+     *   </li>
+     *   <li>
+     *    <b>CV_FM_LMEDS</b> for the LMedS algorithm. \(N \ge 8\)
+     * @param ransacReprojThreshold Parameter used only for RANSAC. It is the maximum distance from a point to an epipolar
+     * line in pixels, beyond which the point is considered an outlier and is not used for computing the
+     * final fundamental matrix. It can be set to something like 1-3, depending on the accuracy of the
+     * point localization, image resolution, and the image noise.
+     * @param confidence Parameter used for the RANSAC and LMedS methods only. It specifies a desirable level
+     * of confidence (probability) that the estimated matrix is correct.
+     * @param maxIters The maximum number of robust method iterations.
+     *   </li>
+     * </ul>
+     *
+     * The epipolar geometry is described by the following equation:
+     *
+     * \([p_2; 1]^T F [p_1; 1] = 0\)
+     *
+     * where \(F\) is a fundamental matrix, \(p_1\) and \(p_2\) are corresponding points in the first and the
+     * second images, respectively.
+     *
+     * The function calculates the fundamental matrix using one of four methods listed above and returns
+     * the found fundamental matrix. Normally just one matrix is found. But in case of the 7-point
+     * algorithm, the function may return up to 3 solutions ( \(9 \times 3\) matrix that stores all 3
+     * matrices sequentially).
+     *
+     * The calculated fundamental matrix may be passed further to computeCorrespondEpilines that finds the
+     * epipolar lines corresponding to the specified points. It can also be passed to
+     * stereoRectifyUncalibrated to compute the rectification transformation. :
+     * <code>
+     *     // Example. Estimation of fundamental matrix using the RANSAC algorithm
+     *     int point_count = 100;
+     *     vector&lt;Point2f&gt; points1(point_count);
+     *     vector&lt;Point2f&gt; points2(point_count);
+     *
+     *     // initialize the points here ...
+     *     for( int i = 0; i &lt; point_count; i++ )
+     *     {
+     *         points1[i] = ...;
+     *         points2[i] = ...;
+     *     }
+     *
+     *     Mat fundamental_matrix =
+     *      findFundamentalMat(points1, points2, FM_RANSAC, 3, 0.99);
+     * </code>
+     * @return automatically generated
+     */
+    public static Mat findFundamentalMat(MatOfPoint2f points1, MatOfPoint2f points2, int method, double ransacReprojThreshold, double confidence, int maxIters) {
+        Mat points1_mat = points1;
+        Mat points2_mat = points2;
+        return new Mat(findFundamentalMat_1(points1_mat.nativeObj, points2_mat.nativeObj, method, ransacReprojThreshold, confidence, maxIters));
+    }
+
+
+    //
+    // C++:  Mat cv::findFundamentalMat(vector_Point2f points1, vector_Point2f points2, int method = FM_RANSAC, double ransacReprojThreshold = 3., double confidence = 0.99, Mat& mask = Mat())
+    //
+
+    public static Mat findFundamentalMat(MatOfPoint2f points1, MatOfPoint2f points2, int method, double ransacReprojThreshold, double confidence, Mat mask) {
+        Mat points1_mat = points1;
+        Mat points2_mat = points2;
+        return new Mat(findFundamentalMat_2(points1_mat.nativeObj, points2_mat.nativeObj, method, ransacReprojThreshold, confidence, mask.nativeObj));
+    }
+
+    public static Mat findFundamentalMat(MatOfPoint2f points1, MatOfPoint2f points2, int method, double ransacReprojThreshold, double confidence) {
+        Mat points1_mat = points1;
+        Mat points2_mat = points2;
+        return new Mat(findFundamentalMat_3(points1_mat.nativeObj, points2_mat.nativeObj, method, ransacReprojThreshold, confidence));
+    }
+
+    public static Mat findFundamentalMat(MatOfPoint2f points1, MatOfPoint2f points2, int method, double ransacReprojThreshold) {
+        Mat points1_mat = points1;
+        Mat points2_mat = points2;
+        return new Mat(findFundamentalMat_4(points1_mat.nativeObj, points2_mat.nativeObj, method, ransacReprojThreshold));
+    }
+
+    public static Mat findFundamentalMat(MatOfPoint2f points1, MatOfPoint2f points2, int method) {
+        Mat points1_mat = points1;
+        Mat points2_mat = points2;
+        return new Mat(findFundamentalMat_5(points1_mat.nativeObj, points2_mat.nativeObj, method));
+    }
+
+    public static Mat findFundamentalMat(MatOfPoint2f points1, MatOfPoint2f points2) {
+        Mat points1_mat = points1;
+        Mat points2_mat = points2;
+        return new Mat(findFundamentalMat_6(points1_mat.nativeObj, points2_mat.nativeObj));
+    }
+
+
+    //
+    // C++:  Mat cv::findHomography(vector_Point2f srcPoints, vector_Point2f dstPoints, int method = 0, double ransacReprojThreshold = 3, Mat& mask = Mat(), int maxIters = 2000, double confidence = 0.995)
+    //
+
+    /**
+     * Finds a perspective transformation between two planes.
+     *
+     * @param srcPoints Coordinates of the points in the original plane, a matrix of the type CV_32FC2
+     * or vector&lt;Point2f&gt; .
+     * @param dstPoints Coordinates of the points in the target plane, a matrix of the type CV_32FC2 or
+     * a vector&lt;Point2f&gt; .
+     * @param method Method used to compute a homography matrix. The following methods are possible:
+     * <ul>
+     *   <li>
+     *    <b>0</b> - a regular method using all the points, i.e., the least squares method
+     *   </li>
+     *   <li>
+     *    <b>RANSAC</b> - RANSAC-based robust method
+     *   </li>
+     *   <li>
+     *    <b>LMEDS</b> - Least-Median robust method
+     *   </li>
+     *   <li>
+     *    <b>RHO</b> - PROSAC-based robust method
+     * @param ransacReprojThreshold Maximum allowed reprojection error to treat a point pair as an inlier
+     * (used in the RANSAC and RHO methods only). That is, if
+     * \(\| \texttt{dstPoints} _i -  \texttt{convertPointsHomogeneous} ( \texttt{H} * \texttt{srcPoints} _i) \|_2  &gt;  \texttt{ransacReprojThreshold}\)
+     * then the point \(i\) is considered as an outlier. If srcPoints and dstPoints are measured in pixels,
+     * it usually makes sense to set this parameter somewhere in the range of 1 to 10.
+     * @param mask Optional output mask set by a robust method ( RANSAC or LMEDS ). Note that the input
+     * mask values are ignored.
+     * @param maxIters The maximum number of RANSAC iterations.
+     * @param confidence Confidence level, between 0 and 1.
+     *   </li>
+     * </ul>
+     *
+     * The function finds and returns the perspective transformation \(H\) between the source and the
+     * destination planes:
+     *
+     * \(s_i  \vecthree{x'_i}{y'_i}{1} \sim H  \vecthree{x_i}{y_i}{1}\)
+     *
+     * so that the back-projection error
+     *
+     * \(\sum _i \left ( x'_i- \frac{h_{11} x_i + h_{12} y_i + h_{13}}{h_{31} x_i + h_{32} y_i + h_{33}} \right )^2+ \left ( y'_i- \frac{h_{21} x_i + h_{22} y_i + h_{23}}{h_{31} x_i + h_{32} y_i + h_{33}} \right )^2\)
+     *
+     * is minimized. If the parameter method is set to the default value 0, the function uses all the point
+     * pairs to compute an initial homography estimate with a simple least-squares scheme.
+     *
+     * However, if not all of the point pairs ( \(srcPoints_i\), \(dstPoints_i\) ) fit the rigid perspective
+     * transformation (that is, there are some outliers), this initial estimate will be poor. In this case,
+     * you can use one of the three robust methods. The methods RANSAC, LMeDS and RHO try many different
+     * random subsets of the corresponding point pairs (of four pairs each, collinear pairs are discarded), estimate the homography matrix
+     * using this subset and a simple least-squares algorithm, and then compute the quality/goodness of the
+     * computed homography (which is the number of inliers for RANSAC or the least median re-projection error for
+     * LMeDS). The best subset is then used to produce the initial estimate of the homography matrix and
+     * the mask of inliers/outliers.
+     *
+     * Regardless of the method, robust or not, the computed homography matrix is refined further (using
+     * inliers only in case of a robust method) with the Levenberg-Marquardt method to reduce the
+     * re-projection error even more.
+     *
+     * The methods RANSAC and RHO can handle practically any ratio of outliers but need a threshold to
+     * distinguish inliers from outliers. The method LMeDS does not need any threshold but it works
+     * correctly only when there are more than 50% of inliers. Finally, if there are no outliers and the
+     * noise is rather small, use the default method (method=0).
+     *
+     * The function is used to find initial intrinsic and extrinsic matrices. Homography matrix is
+     * determined up to a scale. Thus, it is normalized so that \(h_{33}=1\). Note that whenever an \(H\) matrix
+     * cannot be estimated, an empty one will be returned.
+     *
+     * SEE:
+     * getAffineTransform, estimateAffine2D, estimateAffinePartial2D, getPerspectiveTransform, warpPerspective,
+     * perspectiveTransform
+     * @return automatically generated
+     */
+    public static Mat findHomography(MatOfPoint2f srcPoints, MatOfPoint2f dstPoints, int method, double ransacReprojThreshold, Mat mask, int maxIters, double confidence) {
+        Mat srcPoints_mat = srcPoints;
+        Mat dstPoints_mat = dstPoints;
+        return new Mat(findHomography_0(srcPoints_mat.nativeObj, dstPoints_mat.nativeObj, method, ransacReprojThreshold, mask.nativeObj, maxIters, confidence));
+    }
+
+    /**
+     * Finds a perspective transformation between two planes.
+     *
+     * @param srcPoints Coordinates of the points in the original plane, a matrix of the type CV_32FC2
+     * or vector&lt;Point2f&gt; .
+     * @param dstPoints Coordinates of the points in the target plane, a matrix of the type CV_32FC2 or
+     * a vector&lt;Point2f&gt; .
+     * @param method Method used to compute a homography matrix. The following methods are possible:
+     * <ul>
+     *   <li>
+     *    <b>0</b> - a regular method using all the points, i.e., the least squares method
+     *   </li>
+     *   <li>
+     *    <b>RANSAC</b> - RANSAC-based robust method
+     *   </li>
+     *   <li>
+     *    <b>LMEDS</b> - Least-Median robust method
+     *   </li>
+     *   <li>
+     *    <b>RHO</b> - PROSAC-based robust method
+     * @param ransacReprojThreshold Maximum allowed reprojection error to treat a point pair as an inlier
+     * (used in the RANSAC and RHO methods only). That is, if
+     * \(\| \texttt{dstPoints} _i -  \texttt{convertPointsHomogeneous} ( \texttt{H} * \texttt{srcPoints} _i) \|_2  &gt;  \texttt{ransacReprojThreshold}\)
+     * then the point \(i\) is considered as an outlier. If srcPoints and dstPoints are measured in pixels,
+     * it usually makes sense to set this parameter somewhere in the range of 1 to 10.
+     * @param mask Optional output mask set by a robust method ( RANSAC or LMEDS ). Note that the input
+     * mask values are ignored.
+     * @param maxIters The maximum number of RANSAC iterations.
+     *   </li>
+     * </ul>
+     *
+     * The function finds and returns the perspective transformation \(H\) between the source and the
+     * destination planes:
+     *
+     * \(s_i  \vecthree{x'_i}{y'_i}{1} \sim H  \vecthree{x_i}{y_i}{1}\)
+     *
+     * so that the back-projection error
+     *
+     * \(\sum _i \left ( x'_i- \frac{h_{11} x_i + h_{12} y_i + h_{13}}{h_{31} x_i + h_{32} y_i + h_{33}} \right )^2+ \left ( y'_i- \frac{h_{21} x_i + h_{22} y_i + h_{23}}{h_{31} x_i + h_{32} y_i + h_{33}} \right )^2\)
+     *
+     * is minimized. If the parameter method is set to the default value 0, the function uses all the point
+     * pairs to compute an initial homography estimate with a simple least-squares scheme.
+     *
+     * However, if not all of the point pairs ( \(srcPoints_i\), \(dstPoints_i\) ) fit the rigid perspective
+     * transformation (that is, there are some outliers), this initial estimate will be poor. In this case,
+     * you can use one of the three robust methods. The methods RANSAC, LMeDS and RHO try many different
+     * random subsets of the corresponding point pairs (of four pairs each, collinear pairs are discarded), estimate the homography matrix
+     * using this subset and a simple least-squares algorithm, and then compute the quality/goodness of the
+     * computed homography (which is the number of inliers for RANSAC or the least median re-projection error for
+     * LMeDS). The best subset is then used to produce the initial estimate of the homography matrix and
+     * the mask of inliers/outliers.
+     *
+     * Regardless of the method, robust or not, the computed homography matrix is refined further (using
+     * inliers only in case of a robust method) with the Levenberg-Marquardt method to reduce the
+     * re-projection error even more.
+     *
+     * The methods RANSAC and RHO can handle practically any ratio of outliers but need a threshold to
+     * distinguish inliers from outliers. The method LMeDS does not need any threshold but it works
+     * correctly only when there are more than 50% of inliers. Finally, if there are no outliers and the
+     * noise is rather small, use the default method (method=0).
+     *
+     * The function is used to find initial intrinsic and extrinsic matrices. Homography matrix is
+     * determined up to a scale. Thus, it is normalized so that \(h_{33}=1\). Note that whenever an \(H\) matrix
+     * cannot be estimated, an empty one will be returned.
+     *
+     * SEE:
+     * getAffineTransform, estimateAffine2D, estimateAffinePartial2D, getPerspectiveTransform, warpPerspective,
+     * perspectiveTransform
+     * @return automatically generated
+     */
+    public static Mat findHomography(MatOfPoint2f srcPoints, MatOfPoint2f dstPoints, int method, double ransacReprojThreshold, Mat mask, int maxIters) {
+        Mat srcPoints_mat = srcPoints;
+        Mat dstPoints_mat = dstPoints;
+        return new Mat(findHomography_1(srcPoints_mat.nativeObj, dstPoints_mat.nativeObj, method, ransacReprojThreshold, mask.nativeObj, maxIters));
+    }
+
+    /**
+     * Finds a perspective transformation between two planes.
+     *
+     * @param srcPoints Coordinates of the points in the original plane, a matrix of the type CV_32FC2
+     * or vector&lt;Point2f&gt; .
+     * @param dstPoints Coordinates of the points in the target plane, a matrix of the type CV_32FC2 or
+     * a vector&lt;Point2f&gt; .
+     * @param method Method used to compute a homography matrix. The following methods are possible:
+     * <ul>
+     *   <li>
+     *    <b>0</b> - a regular method using all the points, i.e., the least squares method
+     *   </li>
+     *   <li>
+     *    <b>RANSAC</b> - RANSAC-based robust method
+     *   </li>
+     *   <li>
+     *    <b>LMEDS</b> - Least-Median robust method
+     *   </li>
+     *   <li>
+     *    <b>RHO</b> - PROSAC-based robust method
+     * @param ransacReprojThreshold Maximum allowed reprojection error to treat a point pair as an inlier
+     * (used in the RANSAC and RHO methods only). That is, if
+     * \(\| \texttt{dstPoints} _i -  \texttt{convertPointsHomogeneous} ( \texttt{H} * \texttt{srcPoints} _i) \|_2  &gt;  \texttt{ransacReprojThreshold}\)
+     * then the point \(i\) is considered as an outlier. If srcPoints and dstPoints are measured in pixels,
+     * it usually makes sense to set this parameter somewhere in the range of 1 to 10.
+     * @param mask Optional output mask set by a robust method ( RANSAC or LMEDS ). Note that the input
+     * mask values are ignored.
+     *   </li>
+     * </ul>
+     *
+     * The function finds and returns the perspective transformation \(H\) between the source and the
+     * destination planes:
+     *
+     * \(s_i  \vecthree{x'_i}{y'_i}{1} \sim H  \vecthree{x_i}{y_i}{1}\)
+     *
+     * so that the back-projection error
+     *
+     * \(\sum _i \left ( x'_i- \frac{h_{11} x_i + h_{12} y_i + h_{13}}{h_{31} x_i + h_{32} y_i + h_{33}} \right )^2+ \left ( y'_i- \frac{h_{21} x_i + h_{22} y_i + h_{23}}{h_{31} x_i + h_{32} y_i + h_{33}} \right )^2\)
+     *
+     * is minimized. If the parameter method is set to the default value 0, the function uses all the point
+     * pairs to compute an initial homography estimate with a simple least-squares scheme.
+     *
+     * However, if not all of the point pairs ( \(srcPoints_i\), \(dstPoints_i\) ) fit the rigid perspective
+     * transformation (that is, there are some outliers), this initial estimate will be poor. In this case,
+     * you can use one of the three robust methods. The methods RANSAC, LMeDS and RHO try many different
+     * random subsets of the corresponding point pairs (of four pairs each, collinear pairs are discarded), estimate the homography matrix
+     * using this subset and a simple least-squares algorithm, and then compute the quality/goodness of the
+     * computed homography (which is the number of inliers for RANSAC or the least median re-projection error for
+     * LMeDS). The best subset is then used to produce the initial estimate of the homography matrix and
+     * the mask of inliers/outliers.
+     *
+     * Regardless of the method, robust or not, the computed homography matrix is refined further (using
+     * inliers only in case of a robust method) with the Levenberg-Marquardt method to reduce the
+     * re-projection error even more.
+     *
+     * The methods RANSAC and RHO can handle practically any ratio of outliers but need a threshold to
+     * distinguish inliers from outliers. The method LMeDS does not need any threshold but it works
+     * correctly only when there are more than 50% of inliers. Finally, if there are no outliers and the
+     * noise is rather small, use the default method (method=0).
+     *
+     * The function is used to find initial intrinsic and extrinsic matrices. Homography matrix is
+     * determined up to a scale. Thus, it is normalized so that \(h_{33}=1\). Note that whenever an \(H\) matrix
+     * cannot be estimated, an empty one will be returned.
+     *
+     * SEE:
+     * getAffineTransform, estimateAffine2D, estimateAffinePartial2D, getPerspectiveTransform, warpPerspective,
+     * perspectiveTransform
+     * @return automatically generated
+     */
+    public static Mat findHomography(MatOfPoint2f srcPoints, MatOfPoint2f dstPoints, int method, double ransacReprojThreshold, Mat mask) {
+        Mat srcPoints_mat = srcPoints;
+        Mat dstPoints_mat = dstPoints;
+        return new Mat(findHomography_2(srcPoints_mat.nativeObj, dstPoints_mat.nativeObj, method, ransacReprojThreshold, mask.nativeObj));
+    }
+
+    /**
+     * Finds a perspective transformation between two planes.
+     *
+     * @param srcPoints Coordinates of the points in the original plane, a matrix of the type CV_32FC2
+     * or vector&lt;Point2f&gt; .
+     * @param dstPoints Coordinates of the points in the target plane, a matrix of the type CV_32FC2 or
+     * a vector&lt;Point2f&gt; .
+     * @param method Method used to compute a homography matrix. The following methods are possible:
+     * <ul>
+     *   <li>
+     *    <b>0</b> - a regular method using all the points, i.e., the least squares method
+     *   </li>
+     *   <li>
+     *    <b>RANSAC</b> - RANSAC-based robust method
+     *   </li>
+     *   <li>
+     *    <b>LMEDS</b> - Least-Median robust method
+     *   </li>
+     *   <li>
+     *    <b>RHO</b> - PROSAC-based robust method
+     * @param ransacReprojThreshold Maximum allowed reprojection error to treat a point pair as an inlier
+     * (used in the RANSAC and RHO methods only). That is, if
+     * \(\| \texttt{dstPoints} _i -  \texttt{convertPointsHomogeneous} ( \texttt{H} * \texttt{srcPoints} _i) \|_2  &gt;  \texttt{ransacReprojThreshold}\)
+     * then the point \(i\) is considered as an outlier. If srcPoints and dstPoints are measured in pixels,
+     * it usually makes sense to set this parameter somewhere in the range of 1 to 10.
+     * mask values are ignored.
+     *   </li>
+     * </ul>
+     *
+     * The function finds and returns the perspective transformation \(H\) between the source and the
+     * destination planes:
+     *
+     * \(s_i  \vecthree{x'_i}{y'_i}{1} \sim H  \vecthree{x_i}{y_i}{1}\)
+     *
+     * so that the back-projection error
+     *
+     * \(\sum _i \left ( x'_i- \frac{h_{11} x_i + h_{12} y_i + h_{13}}{h_{31} x_i + h_{32} y_i + h_{33}} \right )^2+ \left ( y'_i- \frac{h_{21} x_i + h_{22} y_i + h_{23}}{h_{31} x_i + h_{32} y_i + h_{33}} \right )^2\)
+     *
+     * is minimized. If the parameter method is set to the default value 0, the function uses all the point
+     * pairs to compute an initial homography estimate with a simple least-squares scheme.
+     *
+     * However, if not all of the point pairs ( \(srcPoints_i\), \(dstPoints_i\) ) fit the rigid perspective
+     * transformation (that is, there are some outliers), this initial estimate will be poor. In this case,
+     * you can use one of the three robust methods. The methods RANSAC, LMeDS and RHO try many different
+     * random subsets of the corresponding point pairs (of four pairs each, collinear pairs are discarded), estimate the homography matrix
+     * using this subset and a simple least-squares algorithm, and then compute the quality/goodness of the
+     * computed homography (which is the number of inliers for RANSAC or the least median re-projection error for
+     * LMeDS). The best subset is then used to produce the initial estimate of the homography matrix and
+     * the mask of inliers/outliers.
+     *
+     * Regardless of the method, robust or not, the computed homography matrix is refined further (using
+     * inliers only in case of a robust method) with the Levenberg-Marquardt method to reduce the
+     * re-projection error even more.
+     *
+     * The methods RANSAC and RHO can handle practically any ratio of outliers but need a threshold to
+     * distinguish inliers from outliers. The method LMeDS does not need any threshold but it works
+     * correctly only when there are more than 50% of inliers. Finally, if there are no outliers and the
+     * noise is rather small, use the default method (method=0).
+     *
+     * The function is used to find initial intrinsic and extrinsic matrices. Homography matrix is
+     * determined up to a scale. Thus, it is normalized so that \(h_{33}=1\). Note that whenever an \(H\) matrix
+     * cannot be estimated, an empty one will be returned.
+     *
+     * SEE:
+     * getAffineTransform, estimateAffine2D, estimateAffinePartial2D, getPerspectiveTransform, warpPerspective,
+     * perspectiveTransform
+     * @return automatically generated
+     */
+    public static Mat findHomography(MatOfPoint2f srcPoints, MatOfPoint2f dstPoints, int method, double ransacReprojThreshold) {
+        Mat srcPoints_mat = srcPoints;
+        Mat dstPoints_mat = dstPoints;
+        return new Mat(findHomography_3(srcPoints_mat.nativeObj, dstPoints_mat.nativeObj, method, ransacReprojThreshold));
+    }
+
+    /**
+     * Finds a perspective transformation between two planes.
+     *
+     * @param srcPoints Coordinates of the points in the original plane, a matrix of the type CV_32FC2
+     * or vector&lt;Point2f&gt; .
+     * @param dstPoints Coordinates of the points in the target plane, a matrix of the type CV_32FC2 or
+     * a vector&lt;Point2f&gt; .
+     * @param method Method used to compute a homography matrix. The following methods are possible:
+     * <ul>
+     *   <li>
+     *    <b>0</b> - a regular method using all the points, i.e., the least squares method
+     *   </li>
+     *   <li>
+     *    <b>RANSAC</b> - RANSAC-based robust method
+     *   </li>
+     *   <li>
+     *    <b>LMEDS</b> - Least-Median robust method
+     *   </li>
+     *   <li>
+     *    <b>RHO</b> - PROSAC-based robust method
+     * (used in the RANSAC and RHO methods only). That is, if
+     * \(\| \texttt{dstPoints} _i -  \texttt{convertPointsHomogeneous} ( \texttt{H} * \texttt{srcPoints} _i) \|_2  &gt;  \texttt{ransacReprojThreshold}\)
+     * then the point \(i\) is considered as an outlier. If srcPoints and dstPoints are measured in pixels,
+     * it usually makes sense to set this parameter somewhere in the range of 1 to 10.
+     * mask values are ignored.
+     *   </li>
+     * </ul>
+     *
+     * The function finds and returns the perspective transformation \(H\) between the source and the
+     * destination planes:
+     *
+     * \(s_i  \vecthree{x'_i}{y'_i}{1} \sim H  \vecthree{x_i}{y_i}{1}\)
+     *
+     * so that the back-projection error
+     *
+     * \(\sum _i \left ( x'_i- \frac{h_{11} x_i + h_{12} y_i + h_{13}}{h_{31} x_i + h_{32} y_i + h_{33}} \right )^2+ \left ( y'_i- \frac{h_{21} x_i + h_{22} y_i + h_{23}}{h_{31} x_i + h_{32} y_i + h_{33}} \right )^2\)
+     *
+     * is minimized. If the parameter method is set to the default value 0, the function uses all the point
+     * pairs to compute an initial homography estimate with a simple least-squares scheme.
+     *
+     * However, if not all of the point pairs ( \(srcPoints_i\), \(dstPoints_i\) ) fit the rigid perspective
+     * transformation (that is, there are some outliers), this initial estimate will be poor. In this case,
+     * you can use one of the three robust methods. The methods RANSAC, LMeDS and RHO try many different
+     * random subsets of the corresponding point pairs (of four pairs each, collinear pairs are discarded), estimate the homography matrix
+     * using this subset and a simple least-squares algorithm, and then compute the quality/goodness of the
+     * computed homography (which is the number of inliers for RANSAC or the least median re-projection error for
+     * LMeDS). The best subset is then used to produce the initial estimate of the homography matrix and
+     * the mask of inliers/outliers.
+     *
+     * Regardless of the method, robust or not, the computed homography matrix is refined further (using
+     * inliers only in case of a robust method) with the Levenberg-Marquardt method to reduce the
+     * re-projection error even more.
+     *
+     * The methods RANSAC and RHO can handle practically any ratio of outliers but need a threshold to
+     * distinguish inliers from outliers. The method LMeDS does not need any threshold but it works
+     * correctly only when there are more than 50% of inliers. Finally, if there are no outliers and the
+     * noise is rather small, use the default method (method=0).
+     *
+     * The function is used to find initial intrinsic and extrinsic matrices. Homography matrix is
+     * determined up to a scale. Thus, it is normalized so that \(h_{33}=1\). Note that whenever an \(H\) matrix
+     * cannot be estimated, an empty one will be returned.
+     *
+     * SEE:
+     * getAffineTransform, estimateAffine2D, estimateAffinePartial2D, getPerspectiveTransform, warpPerspective,
+     * perspectiveTransform
+     * @return automatically generated
+     */
+    public static Mat findHomography(MatOfPoint2f srcPoints, MatOfPoint2f dstPoints, int method) {
+        Mat srcPoints_mat = srcPoints;
+        Mat dstPoints_mat = dstPoints;
+        return new Mat(findHomography_4(srcPoints_mat.nativeObj, dstPoints_mat.nativeObj, method));
+    }
+
+    /**
+     * Finds a perspective transformation between two planes.
+     *
+     * @param srcPoints Coordinates of the points in the original plane, a matrix of the type CV_32FC2
+     * or vector&lt;Point2f&gt; .
+     * @param dstPoints Coordinates of the points in the target plane, a matrix of the type CV_32FC2 or
+     * a vector&lt;Point2f&gt; .
+     * <ul>
+     *   <li>
+     *    <b>0</b> - a regular method using all the points, i.e., the least squares method
+     *   </li>
+     *   <li>
+     *    <b>RANSAC</b> - RANSAC-based robust method
+     *   </li>
+     *   <li>
+     *    <b>LMEDS</b> - Least-Median robust method
+     *   </li>
+     *   <li>
+     *    <b>RHO</b> - PROSAC-based robust method
+     * (used in the RANSAC and RHO methods only). That is, if
+     * \(\| \texttt{dstPoints} _i -  \texttt{convertPointsHomogeneous} ( \texttt{H} * \texttt{srcPoints} _i) \|_2  &gt;  \texttt{ransacReprojThreshold}\)
+     * then the point \(i\) is considered as an outlier. If srcPoints and dstPoints are measured in pixels,
+     * it usually makes sense to set this parameter somewhere in the range of 1 to 10.
+     * mask values are ignored.
+     *   </li>
+     * </ul>
+     *
+     * The function finds and returns the perspective transformation \(H\) between the source and the
+     * destination planes:
+     *
+     * \(s_i  \vecthree{x'_i}{y'_i}{1} \sim H  \vecthree{x_i}{y_i}{1}\)
+     *
+     * so that the back-projection error
+     *
+     * \(\sum _i \left ( x'_i- \frac{h_{11} x_i + h_{12} y_i + h_{13}}{h_{31} x_i + h_{32} y_i + h_{33}} \right )^2+ \left ( y'_i- \frac{h_{21} x_i + h_{22} y_i + h_{23}}{h_{31} x_i + h_{32} y_i + h_{33}} \right )^2\)
+     *
+     * is minimized. If the parameter method is set to the default value 0, the function uses all the point
+     * pairs to compute an initial homography estimate with a simple least-squares scheme.
+     *
+     * However, if not all of the point pairs ( \(srcPoints_i\), \(dstPoints_i\) ) fit the rigid perspective
+     * transformation (that is, there are some outliers), this initial estimate will be poor. In this case,
+     * you can use one of the three robust methods. The methods RANSAC, LMeDS and RHO try many different
+     * random subsets of the corresponding point pairs (of four pairs each, collinear pairs are discarded), estimate the homography matrix
+     * using this subset and a simple least-squares algorithm, and then compute the quality/goodness of the
+     * computed homography (which is the number of inliers for RANSAC or the least median re-projection error for
+     * LMeDS). The best subset is then used to produce the initial estimate of the homography matrix and
+     * the mask of inliers/outliers.
+     *
+     * Regardless of the method, robust or not, the computed homography matrix is refined further (using
+     * inliers only in case of a robust method) with the Levenberg-Marquardt method to reduce the
+     * re-projection error even more.
+     *
+     * The methods RANSAC and RHO can handle practically any ratio of outliers but need a threshold to
+     * distinguish inliers from outliers. The method LMeDS does not need any threshold but it works
+     * correctly only when there are more than 50% of inliers. Finally, if there are no outliers and the
+     * noise is rather small, use the default method (method=0).
+     *
+     * The function is used to find initial intrinsic and extrinsic matrices. Homography matrix is
+     * determined up to a scale. Thus, it is normalized so that \(h_{33}=1\). Note that whenever an \(H\) matrix
+     * cannot be estimated, an empty one will be returned.
+     *
+     * SEE:
+     * getAffineTransform, estimateAffine2D, estimateAffinePartial2D, getPerspectiveTransform, warpPerspective,
+     * perspectiveTransform
+     * @return automatically generated
+     */
+    public static Mat findHomography(MatOfPoint2f srcPoints, MatOfPoint2f dstPoints) {
+        Mat srcPoints_mat = srcPoints;
+        Mat dstPoints_mat = dstPoints;
+        return new Mat(findHomography_5(srcPoints_mat.nativeObj, dstPoints_mat.nativeObj));
+    }
+
+
+    //
+    // C++:  Mat cv::getOptimalNewCameraMatrix(Mat cameraMatrix, Mat distCoeffs, Size imageSize, double alpha, Size newImgSize = Size(), Rect* validPixROI = 0, bool centerPrincipalPoint = false)
+    //
+
+    /**
+     * Returns the new camera matrix based on the free scaling parameter.
+     *
+     * @param cameraMatrix Input camera matrix.
+     * @param distCoeffs Input vector of distortion coefficients
+     * \((k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\) of
+     * 4, 5, 8, 12 or 14 elements. If the vector is NULL/empty, the zero distortion coefficients are
+     * assumed.
+     * @param imageSize Original image size.
+     * @param alpha Free scaling parameter between 0 (when all the pixels in the undistorted image are
+     * valid) and 1 (when all the source image pixels are retained in the undistorted image). See
+     * stereoRectify for details.
+     * @param newImgSize Image size after rectification. By default, it is set to imageSize .
+     * @param validPixROI Optional output rectangle that outlines all-good-pixels region in the
+     * undistorted image. See roi1, roi2 description in stereoRectify .
+     * @param centerPrincipalPoint Optional flag that indicates whether in the new camera matrix the
+     * principal point should be at the image center or not. By default, the principal point is chosen to
+     * best fit a subset of the source image (determined by alpha) to the corrected image.
+     * @return new_camera_matrix Output new camera matrix.
+     *
+     * The function computes and returns the optimal new camera matrix based on the free scaling parameter.
+     * By varying this parameter, you may retrieve only sensible pixels alpha=0 , keep all the original
+     * image pixels if there is valuable information in the corners alpha=1 , or get something in between.
+     * When alpha&gt;0 , the undistorted result is likely to have some black pixels corresponding to
+     * "virtual" pixels outside of the captured distorted image. The original camera matrix, distortion
+     * coefficients, the computed new camera matrix, and newImageSize should be passed to
+     * initUndistortRectifyMap to produce the maps for remap .
+     */
+    public static Mat getOptimalNewCameraMatrix(Mat cameraMatrix, Mat distCoeffs, Size imageSize, double alpha, Size newImgSize, Rect validPixROI, boolean centerPrincipalPoint) {
+        double[] validPixROI_out = new double[4];
+        Mat retVal = new Mat(getOptimalNewCameraMatrix_0(cameraMatrix.nativeObj, distCoeffs.nativeObj, imageSize.width, imageSize.height, alpha, newImgSize.width, newImgSize.height, validPixROI_out, centerPrincipalPoint));
+        if(validPixROI!=null){ validPixROI.x = (int)validPixROI_out[0]; validPixROI.y = (int)validPixROI_out[1]; validPixROI.width = (int)validPixROI_out[2]; validPixROI.height = (int)validPixROI_out[3]; } 
+        return retVal;
+    }
+
+    /**
+     * Returns the new camera matrix based on the free scaling parameter.
+     *
+     * @param cameraMatrix Input camera matrix.
+     * @param distCoeffs Input vector of distortion coefficients
+     * \((k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\) of
+     * 4, 5, 8, 12 or 14 elements. If the vector is NULL/empty, the zero distortion coefficients are
+     * assumed.
+     * @param imageSize Original image size.
+     * @param alpha Free scaling parameter between 0 (when all the pixels in the undistorted image are
+     * valid) and 1 (when all the source image pixels are retained in the undistorted image). See
+     * stereoRectify for details.
+     * @param newImgSize Image size after rectification. By default, it is set to imageSize .
+     * @param validPixROI Optional output rectangle that outlines all-good-pixels region in the
+     * undistorted image. See roi1, roi2 description in stereoRectify .
+     * principal point should be at the image center or not. By default, the principal point is chosen to
+     * best fit a subset of the source image (determined by alpha) to the corrected image.
+     * @return new_camera_matrix Output new camera matrix.
+     *
+     * The function computes and returns the optimal new camera matrix based on the free scaling parameter.
+     * By varying this parameter, you may retrieve only sensible pixels alpha=0 , keep all the original
+     * image pixels if there is valuable information in the corners alpha=1 , or get something in between.
+     * When alpha&gt;0 , the undistorted result is likely to have some black pixels corresponding to
+     * "virtual" pixels outside of the captured distorted image. The original camera matrix, distortion
+     * coefficients, the computed new camera matrix, and newImageSize should be passed to
+     * initUndistortRectifyMap to produce the maps for remap .
+     */
+    public static Mat getOptimalNewCameraMatrix(Mat cameraMatrix, Mat distCoeffs, Size imageSize, double alpha, Size newImgSize, Rect validPixROI) {
+        double[] validPixROI_out = new double[4];
+        Mat retVal = new Mat(getOptimalNewCameraMatrix_1(cameraMatrix.nativeObj, distCoeffs.nativeObj, imageSize.width, imageSize.height, alpha, newImgSize.width, newImgSize.height, validPixROI_out));
+        if(validPixROI!=null){ validPixROI.x = (int)validPixROI_out[0]; validPixROI.y = (int)validPixROI_out[1]; validPixROI.width = (int)validPixROI_out[2]; validPixROI.height = (int)validPixROI_out[3]; } 
+        return retVal;
+    }
+
+    /**
+     * Returns the new camera matrix based on the free scaling parameter.
+     *
+     * @param cameraMatrix Input camera matrix.
+     * @param distCoeffs Input vector of distortion coefficients
+     * \((k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\) of
+     * 4, 5, 8, 12 or 14 elements. If the vector is NULL/empty, the zero distortion coefficients are
+     * assumed.
+     * @param imageSize Original image size.
+     * @param alpha Free scaling parameter between 0 (when all the pixels in the undistorted image are
+     * valid) and 1 (when all the source image pixels are retained in the undistorted image). See
+     * stereoRectify for details.
+     * @param newImgSize Image size after rectification. By default, it is set to imageSize .
+     * undistorted image. See roi1, roi2 description in stereoRectify .
+     * principal point should be at the image center or not. By default, the principal point is chosen to
+     * best fit a subset of the source image (determined by alpha) to the corrected image.
+     * @return new_camera_matrix Output new camera matrix.
+     *
+     * The function computes and returns the optimal new camera matrix based on the free scaling parameter.
+     * By varying this parameter, you may retrieve only sensible pixels alpha=0 , keep all the original
+     * image pixels if there is valuable information in the corners alpha=1 , or get something in between.
+     * When alpha&gt;0 , the undistorted result is likely to have some black pixels corresponding to
+     * "virtual" pixels outside of the captured distorted image. The original camera matrix, distortion
+     * coefficients, the computed new camera matrix, and newImageSize should be passed to
+     * initUndistortRectifyMap to produce the maps for remap .
+     */
+    public static Mat getOptimalNewCameraMatrix(Mat cameraMatrix, Mat distCoeffs, Size imageSize, double alpha, Size newImgSize) {
+        return new Mat(getOptimalNewCameraMatrix_2(cameraMatrix.nativeObj, distCoeffs.nativeObj, imageSize.width, imageSize.height, alpha, newImgSize.width, newImgSize.height));
+    }
+
+    /**
+     * Returns the new camera matrix based on the free scaling parameter.
+     *
+     * @param cameraMatrix Input camera matrix.
+     * @param distCoeffs Input vector of distortion coefficients
+     * \((k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\) of
+     * 4, 5, 8, 12 or 14 elements. If the vector is NULL/empty, the zero distortion coefficients are
+     * assumed.
+     * @param imageSize Original image size.
+     * @param alpha Free scaling parameter between 0 (when all the pixels in the undistorted image are
+     * valid) and 1 (when all the source image pixels are retained in the undistorted image). See
+     * stereoRectify for details.
+     * undistorted image. See roi1, roi2 description in stereoRectify .
+     * principal point should be at the image center or not. By default, the principal point is chosen to
+     * best fit a subset of the source image (determined by alpha) to the corrected image.
+     * @return new_camera_matrix Output new camera matrix.
+     *
+     * The function computes and returns the optimal new camera matrix based on the free scaling parameter.
+     * By varying this parameter, you may retrieve only sensible pixels alpha=0 , keep all the original
+     * image pixels if there is valuable information in the corners alpha=1 , or get something in between.
+     * When alpha&gt;0 , the undistorted result is likely to have some black pixels corresponding to
+     * "virtual" pixels outside of the captured distorted image. The original camera matrix, distortion
+     * coefficients, the computed new camera matrix, and newImageSize should be passed to
+     * initUndistortRectifyMap to produce the maps for remap .
+     */
+    public static Mat getOptimalNewCameraMatrix(Mat cameraMatrix, Mat distCoeffs, Size imageSize, double alpha) {
+        return new Mat(getOptimalNewCameraMatrix_3(cameraMatrix.nativeObj, distCoeffs.nativeObj, imageSize.width, imageSize.height, alpha));
+    }
+
+
+    //
+    // C++:  Mat cv::initCameraMatrix2D(vector_vector_Point3f objectPoints, vector_vector_Point2f imagePoints, Size imageSize, double aspectRatio = 1.0)
+    //
+
+    /**
+     * Finds an initial camera matrix from 3D-2D point correspondences.
+     *
+     * @param objectPoints Vector of vectors of the calibration pattern points in the calibration pattern
+     * coordinate space. In the old interface all the per-view vectors are concatenated. See
+     * calibrateCamera for details.
+     * @param imagePoints Vector of vectors of the projections of the calibration pattern points. In the
+     * old interface all the per-view vectors are concatenated.
+     * @param imageSize Image size in pixels used to initialize the principal point.
+     * @param aspectRatio If it is zero or negative, both \(f_x\) and \(f_y\) are estimated independently.
+     * Otherwise, \(f_x = f_y * \texttt{aspectRatio}\) .
+     *
+     * The function estimates and returns an initial camera matrix for the camera calibration process.
+     * Currently, the function only supports planar calibration patterns, which are patterns where each
+     * object point has z-coordinate =0.
+     * @return automatically generated
+     */
+    public static Mat initCameraMatrix2D(List<MatOfPoint3f> objectPoints, List<MatOfPoint2f> imagePoints, Size imageSize, double aspectRatio) {
+        List<Mat> objectPoints_tmplm = new ArrayList<Mat>((objectPoints != null) ? objectPoints.size() : 0);
+        Mat objectPoints_mat = Converters.vector_vector_Point3f_to_Mat(objectPoints, objectPoints_tmplm);
+        List<Mat> imagePoints_tmplm = new ArrayList<Mat>((imagePoints != null) ? imagePoints.size() : 0);
+        Mat imagePoints_mat = Converters.vector_vector_Point2f_to_Mat(imagePoints, imagePoints_tmplm);
+        return new Mat(initCameraMatrix2D_0(objectPoints_mat.nativeObj, imagePoints_mat.nativeObj, imageSize.width, imageSize.height, aspectRatio));
+    }
+
+    /**
+     * Finds an initial camera matrix from 3D-2D point correspondences.
+     *
+     * @param objectPoints Vector of vectors of the calibration pattern points in the calibration pattern
+     * coordinate space. In the old interface all the per-view vectors are concatenated. See
+     * calibrateCamera for details.
+     * @param imagePoints Vector of vectors of the projections of the calibration pattern points. In the
+     * old interface all the per-view vectors are concatenated.
+     * @param imageSize Image size in pixels used to initialize the principal point.
+     * Otherwise, \(f_x = f_y * \texttt{aspectRatio}\) .
+     *
+     * The function estimates and returns an initial camera matrix for the camera calibration process.
+     * Currently, the function only supports planar calibration patterns, which are patterns where each
+     * object point has z-coordinate =0.
+     * @return automatically generated
+     */
+    public static Mat initCameraMatrix2D(List<MatOfPoint3f> objectPoints, List<MatOfPoint2f> imagePoints, Size imageSize) {
+        List<Mat> objectPoints_tmplm = new ArrayList<Mat>((objectPoints != null) ? objectPoints.size() : 0);
+        Mat objectPoints_mat = Converters.vector_vector_Point3f_to_Mat(objectPoints, objectPoints_tmplm);
+        List<Mat> imagePoints_tmplm = new ArrayList<Mat>((imagePoints != null) ? imagePoints.size() : 0);
+        Mat imagePoints_mat = Converters.vector_vector_Point2f_to_Mat(imagePoints, imagePoints_tmplm);
+        return new Mat(initCameraMatrix2D_1(objectPoints_mat.nativeObj, imagePoints_mat.nativeObj, imageSize.width, imageSize.height));
+    }
+
+
+    //
+    // C++:  Rect cv::getValidDisparityROI(Rect roi1, Rect roi2, int minDisparity, int numberOfDisparities, int blockSize)
+    //
+
+    public static Rect getValidDisparityROI(Rect roi1, Rect roi2, int minDisparity, int numberOfDisparities, int blockSize) {
+        return new Rect(getValidDisparityROI_0(roi1.x, roi1.y, roi1.width, roi1.height, roi2.x, roi2.y, roi2.width, roi2.height, minDisparity, numberOfDisparities, blockSize));
+    }
+
+
+    //
+    // C++:  Vec3d cv::RQDecomp3x3(Mat src, Mat& mtxR, Mat& mtxQ, Mat& Qx = Mat(), Mat& Qy = Mat(), Mat& Qz = Mat())
+    //
+
+    /**
+     * Computes an RQ decomposition of 3x3 matrices.
+     *
+     * @param src 3x3 input matrix.
+     * @param mtxR Output 3x3 upper-triangular matrix.
+     * @param mtxQ Output 3x3 orthogonal matrix.
+     * @param Qx Optional output 3x3 rotation matrix around x-axis.
+     * @param Qy Optional output 3x3 rotation matrix around y-axis.
+     * @param Qz Optional output 3x3 rotation matrix around z-axis.
+     *
+     * The function computes a RQ decomposition using the given rotations. This function is used in
+     * decomposeProjectionMatrix to decompose the left 3x3 submatrix of a projection matrix into a camera
+     * and a rotation matrix.
+     *
+     * It optionally returns three rotation matrices, one for each axis, and the three Euler angles in
+     * degrees (as the return value) that could be used in OpenGL. Note, there is always more than one
+     * sequence of rotations about the three principal axes that results in the same orientation of an
+     * object, e.g. see CITE: Slabaugh . Returned tree rotation matrices and corresponding three Euler angles
+     * are only one of the possible solutions.
+     * @return automatically generated
+     */
+    public static double[] RQDecomp3x3(Mat src, Mat mtxR, Mat mtxQ, Mat Qx, Mat Qy, Mat Qz) {
+        return RQDecomp3x3_0(src.nativeObj, mtxR.nativeObj, mtxQ.nativeObj, Qx.nativeObj, Qy.nativeObj, Qz.nativeObj);
+    }
+
+    /**
+     * Computes an RQ decomposition of 3x3 matrices.
+     *
+     * @param src 3x3 input matrix.
+     * @param mtxR Output 3x3 upper-triangular matrix.
+     * @param mtxQ Output 3x3 orthogonal matrix.
+     * @param Qx Optional output 3x3 rotation matrix around x-axis.
+     * @param Qy Optional output 3x3 rotation matrix around y-axis.
+     *
+     * The function computes a RQ decomposition using the given rotations. This function is used in
+     * decomposeProjectionMatrix to decompose the left 3x3 submatrix of a projection matrix into a camera
+     * and a rotation matrix.
+     *
+     * It optionally returns three rotation matrices, one for each axis, and the three Euler angles in
+     * degrees (as the return value) that could be used in OpenGL. Note, there is always more than one
+     * sequence of rotations about the three principal axes that results in the same orientation of an
+     * object, e.g. see CITE: Slabaugh . Returned tree rotation matrices and corresponding three Euler angles
+     * are only one of the possible solutions.
+     * @return automatically generated
+     */
+    public static double[] RQDecomp3x3(Mat src, Mat mtxR, Mat mtxQ, Mat Qx, Mat Qy) {
+        return RQDecomp3x3_1(src.nativeObj, mtxR.nativeObj, mtxQ.nativeObj, Qx.nativeObj, Qy.nativeObj);
+    }
+
+    /**
+     * Computes an RQ decomposition of 3x3 matrices.
+     *
+     * @param src 3x3 input matrix.
+     * @param mtxR Output 3x3 upper-triangular matrix.
+     * @param mtxQ Output 3x3 orthogonal matrix.
+     * @param Qx Optional output 3x3 rotation matrix around x-axis.
+     *
+     * The function computes a RQ decomposition using the given rotations. This function is used in
+     * decomposeProjectionMatrix to decompose the left 3x3 submatrix of a projection matrix into a camera
+     * and a rotation matrix.
+     *
+     * It optionally returns three rotation matrices, one for each axis, and the three Euler angles in
+     * degrees (as the return value) that could be used in OpenGL. Note, there is always more than one
+     * sequence of rotations about the three principal axes that results in the same orientation of an
+     * object, e.g. see CITE: Slabaugh . Returned tree rotation matrices and corresponding three Euler angles
+     * are only one of the possible solutions.
+     * @return automatically generated
+     */
+    public static double[] RQDecomp3x3(Mat src, Mat mtxR, Mat mtxQ, Mat Qx) {
+        return RQDecomp3x3_2(src.nativeObj, mtxR.nativeObj, mtxQ.nativeObj, Qx.nativeObj);
+    }
+
+    /**
+     * Computes an RQ decomposition of 3x3 matrices.
+     *
+     * @param src 3x3 input matrix.
+     * @param mtxR Output 3x3 upper-triangular matrix.
+     * @param mtxQ Output 3x3 orthogonal matrix.
+     *
+     * The function computes a RQ decomposition using the given rotations. This function is used in
+     * decomposeProjectionMatrix to decompose the left 3x3 submatrix of a projection matrix into a camera
+     * and a rotation matrix.
+     *
+     * It optionally returns three rotation matrices, one for each axis, and the three Euler angles in
+     * degrees (as the return value) that could be used in OpenGL. Note, there is always more than one
+     * sequence of rotations about the three principal axes that results in the same orientation of an
+     * object, e.g. see CITE: Slabaugh . Returned tree rotation matrices and corresponding three Euler angles
+     * are only one of the possible solutions.
+     * @return automatically generated
+     */
+    public static double[] RQDecomp3x3(Mat src, Mat mtxR, Mat mtxQ) {
+        return RQDecomp3x3_3(src.nativeObj, mtxR.nativeObj, mtxQ.nativeObj);
+    }
+
+
+    //
+    // C++:  bool cv::find4QuadCornerSubpix(Mat img, Mat& corners, Size region_size)
+    //
+
+    public static boolean find4QuadCornerSubpix(Mat img, Mat corners, Size region_size) {
+        return find4QuadCornerSubpix_0(img.nativeObj, corners.nativeObj, region_size.width, region_size.height);
+    }
+
+
+    //
+    // C++:  bool cv::findChessboardCorners(Mat image, Size patternSize, vector_Point2f& corners, int flags = CALIB_CB_ADAPTIVE_THRESH + CALIB_CB_NORMALIZE_IMAGE)
+    //
+
+    /**
+     * Finds the positions of internal corners of the chessboard.
+     *
+     * @param image Source chessboard view. It must be an 8-bit grayscale or color image.
+     * @param patternSize Number of inner corners per a chessboard row and column
+     * ( patternSize = cvSize(points_per_row,points_per_colum) = cvSize(columns,rows) ).
+     * @param corners Output array of detected corners.
+     * @param flags Various operation flags that can be zero or a combination of the following values:
+     * <ul>
+     *   <li>
+     *    <b>CALIB_CB_ADAPTIVE_THRESH</b> Use adaptive thresholding to convert the image to black
+     * and white, rather than a fixed threshold level (computed from the average image brightness).
+     *   </li>
+     *   <li>
+     *    <b>CALIB_CB_NORMALIZE_IMAGE</b> Normalize the image gamma with equalizeHist before
+     * applying fixed or adaptive thresholding.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_CB_FILTER_QUADS</b> Use additional criteria (like contour area, perimeter,
+     * square-like shape) to filter out false quads extracted at the contour retrieval stage.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_CB_FAST_CHECK</b> Run a fast check on the image that looks for chessboard corners,
+     * and shortcut the call if none is found. This can drastically speed up the call in the
+     * degenerate condition when no chessboard is observed.
+     *   </li>
+     * </ul>
+     *
+     * The function attempts to determine whether the input image is a view of the chessboard pattern and
+     * locate the internal chessboard corners. The function returns a non-zero value if all of the corners
+     * are found and they are placed in a certain order (row by row, left to right in every row).
+     * Otherwise, if the function fails to find all the corners or reorder them, it returns 0. For example,
+     * a regular chessboard has 8 x 8 squares and 7 x 7 internal corners, that is, points where the black
+     * squares touch each other. The detected coordinates are approximate, and to determine their positions
+     * more accurately, the function calls cornerSubPix. You also may use the function cornerSubPix with
+     * different parameters if returned coordinates are not accurate enough.
+     *
+     * Sample usage of detecting and drawing chessboard corners: :
+     * <code>
+     *     Size patternsize(8,6); //interior number of corners
+     *     Mat gray = ....; //source image
+     *     vector&lt;Point2f&gt; corners; //this will be filled by the detected corners
+     *
+     *     //CALIB_CB_FAST_CHECK saves a lot of time on images
+     *     //that do not contain any chessboard corners
+     *     bool patternfound = findChessboardCorners(gray, patternsize, corners,
+     *             CALIB_CB_ADAPTIVE_THRESH + CALIB_CB_NORMALIZE_IMAGE
+     *             + CALIB_CB_FAST_CHECK);
+     *
+     *     if(patternfound)
+     *       cornerSubPix(gray, corners, Size(11, 11), Size(-1, -1),
+     *         TermCriteria(CV_TERMCRIT_EPS + CV_TERMCRIT_ITER, 30, 0.1));
+     *
+     *     drawChessboardCorners(img, patternsize, Mat(corners), patternfound);
+     * </code>
+     * <b>Note:</b> The function requires white space (like a square-thick border, the wider the better) around
+     * the board to make the detection more robust in various environments. Otherwise, if there is no
+     * border and the background is dark, the outer black squares cannot be segmented properly and so the
+     * square grouping and ordering algorithm fails.
+     * @return automatically generated
+     */
+    public static boolean findChessboardCorners(Mat image, Size patternSize, MatOfPoint2f corners, int flags) {
+        Mat corners_mat = corners;
+        return findChessboardCorners_0(image.nativeObj, patternSize.width, patternSize.height, corners_mat.nativeObj, flags);
+    }
+
+    /**
+     * Finds the positions of internal corners of the chessboard.
+     *
+     * @param image Source chessboard view. It must be an 8-bit grayscale or color image.
+     * @param patternSize Number of inner corners per a chessboard row and column
+     * ( patternSize = cvSize(points_per_row,points_per_colum) = cvSize(columns,rows) ).
+     * @param corners Output array of detected corners.
+     * <ul>
+     *   <li>
+     *    <b>CALIB_CB_ADAPTIVE_THRESH</b> Use adaptive thresholding to convert the image to black
+     * and white, rather than a fixed threshold level (computed from the average image brightness).
+     *   </li>
+     *   <li>
+     *    <b>CALIB_CB_NORMALIZE_IMAGE</b> Normalize the image gamma with equalizeHist before
+     * applying fixed or adaptive thresholding.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_CB_FILTER_QUADS</b> Use additional criteria (like contour area, perimeter,
+     * square-like shape) to filter out false quads extracted at the contour retrieval stage.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_CB_FAST_CHECK</b> Run a fast check on the image that looks for chessboard corners,
+     * and shortcut the call if none is found. This can drastically speed up the call in the
+     * degenerate condition when no chessboard is observed.
+     *   </li>
+     * </ul>
+     *
+     * The function attempts to determine whether the input image is a view of the chessboard pattern and
+     * locate the internal chessboard corners. The function returns a non-zero value if all of the corners
+     * are found and they are placed in a certain order (row by row, left to right in every row).
+     * Otherwise, if the function fails to find all the corners or reorder them, it returns 0. For example,
+     * a regular chessboard has 8 x 8 squares and 7 x 7 internal corners, that is, points where the black
+     * squares touch each other. The detected coordinates are approximate, and to determine their positions
+     * more accurately, the function calls cornerSubPix. You also may use the function cornerSubPix with
+     * different parameters if returned coordinates are not accurate enough.
+     *
+     * Sample usage of detecting and drawing chessboard corners: :
+     * <code>
+     *     Size patternsize(8,6); //interior number of corners
+     *     Mat gray = ....; //source image
+     *     vector&lt;Point2f&gt; corners; //this will be filled by the detected corners
+     *
+     *     //CALIB_CB_FAST_CHECK saves a lot of time on images
+     *     //that do not contain any chessboard corners
+     *     bool patternfound = findChessboardCorners(gray, patternsize, corners,
+     *             CALIB_CB_ADAPTIVE_THRESH + CALIB_CB_NORMALIZE_IMAGE
+     *             + CALIB_CB_FAST_CHECK);
+     *
+     *     if(patternfound)
+     *       cornerSubPix(gray, corners, Size(11, 11), Size(-1, -1),
+     *         TermCriteria(CV_TERMCRIT_EPS + CV_TERMCRIT_ITER, 30, 0.1));
+     *
+     *     drawChessboardCorners(img, patternsize, Mat(corners), patternfound);
+     * </code>
+     * <b>Note:</b> The function requires white space (like a square-thick border, the wider the better) around
+     * the board to make the detection more robust in various environments. Otherwise, if there is no
+     * border and the background is dark, the outer black squares cannot be segmented properly and so the
+     * square grouping and ordering algorithm fails.
+     * @return automatically generated
+     */
+    public static boolean findChessboardCorners(Mat image, Size patternSize, MatOfPoint2f corners) {
+        Mat corners_mat = corners;
+        return findChessboardCorners_1(image.nativeObj, patternSize.width, patternSize.height, corners_mat.nativeObj);
+    }
+
+
+    //
+    // C++:  bool cv::findCirclesGrid(Mat image, Size patternSize, Mat& centers, int flags, Ptr_FeatureDetector blobDetector, CirclesGridFinderParameters parameters)
+    //
+
+    // Unknown type 'Ptr_FeatureDetector' (I), skipping the function
+
+
+    //
+    // C++:  bool cv::findCirclesGrid(Mat image, Size patternSize, Mat& centers, int flags = CALIB_CB_SYMMETRIC_GRID, Ptr_FeatureDetector blobDetector = SimpleBlobDetector::create())
+    //
+
+    public static boolean findCirclesGrid(Mat image, Size patternSize, Mat centers, int flags) {
+        return findCirclesGrid_0(image.nativeObj, patternSize.width, patternSize.height, centers.nativeObj, flags);
+    }
+
+    public static boolean findCirclesGrid(Mat image, Size patternSize, Mat centers) {
+        return findCirclesGrid_2(image.nativeObj, patternSize.width, patternSize.height, centers.nativeObj);
+    }
+
+
+    //
+    // C++:  bool cv::findCirclesGrid2(Mat image, Size patternSize, Mat& centers, int flags, Ptr_FeatureDetector blobDetector, CirclesGridFinderParameters2 parameters)
+    //
+
+    // Unknown type 'Ptr_FeatureDetector' (I), skipping the function
+
+
+    //
+    // C++:  bool cv::solvePnP(vector_Point3f objectPoints, vector_Point2f imagePoints, Mat cameraMatrix, vector_double distCoeffs, Mat& rvec, Mat& tvec, bool useExtrinsicGuess = false, int flags = SOLVEPNP_ITERATIVE)
+    //
+
+    /**
+     * Finds an object pose from 3D-2D point correspondences.
+     * This function returns the rotation and the translation vectors that transform a 3D point expressed in the object
+     * coordinate frame to the camera coordinate frame, using different methods:
+     * <ul>
+     *   <li>
+     *  P3P methods (REF: SOLVEPNP_P3P, REF: SOLVEPNP_AP3P): need 4 input points to return a unique solution.
+     *   </li>
+     *   <li>
+     *  REF: SOLVEPNP_IPPE Input points must be &gt;= 4 and object points must be coplanar.
+     *   </li>
+     *   <li>
+     *  REF: SOLVEPNP_IPPE_SQUARE Special case suitable for marker pose estimation.
+     * Number of input points must be 4. Object points must be defined in the following order:
+     *   <ul>
+     *     <li>
+     *    point 0: [-squareLength / 2,  squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *    point 1: [ squareLength / 2,  squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *    point 2: [ squareLength / 2, -squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *    point 3: [-squareLength / 2, -squareLength / 2, 0]
+     *     </li>
+     *   </ul>
+     *   <li>
+     *  for all the other flags, number of input points must be &gt;= 4 and object points can be in any configuration.
+     *   </li>
+     * </ul>
+     *
+     * @param objectPoints Array of object points in the object coordinate space, Nx3 1-channel or
+     * 1xN/Nx1 3-channel, where N is the number of points. vector&lt;Point3d&gt; can be also passed here.
+     * @param imagePoints Array of corresponding image points, Nx2 1-channel or 1xN/Nx1 2-channel,
+     * where N is the number of points. vector&lt;Point2d&gt; can be also passed here.
+     * @param cameraMatrix Input camera matrix \(A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}\) .
+     * @param distCoeffs Input vector of distortion coefficients
+     * \((k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\) of
+     * 4, 5, 8, 12 or 14 elements. If the vector is NULL/empty, the zero distortion coefficients are
+     * assumed.
+     * @param rvec Output rotation vector (see REF: Rodrigues ) that, together with tvec, brings points from
+     * the model coordinate system to the camera coordinate system.
+     * @param tvec Output translation vector.
+     * @param useExtrinsicGuess Parameter used for #SOLVEPNP_ITERATIVE. If true (1), the function uses
+     * the provided rvec and tvec values as initial approximations of the rotation and translation
+     * vectors, respectively, and further optimizes them.
+     * @param flags Method for solving a PnP problem:
+     * <ul>
+     *   <li>
+     *    <b>SOLVEPNP_ITERATIVE</b> Iterative method is based on a Levenberg-Marquardt optimization. In
+     * this case the function finds such a pose that minimizes reprojection error, that is the sum
+     * of squared distances between the observed projections imagePoints and the projected (using
+     * projectPoints ) objectPoints .
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_P3P</b> Method is based on the paper of X.S. Gao, X.-R. Hou, J. Tang, H.-F. Chang
+     * "Complete Solution Classification for the Perspective-Three-Point Problem" (CITE: gao2003complete).
+     * In this case the function requires exactly four object and image points.
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_AP3P</b> Method is based on the paper of T. Ke, S. Roumeliotis
+     * "An Efficient Algebraic Solution to the Perspective-Three-Point Problem" (CITE: Ke17).
+     * In this case the function requires exactly four object and image points.
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_EPNP</b> Method has been introduced by F. Moreno-Noguer, V. Lepetit and P. Fua in the
+     * paper "EPnP: Efficient Perspective-n-Point Camera Pose Estimation" (CITE: lepetit2009epnp).
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_DLS</b> Method is based on the paper of J. Hesch and S. Roumeliotis.
+     * "A Direct Least-Squares (DLS) Method for PnP" (CITE: hesch2011direct).
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_UPNP</b> Method is based on the paper of A. Penate-Sanchez, J. Andrade-Cetto,
+     * F. Moreno-Noguer. "Exhaustive Linearization for Robust Camera Pose and Focal Length
+     * Estimation" (CITE: penate2013exhaustive). In this case the function also estimates the parameters \(f_x\) and \(f_y\)
+     * assuming that both have the same value. Then the cameraMatrix is updated with the estimated
+     * focal length.
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_IPPE</b> Method is based on the paper of T. Collins and A. Bartoli.
+     * "Infinitesimal Plane-Based Pose Estimation" (CITE: Collins14). This method requires coplanar object points.
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_IPPE_SQUARE</b> Method is based on the paper of Toby Collins and Adrien Bartoli.
+     * "Infinitesimal Plane-Based Pose Estimation" (CITE: Collins14). This method is suitable for marker pose estimation.
+     * It requires 4 coplanar object points defined in the following order:
+     *   <ul>
+     *     <li>
+     *    point 0: [-squareLength / 2,  squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *    point 1: [ squareLength / 2,  squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *    point 2: [ squareLength / 2, -squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *    point 3: [-squareLength / 2, -squareLength / 2, 0]
+     *     </li>
+     *   </ul>
+     *
+     * The function estimates the object pose given a set of object points, their corresponding image
+     * projections, as well as the camera matrix and the distortion coefficients, see the figure below
+     * (more precisely, the X-axis of the camera frame is pointing to the right, the Y-axis downward
+     * and the Z-axis forward).
+     *   </li>
+     * </ul>
+     *
+     * ![](pnp.jpg)
+     *
+     * Points expressed in the world frame \( \bf{X}_w \) are projected into the image plane \( \left[ u, v \right] \)
+     * using the perspective projection model \( \Pi \) and the camera intrinsic parameters matrix \( \bf{A} \):
+     *
+     * \(
+     *   \begin{align*}
+     *   \begin{bmatrix}
+     *   u \\
+     *   v \\
+     *   1
+     *   \end{bmatrix} &amp;=
+     *   \bf{A} \hspace{0.1em} \Pi \hspace{0.2em} ^{c}\bf{T}_w
+     *   \begin{bmatrix}
+     *   X_{w} \\
+     *   Y_{w} \\
+     *   Z_{w} \\
+     *   1
+     *   \end{bmatrix} \\
+     *   \begin{bmatrix}
+     *   u \\
+     *   v \\
+     *   1
+     *   \end{bmatrix} &amp;=
+     *   \begin{bmatrix}
+     *   f_x &amp; 0 &amp; c_x \\
+     *   0 &amp; f_y &amp; c_y \\
+     *   0 &amp; 0 &amp; 1
+     *   \end{bmatrix}
+     *   \begin{bmatrix}
+     *   1 &amp; 0 &amp; 0 &amp; 0 \\
+     *   0 &amp; 1 &amp; 0 &amp; 0 \\
+     *   0 &amp; 0 &amp; 1 &amp; 0
+     *   \end{bmatrix}
+     *   \begin{bmatrix}
+     *   r_{11} &amp; r_{12} &amp; r_{13} &amp; t_x \\
+     *   r_{21} &amp; r_{22} &amp; r_{23} &amp; t_y \\
+     *   r_{31} &amp; r_{32} &amp; r_{33} &amp; t_z \\
+     *   0 &amp; 0 &amp; 0 &amp; 1
+     *   \end{bmatrix}
+     *   \begin{bmatrix}
+     *   X_{w} \\
+     *   Y_{w} \\
+     *   Z_{w} \\
+     *   1
+     *   \end{bmatrix}
+     *   \end{align*}
+     * \)
+     *
+     * The estimated pose is thus the rotation ({@code rvec}) and the translation ({@code tvec}) vectors that allow transforming
+     * a 3D point expressed in the world frame into the camera frame:
+     *
+     * \(
+     *   \begin{align*}
+     *   \begin{bmatrix}
+     *   X_c \\
+     *   Y_c \\
+     *   Z_c \\
+     *   1
+     *   \end{bmatrix} &amp;=
+     *   \hspace{0.2em} ^{c}\bf{T}_w
+     *   \begin{bmatrix}
+     *   X_{w} \\
+     *   Y_{w} \\
+     *   Z_{w} \\
+     *   1
+     *   \end{bmatrix} \\
+     *   \begin{bmatrix}
+     *   X_c \\
+     *   Y_c \\
+     *   Z_c \\
+     *   1
+     *   \end{bmatrix} &amp;=
+     *   \begin{bmatrix}
+     *   r_{11} &amp; r_{12} &amp; r_{13} &amp; t_x \\
+     *   r_{21} &amp; r_{22} &amp; r_{23} &amp; t_y \\
+     *   r_{31} &amp; r_{32} &amp; r_{33} &amp; t_z \\
+     *   0 &amp; 0 &amp; 0 &amp; 1
+     *   \end{bmatrix}
+     *   \begin{bmatrix}
+     *   X_{w} \\
+     *   Y_{w} \\
+     *   Z_{w} \\
+     *   1
+     *   \end{bmatrix}
+     *   \end{align*}
+     * \)
+     *
+     * <b>Note:</b>
+     * <ul>
+     *   <li>
+     *       An example of how to use solvePnP for planar augmented reality can be found at
+     *         opencv_source_code/samples/python/plane_ar.py
+     *   </li>
+     *   <li>
+     *       If you are using Python:
+     *   <ul>
+     *     <li>
+     *          Numpy array slices won't work as input because solvePnP requires contiguous
+     *         arrays (enforced by the assertion using cv::Mat::checkVector() around line 55 of
+     *         modules/calib3d/src/solvepnp.cpp version 2.4.9)
+     *     </li>
+     *     <li>
+     *          The P3P algorithm requires image points to be in an array of shape (N,1,2) due
+     *         to its calling of cv::undistortPoints (around line 75 of modules/calib3d/src/solvepnp.cpp version 2.4.9)
+     *         which requires 2-channel information.
+     *     </li>
+     *     <li>
+     *          Thus, given some data D = np.array(...) where D.shape = (N,M), in order to use a subset of
+     *         it as, e.g., imagePoints, one must effectively copy it into a new array: imagePoints =
+     *         np.ascontiguousarray(D[:,:2]).reshape((N,1,2))
+     *     </li>
+     *   </ul>
+     *   <li>
+     *       The methods <b>SOLVEPNP_DLS</b> and <b>SOLVEPNP_UPNP</b> cannot be used as the current implementations are
+     *        unstable and sometimes give completely wrong results. If you pass one of these two
+     *        flags, <b>SOLVEPNP_EPNP</b> method will be used instead.
+     *   </li>
+     *   <li>
+     *       The minimum number of points is 4 in the general case. In the case of <b>SOLVEPNP_P3P</b> and <b>SOLVEPNP_AP3P</b>
+     *        methods, it is required to use exactly 4 points (the first 3 points are used to estimate all the solutions
+     *        of the P3P problem, the last one is used to retain the best solution that minimizes the reprojection error).
+     *   </li>
+     *   <li>
+     *       With <b>SOLVEPNP_ITERATIVE</b> method and {@code useExtrinsicGuess=true}, the minimum number of points is 3 (3 points
+     *        are sufficient to compute a pose but there are up to 4 solutions). The initial solution should be close to the
+     *        global solution to converge.
+     *   </li>
+     *   <li>
+     *       With <b>SOLVEPNP_IPPE</b> input points must be &gt;= 4 and object points must be coplanar.
+     *   </li>
+     *   <li>
+     *       With <b>SOLVEPNP_IPPE_SQUARE</b> this is a special case suitable for marker pose estimation.
+     *        Number of input points must be 4. Object points must be defined in the following order:
+     *   <ul>
+     *     <li>
+     *           point 0: [-squareLength / 2,  squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *           point 1: [ squareLength / 2,  squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *           point 2: [ squareLength / 2, -squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *           point 3: [-squareLength / 2, -squareLength / 2, 0]
+     *     </li>
+     *   </ul>
+     *   </li>
+     * </ul>
+     * @return automatically generated
+     */
+    public static boolean solvePnP(MatOfPoint3f objectPoints, MatOfPoint2f imagePoints, Mat cameraMatrix, MatOfDouble distCoeffs, Mat rvec, Mat tvec, boolean useExtrinsicGuess, int flags) {
+        Mat objectPoints_mat = objectPoints;
+        Mat imagePoints_mat = imagePoints;
+        Mat distCoeffs_mat = distCoeffs;
+        return solvePnP_0(objectPoints_mat.nativeObj, imagePoints_mat.nativeObj, cameraMatrix.nativeObj, distCoeffs_mat.nativeObj, rvec.nativeObj, tvec.nativeObj, useExtrinsicGuess, flags);
+    }
+
+    /**
+     * Finds an object pose from 3D-2D point correspondences.
+     * This function returns the rotation and the translation vectors that transform a 3D point expressed in the object
+     * coordinate frame to the camera coordinate frame, using different methods:
+     * <ul>
+     *   <li>
+     *  P3P methods (REF: SOLVEPNP_P3P, REF: SOLVEPNP_AP3P): need 4 input points to return a unique solution.
+     *   </li>
+     *   <li>
+     *  REF: SOLVEPNP_IPPE Input points must be &gt;= 4 and object points must be coplanar.
+     *   </li>
+     *   <li>
+     *  REF: SOLVEPNP_IPPE_SQUARE Special case suitable for marker pose estimation.
+     * Number of input points must be 4. Object points must be defined in the following order:
+     *   <ul>
+     *     <li>
+     *    point 0: [-squareLength / 2,  squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *    point 1: [ squareLength / 2,  squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *    point 2: [ squareLength / 2, -squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *    point 3: [-squareLength / 2, -squareLength / 2, 0]
+     *     </li>
+     *   </ul>
+     *   <li>
+     *  for all the other flags, number of input points must be &gt;= 4 and object points can be in any configuration.
+     *   </li>
+     * </ul>
+     *
+     * @param objectPoints Array of object points in the object coordinate space, Nx3 1-channel or
+     * 1xN/Nx1 3-channel, where N is the number of points. vector&lt;Point3d&gt; can be also passed here.
+     * @param imagePoints Array of corresponding image points, Nx2 1-channel or 1xN/Nx1 2-channel,
+     * where N is the number of points. vector&lt;Point2d&gt; can be also passed here.
+     * @param cameraMatrix Input camera matrix \(A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}\) .
+     * @param distCoeffs Input vector of distortion coefficients
+     * \((k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\) of
+     * 4, 5, 8, 12 or 14 elements. If the vector is NULL/empty, the zero distortion coefficients are
+     * assumed.
+     * @param rvec Output rotation vector (see REF: Rodrigues ) that, together with tvec, brings points from
+     * the model coordinate system to the camera coordinate system.
+     * @param tvec Output translation vector.
+     * @param useExtrinsicGuess Parameter used for #SOLVEPNP_ITERATIVE. If true (1), the function uses
+     * the provided rvec and tvec values as initial approximations of the rotation and translation
+     * vectors, respectively, and further optimizes them.
+     * <ul>
+     *   <li>
+     *    <b>SOLVEPNP_ITERATIVE</b> Iterative method is based on a Levenberg-Marquardt optimization. In
+     * this case the function finds such a pose that minimizes reprojection error, that is the sum
+     * of squared distances between the observed projections imagePoints and the projected (using
+     * projectPoints ) objectPoints .
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_P3P</b> Method is based on the paper of X.S. Gao, X.-R. Hou, J. Tang, H.-F. Chang
+     * "Complete Solution Classification for the Perspective-Three-Point Problem" (CITE: gao2003complete).
+     * In this case the function requires exactly four object and image points.
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_AP3P</b> Method is based on the paper of T. Ke, S. Roumeliotis
+     * "An Efficient Algebraic Solution to the Perspective-Three-Point Problem" (CITE: Ke17).
+     * In this case the function requires exactly four object and image points.
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_EPNP</b> Method has been introduced by F. Moreno-Noguer, V. Lepetit and P. Fua in the
+     * paper "EPnP: Efficient Perspective-n-Point Camera Pose Estimation" (CITE: lepetit2009epnp).
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_DLS</b> Method is based on the paper of J. Hesch and S. Roumeliotis.
+     * "A Direct Least-Squares (DLS) Method for PnP" (CITE: hesch2011direct).
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_UPNP</b> Method is based on the paper of A. Penate-Sanchez, J. Andrade-Cetto,
+     * F. Moreno-Noguer. "Exhaustive Linearization for Robust Camera Pose and Focal Length
+     * Estimation" (CITE: penate2013exhaustive). In this case the function also estimates the parameters \(f_x\) and \(f_y\)
+     * assuming that both have the same value. Then the cameraMatrix is updated with the estimated
+     * focal length.
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_IPPE</b> Method is based on the paper of T. Collins and A. Bartoli.
+     * "Infinitesimal Plane-Based Pose Estimation" (CITE: Collins14). This method requires coplanar object points.
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_IPPE_SQUARE</b> Method is based on the paper of Toby Collins and Adrien Bartoli.
+     * "Infinitesimal Plane-Based Pose Estimation" (CITE: Collins14). This method is suitable for marker pose estimation.
+     * It requires 4 coplanar object points defined in the following order:
+     *   <ul>
+     *     <li>
+     *    point 0: [-squareLength / 2,  squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *    point 1: [ squareLength / 2,  squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *    point 2: [ squareLength / 2, -squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *    point 3: [-squareLength / 2, -squareLength / 2, 0]
+     *     </li>
+     *   </ul>
+     *
+     * The function estimates the object pose given a set of object points, their corresponding image
+     * projections, as well as the camera matrix and the distortion coefficients, see the figure below
+     * (more precisely, the X-axis of the camera frame is pointing to the right, the Y-axis downward
+     * and the Z-axis forward).
+     *   </li>
+     * </ul>
+     *
+     * ![](pnp.jpg)
+     *
+     * Points expressed in the world frame \( \bf{X}_w \) are projected into the image plane \( \left[ u, v \right] \)
+     * using the perspective projection model \( \Pi \) and the camera intrinsic parameters matrix \( \bf{A} \):
+     *
+     * \(
+     *   \begin{align*}
+     *   \begin{bmatrix}
+     *   u \\
+     *   v \\
+     *   1
+     *   \end{bmatrix} &amp;=
+     *   \bf{A} \hspace{0.1em} \Pi \hspace{0.2em} ^{c}\bf{T}_w
+     *   \begin{bmatrix}
+     *   X_{w} \\
+     *   Y_{w} \\
+     *   Z_{w} \\
+     *   1
+     *   \end{bmatrix} \\
+     *   \begin{bmatrix}
+     *   u \\
+     *   v \\
+     *   1
+     *   \end{bmatrix} &amp;=
+     *   \begin{bmatrix}
+     *   f_x &amp; 0 &amp; c_x \\
+     *   0 &amp; f_y &amp; c_y \\
+     *   0 &amp; 0 &amp; 1
+     *   \end{bmatrix}
+     *   \begin{bmatrix}
+     *   1 &amp; 0 &amp; 0 &amp; 0 \\
+     *   0 &amp; 1 &amp; 0 &amp; 0 \\
+     *   0 &amp; 0 &amp; 1 &amp; 0
+     *   \end{bmatrix}
+     *   \begin{bmatrix}
+     *   r_{11} &amp; r_{12} &amp; r_{13} &amp; t_x \\
+     *   r_{21} &amp; r_{22} &amp; r_{23} &amp; t_y \\
+     *   r_{31} &amp; r_{32} &amp; r_{33} &amp; t_z \\
+     *   0 &amp; 0 &amp; 0 &amp; 1
+     *   \end{bmatrix}
+     *   \begin{bmatrix}
+     *   X_{w} \\
+     *   Y_{w} \\
+     *   Z_{w} \\
+     *   1
+     *   \end{bmatrix}
+     *   \end{align*}
+     * \)
+     *
+     * The estimated pose is thus the rotation ({@code rvec}) and the translation ({@code tvec}) vectors that allow transforming
+     * a 3D point expressed in the world frame into the camera frame:
+     *
+     * \(
+     *   \begin{align*}
+     *   \begin{bmatrix}
+     *   X_c \\
+     *   Y_c \\
+     *   Z_c \\
+     *   1
+     *   \end{bmatrix} &amp;=
+     *   \hspace{0.2em} ^{c}\bf{T}_w
+     *   \begin{bmatrix}
+     *   X_{w} \\
+     *   Y_{w} \\
+     *   Z_{w} \\
+     *   1
+     *   \end{bmatrix} \\
+     *   \begin{bmatrix}
+     *   X_c \\
+     *   Y_c \\
+     *   Z_c \\
+     *   1
+     *   \end{bmatrix} &amp;=
+     *   \begin{bmatrix}
+     *   r_{11} &amp; r_{12} &amp; r_{13} &amp; t_x \\
+     *   r_{21} &amp; r_{22} &amp; r_{23} &amp; t_y \\
+     *   r_{31} &amp; r_{32} &amp; r_{33} &amp; t_z \\
+     *   0 &amp; 0 &amp; 0 &amp; 1
+     *   \end{bmatrix}
+     *   \begin{bmatrix}
+     *   X_{w} \\
+     *   Y_{w} \\
+     *   Z_{w} \\
+     *   1
+     *   \end{bmatrix}
+     *   \end{align*}
+     * \)
+     *
+     * <b>Note:</b>
+     * <ul>
+     *   <li>
+     *       An example of how to use solvePnP for planar augmented reality can be found at
+     *         opencv_source_code/samples/python/plane_ar.py
+     *   </li>
+     *   <li>
+     *       If you are using Python:
+     *   <ul>
+     *     <li>
+     *          Numpy array slices won't work as input because solvePnP requires contiguous
+     *         arrays (enforced by the assertion using cv::Mat::checkVector() around line 55 of
+     *         modules/calib3d/src/solvepnp.cpp version 2.4.9)
+     *     </li>
+     *     <li>
+     *          The P3P algorithm requires image points to be in an array of shape (N,1,2) due
+     *         to its calling of cv::undistortPoints (around line 75 of modules/calib3d/src/solvepnp.cpp version 2.4.9)
+     *         which requires 2-channel information.
+     *     </li>
+     *     <li>
+     *          Thus, given some data D = np.array(...) where D.shape = (N,M), in order to use a subset of
+     *         it as, e.g., imagePoints, one must effectively copy it into a new array: imagePoints =
+     *         np.ascontiguousarray(D[:,:2]).reshape((N,1,2))
+     *     </li>
+     *   </ul>
+     *   <li>
+     *       The methods <b>SOLVEPNP_DLS</b> and <b>SOLVEPNP_UPNP</b> cannot be used as the current implementations are
+     *        unstable and sometimes give completely wrong results. If you pass one of these two
+     *        flags, <b>SOLVEPNP_EPNP</b> method will be used instead.
+     *   </li>
+     *   <li>
+     *       The minimum number of points is 4 in the general case. In the case of <b>SOLVEPNP_P3P</b> and <b>SOLVEPNP_AP3P</b>
+     *        methods, it is required to use exactly 4 points (the first 3 points are used to estimate all the solutions
+     *        of the P3P problem, the last one is used to retain the best solution that minimizes the reprojection error).
+     *   </li>
+     *   <li>
+     *       With <b>SOLVEPNP_ITERATIVE</b> method and {@code useExtrinsicGuess=true}, the minimum number of points is 3 (3 points
+     *        are sufficient to compute a pose but there are up to 4 solutions). The initial solution should be close to the
+     *        global solution to converge.
+     *   </li>
+     *   <li>
+     *       With <b>SOLVEPNP_IPPE</b> input points must be &gt;= 4 and object points must be coplanar.
+     *   </li>
+     *   <li>
+     *       With <b>SOLVEPNP_IPPE_SQUARE</b> this is a special case suitable for marker pose estimation.
+     *        Number of input points must be 4. Object points must be defined in the following order:
+     *   <ul>
+     *     <li>
+     *           point 0: [-squareLength / 2,  squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *           point 1: [ squareLength / 2,  squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *           point 2: [ squareLength / 2, -squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *           point 3: [-squareLength / 2, -squareLength / 2, 0]
+     *     </li>
+     *   </ul>
+     *   </li>
+     * </ul>
+     * @return automatically generated
+     */
+    public static boolean solvePnP(MatOfPoint3f objectPoints, MatOfPoint2f imagePoints, Mat cameraMatrix, MatOfDouble distCoeffs, Mat rvec, Mat tvec, boolean useExtrinsicGuess) {
+        Mat objectPoints_mat = objectPoints;
+        Mat imagePoints_mat = imagePoints;
+        Mat distCoeffs_mat = distCoeffs;
+        return solvePnP_1(objectPoints_mat.nativeObj, imagePoints_mat.nativeObj, cameraMatrix.nativeObj, distCoeffs_mat.nativeObj, rvec.nativeObj, tvec.nativeObj, useExtrinsicGuess);
+    }
+
+    /**
+     * Finds an object pose from 3D-2D point correspondences.
+     * This function returns the rotation and the translation vectors that transform a 3D point expressed in the object
+     * coordinate frame to the camera coordinate frame, using different methods:
+     * <ul>
+     *   <li>
+     *  P3P methods (REF: SOLVEPNP_P3P, REF: SOLVEPNP_AP3P): need 4 input points to return a unique solution.
+     *   </li>
+     *   <li>
+     *  REF: SOLVEPNP_IPPE Input points must be &gt;= 4 and object points must be coplanar.
+     *   </li>
+     *   <li>
+     *  REF: SOLVEPNP_IPPE_SQUARE Special case suitable for marker pose estimation.
+     * Number of input points must be 4. Object points must be defined in the following order:
+     *   <ul>
+     *     <li>
+     *    point 0: [-squareLength / 2,  squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *    point 1: [ squareLength / 2,  squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *    point 2: [ squareLength / 2, -squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *    point 3: [-squareLength / 2, -squareLength / 2, 0]
+     *     </li>
+     *   </ul>
+     *   <li>
+     *  for all the other flags, number of input points must be &gt;= 4 and object points can be in any configuration.
+     *   </li>
+     * </ul>
+     *
+     * @param objectPoints Array of object points in the object coordinate space, Nx3 1-channel or
+     * 1xN/Nx1 3-channel, where N is the number of points. vector&lt;Point3d&gt; can be also passed here.
+     * @param imagePoints Array of corresponding image points, Nx2 1-channel or 1xN/Nx1 2-channel,
+     * where N is the number of points. vector&lt;Point2d&gt; can be also passed here.
+     * @param cameraMatrix Input camera matrix \(A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}\) .
+     * @param distCoeffs Input vector of distortion coefficients
+     * \((k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\) of
+     * 4, 5, 8, 12 or 14 elements. If the vector is NULL/empty, the zero distortion coefficients are
+     * assumed.
+     * @param rvec Output rotation vector (see REF: Rodrigues ) that, together with tvec, brings points from
+     * the model coordinate system to the camera coordinate system.
+     * @param tvec Output translation vector.
+     * the provided rvec and tvec values as initial approximations of the rotation and translation
+     * vectors, respectively, and further optimizes them.
+     * <ul>
+     *   <li>
+     *    <b>SOLVEPNP_ITERATIVE</b> Iterative method is based on a Levenberg-Marquardt optimization. In
+     * this case the function finds such a pose that minimizes reprojection error, that is the sum
+     * of squared distances between the observed projections imagePoints and the projected (using
+     * projectPoints ) objectPoints .
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_P3P</b> Method is based on the paper of X.S. Gao, X.-R. Hou, J. Tang, H.-F. Chang
+     * "Complete Solution Classification for the Perspective-Three-Point Problem" (CITE: gao2003complete).
+     * In this case the function requires exactly four object and image points.
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_AP3P</b> Method is based on the paper of T. Ke, S. Roumeliotis
+     * "An Efficient Algebraic Solution to the Perspective-Three-Point Problem" (CITE: Ke17).
+     * In this case the function requires exactly four object and image points.
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_EPNP</b> Method has been introduced by F. Moreno-Noguer, V. Lepetit and P. Fua in the
+     * paper "EPnP: Efficient Perspective-n-Point Camera Pose Estimation" (CITE: lepetit2009epnp).
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_DLS</b> Method is based on the paper of J. Hesch and S. Roumeliotis.
+     * "A Direct Least-Squares (DLS) Method for PnP" (CITE: hesch2011direct).
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_UPNP</b> Method is based on the paper of A. Penate-Sanchez, J. Andrade-Cetto,
+     * F. Moreno-Noguer. "Exhaustive Linearization for Robust Camera Pose and Focal Length
+     * Estimation" (CITE: penate2013exhaustive). In this case the function also estimates the parameters \(f_x\) and \(f_y\)
+     * assuming that both have the same value. Then the cameraMatrix is updated with the estimated
+     * focal length.
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_IPPE</b> Method is based on the paper of T. Collins and A. Bartoli.
+     * "Infinitesimal Plane-Based Pose Estimation" (CITE: Collins14). This method requires coplanar object points.
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_IPPE_SQUARE</b> Method is based on the paper of Toby Collins and Adrien Bartoli.
+     * "Infinitesimal Plane-Based Pose Estimation" (CITE: Collins14). This method is suitable for marker pose estimation.
+     * It requires 4 coplanar object points defined in the following order:
+     *   <ul>
+     *     <li>
+     *    point 0: [-squareLength / 2,  squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *    point 1: [ squareLength / 2,  squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *    point 2: [ squareLength / 2, -squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *    point 3: [-squareLength / 2, -squareLength / 2, 0]
+     *     </li>
+     *   </ul>
+     *
+     * The function estimates the object pose given a set of object points, their corresponding image
+     * projections, as well as the camera matrix and the distortion coefficients, see the figure below
+     * (more precisely, the X-axis of the camera frame is pointing to the right, the Y-axis downward
+     * and the Z-axis forward).
+     *   </li>
+     * </ul>
+     *
+     * ![](pnp.jpg)
+     *
+     * Points expressed in the world frame \( \bf{X}_w \) are projected into the image plane \( \left[ u, v \right] \)
+     * using the perspective projection model \( \Pi \) and the camera intrinsic parameters matrix \( \bf{A} \):
+     *
+     * \(
+     *   \begin{align*}
+     *   \begin{bmatrix}
+     *   u \\
+     *   v \\
+     *   1
+     *   \end{bmatrix} &amp;=
+     *   \bf{A} \hspace{0.1em} \Pi \hspace{0.2em} ^{c}\bf{T}_w
+     *   \begin{bmatrix}
+     *   X_{w} \\
+     *   Y_{w} \\
+     *   Z_{w} \\
+     *   1
+     *   \end{bmatrix} \\
+     *   \begin{bmatrix}
+     *   u \\
+     *   v \\
+     *   1
+     *   \end{bmatrix} &amp;=
+     *   \begin{bmatrix}
+     *   f_x &amp; 0 &amp; c_x \\
+     *   0 &amp; f_y &amp; c_y \\
+     *   0 &amp; 0 &amp; 1
+     *   \end{bmatrix}
+     *   \begin{bmatrix}
+     *   1 &amp; 0 &amp; 0 &amp; 0 \\
+     *   0 &amp; 1 &amp; 0 &amp; 0 \\
+     *   0 &amp; 0 &amp; 1 &amp; 0
+     *   \end{bmatrix}
+     *   \begin{bmatrix}
+     *   r_{11} &amp; r_{12} &amp; r_{13} &amp; t_x \\
+     *   r_{21} &amp; r_{22} &amp; r_{23} &amp; t_y \\
+     *   r_{31} &amp; r_{32} &amp; r_{33} &amp; t_z \\
+     *   0 &amp; 0 &amp; 0 &amp; 1
+     *   \end{bmatrix}
+     *   \begin{bmatrix}
+     *   X_{w} \\
+     *   Y_{w} \\
+     *   Z_{w} \\
+     *   1
+     *   \end{bmatrix}
+     *   \end{align*}
+     * \)
+     *
+     * The estimated pose is thus the rotation ({@code rvec}) and the translation ({@code tvec}) vectors that allow transforming
+     * a 3D point expressed in the world frame into the camera frame:
+     *
+     * \(
+     *   \begin{align*}
+     *   \begin{bmatrix}
+     *   X_c \\
+     *   Y_c \\
+     *   Z_c \\
+     *   1
+     *   \end{bmatrix} &amp;=
+     *   \hspace{0.2em} ^{c}\bf{T}_w
+     *   \begin{bmatrix}
+     *   X_{w} \\
+     *   Y_{w} \\
+     *   Z_{w} \\
+     *   1
+     *   \end{bmatrix} \\
+     *   \begin{bmatrix}
+     *   X_c \\
+     *   Y_c \\
+     *   Z_c \\
+     *   1
+     *   \end{bmatrix} &amp;=
+     *   \begin{bmatrix}
+     *   r_{11} &amp; r_{12} &amp; r_{13} &amp; t_x \\
+     *   r_{21} &amp; r_{22} &amp; r_{23} &amp; t_y \\
+     *   r_{31} &amp; r_{32} &amp; r_{33} &amp; t_z \\
+     *   0 &amp; 0 &amp; 0 &amp; 1
+     *   \end{bmatrix}
+     *   \begin{bmatrix}
+     *   X_{w} \\
+     *   Y_{w} \\
+     *   Z_{w} \\
+     *   1
+     *   \end{bmatrix}
+     *   \end{align*}
+     * \)
+     *
+     * <b>Note:</b>
+     * <ul>
+     *   <li>
+     *       An example of how to use solvePnP for planar augmented reality can be found at
+     *         opencv_source_code/samples/python/plane_ar.py
+     *   </li>
+     *   <li>
+     *       If you are using Python:
+     *   <ul>
+     *     <li>
+     *          Numpy array slices won't work as input because solvePnP requires contiguous
+     *         arrays (enforced by the assertion using cv::Mat::checkVector() around line 55 of
+     *         modules/calib3d/src/solvepnp.cpp version 2.4.9)
+     *     </li>
+     *     <li>
+     *          The P3P algorithm requires image points to be in an array of shape (N,1,2) due
+     *         to its calling of cv::undistortPoints (around line 75 of modules/calib3d/src/solvepnp.cpp version 2.4.9)
+     *         which requires 2-channel information.
+     *     </li>
+     *     <li>
+     *          Thus, given some data D = np.array(...) where D.shape = (N,M), in order to use a subset of
+     *         it as, e.g., imagePoints, one must effectively copy it into a new array: imagePoints =
+     *         np.ascontiguousarray(D[:,:2]).reshape((N,1,2))
+     *     </li>
+     *   </ul>
+     *   <li>
+     *       The methods <b>SOLVEPNP_DLS</b> and <b>SOLVEPNP_UPNP</b> cannot be used as the current implementations are
+     *        unstable and sometimes give completely wrong results. If you pass one of these two
+     *        flags, <b>SOLVEPNP_EPNP</b> method will be used instead.
+     *   </li>
+     *   <li>
+     *       The minimum number of points is 4 in the general case. In the case of <b>SOLVEPNP_P3P</b> and <b>SOLVEPNP_AP3P</b>
+     *        methods, it is required to use exactly 4 points (the first 3 points are used to estimate all the solutions
+     *        of the P3P problem, the last one is used to retain the best solution that minimizes the reprojection error).
+     *   </li>
+     *   <li>
+     *       With <b>SOLVEPNP_ITERATIVE</b> method and {@code useExtrinsicGuess=true}, the minimum number of points is 3 (3 points
+     *        are sufficient to compute a pose but there are up to 4 solutions). The initial solution should be close to the
+     *        global solution to converge.
+     *   </li>
+     *   <li>
+     *       With <b>SOLVEPNP_IPPE</b> input points must be &gt;= 4 and object points must be coplanar.
+     *   </li>
+     *   <li>
+     *       With <b>SOLVEPNP_IPPE_SQUARE</b> this is a special case suitable for marker pose estimation.
+     *        Number of input points must be 4. Object points must be defined in the following order:
+     *   <ul>
+     *     <li>
+     *           point 0: [-squareLength / 2,  squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *           point 1: [ squareLength / 2,  squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *           point 2: [ squareLength / 2, -squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *           point 3: [-squareLength / 2, -squareLength / 2, 0]
+     *     </li>
+     *   </ul>
+     *   </li>
+     * </ul>
+     * @return automatically generated
+     */
+    public static boolean solvePnP(MatOfPoint3f objectPoints, MatOfPoint2f imagePoints, Mat cameraMatrix, MatOfDouble distCoeffs, Mat rvec, Mat tvec) {
+        Mat objectPoints_mat = objectPoints;
+        Mat imagePoints_mat = imagePoints;
+        Mat distCoeffs_mat = distCoeffs;
+        return solvePnP_2(objectPoints_mat.nativeObj, imagePoints_mat.nativeObj, cameraMatrix.nativeObj, distCoeffs_mat.nativeObj, rvec.nativeObj, tvec.nativeObj);
+    }
+
+
+    //
+    // C++:  bool cv::solvePnPRansac(vector_Point3f objectPoints, vector_Point2f imagePoints, Mat cameraMatrix, vector_double distCoeffs, Mat& rvec, Mat& tvec, bool useExtrinsicGuess = false, int iterationsCount = 100, float reprojectionError = 8.0, double confidence = 0.99, Mat& inliers = Mat(), int flags = SOLVEPNP_ITERATIVE)
+    //
+
+    /**
+     * Finds an object pose from 3D-2D point correspondences using the RANSAC scheme.
+     *
+     * @param objectPoints Array of object points in the object coordinate space, Nx3 1-channel or
+     * 1xN/Nx1 3-channel, where N is the number of points. vector&lt;Point3d&gt; can be also passed here.
+     * @param imagePoints Array of corresponding image points, Nx2 1-channel or 1xN/Nx1 2-channel,
+     * where N is the number of points. vector&lt;Point2d&gt; can be also passed here.
+     * @param cameraMatrix Input camera matrix \(A = \vecthreethree{fx}{0}{cx}{0}{fy}{cy}{0}{0}{1}\) .
+     * @param distCoeffs Input vector of distortion coefficients
+     * \((k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\) of
+     * 4, 5, 8, 12 or 14 elements. If the vector is NULL/empty, the zero distortion coefficients are
+     * assumed.
+     * @param rvec Output rotation vector (see REF: Rodrigues ) that, together with tvec, brings points from
+     * the model coordinate system to the camera coordinate system.
+     * @param tvec Output translation vector.
+     * @param useExtrinsicGuess Parameter used for REF: SOLVEPNP_ITERATIVE. If true (1), the function uses
+     * the provided rvec and tvec values as initial approximations of the rotation and translation
+     * vectors, respectively, and further optimizes them.
+     * @param iterationsCount Number of iterations.
+     * @param reprojectionError Inlier threshold value used by the RANSAC procedure. The parameter value
+     * is the maximum allowed distance between the observed and computed point projections to consider it
+     * an inlier.
+     * @param confidence The probability that the algorithm produces a useful result.
+     * @param inliers Output vector that contains indices of inliers in objectPoints and imagePoints .
+     * @param flags Method for solving a PnP problem (see REF: solvePnP ).
+     *
+     * The function estimates an object pose given a set of object points, their corresponding image
+     * projections, as well as the camera matrix and the distortion coefficients. This function finds such
+     * a pose that minimizes reprojection error, that is, the sum of squared distances between the observed
+     * projections imagePoints and the projected (using REF: projectPoints ) objectPoints. The use of RANSAC
+     * makes the function resistant to outliers.
+     *
+     * <b>Note:</b>
+     * <ul>
+     *   <li>
+     *       An example of how to use solvePNPRansac for object detection can be found at
+     *         opencv_source_code/samples/cpp/tutorial_code/calib3d/real_time_pose_estimation/
+     *   </li>
+     *   <li>
+     *       The default method used to estimate the camera pose for the Minimal Sample Sets step
+     *        is #SOLVEPNP_EPNP. Exceptions are:
+     *   <ul>
+     *     <li>
+     *           if you choose #SOLVEPNP_P3P or #SOLVEPNP_AP3P, these methods will be used.
+     *     </li>
+     *     <li>
+     *           if the number of input points is equal to 4, #SOLVEPNP_P3P is used.
+     *     </li>
+     *   </ul>
+     *   <li>
+     *       The method used to estimate the camera pose using all the inliers is defined by the
+     *        flags parameters unless it is equal to #SOLVEPNP_P3P or #SOLVEPNP_AP3P. In this case,
+     *        the method #SOLVEPNP_EPNP will be used instead.
+     *   </li>
+     * </ul>
+     * @return automatically generated
+     */
+    public static boolean solvePnPRansac(MatOfPoint3f objectPoints, MatOfPoint2f imagePoints, Mat cameraMatrix, MatOfDouble distCoeffs, Mat rvec, Mat tvec, boolean useExtrinsicGuess, int iterationsCount, float reprojectionError, double confidence, Mat inliers, int flags) {
+        Mat objectPoints_mat = objectPoints;
+        Mat imagePoints_mat = imagePoints;
+        Mat distCoeffs_mat = distCoeffs;
+        return solvePnPRansac_0(objectPoints_mat.nativeObj, imagePoints_mat.nativeObj, cameraMatrix.nativeObj, distCoeffs_mat.nativeObj, rvec.nativeObj, tvec.nativeObj, useExtrinsicGuess, iterationsCount, reprojectionError, confidence, inliers.nativeObj, flags);
+    }
+
+    /**
+     * Finds an object pose from 3D-2D point correspondences using the RANSAC scheme.
+     *
+     * @param objectPoints Array of object points in the object coordinate space, Nx3 1-channel or
+     * 1xN/Nx1 3-channel, where N is the number of points. vector&lt;Point3d&gt; can be also passed here.
+     * @param imagePoints Array of corresponding image points, Nx2 1-channel or 1xN/Nx1 2-channel,
+     * where N is the number of points. vector&lt;Point2d&gt; can be also passed here.
+     * @param cameraMatrix Input camera matrix \(A = \vecthreethree{fx}{0}{cx}{0}{fy}{cy}{0}{0}{1}\) .
+     * @param distCoeffs Input vector of distortion coefficients
+     * \((k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\) of
+     * 4, 5, 8, 12 or 14 elements. If the vector is NULL/empty, the zero distortion coefficients are
+     * assumed.
+     * @param rvec Output rotation vector (see REF: Rodrigues ) that, together with tvec, brings points from
+     * the model coordinate system to the camera coordinate system.
+     * @param tvec Output translation vector.
+     * @param useExtrinsicGuess Parameter used for REF: SOLVEPNP_ITERATIVE. If true (1), the function uses
+     * the provided rvec and tvec values as initial approximations of the rotation and translation
+     * vectors, respectively, and further optimizes them.
+     * @param iterationsCount Number of iterations.
+     * @param reprojectionError Inlier threshold value used by the RANSAC procedure. The parameter value
+     * is the maximum allowed distance between the observed and computed point projections to consider it
+     * an inlier.
+     * @param confidence The probability that the algorithm produces a useful result.
+     * @param inliers Output vector that contains indices of inliers in objectPoints and imagePoints .
+     *
+     * The function estimates an object pose given a set of object points, their corresponding image
+     * projections, as well as the camera matrix and the distortion coefficients. This function finds such
+     * a pose that minimizes reprojection error, that is, the sum of squared distances between the observed
+     * projections imagePoints and the projected (using REF: projectPoints ) objectPoints. The use of RANSAC
+     * makes the function resistant to outliers.
+     *
+     * <b>Note:</b>
+     * <ul>
+     *   <li>
+     *       An example of how to use solvePNPRansac for object detection can be found at
+     *         opencv_source_code/samples/cpp/tutorial_code/calib3d/real_time_pose_estimation/
+     *   </li>
+     *   <li>
+     *       The default method used to estimate the camera pose for the Minimal Sample Sets step
+     *        is #SOLVEPNP_EPNP. Exceptions are:
+     *   <ul>
+     *     <li>
+     *           if you choose #SOLVEPNP_P3P or #SOLVEPNP_AP3P, these methods will be used.
+     *     </li>
+     *     <li>
+     *           if the number of input points is equal to 4, #SOLVEPNP_P3P is used.
+     *     </li>
+     *   </ul>
+     *   <li>
+     *       The method used to estimate the camera pose using all the inliers is defined by the
+     *        flags parameters unless it is equal to #SOLVEPNP_P3P or #SOLVEPNP_AP3P. In this case,
+     *        the method #SOLVEPNP_EPNP will be used instead.
+     *   </li>
+     * </ul>
+     * @return automatically generated
+     */
+    public static boolean solvePnPRansac(MatOfPoint3f objectPoints, MatOfPoint2f imagePoints, Mat cameraMatrix, MatOfDouble distCoeffs, Mat rvec, Mat tvec, boolean useExtrinsicGuess, int iterationsCount, float reprojectionError, double confidence, Mat inliers) {
+        Mat objectPoints_mat = objectPoints;
+        Mat imagePoints_mat = imagePoints;
+        Mat distCoeffs_mat = distCoeffs;
+        return solvePnPRansac_1(objectPoints_mat.nativeObj, imagePoints_mat.nativeObj, cameraMatrix.nativeObj, distCoeffs_mat.nativeObj, rvec.nativeObj, tvec.nativeObj, useExtrinsicGuess, iterationsCount, reprojectionError, confidence, inliers.nativeObj);
+    }
+
+    /**
+     * Finds an object pose from 3D-2D point correspondences using the RANSAC scheme.
+     *
+     * @param objectPoints Array of object points in the object coordinate space, Nx3 1-channel or
+     * 1xN/Nx1 3-channel, where N is the number of points. vector&lt;Point3d&gt; can be also passed here.
+     * @param imagePoints Array of corresponding image points, Nx2 1-channel or 1xN/Nx1 2-channel,
+     * where N is the number of points. vector&lt;Point2d&gt; can be also passed here.
+     * @param cameraMatrix Input camera matrix \(A = \vecthreethree{fx}{0}{cx}{0}{fy}{cy}{0}{0}{1}\) .
+     * @param distCoeffs Input vector of distortion coefficients
+     * \((k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\) of
+     * 4, 5, 8, 12 or 14 elements. If the vector is NULL/empty, the zero distortion coefficients are
+     * assumed.
+     * @param rvec Output rotation vector (see REF: Rodrigues ) that, together with tvec, brings points from
+     * the model coordinate system to the camera coordinate system.
+     * @param tvec Output translation vector.
+     * @param useExtrinsicGuess Parameter used for REF: SOLVEPNP_ITERATIVE. If true (1), the function uses
+     * the provided rvec and tvec values as initial approximations of the rotation and translation
+     * vectors, respectively, and further optimizes them.
+     * @param iterationsCount Number of iterations.
+     * @param reprojectionError Inlier threshold value used by the RANSAC procedure. The parameter value
+     * is the maximum allowed distance between the observed and computed point projections to consider it
+     * an inlier.
+     * @param confidence The probability that the algorithm produces a useful result.
+     *
+     * The function estimates an object pose given a set of object points, their corresponding image
+     * projections, as well as the camera matrix and the distortion coefficients. This function finds such
+     * a pose that minimizes reprojection error, that is, the sum of squared distances between the observed
+     * projections imagePoints and the projected (using REF: projectPoints ) objectPoints. The use of RANSAC
+     * makes the function resistant to outliers.
+     *
+     * <b>Note:</b>
+     * <ul>
+     *   <li>
+     *       An example of how to use solvePNPRansac for object detection can be found at
+     *         opencv_source_code/samples/cpp/tutorial_code/calib3d/real_time_pose_estimation/
+     *   </li>
+     *   <li>
+     *       The default method used to estimate the camera pose for the Minimal Sample Sets step
+     *        is #SOLVEPNP_EPNP. Exceptions are:
+     *   <ul>
+     *     <li>
+     *           if you choose #SOLVEPNP_P3P or #SOLVEPNP_AP3P, these methods will be used.
+     *     </li>
+     *     <li>
+     *           if the number of input points is equal to 4, #SOLVEPNP_P3P is used.
+     *     </li>
+     *   </ul>
+     *   <li>
+     *       The method used to estimate the camera pose using all the inliers is defined by the
+     *        flags parameters unless it is equal to #SOLVEPNP_P3P or #SOLVEPNP_AP3P. In this case,
+     *        the method #SOLVEPNP_EPNP will be used instead.
+     *   </li>
+     * </ul>
+     * @return automatically generated
+     */
+    public static boolean solvePnPRansac(MatOfPoint3f objectPoints, MatOfPoint2f imagePoints, Mat cameraMatrix, MatOfDouble distCoeffs, Mat rvec, Mat tvec, boolean useExtrinsicGuess, int iterationsCount, float reprojectionError, double confidence) {
+        Mat objectPoints_mat = objectPoints;
+        Mat imagePoints_mat = imagePoints;
+        Mat distCoeffs_mat = distCoeffs;
+        return solvePnPRansac_2(objectPoints_mat.nativeObj, imagePoints_mat.nativeObj, cameraMatrix.nativeObj, distCoeffs_mat.nativeObj, rvec.nativeObj, tvec.nativeObj, useExtrinsicGuess, iterationsCount, reprojectionError, confidence);
+    }
+
+    /**
+     * Finds an object pose from 3D-2D point correspondences using the RANSAC scheme.
+     *
+     * @param objectPoints Array of object points in the object coordinate space, Nx3 1-channel or
+     * 1xN/Nx1 3-channel, where N is the number of points. vector&lt;Point3d&gt; can be also passed here.
+     * @param imagePoints Array of corresponding image points, Nx2 1-channel or 1xN/Nx1 2-channel,
+     * where N is the number of points. vector&lt;Point2d&gt; can be also passed here.
+     * @param cameraMatrix Input camera matrix \(A = \vecthreethree{fx}{0}{cx}{0}{fy}{cy}{0}{0}{1}\) .
+     * @param distCoeffs Input vector of distortion coefficients
+     * \((k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\) of
+     * 4, 5, 8, 12 or 14 elements. If the vector is NULL/empty, the zero distortion coefficients are
+     * assumed.
+     * @param rvec Output rotation vector (see REF: Rodrigues ) that, together with tvec, brings points from
+     * the model coordinate system to the camera coordinate system.
+     * @param tvec Output translation vector.
+     * @param useExtrinsicGuess Parameter used for REF: SOLVEPNP_ITERATIVE. If true (1), the function uses
+     * the provided rvec and tvec values as initial approximations of the rotation and translation
+     * vectors, respectively, and further optimizes them.
+     * @param iterationsCount Number of iterations.
+     * @param reprojectionError Inlier threshold value used by the RANSAC procedure. The parameter value
+     * is the maximum allowed distance between the observed and computed point projections to consider it
+     * an inlier.
+     *
+     * The function estimates an object pose given a set of object points, their corresponding image
+     * projections, as well as the camera matrix and the distortion coefficients. This function finds such
+     * a pose that minimizes reprojection error, that is, the sum of squared distances between the observed
+     * projections imagePoints and the projected (using REF: projectPoints ) objectPoints. The use of RANSAC
+     * makes the function resistant to outliers.
+     *
+     * <b>Note:</b>
+     * <ul>
+     *   <li>
+     *       An example of how to use solvePNPRansac for object detection can be found at
+     *         opencv_source_code/samples/cpp/tutorial_code/calib3d/real_time_pose_estimation/
+     *   </li>
+     *   <li>
+     *       The default method used to estimate the camera pose for the Minimal Sample Sets step
+     *        is #SOLVEPNP_EPNP. Exceptions are:
+     *   <ul>
+     *     <li>
+     *           if you choose #SOLVEPNP_P3P or #SOLVEPNP_AP3P, these methods will be used.
+     *     </li>
+     *     <li>
+     *           if the number of input points is equal to 4, #SOLVEPNP_P3P is used.
+     *     </li>
+     *   </ul>
+     *   <li>
+     *       The method used to estimate the camera pose using all the inliers is defined by the
+     *        flags parameters unless it is equal to #SOLVEPNP_P3P or #SOLVEPNP_AP3P. In this case,
+     *        the method #SOLVEPNP_EPNP will be used instead.
+     *   </li>
+     * </ul>
+     * @return automatically generated
+     */
+    public static boolean solvePnPRansac(MatOfPoint3f objectPoints, MatOfPoint2f imagePoints, Mat cameraMatrix, MatOfDouble distCoeffs, Mat rvec, Mat tvec, boolean useExtrinsicGuess, int iterationsCount, float reprojectionError) {
+        Mat objectPoints_mat = objectPoints;
+        Mat imagePoints_mat = imagePoints;
+        Mat distCoeffs_mat = distCoeffs;
+        return solvePnPRansac_3(objectPoints_mat.nativeObj, imagePoints_mat.nativeObj, cameraMatrix.nativeObj, distCoeffs_mat.nativeObj, rvec.nativeObj, tvec.nativeObj, useExtrinsicGuess, iterationsCount, reprojectionError);
+    }
+
+    /**
+     * Finds an object pose from 3D-2D point correspondences using the RANSAC scheme.
+     *
+     * @param objectPoints Array of object points in the object coordinate space, Nx3 1-channel or
+     * 1xN/Nx1 3-channel, where N is the number of points. vector&lt;Point3d&gt; can be also passed here.
+     * @param imagePoints Array of corresponding image points, Nx2 1-channel or 1xN/Nx1 2-channel,
+     * where N is the number of points. vector&lt;Point2d&gt; can be also passed here.
+     * @param cameraMatrix Input camera matrix \(A = \vecthreethree{fx}{0}{cx}{0}{fy}{cy}{0}{0}{1}\) .
+     * @param distCoeffs Input vector of distortion coefficients
+     * \((k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\) of
+     * 4, 5, 8, 12 or 14 elements. If the vector is NULL/empty, the zero distortion coefficients are
+     * assumed.
+     * @param rvec Output rotation vector (see REF: Rodrigues ) that, together with tvec, brings points from
+     * the model coordinate system to the camera coordinate system.
+     * @param tvec Output translation vector.
+     * @param useExtrinsicGuess Parameter used for REF: SOLVEPNP_ITERATIVE. If true (1), the function uses
+     * the provided rvec and tvec values as initial approximations of the rotation and translation
+     * vectors, respectively, and further optimizes them.
+     * @param iterationsCount Number of iterations.
+     * is the maximum allowed distance between the observed and computed point projections to consider it
+     * an inlier.
+     *
+     * The function estimates an object pose given a set of object points, their corresponding image
+     * projections, as well as the camera matrix and the distortion coefficients. This function finds such
+     * a pose that minimizes reprojection error, that is, the sum of squared distances between the observed
+     * projections imagePoints and the projected (using REF: projectPoints ) objectPoints. The use of RANSAC
+     * makes the function resistant to outliers.
+     *
+     * <b>Note:</b>
+     * <ul>
+     *   <li>
+     *       An example of how to use solvePNPRansac for object detection can be found at
+     *         opencv_source_code/samples/cpp/tutorial_code/calib3d/real_time_pose_estimation/
+     *   </li>
+     *   <li>
+     *       The default method used to estimate the camera pose for the Minimal Sample Sets step
+     *        is #SOLVEPNP_EPNP. Exceptions are:
+     *   <ul>
+     *     <li>
+     *           if you choose #SOLVEPNP_P3P or #SOLVEPNP_AP3P, these methods will be used.
+     *     </li>
+     *     <li>
+     *           if the number of input points is equal to 4, #SOLVEPNP_P3P is used.
+     *     </li>
+     *   </ul>
+     *   <li>
+     *       The method used to estimate the camera pose using all the inliers is defined by the
+     *        flags parameters unless it is equal to #SOLVEPNP_P3P or #SOLVEPNP_AP3P. In this case,
+     *        the method #SOLVEPNP_EPNP will be used instead.
+     *   </li>
+     * </ul>
+     * @return automatically generated
+     */
+    public static boolean solvePnPRansac(MatOfPoint3f objectPoints, MatOfPoint2f imagePoints, Mat cameraMatrix, MatOfDouble distCoeffs, Mat rvec, Mat tvec, boolean useExtrinsicGuess, int iterationsCount) {
+        Mat objectPoints_mat = objectPoints;
+        Mat imagePoints_mat = imagePoints;
+        Mat distCoeffs_mat = distCoeffs;
+        return solvePnPRansac_4(objectPoints_mat.nativeObj, imagePoints_mat.nativeObj, cameraMatrix.nativeObj, distCoeffs_mat.nativeObj, rvec.nativeObj, tvec.nativeObj, useExtrinsicGuess, iterationsCount);
+    }
+
+    /**
+     * Finds an object pose from 3D-2D point correspondences using the RANSAC scheme.
+     *
+     * @param objectPoints Array of object points in the object coordinate space, Nx3 1-channel or
+     * 1xN/Nx1 3-channel, where N is the number of points. vector&lt;Point3d&gt; can be also passed here.
+     * @param imagePoints Array of corresponding image points, Nx2 1-channel or 1xN/Nx1 2-channel,
+     * where N is the number of points. vector&lt;Point2d&gt; can be also passed here.
+     * @param cameraMatrix Input camera matrix \(A = \vecthreethree{fx}{0}{cx}{0}{fy}{cy}{0}{0}{1}\) .
+     * @param distCoeffs Input vector of distortion coefficients
+     * \((k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\) of
+     * 4, 5, 8, 12 or 14 elements. If the vector is NULL/empty, the zero distortion coefficients are
+     * assumed.
+     * @param rvec Output rotation vector (see REF: Rodrigues ) that, together with tvec, brings points from
+     * the model coordinate system to the camera coordinate system.
+     * @param tvec Output translation vector.
+     * @param useExtrinsicGuess Parameter used for REF: SOLVEPNP_ITERATIVE. If true (1), the function uses
+     * the provided rvec and tvec values as initial approximations of the rotation and translation
+     * vectors, respectively, and further optimizes them.
+     * is the maximum allowed distance between the observed and computed point projections to consider it
+     * an inlier.
+     *
+     * The function estimates an object pose given a set of object points, their corresponding image
+     * projections, as well as the camera matrix and the distortion coefficients. This function finds such
+     * a pose that minimizes reprojection error, that is, the sum of squared distances between the observed
+     * projections imagePoints and the projected (using REF: projectPoints ) objectPoints. The use of RANSAC
+     * makes the function resistant to outliers.
+     *
+     * <b>Note:</b>
+     * <ul>
+     *   <li>
+     *       An example of how to use solvePNPRansac for object detection can be found at
+     *         opencv_source_code/samples/cpp/tutorial_code/calib3d/real_time_pose_estimation/
+     *   </li>
+     *   <li>
+     *       The default method used to estimate the camera pose for the Minimal Sample Sets step
+     *        is #SOLVEPNP_EPNP. Exceptions are:
+     *   <ul>
+     *     <li>
+     *           if you choose #SOLVEPNP_P3P or #SOLVEPNP_AP3P, these methods will be used.
+     *     </li>
+     *     <li>
+     *           if the number of input points is equal to 4, #SOLVEPNP_P3P is used.
+     *     </li>
+     *   </ul>
+     *   <li>
+     *       The method used to estimate the camera pose using all the inliers is defined by the
+     *        flags parameters unless it is equal to #SOLVEPNP_P3P or #SOLVEPNP_AP3P. In this case,
+     *        the method #SOLVEPNP_EPNP will be used instead.
+     *   </li>
+     * </ul>
+     * @return automatically generated
+     */
+    public static boolean solvePnPRansac(MatOfPoint3f objectPoints, MatOfPoint2f imagePoints, Mat cameraMatrix, MatOfDouble distCoeffs, Mat rvec, Mat tvec, boolean useExtrinsicGuess) {
+        Mat objectPoints_mat = objectPoints;
+        Mat imagePoints_mat = imagePoints;
+        Mat distCoeffs_mat = distCoeffs;
+        return solvePnPRansac_5(objectPoints_mat.nativeObj, imagePoints_mat.nativeObj, cameraMatrix.nativeObj, distCoeffs_mat.nativeObj, rvec.nativeObj, tvec.nativeObj, useExtrinsicGuess);
+    }
+
+    /**
+     * Finds an object pose from 3D-2D point correspondences using the RANSAC scheme.
+     *
+     * @param objectPoints Array of object points in the object coordinate space, Nx3 1-channel or
+     * 1xN/Nx1 3-channel, where N is the number of points. vector&lt;Point3d&gt; can be also passed here.
+     * @param imagePoints Array of corresponding image points, Nx2 1-channel or 1xN/Nx1 2-channel,
+     * where N is the number of points. vector&lt;Point2d&gt; can be also passed here.
+     * @param cameraMatrix Input camera matrix \(A = \vecthreethree{fx}{0}{cx}{0}{fy}{cy}{0}{0}{1}\) .
+     * @param distCoeffs Input vector of distortion coefficients
+     * \((k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\) of
+     * 4, 5, 8, 12 or 14 elements. If the vector is NULL/empty, the zero distortion coefficients are
+     * assumed.
+     * @param rvec Output rotation vector (see REF: Rodrigues ) that, together with tvec, brings points from
+     * the model coordinate system to the camera coordinate system.
+     * @param tvec Output translation vector.
+     * the provided rvec and tvec values as initial approximations of the rotation and translation
+     * vectors, respectively, and further optimizes them.
+     * is the maximum allowed distance between the observed and computed point projections to consider it
+     * an inlier.
+     *
+     * The function estimates an object pose given a set of object points, their corresponding image
+     * projections, as well as the camera matrix and the distortion coefficients. This function finds such
+     * a pose that minimizes reprojection error, that is, the sum of squared distances between the observed
+     * projections imagePoints and the projected (using REF: projectPoints ) objectPoints. The use of RANSAC
+     * makes the function resistant to outliers.
+     *
+     * <b>Note:</b>
+     * <ul>
+     *   <li>
+     *       An example of how to use solvePNPRansac for object detection can be found at
+     *         opencv_source_code/samples/cpp/tutorial_code/calib3d/real_time_pose_estimation/
+     *   </li>
+     *   <li>
+     *       The default method used to estimate the camera pose for the Minimal Sample Sets step
+     *        is #SOLVEPNP_EPNP. Exceptions are:
+     *   <ul>
+     *     <li>
+     *           if you choose #SOLVEPNP_P3P or #SOLVEPNP_AP3P, these methods will be used.
+     *     </li>
+     *     <li>
+     *           if the number of input points is equal to 4, #SOLVEPNP_P3P is used.
+     *     </li>
+     *   </ul>
+     *   <li>
+     *       The method used to estimate the camera pose using all the inliers is defined by the
+     *        flags parameters unless it is equal to #SOLVEPNP_P3P or #SOLVEPNP_AP3P. In this case,
+     *        the method #SOLVEPNP_EPNP will be used instead.
+     *   </li>
+     * </ul>
+     * @return automatically generated
+     */
+    public static boolean solvePnPRansac(MatOfPoint3f objectPoints, MatOfPoint2f imagePoints, Mat cameraMatrix, MatOfDouble distCoeffs, Mat rvec, Mat tvec) {
+        Mat objectPoints_mat = objectPoints;
+        Mat imagePoints_mat = imagePoints;
+        Mat distCoeffs_mat = distCoeffs;
+        return solvePnPRansac_6(objectPoints_mat.nativeObj, imagePoints_mat.nativeObj, cameraMatrix.nativeObj, distCoeffs_mat.nativeObj, rvec.nativeObj, tvec.nativeObj);
+    }
+
+
+    //
+    // C++:  bool cv::stereoRectifyUncalibrated(Mat points1, Mat points2, Mat F, Size imgSize, Mat& H1, Mat& H2, double threshold = 5)
+    //
+
+    /**
+     * Computes a rectification transform for an uncalibrated stereo camera.
+     *
+     * @param points1 Array of feature points in the first image.
+     * @param points2 The corresponding points in the second image. The same formats as in
+     * findFundamentalMat are supported.
+     * @param F Input fundamental matrix. It can be computed from the same set of point pairs using
+     * findFundamentalMat .
+     * @param imgSize Size of the image.
+     * @param H1 Output rectification homography matrix for the first image.
+     * @param H2 Output rectification homography matrix for the second image.
+     * @param threshold Optional threshold used to filter out the outliers. If the parameter is greater
+     * than zero, all the point pairs that do not comply with the epipolar geometry (that is, the points
+     * for which \(|\texttt{points2[i]}^T*\texttt{F}*\texttt{points1[i]}|&gt;\texttt{threshold}\) ) are
+     * rejected prior to computing the homographies. Otherwise, all the points are considered inliers.
+     *
+     * The function computes the rectification transformations without knowing intrinsic parameters of the
+     * cameras and their relative position in the space, which explains the suffix "uncalibrated". Another
+     * related difference from stereoRectify is that the function outputs not the rectification
+     * transformations in the object (3D) space, but the planar perspective transformations encoded by the
+     * homography matrices H1 and H2 . The function implements the algorithm CITE: Hartley99 .
+     *
+     * <b>Note:</b>
+     *    While the algorithm does not need to know the intrinsic parameters of the cameras, it heavily
+     *     depends on the epipolar geometry. Therefore, if the camera lenses have a significant distortion,
+     *     it would be better to correct it before computing the fundamental matrix and calling this
+     *     function. For example, distortion coefficients can be estimated for each head of stereo camera
+     *     separately by using calibrateCamera . Then, the images can be corrected using undistort , or
+     *     just the point coordinates can be corrected with undistortPoints .
+     * @return automatically generated
+     */
+    public static boolean stereoRectifyUncalibrated(Mat points1, Mat points2, Mat F, Size imgSize, Mat H1, Mat H2, double threshold) {
+        return stereoRectifyUncalibrated_0(points1.nativeObj, points2.nativeObj, F.nativeObj, imgSize.width, imgSize.height, H1.nativeObj, H2.nativeObj, threshold);
+    }
+
+    /**
+     * Computes a rectification transform for an uncalibrated stereo camera.
+     *
+     * @param points1 Array of feature points in the first image.
+     * @param points2 The corresponding points in the second image. The same formats as in
+     * findFundamentalMat are supported.
+     * @param F Input fundamental matrix. It can be computed from the same set of point pairs using
+     * findFundamentalMat .
+     * @param imgSize Size of the image.
+     * @param H1 Output rectification homography matrix for the first image.
+     * @param H2 Output rectification homography matrix for the second image.
+     * than zero, all the point pairs that do not comply with the epipolar geometry (that is, the points
+     * for which \(|\texttt{points2[i]}^T*\texttt{F}*\texttt{points1[i]}|&gt;\texttt{threshold}\) ) are
+     * rejected prior to computing the homographies. Otherwise, all the points are considered inliers.
+     *
+     * The function computes the rectification transformations without knowing intrinsic parameters of the
+     * cameras and their relative position in the space, which explains the suffix "uncalibrated". Another
+     * related difference from stereoRectify is that the function outputs not the rectification
+     * transformations in the object (3D) space, but the planar perspective transformations encoded by the
+     * homography matrices H1 and H2 . The function implements the algorithm CITE: Hartley99 .
+     *
+     * <b>Note:</b>
+     *    While the algorithm does not need to know the intrinsic parameters of the cameras, it heavily
+     *     depends on the epipolar geometry. Therefore, if the camera lenses have a significant distortion,
+     *     it would be better to correct it before computing the fundamental matrix and calling this
+     *     function. For example, distortion coefficients can be estimated for each head of stereo camera
+     *     separately by using calibrateCamera . Then, the images can be corrected using undistort , or
+     *     just the point coordinates can be corrected with undistortPoints .
+     * @return automatically generated
+     */
+    public static boolean stereoRectifyUncalibrated(Mat points1, Mat points2, Mat F, Size imgSize, Mat H1, Mat H2) {
+        return stereoRectifyUncalibrated_1(points1.nativeObj, points2.nativeObj, F.nativeObj, imgSize.width, imgSize.height, H1.nativeObj, H2.nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::calibrateCamera(vector_Mat objectPoints, vector_Mat imagePoints, Size imageSize, Mat& cameraMatrix, Mat& distCoeffs, vector_Mat& rvecs, vector_Mat& tvecs, Mat& stdDeviationsIntrinsics, Mat& stdDeviationsExtrinsics, Mat& perViewErrors, int flags = 0, TermCriteria criteria = TermCriteria( TermCriteria::COUNT + TermCriteria::EPS, 30, DBL_EPSILON))
+    //
+
+    /**
+     * Finds the camera intrinsic and extrinsic parameters from several views of a calibration
+     * pattern.
+     *
+     * @param objectPoints In the new interface it is a vector of vectors of calibration pattern points in
+     * the calibration pattern coordinate space (e.g. std::vector&lt;std::vector&lt;cv::Vec3f&gt;&gt;). The outer
+     * vector contains as many elements as the number of pattern views. If the same calibration pattern
+     * is shown in each view and it is fully visible, all the vectors will be the same. Although, it is
+     * possible to use partially occluded patterns or even different patterns in different views. Then,
+     * the vectors will be different. Although the points are 3D, they all lie in the calibration pattern's
+     * XY coordinate plane (thus 0 in the Z-coordinate), if the used calibration pattern is a planar rig.
+     * In the old interface all the vectors of object points from different views are concatenated
+     * together.
+     * @param imagePoints In the new interface it is a vector of vectors of the projections of calibration
+     * pattern points (e.g. std::vector&lt;std::vector&lt;cv::Vec2f&gt;&gt;). imagePoints.size() and
+     * objectPoints.size(), and imagePoints[i].size() and objectPoints[i].size() for each i, must be equal,
+     * respectively. In the old interface all the vectors of object points from different views are
+     * concatenated together.
+     * @param imageSize Size of the image used only to initialize the intrinsic camera matrix.
+     * @param cameraMatrix Input/output 3x3 floating-point camera matrix
+     * \(A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}\) . If CV\_CALIB\_USE\_INTRINSIC\_GUESS
+     * and/or CALIB_FIX_ASPECT_RATIO are specified, some or all of fx, fy, cx, cy must be
+     * initialized before calling the function.
+     * @param distCoeffs Input/output vector of distortion coefficients
+     * \((k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\) of
+     * 4, 5, 8, 12 or 14 elements.
+     * @param rvecs Output vector of rotation vectors (REF: Rodrigues ) estimated for each pattern view
+     * (e.g. std::vector&lt;cv::Mat&gt;&gt;). That is, each i-th rotation vector together with the corresponding
+     * i-th translation vector (see the next output parameter description) brings the calibration pattern
+     * from the object coordinate space (in which object points are specified) to the camera coordinate
+     * space. In more technical terms, the tuple of the i-th rotation and translation vector performs
+     * a change of basis from object coordinate space to camera coordinate space. Due to its duality, this
+     * tuple is equivalent to the position of the calibration pattern with respect to the camera coordinate
+     * space.
+     * @param tvecs Output vector of translation vectors estimated for each pattern view, see parameter
+     * describtion above.
+     * @param stdDeviationsIntrinsics Output vector of standard deviations estimated for intrinsic
+     * parameters. Order of deviations values:
+     * \((f_x, f_y, c_x, c_y, k_1, k_2, p_1, p_2, k_3, k_4, k_5, k_6 , s_1, s_2, s_3,
+     *  s_4, \tau_x, \tau_y)\) If one of parameters is not estimated, it's deviation is equals to zero.
+     * @param stdDeviationsExtrinsics Output vector of standard deviations estimated for extrinsic
+     * parameters. Order of deviations values: \((R_0, T_0, \dotsc , R_{M - 1}, T_{M - 1})\) where M is
+     * the number of pattern views. \(R_i, T_i\) are concatenated 1x3 vectors.
+     *  @param perViewErrors Output vector of the RMS re-projection error estimated for each pattern view.
+     * @param flags Different flags that may be zero or a combination of the following values:
+     * <ul>
+     *   <li>
+     *    <b>CALIB_USE_INTRINSIC_GUESS</b> cameraMatrix contains valid initial values of
+     * fx, fy, cx, cy that are optimized further. Otherwise, (cx, cy) is initially set to the image
+     * center ( imageSize is used), and focal distances are computed in a least-squares fashion.
+     * Note, that if intrinsic parameters are known, there is no need to use this function just to
+     * estimate extrinsic parameters. Use solvePnP instead.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_FIX_PRINCIPAL_POINT</b> The principal point is not changed during the global
+     * optimization. It stays at the center or at a different location specified when
+     * CALIB_USE_INTRINSIC_GUESS is set too.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_FIX_ASPECT_RATIO</b> The functions consider only fy as a free parameter. The
+     * ratio fx/fy stays the same as in the input cameraMatrix . When
+     * CALIB_USE_INTRINSIC_GUESS is not set, the actual input values of fx and fy are
+     * ignored, only their ratio is computed and used further.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_ZERO_TANGENT_DIST</b> Tangential distortion coefficients \((p_1, p_2)\) are set
+     * to zeros and stay zero.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_FIX_K1,...,CALIB_FIX_K6</b> The corresponding radial distortion
+     * coefficient is not changed during the optimization. If CALIB_USE_INTRINSIC_GUESS is
+     * set, the coefficient from the supplied distCoeffs matrix is used. Otherwise, it is set to 0.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_RATIONAL_MODEL</b> Coefficients k4, k5, and k6 are enabled. To provide the
+     * backward compatibility, this extra flag should be explicitly specified to make the
+     * calibration function use the rational model and return 8 coefficients. If the flag is not
+     * set, the function computes and returns only 5 distortion coefficients.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_THIN_PRISM_MODEL</b> Coefficients s1, s2, s3 and s4 are enabled. To provide the
+     * backward compatibility, this extra flag should be explicitly specified to make the
+     * calibration function use the thin prism model and return 12 coefficients. If the flag is not
+     * set, the function computes and returns only 5 distortion coefficients.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_FIX_S1_S2_S3_S4</b> The thin prism distortion coefficients are not changed during
+     * the optimization. If CALIB_USE_INTRINSIC_GUESS is set, the coefficient from the
+     * supplied distCoeffs matrix is used. Otherwise, it is set to 0.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_TILTED_MODEL</b> Coefficients tauX and tauY are enabled. To provide the
+     * backward compatibility, this extra flag should be explicitly specified to make the
+     * calibration function use the tilted sensor model and return 14 coefficients. If the flag is not
+     * set, the function computes and returns only 5 distortion coefficients.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_FIX_TAUX_TAUY</b> The coefficients of the tilted sensor model are not changed during
+     * the optimization. If CALIB_USE_INTRINSIC_GUESS is set, the coefficient from the
+     * supplied distCoeffs matrix is used. Otherwise, it is set to 0.
+     * @param criteria Termination criteria for the iterative optimization algorithm.
+     *   </li>
+     * </ul>
+     *
+     * @return the overall RMS re-projection error.
+     *
+     * The function estimates the intrinsic camera parameters and extrinsic parameters for each of the
+     * views. The algorithm is based on CITE: Zhang2000 and CITE: BouguetMCT . The coordinates of 3D object
+     * points and their corresponding 2D projections in each view must be specified. That may be achieved
+     * by using an object with known geometry and easily detectable feature points. Such an object is
+     * called a calibration rig or calibration pattern, and OpenCV has built-in support for a chessboard as
+     * a calibration rig (see REF: findChessboardCorners). Currently, initialization of intrinsic
+     * parameters (when CALIB_USE_INTRINSIC_GUESS is not set) is only implemented for planar calibration
+     * patterns (where Z-coordinates of the object points must be all zeros). 3D calibration rigs can also
+     * be used as long as initial cameraMatrix is provided.
+     *
+     * The algorithm performs the following steps:
+     *
+     * <ul>
+     *   <li>
+     *    Compute the initial intrinsic parameters (the option only available for planar calibration
+     *     patterns) or read them from the input parameters. The distortion coefficients are all set to
+     *     zeros initially unless some of CALIB_FIX_K? are specified.
+     *   </li>
+     * </ul>
+     *
+     * <ul>
+     *   <li>
+     *    Estimate the initial camera pose as if the intrinsic parameters have been already known. This is
+     *     done using solvePnP .
+     *   </li>
+     * </ul>
+     *
+     * <ul>
+     *   <li>
+     *    Run the global Levenberg-Marquardt optimization algorithm to minimize the reprojection error,
+     *     that is, the total sum of squared distances between the observed feature points imagePoints and
+     *     the projected (using the current estimates for camera parameters and the poses) object points
+     *     objectPoints. See projectPoints for details.
+     *   </li>
+     * </ul>
+     *
+     * <b>Note:</b>
+     *     If you use a non-square (i.e. non-N-by-N) grid and REF: findChessboardCorners for calibration,
+     *     and REF: calibrateCamera returns bad values (zero distortion coefficients, \(c_x\) and
+     *     \(c_y\) very far from the image center, and/or large differences between \(f_x\) and
+     *     \(f_y\) (ratios of 10:1 or more)), then you are probably using patternSize=cvSize(rows,cols)
+     *     instead of using patternSize=cvSize(cols,rows) in REF: findChessboardCorners.
+     *
+     * SEE:
+     *    findChessboardCorners, solvePnP, initCameraMatrix2D, stereoCalibrate, undistort
+     */
+    public static double calibrateCameraExtended(List<Mat> objectPoints, List<Mat> imagePoints, Size imageSize, Mat cameraMatrix, Mat distCoeffs, List<Mat> rvecs, List<Mat> tvecs, Mat stdDeviationsIntrinsics, Mat stdDeviationsExtrinsics, Mat perViewErrors, int flags, TermCriteria criteria) {
+        Mat objectPoints_mat = Converters.vector_Mat_to_Mat(objectPoints);
+        Mat imagePoints_mat = Converters.vector_Mat_to_Mat(imagePoints);
+        Mat rvecs_mat = new Mat();
+        Mat tvecs_mat = new Mat();
+        double retVal = calibrateCameraExtended_0(objectPoints_mat.nativeObj, imagePoints_mat.nativeObj, imageSize.width, imageSize.height, cameraMatrix.nativeObj, distCoeffs.nativeObj, rvecs_mat.nativeObj, tvecs_mat.nativeObj, stdDeviationsIntrinsics.nativeObj, stdDeviationsExtrinsics.nativeObj, perViewErrors.nativeObj, flags, criteria.type, criteria.maxCount, criteria.epsilon);
+        Converters.Mat_to_vector_Mat(rvecs_mat, rvecs);
+        rvecs_mat.release();
+        Converters.Mat_to_vector_Mat(tvecs_mat, tvecs);
+        tvecs_mat.release();
+        return retVal;
+    }
+
+    /**
+     * Finds the camera intrinsic and extrinsic parameters from several views of a calibration
+     * pattern.
+     *
+     * @param objectPoints In the new interface it is a vector of vectors of calibration pattern points in
+     * the calibration pattern coordinate space (e.g. std::vector&lt;std::vector&lt;cv::Vec3f&gt;&gt;). The outer
+     * vector contains as many elements as the number of pattern views. If the same calibration pattern
+     * is shown in each view and it is fully visible, all the vectors will be the same. Although, it is
+     * possible to use partially occluded patterns or even different patterns in different views. Then,
+     * the vectors will be different. Although the points are 3D, they all lie in the calibration pattern's
+     * XY coordinate plane (thus 0 in the Z-coordinate), if the used calibration pattern is a planar rig.
+     * In the old interface all the vectors of object points from different views are concatenated
+     * together.
+     * @param imagePoints In the new interface it is a vector of vectors of the projections of calibration
+     * pattern points (e.g. std::vector&lt;std::vector&lt;cv::Vec2f&gt;&gt;). imagePoints.size() and
+     * objectPoints.size(), and imagePoints[i].size() and objectPoints[i].size() for each i, must be equal,
+     * respectively. In the old interface all the vectors of object points from different views are
+     * concatenated together.
+     * @param imageSize Size of the image used only to initialize the intrinsic camera matrix.
+     * @param cameraMatrix Input/output 3x3 floating-point camera matrix
+     * \(A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}\) . If CV\_CALIB\_USE\_INTRINSIC\_GUESS
+     * and/or CALIB_FIX_ASPECT_RATIO are specified, some or all of fx, fy, cx, cy must be
+     * initialized before calling the function.
+     * @param distCoeffs Input/output vector of distortion coefficients
+     * \((k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\) of
+     * 4, 5, 8, 12 or 14 elements.
+     * @param rvecs Output vector of rotation vectors (REF: Rodrigues ) estimated for each pattern view
+     * (e.g. std::vector&lt;cv::Mat&gt;&gt;). That is, each i-th rotation vector together with the corresponding
+     * i-th translation vector (see the next output parameter description) brings the calibration pattern
+     * from the object coordinate space (in which object points are specified) to the camera coordinate
+     * space. In more technical terms, the tuple of the i-th rotation and translation vector performs
+     * a change of basis from object coordinate space to camera coordinate space. Due to its duality, this
+     * tuple is equivalent to the position of the calibration pattern with respect to the camera coordinate
+     * space.
+     * @param tvecs Output vector of translation vectors estimated for each pattern view, see parameter
+     * describtion above.
+     * @param stdDeviationsIntrinsics Output vector of standard deviations estimated for intrinsic
+     * parameters. Order of deviations values:
+     * \((f_x, f_y, c_x, c_y, k_1, k_2, p_1, p_2, k_3, k_4, k_5, k_6 , s_1, s_2, s_3,
+     *  s_4, \tau_x, \tau_y)\) If one of parameters is not estimated, it's deviation is equals to zero.
+     * @param stdDeviationsExtrinsics Output vector of standard deviations estimated for extrinsic
+     * parameters. Order of deviations values: \((R_0, T_0, \dotsc , R_{M - 1}, T_{M - 1})\) where M is
+     * the number of pattern views. \(R_i, T_i\) are concatenated 1x3 vectors.
+     *  @param perViewErrors Output vector of the RMS re-projection error estimated for each pattern view.
+     * @param flags Different flags that may be zero or a combination of the following values:
+     * <ul>
+     *   <li>
+     *    <b>CALIB_USE_INTRINSIC_GUESS</b> cameraMatrix contains valid initial values of
+     * fx, fy, cx, cy that are optimized further. Otherwise, (cx, cy) is initially set to the image
+     * center ( imageSize is used), and focal distances are computed in a least-squares fashion.
+     * Note, that if intrinsic parameters are known, there is no need to use this function just to
+     * estimate extrinsic parameters. Use solvePnP instead.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_FIX_PRINCIPAL_POINT</b> The principal point is not changed during the global
+     * optimization. It stays at the center or at a different location specified when
+     * CALIB_USE_INTRINSIC_GUESS is set too.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_FIX_ASPECT_RATIO</b> The functions consider only fy as a free parameter. The
+     * ratio fx/fy stays the same as in the input cameraMatrix . When
+     * CALIB_USE_INTRINSIC_GUESS is not set, the actual input values of fx and fy are
+     * ignored, only their ratio is computed and used further.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_ZERO_TANGENT_DIST</b> Tangential distortion coefficients \((p_1, p_2)\) are set
+     * to zeros and stay zero.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_FIX_K1,...,CALIB_FIX_K6</b> The corresponding radial distortion
+     * coefficient is not changed during the optimization. If CALIB_USE_INTRINSIC_GUESS is
+     * set, the coefficient from the supplied distCoeffs matrix is used. Otherwise, it is set to 0.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_RATIONAL_MODEL</b> Coefficients k4, k5, and k6 are enabled. To provide the
+     * backward compatibility, this extra flag should be explicitly specified to make the
+     * calibration function use the rational model and return 8 coefficients. If the flag is not
+     * set, the function computes and returns only 5 distortion coefficients.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_THIN_PRISM_MODEL</b> Coefficients s1, s2, s3 and s4 are enabled. To provide the
+     * backward compatibility, this extra flag should be explicitly specified to make the
+     * calibration function use the thin prism model and return 12 coefficients. If the flag is not
+     * set, the function computes and returns only 5 distortion coefficients.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_FIX_S1_S2_S3_S4</b> The thin prism distortion coefficients are not changed during
+     * the optimization. If CALIB_USE_INTRINSIC_GUESS is set, the coefficient from the
+     * supplied distCoeffs matrix is used. Otherwise, it is set to 0.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_TILTED_MODEL</b> Coefficients tauX and tauY are enabled. To provide the
+     * backward compatibility, this extra flag should be explicitly specified to make the
+     * calibration function use the tilted sensor model and return 14 coefficients. If the flag is not
+     * set, the function computes and returns only 5 distortion coefficients.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_FIX_TAUX_TAUY</b> The coefficients of the tilted sensor model are not changed during
+     * the optimization. If CALIB_USE_INTRINSIC_GUESS is set, the coefficient from the
+     * supplied distCoeffs matrix is used. Otherwise, it is set to 0.
+     *   </li>
+     * </ul>
+     *
+     * @return the overall RMS re-projection error.
+     *
+     * The function estimates the intrinsic camera parameters and extrinsic parameters for each of the
+     * views. The algorithm is based on CITE: Zhang2000 and CITE: BouguetMCT . The coordinates of 3D object
+     * points and their corresponding 2D projections in each view must be specified. That may be achieved
+     * by using an object with known geometry and easily detectable feature points. Such an object is
+     * called a calibration rig or calibration pattern, and OpenCV has built-in support for a chessboard as
+     * a calibration rig (see REF: findChessboardCorners). Currently, initialization of intrinsic
+     * parameters (when CALIB_USE_INTRINSIC_GUESS is not set) is only implemented for planar calibration
+     * patterns (where Z-coordinates of the object points must be all zeros). 3D calibration rigs can also
+     * be used as long as initial cameraMatrix is provided.
+     *
+     * The algorithm performs the following steps:
+     *
+     * <ul>
+     *   <li>
+     *    Compute the initial intrinsic parameters (the option only available for planar calibration
+     *     patterns) or read them from the input parameters. The distortion coefficients are all set to
+     *     zeros initially unless some of CALIB_FIX_K? are specified.
+     *   </li>
+     * </ul>
+     *
+     * <ul>
+     *   <li>
+     *    Estimate the initial camera pose as if the intrinsic parameters have been already known. This is
+     *     done using solvePnP .
+     *   </li>
+     * </ul>
+     *
+     * <ul>
+     *   <li>
+     *    Run the global Levenberg-Marquardt optimization algorithm to minimize the reprojection error,
+     *     that is, the total sum of squared distances between the observed feature points imagePoints and
+     *     the projected (using the current estimates for camera parameters and the poses) object points
+     *     objectPoints. See projectPoints for details.
+     *   </li>
+     * </ul>
+     *
+     * <b>Note:</b>
+     *     If you use a non-square (i.e. non-N-by-N) grid and REF: findChessboardCorners for calibration,
+     *     and REF: calibrateCamera returns bad values (zero distortion coefficients, \(c_x\) and
+     *     \(c_y\) very far from the image center, and/or large differences between \(f_x\) and
+     *     \(f_y\) (ratios of 10:1 or more)), then you are probably using patternSize=cvSize(rows,cols)
+     *     instead of using patternSize=cvSize(cols,rows) in REF: findChessboardCorners.
+     *
+     * SEE:
+     *    findChessboardCorners, solvePnP, initCameraMatrix2D, stereoCalibrate, undistort
+     */
+    public static double calibrateCameraExtended(List<Mat> objectPoints, List<Mat> imagePoints, Size imageSize, Mat cameraMatrix, Mat distCoeffs, List<Mat> rvecs, List<Mat> tvecs, Mat stdDeviationsIntrinsics, Mat stdDeviationsExtrinsics, Mat perViewErrors, int flags) {
+        Mat objectPoints_mat = Converters.vector_Mat_to_Mat(objectPoints);
+        Mat imagePoints_mat = Converters.vector_Mat_to_Mat(imagePoints);
+        Mat rvecs_mat = new Mat();
+        Mat tvecs_mat = new Mat();
+        double retVal = calibrateCameraExtended_1(objectPoints_mat.nativeObj, imagePoints_mat.nativeObj, imageSize.width, imageSize.height, cameraMatrix.nativeObj, distCoeffs.nativeObj, rvecs_mat.nativeObj, tvecs_mat.nativeObj, stdDeviationsIntrinsics.nativeObj, stdDeviationsExtrinsics.nativeObj, perViewErrors.nativeObj, flags);
+        Converters.Mat_to_vector_Mat(rvecs_mat, rvecs);
+        rvecs_mat.release();
+        Converters.Mat_to_vector_Mat(tvecs_mat, tvecs);
+        tvecs_mat.release();
+        return retVal;
+    }
+
+    /**
+     * Finds the camera intrinsic and extrinsic parameters from several views of a calibration
+     * pattern.
+     *
+     * @param objectPoints In the new interface it is a vector of vectors of calibration pattern points in
+     * the calibration pattern coordinate space (e.g. std::vector&lt;std::vector&lt;cv::Vec3f&gt;&gt;). The outer
+     * vector contains as many elements as the number of pattern views. If the same calibration pattern
+     * is shown in each view and it is fully visible, all the vectors will be the same. Although, it is
+     * possible to use partially occluded patterns or even different patterns in different views. Then,
+     * the vectors will be different. Although the points are 3D, they all lie in the calibration pattern's
+     * XY coordinate plane (thus 0 in the Z-coordinate), if the used calibration pattern is a planar rig.
+     * In the old interface all the vectors of object points from different views are concatenated
+     * together.
+     * @param imagePoints In the new interface it is a vector of vectors of the projections of calibration
+     * pattern points (e.g. std::vector&lt;std::vector&lt;cv::Vec2f&gt;&gt;). imagePoints.size() and
+     * objectPoints.size(), and imagePoints[i].size() and objectPoints[i].size() for each i, must be equal,
+     * respectively. In the old interface all the vectors of object points from different views are
+     * concatenated together.
+     * @param imageSize Size of the image used only to initialize the intrinsic camera matrix.
+     * @param cameraMatrix Input/output 3x3 floating-point camera matrix
+     * \(A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}\) . If CV\_CALIB\_USE\_INTRINSIC\_GUESS
+     * and/or CALIB_FIX_ASPECT_RATIO are specified, some or all of fx, fy, cx, cy must be
+     * initialized before calling the function.
+     * @param distCoeffs Input/output vector of distortion coefficients
+     * \((k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\) of
+     * 4, 5, 8, 12 or 14 elements.
+     * @param rvecs Output vector of rotation vectors (REF: Rodrigues ) estimated for each pattern view
+     * (e.g. std::vector&lt;cv::Mat&gt;&gt;). That is, each i-th rotation vector together with the corresponding
+     * i-th translation vector (see the next output parameter description) brings the calibration pattern
+     * from the object coordinate space (in which object points are specified) to the camera coordinate
+     * space. In more technical terms, the tuple of the i-th rotation and translation vector performs
+     * a change of basis from object coordinate space to camera coordinate space. Due to its duality, this
+     * tuple is equivalent to the position of the calibration pattern with respect to the camera coordinate
+     * space.
+     * @param tvecs Output vector of translation vectors estimated for each pattern view, see parameter
+     * describtion above.
+     * @param stdDeviationsIntrinsics Output vector of standard deviations estimated for intrinsic
+     * parameters. Order of deviations values:
+     * \((f_x, f_y, c_x, c_y, k_1, k_2, p_1, p_2, k_3, k_4, k_5, k_6 , s_1, s_2, s_3,
+     *  s_4, \tau_x, \tau_y)\) If one of parameters is not estimated, it's deviation is equals to zero.
+     * @param stdDeviationsExtrinsics Output vector of standard deviations estimated for extrinsic
+     * parameters. Order of deviations values: \((R_0, T_0, \dotsc , R_{M - 1}, T_{M - 1})\) where M is
+     * the number of pattern views. \(R_i, T_i\) are concatenated 1x3 vectors.
+     *  @param perViewErrors Output vector of the RMS re-projection error estimated for each pattern view.
+     * <ul>
+     *   <li>
+     *    <b>CALIB_USE_INTRINSIC_GUESS</b> cameraMatrix contains valid initial values of
+     * fx, fy, cx, cy that are optimized further. Otherwise, (cx, cy) is initially set to the image
+     * center ( imageSize is used), and focal distances are computed in a least-squares fashion.
+     * Note, that if intrinsic parameters are known, there is no need to use this function just to
+     * estimate extrinsic parameters. Use solvePnP instead.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_FIX_PRINCIPAL_POINT</b> The principal point is not changed during the global
+     * optimization. It stays at the center or at a different location specified when
+     * CALIB_USE_INTRINSIC_GUESS is set too.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_FIX_ASPECT_RATIO</b> The functions consider only fy as a free parameter. The
+     * ratio fx/fy stays the same as in the input cameraMatrix . When
+     * CALIB_USE_INTRINSIC_GUESS is not set, the actual input values of fx and fy are
+     * ignored, only their ratio is computed and used further.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_ZERO_TANGENT_DIST</b> Tangential distortion coefficients \((p_1, p_2)\) are set
+     * to zeros and stay zero.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_FIX_K1,...,CALIB_FIX_K6</b> The corresponding radial distortion
+     * coefficient is not changed during the optimization. If CALIB_USE_INTRINSIC_GUESS is
+     * set, the coefficient from the supplied distCoeffs matrix is used. Otherwise, it is set to 0.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_RATIONAL_MODEL</b> Coefficients k4, k5, and k6 are enabled. To provide the
+     * backward compatibility, this extra flag should be explicitly specified to make the
+     * calibration function use the rational model and return 8 coefficients. If the flag is not
+     * set, the function computes and returns only 5 distortion coefficients.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_THIN_PRISM_MODEL</b> Coefficients s1, s2, s3 and s4 are enabled. To provide the
+     * backward compatibility, this extra flag should be explicitly specified to make the
+     * calibration function use the thin prism model and return 12 coefficients. If the flag is not
+     * set, the function computes and returns only 5 distortion coefficients.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_FIX_S1_S2_S3_S4</b> The thin prism distortion coefficients are not changed during
+     * the optimization. If CALIB_USE_INTRINSIC_GUESS is set, the coefficient from the
+     * supplied distCoeffs matrix is used. Otherwise, it is set to 0.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_TILTED_MODEL</b> Coefficients tauX and tauY are enabled. To provide the
+     * backward compatibility, this extra flag should be explicitly specified to make the
+     * calibration function use the tilted sensor model and return 14 coefficients. If the flag is not
+     * set, the function computes and returns only 5 distortion coefficients.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_FIX_TAUX_TAUY</b> The coefficients of the tilted sensor model are not changed during
+     * the optimization. If CALIB_USE_INTRINSIC_GUESS is set, the coefficient from the
+     * supplied distCoeffs matrix is used. Otherwise, it is set to 0.
+     *   </li>
+     * </ul>
+     *
+     * @return the overall RMS re-projection error.
+     *
+     * The function estimates the intrinsic camera parameters and extrinsic parameters for each of the
+     * views. The algorithm is based on CITE: Zhang2000 and CITE: BouguetMCT . The coordinates of 3D object
+     * points and their corresponding 2D projections in each view must be specified. That may be achieved
+     * by using an object with known geometry and easily detectable feature points. Such an object is
+     * called a calibration rig or calibration pattern, and OpenCV has built-in support for a chessboard as
+     * a calibration rig (see REF: findChessboardCorners). Currently, initialization of intrinsic
+     * parameters (when CALIB_USE_INTRINSIC_GUESS is not set) is only implemented for planar calibration
+     * patterns (where Z-coordinates of the object points must be all zeros). 3D calibration rigs can also
+     * be used as long as initial cameraMatrix is provided.
+     *
+     * The algorithm performs the following steps:
+     *
+     * <ul>
+     *   <li>
+     *    Compute the initial intrinsic parameters (the option only available for planar calibration
+     *     patterns) or read them from the input parameters. The distortion coefficients are all set to
+     *     zeros initially unless some of CALIB_FIX_K? are specified.
+     *   </li>
+     * </ul>
+     *
+     * <ul>
+     *   <li>
+     *    Estimate the initial camera pose as if the intrinsic parameters have been already known. This is
+     *     done using solvePnP .
+     *   </li>
+     * </ul>
+     *
+     * <ul>
+     *   <li>
+     *    Run the global Levenberg-Marquardt optimization algorithm to minimize the reprojection error,
+     *     that is, the total sum of squared distances between the observed feature points imagePoints and
+     *     the projected (using the current estimates for camera parameters and the poses) object points
+     *     objectPoints. See projectPoints for details.
+     *   </li>
+     * </ul>
+     *
+     * <b>Note:</b>
+     *     If you use a non-square (i.e. non-N-by-N) grid and REF: findChessboardCorners for calibration,
+     *     and REF: calibrateCamera returns bad values (zero distortion coefficients, \(c_x\) and
+     *     \(c_y\) very far from the image center, and/or large differences between \(f_x\) and
+     *     \(f_y\) (ratios of 10:1 or more)), then you are probably using patternSize=cvSize(rows,cols)
+     *     instead of using patternSize=cvSize(cols,rows) in REF: findChessboardCorners.
+     *
+     * SEE:
+     *    findChessboardCorners, solvePnP, initCameraMatrix2D, stereoCalibrate, undistort
+     */
+    public static double calibrateCameraExtended(List<Mat> objectPoints, List<Mat> imagePoints, Size imageSize, Mat cameraMatrix, Mat distCoeffs, List<Mat> rvecs, List<Mat> tvecs, Mat stdDeviationsIntrinsics, Mat stdDeviationsExtrinsics, Mat perViewErrors) {
+        Mat objectPoints_mat = Converters.vector_Mat_to_Mat(objectPoints);
+        Mat imagePoints_mat = Converters.vector_Mat_to_Mat(imagePoints);
+        Mat rvecs_mat = new Mat();
+        Mat tvecs_mat = new Mat();
+        double retVal = calibrateCameraExtended_2(objectPoints_mat.nativeObj, imagePoints_mat.nativeObj, imageSize.width, imageSize.height, cameraMatrix.nativeObj, distCoeffs.nativeObj, rvecs_mat.nativeObj, tvecs_mat.nativeObj, stdDeviationsIntrinsics.nativeObj, stdDeviationsExtrinsics.nativeObj, perViewErrors.nativeObj);
+        Converters.Mat_to_vector_Mat(rvecs_mat, rvecs);
+        rvecs_mat.release();
+        Converters.Mat_to_vector_Mat(tvecs_mat, tvecs);
+        tvecs_mat.release();
+        return retVal;
+    }
+
+
+    //
+    // C++:  double cv::calibrateCamera(vector_Mat objectPoints, vector_Mat imagePoints, Size imageSize, Mat& cameraMatrix, Mat& distCoeffs, vector_Mat& rvecs, vector_Mat& tvecs, int flags = 0, TermCriteria criteria = TermCriteria( TermCriteria::COUNT + TermCriteria::EPS, 30, DBL_EPSILON))
+    //
+
+    /**
+     *  double calibrateCamera( InputArrayOfArrays objectPoints,
+     *                                      InputArrayOfArrays imagePoints, Size imageSize,
+     *                                      InputOutputArray cameraMatrix, InputOutputArray distCoeffs,
+     *                                      OutputArrayOfArrays rvecs, OutputArrayOfArrays tvecs,
+     *                                      OutputArray stdDeviations, OutputArray perViewErrors,
+     *                                      int flags = 0, TermCriteria criteria = TermCriteria(
+     *                                         TermCriteria::COUNT + TermCriteria::EPS, 30, DBL_EPSILON) )
+     * @param objectPoints automatically generated
+     * @param imagePoints automatically generated
+     * @param imageSize automatically generated
+     * @param cameraMatrix automatically generated
+     * @param distCoeffs automatically generated
+     * @param rvecs automatically generated
+     * @param tvecs automatically generated
+     * @param flags automatically generated
+     * @param criteria automatically generated
+     * @return automatically generated
+     */
+    public static double calibrateCamera(List<Mat> objectPoints, List<Mat> imagePoints, Size imageSize, Mat cameraMatrix, Mat distCoeffs, List<Mat> rvecs, List<Mat> tvecs, int flags, TermCriteria criteria) {
+        Mat objectPoints_mat = Converters.vector_Mat_to_Mat(objectPoints);
+        Mat imagePoints_mat = Converters.vector_Mat_to_Mat(imagePoints);
+        Mat rvecs_mat = new Mat();
+        Mat tvecs_mat = new Mat();
+        double retVal = calibrateCamera_0(objectPoints_mat.nativeObj, imagePoints_mat.nativeObj, imageSize.width, imageSize.height, cameraMatrix.nativeObj, distCoeffs.nativeObj, rvecs_mat.nativeObj, tvecs_mat.nativeObj, flags, criteria.type, criteria.maxCount, criteria.epsilon);
+        Converters.Mat_to_vector_Mat(rvecs_mat, rvecs);
+        rvecs_mat.release();
+        Converters.Mat_to_vector_Mat(tvecs_mat, tvecs);
+        tvecs_mat.release();
+        return retVal;
+    }
+
+    /**
+     *  double calibrateCamera( InputArrayOfArrays objectPoints,
+     *                                      InputArrayOfArrays imagePoints, Size imageSize,
+     *                                      InputOutputArray cameraMatrix, InputOutputArray distCoeffs,
+     *                                      OutputArrayOfArrays rvecs, OutputArrayOfArrays tvecs,
+     *                                      OutputArray stdDeviations, OutputArray perViewErrors,
+     *                                      int flags = 0, TermCriteria criteria = TermCriteria(
+     *                                         TermCriteria::COUNT + TermCriteria::EPS, 30, DBL_EPSILON) )
+     * @param objectPoints automatically generated
+     * @param imagePoints automatically generated
+     * @param imageSize automatically generated
+     * @param cameraMatrix automatically generated
+     * @param distCoeffs automatically generated
+     * @param rvecs automatically generated
+     * @param tvecs automatically generated
+     * @param flags automatically generated
+     * @return automatically generated
+     */
+    public static double calibrateCamera(List<Mat> objectPoints, List<Mat> imagePoints, Size imageSize, Mat cameraMatrix, Mat distCoeffs, List<Mat> rvecs, List<Mat> tvecs, int flags) {
+        Mat objectPoints_mat = Converters.vector_Mat_to_Mat(objectPoints);
+        Mat imagePoints_mat = Converters.vector_Mat_to_Mat(imagePoints);
+        Mat rvecs_mat = new Mat();
+        Mat tvecs_mat = new Mat();
+        double retVal = calibrateCamera_1(objectPoints_mat.nativeObj, imagePoints_mat.nativeObj, imageSize.width, imageSize.height, cameraMatrix.nativeObj, distCoeffs.nativeObj, rvecs_mat.nativeObj, tvecs_mat.nativeObj, flags);
+        Converters.Mat_to_vector_Mat(rvecs_mat, rvecs);
+        rvecs_mat.release();
+        Converters.Mat_to_vector_Mat(tvecs_mat, tvecs);
+        tvecs_mat.release();
+        return retVal;
+    }
+
+    /**
+     *  double calibrateCamera( InputArrayOfArrays objectPoints,
+     *                                      InputArrayOfArrays imagePoints, Size imageSize,
+     *                                      InputOutputArray cameraMatrix, InputOutputArray distCoeffs,
+     *                                      OutputArrayOfArrays rvecs, OutputArrayOfArrays tvecs,
+     *                                      OutputArray stdDeviations, OutputArray perViewErrors,
+     *                                      int flags = 0, TermCriteria criteria = TermCriteria(
+     *                                         TermCriteria::COUNT + TermCriteria::EPS, 30, DBL_EPSILON) )
+     * @param objectPoints automatically generated
+     * @param imagePoints automatically generated
+     * @param imageSize automatically generated
+     * @param cameraMatrix automatically generated
+     * @param distCoeffs automatically generated
+     * @param rvecs automatically generated
+     * @param tvecs automatically generated
+     * @return automatically generated
+     */
+    public static double calibrateCamera(List<Mat> objectPoints, List<Mat> imagePoints, Size imageSize, Mat cameraMatrix, Mat distCoeffs, List<Mat> rvecs, List<Mat> tvecs) {
+        Mat objectPoints_mat = Converters.vector_Mat_to_Mat(objectPoints);
+        Mat imagePoints_mat = Converters.vector_Mat_to_Mat(imagePoints);
+        Mat rvecs_mat = new Mat();
+        Mat tvecs_mat = new Mat();
+        double retVal = calibrateCamera_2(objectPoints_mat.nativeObj, imagePoints_mat.nativeObj, imageSize.width, imageSize.height, cameraMatrix.nativeObj, distCoeffs.nativeObj, rvecs_mat.nativeObj, tvecs_mat.nativeObj);
+        Converters.Mat_to_vector_Mat(rvecs_mat, rvecs);
+        rvecs_mat.release();
+        Converters.Mat_to_vector_Mat(tvecs_mat, tvecs);
+        tvecs_mat.release();
+        return retVal;
+    }
+
+
+    //
+    // C++:  double cv::sampsonDistance(Mat pt1, Mat pt2, Mat F)
+    //
+
+    /**
+     * Calculates the Sampson Distance between two points.
+     *
+     * The function cv::sampsonDistance calculates and returns the first order approximation of the geometric error as:
+     * \(
+     * sd( \texttt{pt1} , \texttt{pt2} )=
+     * \frac{(\texttt{pt2}^t \cdot \texttt{F} \cdot \texttt{pt1})^2}
+     * {((\texttt{F} \cdot \texttt{pt1})(0))^2 +
+     * ((\texttt{F} \cdot \texttt{pt1})(1))^2 +
+     * ((\texttt{F}^t \cdot \texttt{pt2})(0))^2 +
+     * ((\texttt{F}^t \cdot \texttt{pt2})(1))^2}
+     * \)
+     * The fundamental matrix may be calculated using the cv::findFundamentalMat function. See CITE: HartleyZ00 11.4.3 for details.
+     * @param pt1 first homogeneous 2d point
+     * @param pt2 second homogeneous 2d point
+     * @param F fundamental matrix
+     * @return The computed Sampson distance.
+     */
+    public static double sampsonDistance(Mat pt1, Mat pt2, Mat F) {
+        return sampsonDistance_0(pt1.nativeObj, pt2.nativeObj, F.nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::stereoCalibrate(vector_Mat objectPoints, vector_Mat imagePoints1, vector_Mat imagePoints2, Mat& cameraMatrix1, Mat& distCoeffs1, Mat& cameraMatrix2, Mat& distCoeffs2, Size imageSize, Mat& R, Mat& T, Mat& E, Mat& F, Mat& perViewErrors, int flags = CALIB_FIX_INTRINSIC, TermCriteria criteria = TermCriteria(TermCriteria::COUNT+TermCriteria::EPS, 30, 1e-6))
+    //
+
+    /**
+     * Calibrates a stereo camera set up. This function finds the intrinsic parameters
+     * for each of the two cameras and the extrinsic parameters between the two cameras.
+     *
+     * @param objectPoints Vector of vectors of the calibration pattern points. The same structure as
+     * in REF: calibrateCamera. For each pattern view, both cameras need to see the same object
+     * points. Therefore, objectPoints.size(), imagePoints1.size(), and imagePoints2.size() need to be
+     * equal as well as objectPoints[i].size(), imagePoints1[i].size(), and imagePoints2[i].size() need to
+     * be equal for each i.
+     * @param imagePoints1 Vector of vectors of the projections of the calibration pattern points,
+     * observed by the first camera. The same structure as in REF: calibrateCamera.
+     * @param imagePoints2 Vector of vectors of the projections of the calibration pattern points,
+     * observed by the second camera. The same structure as in REF: calibrateCamera.
+     * @param cameraMatrix1 Input/output camera matrix for the first camera, the same as in
+     * REF: calibrateCamera. Furthermore, for the stereo case, additional flags may be used, see below.
+     * @param distCoeffs1 Input/output vector of distortion coefficients, the same as in
+     * REF: calibrateCamera.
+     * @param cameraMatrix2 Input/output second camera matrix for the second camera. See description for
+     * cameraMatrix1.
+     * @param distCoeffs2 Input/output lens distortion coefficients for the second camera. See
+     * description for distCoeffs1.
+     * @param imageSize Size of the image used only to initialize the intrinsic camera matrices.
+     * @param R Output rotation matrix. Together with the translation vector T, this matrix brings
+     * points given in the first camera's coordinate system to points in the second camera's
+     * coordinate system. In more technical terms, the tuple of R and T performs a change of basis
+     * from the first camera's coordinate system to the second camera's coordinate system. Due to its
+     * duality, this tuple is equivalent to the position of the first camera with respect to the
+     * second camera coordinate system.
+     * @param T Output translation vector, see description above.
+     * @param E Output essential matrix.
+     * @param F Output fundamental matrix.
+     * @param perViewErrors Output vector of the RMS re-projection error estimated for each pattern view.
+     * @param flags Different flags that may be zero or a combination of the following values:
+     * <ul>
+     *   <li>
+     *    <b>CALIB_FIX_INTRINSIC</b> Fix cameraMatrix? and distCoeffs? so that only R, T, E, and F
+     * matrices are estimated.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_USE_INTRINSIC_GUESS</b> Optimize some or all of the intrinsic parameters
+     * according to the specified flags. Initial values are provided by the user.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_USE_EXTRINSIC_GUESS</b> R and T contain valid initial values that are optimized further.
+     * Otherwise R and T are initialized to the median value of the pattern views (each dimension separately).
+     *   </li>
+     *   <li>
+     *    <b>CALIB_FIX_PRINCIPAL_POINT</b> Fix the principal points during the optimization.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_FIX_FOCAL_LENGTH</b> Fix \(f^{(j)}_x\) and \(f^{(j)}_y\) .
+     *   </li>
+     *   <li>
+     *    <b>CALIB_FIX_ASPECT_RATIO</b> Optimize \(f^{(j)}_y\) . Fix the ratio \(f^{(j)}_x/f^{(j)}_y\)
+     * .
+     *   </li>
+     *   <li>
+     *    <b>CALIB_SAME_FOCAL_LENGTH</b> Enforce \(f^{(0)}_x=f^{(1)}_x\) and \(f^{(0)}_y=f^{(1)}_y\) .
+     *   </li>
+     *   <li>
+     *    <b>CALIB_ZERO_TANGENT_DIST</b> Set tangential distortion coefficients for each camera to
+     * zeros and fix there.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_FIX_K1,...,CALIB_FIX_K6</b> Do not change the corresponding radial
+     * distortion coefficient during the optimization. If CALIB_USE_INTRINSIC_GUESS is set,
+     * the coefficient from the supplied distCoeffs matrix is used. Otherwise, it is set to 0.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_RATIONAL_MODEL</b> Enable coefficients k4, k5, and k6. To provide the backward
+     * compatibility, this extra flag should be explicitly specified to make the calibration
+     * function use the rational model and return 8 coefficients. If the flag is not set, the
+     * function computes and returns only 5 distortion coefficients.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_THIN_PRISM_MODEL</b> Coefficients s1, s2, s3 and s4 are enabled. To provide the
+     * backward compatibility, this extra flag should be explicitly specified to make the
+     * calibration function use the thin prism model and return 12 coefficients. If the flag is not
+     * set, the function computes and returns only 5 distortion coefficients.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_FIX_S1_S2_S3_S4</b> The thin prism distortion coefficients are not changed during
+     * the optimization. If CALIB_USE_INTRINSIC_GUESS is set, the coefficient from the
+     * supplied distCoeffs matrix is used. Otherwise, it is set to 0.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_TILTED_MODEL</b> Coefficients tauX and tauY are enabled. To provide the
+     * backward compatibility, this extra flag should be explicitly specified to make the
+     * calibration function use the tilted sensor model and return 14 coefficients. If the flag is not
+     * set, the function computes and returns only 5 distortion coefficients.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_FIX_TAUX_TAUY</b> The coefficients of the tilted sensor model are not changed during
+     * the optimization. If CALIB_USE_INTRINSIC_GUESS is set, the coefficient from the
+     * supplied distCoeffs matrix is used. Otherwise, it is set to 0.
+     * @param criteria Termination criteria for the iterative optimization algorithm.
+     *   </li>
+     * </ul>
+     *
+     * The function estimates the transformation between two cameras making a stereo pair. If one computes
+     * the poses of an object relative to the first camera and to the second camera,
+     * ( \(R_1\),\(T_1\) ) and (\(R_2\),\(T_2\)), respectively, for a stereo camera where the
+     * relative position and orientation between the two cameras are fixed, then those poses definitely
+     * relate to each other. This means, if the relative position and orientation (\(R\),\(T\)) of the
+     * two cameras is known, it is possible to compute (\(R_2\),\(T_2\)) when (\(R_1\),\(T_1\)) is
+     * given. This is what the described function does. It computes (\(R\),\(T\)) such that:
+     *
+     * \(R_2=R R_1\)
+     * \(T_2=R T_1 + T.\)
+     *
+     * Therefore, one can compute the coordinate representation of a 3D point for the second camera's
+     * coordinate system when given the point's coordinate representation in the first camera's coordinate
+     * system:
+     *
+     * \(\begin{bmatrix}
+     * X_2 \\
+     * Y_2 \\
+     * Z_2 \\
+     * 1
+     * \end{bmatrix} = \begin{bmatrix}
+     * R &amp; T \\
+     * 0 &amp; 1
+     * \end{bmatrix} \begin{bmatrix}
+     * X_1 \\
+     * Y_1 \\
+     * Z_1 \\
+     * 1
+     * \end{bmatrix}.\)
+     *
+     *
+     * Optionally, it computes the essential matrix E:
+     *
+     * \(E= \vecthreethree{0}{-T_2}{T_1}{T_2}{0}{-T_0}{-T_1}{T_0}{0} R\)
+     *
+     * where \(T_i\) are components of the translation vector \(T\) : \(T=[T_0, T_1, T_2]^T\) .
+     * And the function can also compute the fundamental matrix F:
+     *
+     * \(F = cameraMatrix2^{-T}\cdot E \cdot cameraMatrix1^{-1}\)
+     *
+     * Besides the stereo-related information, the function can also perform a full calibration of each of
+     * the two cameras. However, due to the high dimensionality of the parameter space and noise in the
+     * input data, the function can diverge from the correct solution. If the intrinsic parameters can be
+     * estimated with high accuracy for each of the cameras individually (for example, using
+     * calibrateCamera ), you are recommended to do so and then pass CALIB_FIX_INTRINSIC flag to the
+     * function along with the computed intrinsic parameters. Otherwise, if all the parameters are
+     * estimated at once, it makes sense to restrict some parameters, for example, pass
+     * CALIB_SAME_FOCAL_LENGTH and CALIB_ZERO_TANGENT_DIST flags, which is usually a
+     * reasonable assumption.
+     *
+     * Similarly to calibrateCamera, the function minimizes the total re-projection error for all the
+     * points in all the available views from both cameras. The function returns the final value of the
+     * re-projection error.
+     * @return automatically generated
+     */
+    public static double stereoCalibrateExtended(List<Mat> objectPoints, List<Mat> imagePoints1, List<Mat> imagePoints2, Mat cameraMatrix1, Mat distCoeffs1, Mat cameraMatrix2, Mat distCoeffs2, Size imageSize, Mat R, Mat T, Mat E, Mat F, Mat perViewErrors, int flags, TermCriteria criteria) {
+        Mat objectPoints_mat = Converters.vector_Mat_to_Mat(objectPoints);
+        Mat imagePoints1_mat = Converters.vector_Mat_to_Mat(imagePoints1);
+        Mat imagePoints2_mat = Converters.vector_Mat_to_Mat(imagePoints2);
+        return stereoCalibrateExtended_0(objectPoints_mat.nativeObj, imagePoints1_mat.nativeObj, imagePoints2_mat.nativeObj, cameraMatrix1.nativeObj, distCoeffs1.nativeObj, cameraMatrix2.nativeObj, distCoeffs2.nativeObj, imageSize.width, imageSize.height, R.nativeObj, T.nativeObj, E.nativeObj, F.nativeObj, perViewErrors.nativeObj, flags, criteria.type, criteria.maxCount, criteria.epsilon);
+    }
+
+    /**
+     * Calibrates a stereo camera set up. This function finds the intrinsic parameters
+     * for each of the two cameras and the extrinsic parameters between the two cameras.
+     *
+     * @param objectPoints Vector of vectors of the calibration pattern points. The same structure as
+     * in REF: calibrateCamera. For each pattern view, both cameras need to see the same object
+     * points. Therefore, objectPoints.size(), imagePoints1.size(), and imagePoints2.size() need to be
+     * equal as well as objectPoints[i].size(), imagePoints1[i].size(), and imagePoints2[i].size() need to
+     * be equal for each i.
+     * @param imagePoints1 Vector of vectors of the projections of the calibration pattern points,
+     * observed by the first camera. The same structure as in REF: calibrateCamera.
+     * @param imagePoints2 Vector of vectors of the projections of the calibration pattern points,
+     * observed by the second camera. The same structure as in REF: calibrateCamera.
+     * @param cameraMatrix1 Input/output camera matrix for the first camera, the same as in
+     * REF: calibrateCamera. Furthermore, for the stereo case, additional flags may be used, see below.
+     * @param distCoeffs1 Input/output vector of distortion coefficients, the same as in
+     * REF: calibrateCamera.
+     * @param cameraMatrix2 Input/output second camera matrix for the second camera. See description for
+     * cameraMatrix1.
+     * @param distCoeffs2 Input/output lens distortion coefficients for the second camera. See
+     * description for distCoeffs1.
+     * @param imageSize Size of the image used only to initialize the intrinsic camera matrices.
+     * @param R Output rotation matrix. Together with the translation vector T, this matrix brings
+     * points given in the first camera's coordinate system to points in the second camera's
+     * coordinate system. In more technical terms, the tuple of R and T performs a change of basis
+     * from the first camera's coordinate system to the second camera's coordinate system. Due to its
+     * duality, this tuple is equivalent to the position of the first camera with respect to the
+     * second camera coordinate system.
+     * @param T Output translation vector, see description above.
+     * @param E Output essential matrix.
+     * @param F Output fundamental matrix.
+     * @param perViewErrors Output vector of the RMS re-projection error estimated for each pattern view.
+     * @param flags Different flags that may be zero or a combination of the following values:
+     * <ul>
+     *   <li>
+     *    <b>CALIB_FIX_INTRINSIC</b> Fix cameraMatrix? and distCoeffs? so that only R, T, E, and F
+     * matrices are estimated.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_USE_INTRINSIC_GUESS</b> Optimize some or all of the intrinsic parameters
+     * according to the specified flags. Initial values are provided by the user.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_USE_EXTRINSIC_GUESS</b> R and T contain valid initial values that are optimized further.
+     * Otherwise R and T are initialized to the median value of the pattern views (each dimension separately).
+     *   </li>
+     *   <li>
+     *    <b>CALIB_FIX_PRINCIPAL_POINT</b> Fix the principal points during the optimization.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_FIX_FOCAL_LENGTH</b> Fix \(f^{(j)}_x\) and \(f^{(j)}_y\) .
+     *   </li>
+     *   <li>
+     *    <b>CALIB_FIX_ASPECT_RATIO</b> Optimize \(f^{(j)}_y\) . Fix the ratio \(f^{(j)}_x/f^{(j)}_y\)
+     * .
+     *   </li>
+     *   <li>
+     *    <b>CALIB_SAME_FOCAL_LENGTH</b> Enforce \(f^{(0)}_x=f^{(1)}_x\) and \(f^{(0)}_y=f^{(1)}_y\) .
+     *   </li>
+     *   <li>
+     *    <b>CALIB_ZERO_TANGENT_DIST</b> Set tangential distortion coefficients for each camera to
+     * zeros and fix there.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_FIX_K1,...,CALIB_FIX_K6</b> Do not change the corresponding radial
+     * distortion coefficient during the optimization. If CALIB_USE_INTRINSIC_GUESS is set,
+     * the coefficient from the supplied distCoeffs matrix is used. Otherwise, it is set to 0.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_RATIONAL_MODEL</b> Enable coefficients k4, k5, and k6. To provide the backward
+     * compatibility, this extra flag should be explicitly specified to make the calibration
+     * function use the rational model and return 8 coefficients. If the flag is not set, the
+     * function computes and returns only 5 distortion coefficients.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_THIN_PRISM_MODEL</b> Coefficients s1, s2, s3 and s4 are enabled. To provide the
+     * backward compatibility, this extra flag should be explicitly specified to make the
+     * calibration function use the thin prism model and return 12 coefficients. If the flag is not
+     * set, the function computes and returns only 5 distortion coefficients.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_FIX_S1_S2_S3_S4</b> The thin prism distortion coefficients are not changed during
+     * the optimization. If CALIB_USE_INTRINSIC_GUESS is set, the coefficient from the
+     * supplied distCoeffs matrix is used. Otherwise, it is set to 0.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_TILTED_MODEL</b> Coefficients tauX and tauY are enabled. To provide the
+     * backward compatibility, this extra flag should be explicitly specified to make the
+     * calibration function use the tilted sensor model and return 14 coefficients. If the flag is not
+     * set, the function computes and returns only 5 distortion coefficients.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_FIX_TAUX_TAUY</b> The coefficients of the tilted sensor model are not changed during
+     * the optimization. If CALIB_USE_INTRINSIC_GUESS is set, the coefficient from the
+     * supplied distCoeffs matrix is used. Otherwise, it is set to 0.
+     *   </li>
+     * </ul>
+     *
+     * The function estimates the transformation between two cameras making a stereo pair. If one computes
+     * the poses of an object relative to the first camera and to the second camera,
+     * ( \(R_1\),\(T_1\) ) and (\(R_2\),\(T_2\)), respectively, for a stereo camera where the
+     * relative position and orientation between the two cameras are fixed, then those poses definitely
+     * relate to each other. This means, if the relative position and orientation (\(R\),\(T\)) of the
+     * two cameras is known, it is possible to compute (\(R_2\),\(T_2\)) when (\(R_1\),\(T_1\)) is
+     * given. This is what the described function does. It computes (\(R\),\(T\)) such that:
+     *
+     * \(R_2=R R_1\)
+     * \(T_2=R T_1 + T.\)
+     *
+     * Therefore, one can compute the coordinate representation of a 3D point for the second camera's
+     * coordinate system when given the point's coordinate representation in the first camera's coordinate
+     * system:
+     *
+     * \(\begin{bmatrix}
+     * X_2 \\
+     * Y_2 \\
+     * Z_2 \\
+     * 1
+     * \end{bmatrix} = \begin{bmatrix}
+     * R &amp; T \\
+     * 0 &amp; 1
+     * \end{bmatrix} \begin{bmatrix}
+     * X_1 \\
+     * Y_1 \\
+     * Z_1 \\
+     * 1
+     * \end{bmatrix}.\)
+     *
+     *
+     * Optionally, it computes the essential matrix E:
+     *
+     * \(E= \vecthreethree{0}{-T_2}{T_1}{T_2}{0}{-T_0}{-T_1}{T_0}{0} R\)
+     *
+     * where \(T_i\) are components of the translation vector \(T\) : \(T=[T_0, T_1, T_2]^T\) .
+     * And the function can also compute the fundamental matrix F:
+     *
+     * \(F = cameraMatrix2^{-T}\cdot E \cdot cameraMatrix1^{-1}\)
+     *
+     * Besides the stereo-related information, the function can also perform a full calibration of each of
+     * the two cameras. However, due to the high dimensionality of the parameter space and noise in the
+     * input data, the function can diverge from the correct solution. If the intrinsic parameters can be
+     * estimated with high accuracy for each of the cameras individually (for example, using
+     * calibrateCamera ), you are recommended to do so and then pass CALIB_FIX_INTRINSIC flag to the
+     * function along with the computed intrinsic parameters. Otherwise, if all the parameters are
+     * estimated at once, it makes sense to restrict some parameters, for example, pass
+     * CALIB_SAME_FOCAL_LENGTH and CALIB_ZERO_TANGENT_DIST flags, which is usually a
+     * reasonable assumption.
+     *
+     * Similarly to calibrateCamera, the function minimizes the total re-projection error for all the
+     * points in all the available views from both cameras. The function returns the final value of the
+     * re-projection error.
+     * @return automatically generated
+     */
+    public static double stereoCalibrateExtended(List<Mat> objectPoints, List<Mat> imagePoints1, List<Mat> imagePoints2, Mat cameraMatrix1, Mat distCoeffs1, Mat cameraMatrix2, Mat distCoeffs2, Size imageSize, Mat R, Mat T, Mat E, Mat F, Mat perViewErrors, int flags) {
+        Mat objectPoints_mat = Converters.vector_Mat_to_Mat(objectPoints);
+        Mat imagePoints1_mat = Converters.vector_Mat_to_Mat(imagePoints1);
+        Mat imagePoints2_mat = Converters.vector_Mat_to_Mat(imagePoints2);
+        return stereoCalibrateExtended_1(objectPoints_mat.nativeObj, imagePoints1_mat.nativeObj, imagePoints2_mat.nativeObj, cameraMatrix1.nativeObj, distCoeffs1.nativeObj, cameraMatrix2.nativeObj, distCoeffs2.nativeObj, imageSize.width, imageSize.height, R.nativeObj, T.nativeObj, E.nativeObj, F.nativeObj, perViewErrors.nativeObj, flags);
+    }
+
+    /**
+     * Calibrates a stereo camera set up. This function finds the intrinsic parameters
+     * for each of the two cameras and the extrinsic parameters between the two cameras.
+     *
+     * @param objectPoints Vector of vectors of the calibration pattern points. The same structure as
+     * in REF: calibrateCamera. For each pattern view, both cameras need to see the same object
+     * points. Therefore, objectPoints.size(), imagePoints1.size(), and imagePoints2.size() need to be
+     * equal as well as objectPoints[i].size(), imagePoints1[i].size(), and imagePoints2[i].size() need to
+     * be equal for each i.
+     * @param imagePoints1 Vector of vectors of the projections of the calibration pattern points,
+     * observed by the first camera. The same structure as in REF: calibrateCamera.
+     * @param imagePoints2 Vector of vectors of the projections of the calibration pattern points,
+     * observed by the second camera. The same structure as in REF: calibrateCamera.
+     * @param cameraMatrix1 Input/output camera matrix for the first camera, the same as in
+     * REF: calibrateCamera. Furthermore, for the stereo case, additional flags may be used, see below.
+     * @param distCoeffs1 Input/output vector of distortion coefficients, the same as in
+     * REF: calibrateCamera.
+     * @param cameraMatrix2 Input/output second camera matrix for the second camera. See description for
+     * cameraMatrix1.
+     * @param distCoeffs2 Input/output lens distortion coefficients for the second camera. See
+     * description for distCoeffs1.
+     * @param imageSize Size of the image used only to initialize the intrinsic camera matrices.
+     * @param R Output rotation matrix. Together with the translation vector T, this matrix brings
+     * points given in the first camera's coordinate system to points in the second camera's
+     * coordinate system. In more technical terms, the tuple of R and T performs a change of basis
+     * from the first camera's coordinate system to the second camera's coordinate system. Due to its
+     * duality, this tuple is equivalent to the position of the first camera with respect to the
+     * second camera coordinate system.
+     * @param T Output translation vector, see description above.
+     * @param E Output essential matrix.
+     * @param F Output fundamental matrix.
+     * @param perViewErrors Output vector of the RMS re-projection error estimated for each pattern view.
+     * <ul>
+     *   <li>
+     *    <b>CALIB_FIX_INTRINSIC</b> Fix cameraMatrix? and distCoeffs? so that only R, T, E, and F
+     * matrices are estimated.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_USE_INTRINSIC_GUESS</b> Optimize some or all of the intrinsic parameters
+     * according to the specified flags. Initial values are provided by the user.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_USE_EXTRINSIC_GUESS</b> R and T contain valid initial values that are optimized further.
+     * Otherwise R and T are initialized to the median value of the pattern views (each dimension separately).
+     *   </li>
+     *   <li>
+     *    <b>CALIB_FIX_PRINCIPAL_POINT</b> Fix the principal points during the optimization.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_FIX_FOCAL_LENGTH</b> Fix \(f^{(j)}_x\) and \(f^{(j)}_y\) .
+     *   </li>
+     *   <li>
+     *    <b>CALIB_FIX_ASPECT_RATIO</b> Optimize \(f^{(j)}_y\) . Fix the ratio \(f^{(j)}_x/f^{(j)}_y\)
+     * .
+     *   </li>
+     *   <li>
+     *    <b>CALIB_SAME_FOCAL_LENGTH</b> Enforce \(f^{(0)}_x=f^{(1)}_x\) and \(f^{(0)}_y=f^{(1)}_y\) .
+     *   </li>
+     *   <li>
+     *    <b>CALIB_ZERO_TANGENT_DIST</b> Set tangential distortion coefficients for each camera to
+     * zeros and fix there.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_FIX_K1,...,CALIB_FIX_K6</b> Do not change the corresponding radial
+     * distortion coefficient during the optimization. If CALIB_USE_INTRINSIC_GUESS is set,
+     * the coefficient from the supplied distCoeffs matrix is used. Otherwise, it is set to 0.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_RATIONAL_MODEL</b> Enable coefficients k4, k5, and k6. To provide the backward
+     * compatibility, this extra flag should be explicitly specified to make the calibration
+     * function use the rational model and return 8 coefficients. If the flag is not set, the
+     * function computes and returns only 5 distortion coefficients.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_THIN_PRISM_MODEL</b> Coefficients s1, s2, s3 and s4 are enabled. To provide the
+     * backward compatibility, this extra flag should be explicitly specified to make the
+     * calibration function use the thin prism model and return 12 coefficients. If the flag is not
+     * set, the function computes and returns only 5 distortion coefficients.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_FIX_S1_S2_S3_S4</b> The thin prism distortion coefficients are not changed during
+     * the optimization. If CALIB_USE_INTRINSIC_GUESS is set, the coefficient from the
+     * supplied distCoeffs matrix is used. Otherwise, it is set to 0.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_TILTED_MODEL</b> Coefficients tauX and tauY are enabled. To provide the
+     * backward compatibility, this extra flag should be explicitly specified to make the
+     * calibration function use the tilted sensor model and return 14 coefficients. If the flag is not
+     * set, the function computes and returns only 5 distortion coefficients.
+     *   </li>
+     *   <li>
+     *    <b>CALIB_FIX_TAUX_TAUY</b> The coefficients of the tilted sensor model are not changed during
+     * the optimization. If CALIB_USE_INTRINSIC_GUESS is set, the coefficient from the
+     * supplied distCoeffs matrix is used. Otherwise, it is set to 0.
+     *   </li>
+     * </ul>
+     *
+     * The function estimates the transformation between two cameras making a stereo pair. If one computes
+     * the poses of an object relative to the first camera and to the second camera,
+     * ( \(R_1\),\(T_1\) ) and (\(R_2\),\(T_2\)), respectively, for a stereo camera where the
+     * relative position and orientation between the two cameras are fixed, then those poses definitely
+     * relate to each other. This means, if the relative position and orientation (\(R\),\(T\)) of the
+     * two cameras is known, it is possible to compute (\(R_2\),\(T_2\)) when (\(R_1\),\(T_1\)) is
+     * given. This is what the described function does. It computes (\(R\),\(T\)) such that:
+     *
+     * \(R_2=R R_1\)
+     * \(T_2=R T_1 + T.\)
+     *
+     * Therefore, one can compute the coordinate representation of a 3D point for the second camera's
+     * coordinate system when given the point's coordinate representation in the first camera's coordinate
+     * system:
+     *
+     * \(\begin{bmatrix}
+     * X_2 \\
+     * Y_2 \\
+     * Z_2 \\
+     * 1
+     * \end{bmatrix} = \begin{bmatrix}
+     * R &amp; T \\
+     * 0 &amp; 1
+     * \end{bmatrix} \begin{bmatrix}
+     * X_1 \\
+     * Y_1 \\
+     * Z_1 \\
+     * 1
+     * \end{bmatrix}.\)
+     *
+     *
+     * Optionally, it computes the essential matrix E:
+     *
+     * \(E= \vecthreethree{0}{-T_2}{T_1}{T_2}{0}{-T_0}{-T_1}{T_0}{0} R\)
+     *
+     * where \(T_i\) are components of the translation vector \(T\) : \(T=[T_0, T_1, T_2]^T\) .
+     * And the function can also compute the fundamental matrix F:
+     *
+     * \(F = cameraMatrix2^{-T}\cdot E \cdot cameraMatrix1^{-1}\)
+     *
+     * Besides the stereo-related information, the function can also perform a full calibration of each of
+     * the two cameras. However, due to the high dimensionality of the parameter space and noise in the
+     * input data, the function can diverge from the correct solution. If the intrinsic parameters can be
+     * estimated with high accuracy for each of the cameras individually (for example, using
+     * calibrateCamera ), you are recommended to do so and then pass CALIB_FIX_INTRINSIC flag to the
+     * function along with the computed intrinsic parameters. Otherwise, if all the parameters are
+     * estimated at once, it makes sense to restrict some parameters, for example, pass
+     * CALIB_SAME_FOCAL_LENGTH and CALIB_ZERO_TANGENT_DIST flags, which is usually a
+     * reasonable assumption.
+     *
+     * Similarly to calibrateCamera, the function minimizes the total re-projection error for all the
+     * points in all the available views from both cameras. The function returns the final value of the
+     * re-projection error.
+     * @return automatically generated
+     */
+    public static double stereoCalibrateExtended(List<Mat> objectPoints, List<Mat> imagePoints1, List<Mat> imagePoints2, Mat cameraMatrix1, Mat distCoeffs1, Mat cameraMatrix2, Mat distCoeffs2, Size imageSize, Mat R, Mat T, Mat E, Mat F, Mat perViewErrors) {
+        Mat objectPoints_mat = Converters.vector_Mat_to_Mat(objectPoints);
+        Mat imagePoints1_mat = Converters.vector_Mat_to_Mat(imagePoints1);
+        Mat imagePoints2_mat = Converters.vector_Mat_to_Mat(imagePoints2);
+        return stereoCalibrateExtended_2(objectPoints_mat.nativeObj, imagePoints1_mat.nativeObj, imagePoints2_mat.nativeObj, cameraMatrix1.nativeObj, distCoeffs1.nativeObj, cameraMatrix2.nativeObj, distCoeffs2.nativeObj, imageSize.width, imageSize.height, R.nativeObj, T.nativeObj, E.nativeObj, F.nativeObj, perViewErrors.nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::stereoCalibrate(vector_Mat objectPoints, vector_Mat imagePoints1, vector_Mat imagePoints2, Mat& cameraMatrix1, Mat& distCoeffs1, Mat& cameraMatrix2, Mat& distCoeffs2, Size imageSize, Mat& R, Mat& T, Mat& E, Mat& F, int flags = CALIB_FIX_INTRINSIC, TermCriteria criteria = TermCriteria(TermCriteria::COUNT+TermCriteria::EPS, 30, 1e-6))
+    //
+
+    public static double stereoCalibrate(List<Mat> objectPoints, List<Mat> imagePoints1, List<Mat> imagePoints2, Mat cameraMatrix1, Mat distCoeffs1, Mat cameraMatrix2, Mat distCoeffs2, Size imageSize, Mat R, Mat T, Mat E, Mat F, int flags, TermCriteria criteria) {
+        Mat objectPoints_mat = Converters.vector_Mat_to_Mat(objectPoints);
+        Mat imagePoints1_mat = Converters.vector_Mat_to_Mat(imagePoints1);
+        Mat imagePoints2_mat = Converters.vector_Mat_to_Mat(imagePoints2);
+        return stereoCalibrate_0(objectPoints_mat.nativeObj, imagePoints1_mat.nativeObj, imagePoints2_mat.nativeObj, cameraMatrix1.nativeObj, distCoeffs1.nativeObj, cameraMatrix2.nativeObj, distCoeffs2.nativeObj, imageSize.width, imageSize.height, R.nativeObj, T.nativeObj, E.nativeObj, F.nativeObj, flags, criteria.type, criteria.maxCount, criteria.epsilon);
+    }
+
+    public static double stereoCalibrate(List<Mat> objectPoints, List<Mat> imagePoints1, List<Mat> imagePoints2, Mat cameraMatrix1, Mat distCoeffs1, Mat cameraMatrix2, Mat distCoeffs2, Size imageSize, Mat R, Mat T, Mat E, Mat F, int flags) {
+        Mat objectPoints_mat = Converters.vector_Mat_to_Mat(objectPoints);
+        Mat imagePoints1_mat = Converters.vector_Mat_to_Mat(imagePoints1);
+        Mat imagePoints2_mat = Converters.vector_Mat_to_Mat(imagePoints2);
+        return stereoCalibrate_1(objectPoints_mat.nativeObj, imagePoints1_mat.nativeObj, imagePoints2_mat.nativeObj, cameraMatrix1.nativeObj, distCoeffs1.nativeObj, cameraMatrix2.nativeObj, distCoeffs2.nativeObj, imageSize.width, imageSize.height, R.nativeObj, T.nativeObj, E.nativeObj, F.nativeObj, flags);
+    }
+
+    public static double stereoCalibrate(List<Mat> objectPoints, List<Mat> imagePoints1, List<Mat> imagePoints2, Mat cameraMatrix1, Mat distCoeffs1, Mat cameraMatrix2, Mat distCoeffs2, Size imageSize, Mat R, Mat T, Mat E, Mat F) {
+        Mat objectPoints_mat = Converters.vector_Mat_to_Mat(objectPoints);
+        Mat imagePoints1_mat = Converters.vector_Mat_to_Mat(imagePoints1);
+        Mat imagePoints2_mat = Converters.vector_Mat_to_Mat(imagePoints2);
+        return stereoCalibrate_2(objectPoints_mat.nativeObj, imagePoints1_mat.nativeObj, imagePoints2_mat.nativeObj, cameraMatrix1.nativeObj, distCoeffs1.nativeObj, cameraMatrix2.nativeObj, distCoeffs2.nativeObj, imageSize.width, imageSize.height, R.nativeObj, T.nativeObj, E.nativeObj, F.nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::fisheye::calibrate(vector_Mat objectPoints, vector_Mat imagePoints, Size image_size, Mat& K, Mat& D, vector_Mat& rvecs, vector_Mat& tvecs, int flags = 0, TermCriteria criteria = TermCriteria(TermCriteria::COUNT + TermCriteria::EPS, 100, DBL_EPSILON))
+    //
+
+    /**
+     * Performs camera calibaration
+     *
+     *     @param objectPoints vector of vectors of calibration pattern points in the calibration pattern
+     *     coordinate space.
+     *     @param imagePoints vector of vectors of the projections of calibration pattern points.
+     *     imagePoints.size() and objectPoints.size() and imagePoints[i].size() must be equal to
+     *     objectPoints[i].size() for each i.
+     *     @param image_size Size of the image used only to initialize the intrinsic camera matrix.
+     *     @param K Output 3x3 floating-point camera matrix
+     *     \(A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}\) . If
+     *     fisheye::CALIB_USE_INTRINSIC_GUESS/ is specified, some or all of fx, fy, cx, cy must be
+     *     initialized before calling the function.
+     *     @param D Output vector of distortion coefficients \((k_1, k_2, k_3, k_4)\).
+     *     @param rvecs Output vector of rotation vectors (see Rodrigues ) estimated for each pattern view.
+     *     That is, each k-th rotation vector together with the corresponding k-th translation vector (see
+     *     the next output parameter description) brings the calibration pattern from the model coordinate
+     *     space (in which object points are specified) to the world coordinate space, that is, a real
+     *     position of the calibration pattern in the k-th pattern view (k=0.. *M* -1).
+     *     @param tvecs Output vector of translation vectors estimated for each pattern view.
+     *     @param flags Different flags that may be zero or a combination of the following values:
+     * <ul>
+     *   <li>
+     *        <b>fisheye::CALIB_USE_INTRINSIC_GUESS</b> cameraMatrix contains valid initial values of
+     *     fx, fy, cx, cy that are optimized further. Otherwise, (cx, cy) is initially set to the image
+     *     center ( imageSize is used), and focal distances are computed in a least-squares fashion.
+     *   </li>
+     *   <li>
+     *        <b>fisheye::CALIB_RECOMPUTE_EXTRINSIC</b> Extrinsic will be recomputed after each iteration
+     *     of intrinsic optimization.
+     *   </li>
+     *   <li>
+     *        <b>fisheye::CALIB_CHECK_COND</b> The functions will check validity of condition number.
+     *   </li>
+     *   <li>
+     *        <b>fisheye::CALIB_FIX_SKEW</b> Skew coefficient (alpha) is set to zero and stay zero.
+     *   </li>
+     *   <li>
+     *        <b>fisheye::CALIB_FIX_K1..fisheye::CALIB_FIX_K4</b> Selected distortion coefficients
+     *     are set to zeros and stay zero.
+     *   </li>
+     *   <li>
+     *        <b>fisheye::CALIB_FIX_PRINCIPAL_POINT</b> The principal point is not changed during the global
+     * optimization. It stays at the center or at a different location specified when CALIB_USE_INTRINSIC_GUESS is set too.
+     *     @param criteria Termination criteria for the iterative optimization algorithm.
+     *   </li>
+     * </ul>
+     * @return automatically generated
+     */
+    public static double fisheye_calibrate(List<Mat> objectPoints, List<Mat> imagePoints, Size image_size, Mat K, Mat D, List<Mat> rvecs, List<Mat> tvecs, int flags, TermCriteria criteria) {
+        Mat objectPoints_mat = Converters.vector_Mat_to_Mat(objectPoints);
+        Mat imagePoints_mat = Converters.vector_Mat_to_Mat(imagePoints);
+        Mat rvecs_mat = new Mat();
+        Mat tvecs_mat = new Mat();
+        double retVal = fisheye_calibrate_0(objectPoints_mat.nativeObj, imagePoints_mat.nativeObj, image_size.width, image_size.height, K.nativeObj, D.nativeObj, rvecs_mat.nativeObj, tvecs_mat.nativeObj, flags, criteria.type, criteria.maxCount, criteria.epsilon);
+        Converters.Mat_to_vector_Mat(rvecs_mat, rvecs);
+        rvecs_mat.release();
+        Converters.Mat_to_vector_Mat(tvecs_mat, tvecs);
+        tvecs_mat.release();
+        return retVal;
+    }
+
+    /**
+     * Performs camera calibaration
+     *
+     *     @param objectPoints vector of vectors of calibration pattern points in the calibration pattern
+     *     coordinate space.
+     *     @param imagePoints vector of vectors of the projections of calibration pattern points.
+     *     imagePoints.size() and objectPoints.size() and imagePoints[i].size() must be equal to
+     *     objectPoints[i].size() for each i.
+     *     @param image_size Size of the image used only to initialize the intrinsic camera matrix.
+     *     @param K Output 3x3 floating-point camera matrix
+     *     \(A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}\) . If
+     *     fisheye::CALIB_USE_INTRINSIC_GUESS/ is specified, some or all of fx, fy, cx, cy must be
+     *     initialized before calling the function.
+     *     @param D Output vector of distortion coefficients \((k_1, k_2, k_3, k_4)\).
+     *     @param rvecs Output vector of rotation vectors (see Rodrigues ) estimated for each pattern view.
+     *     That is, each k-th rotation vector together with the corresponding k-th translation vector (see
+     *     the next output parameter description) brings the calibration pattern from the model coordinate
+     *     space (in which object points are specified) to the world coordinate space, that is, a real
+     *     position of the calibration pattern in the k-th pattern view (k=0.. *M* -1).
+     *     @param tvecs Output vector of translation vectors estimated for each pattern view.
+     *     @param flags Different flags that may be zero or a combination of the following values:
+     * <ul>
+     *   <li>
+     *        <b>fisheye::CALIB_USE_INTRINSIC_GUESS</b> cameraMatrix contains valid initial values of
+     *     fx, fy, cx, cy that are optimized further. Otherwise, (cx, cy) is initially set to the image
+     *     center ( imageSize is used), and focal distances are computed in a least-squares fashion.
+     *   </li>
+     *   <li>
+     *        <b>fisheye::CALIB_RECOMPUTE_EXTRINSIC</b> Extrinsic will be recomputed after each iteration
+     *     of intrinsic optimization.
+     *   </li>
+     *   <li>
+     *        <b>fisheye::CALIB_CHECK_COND</b> The functions will check validity of condition number.
+     *   </li>
+     *   <li>
+     *        <b>fisheye::CALIB_FIX_SKEW</b> Skew coefficient (alpha) is set to zero and stay zero.
+     *   </li>
+     *   <li>
+     *        <b>fisheye::CALIB_FIX_K1..fisheye::CALIB_FIX_K4</b> Selected distortion coefficients
+     *     are set to zeros and stay zero.
+     *   </li>
+     *   <li>
+     *        <b>fisheye::CALIB_FIX_PRINCIPAL_POINT</b> The principal point is not changed during the global
+     * optimization. It stays at the center or at a different location specified when CALIB_USE_INTRINSIC_GUESS is set too.
+     *   </li>
+     * </ul>
+     * @return automatically generated
+     */
+    public static double fisheye_calibrate(List<Mat> objectPoints, List<Mat> imagePoints, Size image_size, Mat K, Mat D, List<Mat> rvecs, List<Mat> tvecs, int flags) {
+        Mat objectPoints_mat = Converters.vector_Mat_to_Mat(objectPoints);
+        Mat imagePoints_mat = Converters.vector_Mat_to_Mat(imagePoints);
+        Mat rvecs_mat = new Mat();
+        Mat tvecs_mat = new Mat();
+        double retVal = fisheye_calibrate_1(objectPoints_mat.nativeObj, imagePoints_mat.nativeObj, image_size.width, image_size.height, K.nativeObj, D.nativeObj, rvecs_mat.nativeObj, tvecs_mat.nativeObj, flags);
+        Converters.Mat_to_vector_Mat(rvecs_mat, rvecs);
+        rvecs_mat.release();
+        Converters.Mat_to_vector_Mat(tvecs_mat, tvecs);
+        tvecs_mat.release();
+        return retVal;
+    }
+
+    /**
+     * Performs camera calibaration
+     *
+     *     @param objectPoints vector of vectors of calibration pattern points in the calibration pattern
+     *     coordinate space.
+     *     @param imagePoints vector of vectors of the projections of calibration pattern points.
+     *     imagePoints.size() and objectPoints.size() and imagePoints[i].size() must be equal to
+     *     objectPoints[i].size() for each i.
+     *     @param image_size Size of the image used only to initialize the intrinsic camera matrix.
+     *     @param K Output 3x3 floating-point camera matrix
+     *     \(A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}\) . If
+     *     fisheye::CALIB_USE_INTRINSIC_GUESS/ is specified, some or all of fx, fy, cx, cy must be
+     *     initialized before calling the function.
+     *     @param D Output vector of distortion coefficients \((k_1, k_2, k_3, k_4)\).
+     *     @param rvecs Output vector of rotation vectors (see Rodrigues ) estimated for each pattern view.
+     *     That is, each k-th rotation vector together with the corresponding k-th translation vector (see
+     *     the next output parameter description) brings the calibration pattern from the model coordinate
+     *     space (in which object points are specified) to the world coordinate space, that is, a real
+     *     position of the calibration pattern in the k-th pattern view (k=0.. *M* -1).
+     *     @param tvecs Output vector of translation vectors estimated for each pattern view.
+     * <ul>
+     *   <li>
+     *        <b>fisheye::CALIB_USE_INTRINSIC_GUESS</b> cameraMatrix contains valid initial values of
+     *     fx, fy, cx, cy that are optimized further. Otherwise, (cx, cy) is initially set to the image
+     *     center ( imageSize is used), and focal distances are computed in a least-squares fashion.
+     *   </li>
+     *   <li>
+     *        <b>fisheye::CALIB_RECOMPUTE_EXTRINSIC</b> Extrinsic will be recomputed after each iteration
+     *     of intrinsic optimization.
+     *   </li>
+     *   <li>
+     *        <b>fisheye::CALIB_CHECK_COND</b> The functions will check validity of condition number.
+     *   </li>
+     *   <li>
+     *        <b>fisheye::CALIB_FIX_SKEW</b> Skew coefficient (alpha) is set to zero and stay zero.
+     *   </li>
+     *   <li>
+     *        <b>fisheye::CALIB_FIX_K1..fisheye::CALIB_FIX_K4</b> Selected distortion coefficients
+     *     are set to zeros and stay zero.
+     *   </li>
+     *   <li>
+     *        <b>fisheye::CALIB_FIX_PRINCIPAL_POINT</b> The principal point is not changed during the global
+     * optimization. It stays at the center or at a different location specified when CALIB_USE_INTRINSIC_GUESS is set too.
+     *   </li>
+     * </ul>
+     * @return automatically generated
+     */
+    public static double fisheye_calibrate(List<Mat> objectPoints, List<Mat> imagePoints, Size image_size, Mat K, Mat D, List<Mat> rvecs, List<Mat> tvecs) {
+        Mat objectPoints_mat = Converters.vector_Mat_to_Mat(objectPoints);
+        Mat imagePoints_mat = Converters.vector_Mat_to_Mat(imagePoints);
+        Mat rvecs_mat = new Mat();
+        Mat tvecs_mat = new Mat();
+        double retVal = fisheye_calibrate_2(objectPoints_mat.nativeObj, imagePoints_mat.nativeObj, image_size.width, image_size.height, K.nativeObj, D.nativeObj, rvecs_mat.nativeObj, tvecs_mat.nativeObj);
+        Converters.Mat_to_vector_Mat(rvecs_mat, rvecs);
+        rvecs_mat.release();
+        Converters.Mat_to_vector_Mat(tvecs_mat, tvecs);
+        tvecs_mat.release();
+        return retVal;
+    }
+
+
+    //
+    // C++:  double cv::fisheye::stereoCalibrate(vector_Mat objectPoints, vector_Mat imagePoints1, vector_Mat imagePoints2, Mat& K1, Mat& D1, Mat& K2, Mat& D2, Size imageSize, Mat& R, Mat& T, int flags = fisheye::CALIB_FIX_INTRINSIC, TermCriteria criteria = TermCriteria(TermCriteria::COUNT + TermCriteria::EPS, 100, DBL_EPSILON))
+    //
+
+    /**
+     * Performs stereo calibration
+     *
+     *     @param objectPoints Vector of vectors of the calibration pattern points.
+     *     @param imagePoints1 Vector of vectors of the projections of the calibration pattern points,
+     *     observed by the first camera.
+     *     @param imagePoints2 Vector of vectors of the projections of the calibration pattern points,
+     *     observed by the second camera.
+     *     @param K1 Input/output first camera matrix:
+     *     \(\vecthreethree{f_x^{(j)}}{0}{c_x^{(j)}}{0}{f_y^{(j)}}{c_y^{(j)}}{0}{0}{1}\) , \(j = 0,\, 1\) . If
+     *     any of fisheye::CALIB_USE_INTRINSIC_GUESS , fisheye::CALIB_FIX_INTRINSIC are specified,
+     *     some or all of the matrix components must be initialized.
+     *     @param D1 Input/output vector of distortion coefficients \((k_1, k_2, k_3, k_4)\) of 4 elements.
+     *     @param K2 Input/output second camera matrix. The parameter is similar to K1 .
+     *     @param D2 Input/output lens distortion coefficients for the second camera. The parameter is
+     *     similar to D1 .
+     *     @param imageSize Size of the image used only to initialize intrinsic camera matrix.
+     *     @param R Output rotation matrix between the 1st and the 2nd camera coordinate systems.
+     *     @param T Output translation vector between the coordinate systems of the cameras.
+     *     @param flags Different flags that may be zero or a combination of the following values:
+     * <ul>
+     *   <li>
+     *        <b>fisheye::CALIB_FIX_INTRINSIC</b> Fix K1, K2? and D1, D2? so that only R, T matrices
+     *     are estimated.
+     *   </li>
+     *   <li>
+     *        <b>fisheye::CALIB_USE_INTRINSIC_GUESS</b> K1, K2 contains valid initial values of
+     *     fx, fy, cx, cy that are optimized further. Otherwise, (cx, cy) is initially set to the image
+     *     center (imageSize is used), and focal distances are computed in a least-squares fashion.
+     *   </li>
+     *   <li>
+     *        <b>fisheye::CALIB_RECOMPUTE_EXTRINSIC</b> Extrinsic will be recomputed after each iteration
+     *     of intrinsic optimization.
+     *   </li>
+     *   <li>
+     *        <b>fisheye::CALIB_CHECK_COND</b> The functions will check validity of condition number.
+     *   </li>
+     *   <li>
+     *        <b>fisheye::CALIB_FIX_SKEW</b> Skew coefficient (alpha) is set to zero and stay zero.
+     *   </li>
+     *   <li>
+     *        <b>fisheye::CALIB_FIX_K1..4</b> Selected distortion coefficients are set to zeros and stay
+     *     zero.
+     *     @param criteria Termination criteria for the iterative optimization algorithm.
+     *   </li>
+     * </ul>
+     * @return automatically generated
+     */
+    public static double fisheye_stereoCalibrate(List<Mat> objectPoints, List<Mat> imagePoints1, List<Mat> imagePoints2, Mat K1, Mat D1, Mat K2, Mat D2, Size imageSize, Mat R, Mat T, int flags, TermCriteria criteria) {
+        Mat objectPoints_mat = Converters.vector_Mat_to_Mat(objectPoints);
+        Mat imagePoints1_mat = Converters.vector_Mat_to_Mat(imagePoints1);
+        Mat imagePoints2_mat = Converters.vector_Mat_to_Mat(imagePoints2);
+        return fisheye_stereoCalibrate_0(objectPoints_mat.nativeObj, imagePoints1_mat.nativeObj, imagePoints2_mat.nativeObj, K1.nativeObj, D1.nativeObj, K2.nativeObj, D2.nativeObj, imageSize.width, imageSize.height, R.nativeObj, T.nativeObj, flags, criteria.type, criteria.maxCount, criteria.epsilon);
+    }
+
+    /**
+     * Performs stereo calibration
+     *
+     *     @param objectPoints Vector of vectors of the calibration pattern points.
+     *     @param imagePoints1 Vector of vectors of the projections of the calibration pattern points,
+     *     observed by the first camera.
+     *     @param imagePoints2 Vector of vectors of the projections of the calibration pattern points,
+     *     observed by the second camera.
+     *     @param K1 Input/output first camera matrix:
+     *     \(\vecthreethree{f_x^{(j)}}{0}{c_x^{(j)}}{0}{f_y^{(j)}}{c_y^{(j)}}{0}{0}{1}\) , \(j = 0,\, 1\) . If
+     *     any of fisheye::CALIB_USE_INTRINSIC_GUESS , fisheye::CALIB_FIX_INTRINSIC are specified,
+     *     some or all of the matrix components must be initialized.
+     *     @param D1 Input/output vector of distortion coefficients \((k_1, k_2, k_3, k_4)\) of 4 elements.
+     *     @param K2 Input/output second camera matrix. The parameter is similar to K1 .
+     *     @param D2 Input/output lens distortion coefficients for the second camera. The parameter is
+     *     similar to D1 .
+     *     @param imageSize Size of the image used only to initialize intrinsic camera matrix.
+     *     @param R Output rotation matrix between the 1st and the 2nd camera coordinate systems.
+     *     @param T Output translation vector between the coordinate systems of the cameras.
+     *     @param flags Different flags that may be zero or a combination of the following values:
+     * <ul>
+     *   <li>
+     *        <b>fisheye::CALIB_FIX_INTRINSIC</b> Fix K1, K2? and D1, D2? so that only R, T matrices
+     *     are estimated.
+     *   </li>
+     *   <li>
+     *        <b>fisheye::CALIB_USE_INTRINSIC_GUESS</b> K1, K2 contains valid initial values of
+     *     fx, fy, cx, cy that are optimized further. Otherwise, (cx, cy) is initially set to the image
+     *     center (imageSize is used), and focal distances are computed in a least-squares fashion.
+     *   </li>
+     *   <li>
+     *        <b>fisheye::CALIB_RECOMPUTE_EXTRINSIC</b> Extrinsic will be recomputed after each iteration
+     *     of intrinsic optimization.
+     *   </li>
+     *   <li>
+     *        <b>fisheye::CALIB_CHECK_COND</b> The functions will check validity of condition number.
+     *   </li>
+     *   <li>
+     *        <b>fisheye::CALIB_FIX_SKEW</b> Skew coefficient (alpha) is set to zero and stay zero.
+     *   </li>
+     *   <li>
+     *        <b>fisheye::CALIB_FIX_K1..4</b> Selected distortion coefficients are set to zeros and stay
+     *     zero.
+     *   </li>
+     * </ul>
+     * @return automatically generated
+     */
+    public static double fisheye_stereoCalibrate(List<Mat> objectPoints, List<Mat> imagePoints1, List<Mat> imagePoints2, Mat K1, Mat D1, Mat K2, Mat D2, Size imageSize, Mat R, Mat T, int flags) {
+        Mat objectPoints_mat = Converters.vector_Mat_to_Mat(objectPoints);
+        Mat imagePoints1_mat = Converters.vector_Mat_to_Mat(imagePoints1);
+        Mat imagePoints2_mat = Converters.vector_Mat_to_Mat(imagePoints2);
+        return fisheye_stereoCalibrate_1(objectPoints_mat.nativeObj, imagePoints1_mat.nativeObj, imagePoints2_mat.nativeObj, K1.nativeObj, D1.nativeObj, K2.nativeObj, D2.nativeObj, imageSize.width, imageSize.height, R.nativeObj, T.nativeObj, flags);
+    }
+
+    /**
+     * Performs stereo calibration
+     *
+     *     @param objectPoints Vector of vectors of the calibration pattern points.
+     *     @param imagePoints1 Vector of vectors of the projections of the calibration pattern points,
+     *     observed by the first camera.
+     *     @param imagePoints2 Vector of vectors of the projections of the calibration pattern points,
+     *     observed by the second camera.
+     *     @param K1 Input/output first camera matrix:
+     *     \(\vecthreethree{f_x^{(j)}}{0}{c_x^{(j)}}{0}{f_y^{(j)}}{c_y^{(j)}}{0}{0}{1}\) , \(j = 0,\, 1\) . If
+     *     any of fisheye::CALIB_USE_INTRINSIC_GUESS , fisheye::CALIB_FIX_INTRINSIC are specified,
+     *     some or all of the matrix components must be initialized.
+     *     @param D1 Input/output vector of distortion coefficients \((k_1, k_2, k_3, k_4)\) of 4 elements.
+     *     @param K2 Input/output second camera matrix. The parameter is similar to K1 .
+     *     @param D2 Input/output lens distortion coefficients for the second camera. The parameter is
+     *     similar to D1 .
+     *     @param imageSize Size of the image used only to initialize intrinsic camera matrix.
+     *     @param R Output rotation matrix between the 1st and the 2nd camera coordinate systems.
+     *     @param T Output translation vector between the coordinate systems of the cameras.
+     * <ul>
+     *   <li>
+     *        <b>fisheye::CALIB_FIX_INTRINSIC</b> Fix K1, K2? and D1, D2? so that only R, T matrices
+     *     are estimated.
+     *   </li>
+     *   <li>
+     *        <b>fisheye::CALIB_USE_INTRINSIC_GUESS</b> K1, K2 contains valid initial values of
+     *     fx, fy, cx, cy that are optimized further. Otherwise, (cx, cy) is initially set to the image
+     *     center (imageSize is used), and focal distances are computed in a least-squares fashion.
+     *   </li>
+     *   <li>
+     *        <b>fisheye::CALIB_RECOMPUTE_EXTRINSIC</b> Extrinsic will be recomputed after each iteration
+     *     of intrinsic optimization.
+     *   </li>
+     *   <li>
+     *        <b>fisheye::CALIB_CHECK_COND</b> The functions will check validity of condition number.
+     *   </li>
+     *   <li>
+     *        <b>fisheye::CALIB_FIX_SKEW</b> Skew coefficient (alpha) is set to zero and stay zero.
+     *   </li>
+     *   <li>
+     *        <b>fisheye::CALIB_FIX_K1..4</b> Selected distortion coefficients are set to zeros and stay
+     *     zero.
+     *   </li>
+     * </ul>
+     * @return automatically generated
+     */
+    public static double fisheye_stereoCalibrate(List<Mat> objectPoints, List<Mat> imagePoints1, List<Mat> imagePoints2, Mat K1, Mat D1, Mat K2, Mat D2, Size imageSize, Mat R, Mat T) {
+        Mat objectPoints_mat = Converters.vector_Mat_to_Mat(objectPoints);
+        Mat imagePoints1_mat = Converters.vector_Mat_to_Mat(imagePoints1);
+        Mat imagePoints2_mat = Converters.vector_Mat_to_Mat(imagePoints2);
+        return fisheye_stereoCalibrate_2(objectPoints_mat.nativeObj, imagePoints1_mat.nativeObj, imagePoints2_mat.nativeObj, K1.nativeObj, D1.nativeObj, K2.nativeObj, D2.nativeObj, imageSize.width, imageSize.height, R.nativeObj, T.nativeObj);
+    }
+
+
+    //
+    // C++:  float cv::rectify3Collinear(Mat cameraMatrix1, Mat distCoeffs1, Mat cameraMatrix2, Mat distCoeffs2, Mat cameraMatrix3, Mat distCoeffs3, vector_Mat imgpt1, vector_Mat imgpt3, Size imageSize, Mat R12, Mat T12, Mat R13, Mat T13, Mat& R1, Mat& R2, Mat& R3, Mat& P1, Mat& P2, Mat& P3, Mat& Q, double alpha, Size newImgSize, Rect* roi1, Rect* roi2, int flags)
+    //
+
+    public static float rectify3Collinear(Mat cameraMatrix1, Mat distCoeffs1, Mat cameraMatrix2, Mat distCoeffs2, Mat cameraMatrix3, Mat distCoeffs3, List<Mat> imgpt1, List<Mat> imgpt3, Size imageSize, Mat R12, Mat T12, Mat R13, Mat T13, Mat R1, Mat R2, Mat R3, Mat P1, Mat P2, Mat P3, Mat Q, double alpha, Size newImgSize, Rect roi1, Rect roi2, int flags) {
+        Mat imgpt1_mat = Converters.vector_Mat_to_Mat(imgpt1);
+        Mat imgpt3_mat = Converters.vector_Mat_to_Mat(imgpt3);
+        double[] roi1_out = new double[4];
+        double[] roi2_out = new double[4];
+        float retVal = rectify3Collinear_0(cameraMatrix1.nativeObj, distCoeffs1.nativeObj, cameraMatrix2.nativeObj, distCoeffs2.nativeObj, cameraMatrix3.nativeObj, distCoeffs3.nativeObj, imgpt1_mat.nativeObj, imgpt3_mat.nativeObj, imageSize.width, imageSize.height, R12.nativeObj, T12.nativeObj, R13.nativeObj, T13.nativeObj, R1.nativeObj, R2.nativeObj, R3.nativeObj, P1.nativeObj, P2.nativeObj, P3.nativeObj, Q.nativeObj, alpha, newImgSize.width, newImgSize.height, roi1_out, roi2_out, flags);
+        if(roi1!=null){ roi1.x = (int)roi1_out[0]; roi1.y = (int)roi1_out[1]; roi1.width = (int)roi1_out[2]; roi1.height = (int)roi1_out[3]; } 
+        if(roi2!=null){ roi2.x = (int)roi2_out[0]; roi2.y = (int)roi2_out[1]; roi2.width = (int)roi2_out[2]; roi2.height = (int)roi2_out[3]; } 
+        return retVal;
+    }
+
+
+    //
+    // C++:  int cv::decomposeHomographyMat(Mat H, Mat K, vector_Mat& rotations, vector_Mat& translations, vector_Mat& normals)
+    //
+
+    /**
+     * Decompose a homography matrix to rotation(s), translation(s) and plane normal(s).
+     *
+     * @param H The input homography matrix between two images.
+     * @param K The input intrinsic camera calibration matrix.
+     * @param rotations Array of rotation matrices.
+     * @param translations Array of translation matrices.
+     * @param normals Array of plane normal matrices.
+     *
+     * This function extracts relative camera motion between two views of a planar object and returns up to
+     * four mathematical solution tuples of rotation, translation, and plane normal. The decomposition of
+     * the homography matrix H is described in detail in CITE: Malis.
+     *
+     * If the homography H, induced by the plane, gives the constraint
+     * \(s_i \vecthree{x'_i}{y'_i}{1} \sim H \vecthree{x_i}{y_i}{1}\) on the source image points
+     * \(p_i\) and the destination image points \(p'_i\), then the tuple of rotations[k] and
+     * translations[k] is a change of basis from the source camera's coordinate system to the destination
+     * camera's coordinate system. However, by decomposing H, one can only get the translation normalized
+     * by the (typically unknown) depth of the scene, i.e. its direction but with normalized length.
+     *
+     * If point correspondences are available, at least two solutions may further be invalidated, by
+     * applying positive depth constraint, i.e. all points must be in front of the camera.
+     * @return automatically generated
+     */
+    public static int decomposeHomographyMat(Mat H, Mat K, List<Mat> rotations, List<Mat> translations, List<Mat> normals) {
+        Mat rotations_mat = new Mat();
+        Mat translations_mat = new Mat();
+        Mat normals_mat = new Mat();
+        int retVal = decomposeHomographyMat_0(H.nativeObj, K.nativeObj, rotations_mat.nativeObj, translations_mat.nativeObj, normals_mat.nativeObj);
+        Converters.Mat_to_vector_Mat(rotations_mat, rotations);
+        rotations_mat.release();
+        Converters.Mat_to_vector_Mat(translations_mat, translations);
+        translations_mat.release();
+        Converters.Mat_to_vector_Mat(normals_mat, normals);
+        normals_mat.release();
+        return retVal;
+    }
+
+
+    //
+    // C++:  int cv::estimateAffine3D(Mat src, Mat dst, Mat& out, Mat& inliers, double ransacThreshold = 3, double confidence = 0.99)
+    //
+
+    /**
+     * Computes an optimal affine transformation between two 3D point sets.
+     *
+     * It computes
+     * \(
+     * \begin{bmatrix}
+     * x\\
+     * y\\
+     * z\\
+     * \end{bmatrix}
+     * =
+     * \begin{bmatrix}
+     * a_{11} &amp; a_{12} &amp; a_{13}\\
+     * a_{21} &amp; a_{22} &amp; a_{23}\\
+     * a_{31} &amp; a_{32} &amp; a_{33}\\
+     * \end{bmatrix}
+     * \begin{bmatrix}
+     * X\\
+     * Y\\
+     * Z\\
+     * \end{bmatrix}
+     * +
+     * \begin{bmatrix}
+     * b_1\\
+     * b_2\\
+     * b_3\\
+     * \end{bmatrix}
+     * \)
+     *
+     * @param src First input 3D point set containing \((X,Y,Z)\).
+     * @param dst Second input 3D point set containing \((x,y,z)\).
+     * @param out Output 3D affine transformation matrix \(3 \times 4\) of the form
+     * \(
+     * \begin{bmatrix}
+     * a_{11} &amp; a_{12} &amp; a_{13} &amp; b_1\\
+     * a_{21} &amp; a_{22} &amp; a_{23} &amp; b_2\\
+     * a_{31} &amp; a_{32} &amp; a_{33} &amp; b_3\\
+     * \end{bmatrix}
+     * \)
+     * @param inliers Output vector indicating which points are inliers (1-inlier, 0-outlier).
+     * @param ransacThreshold Maximum reprojection error in the RANSAC algorithm to consider a point as
+     * an inlier.
+     * @param confidence Confidence level, between 0 and 1, for the estimated transformation. Anything
+     * between 0.95 and 0.99 is usually good enough. Values too close to 1 can slow down the estimation
+     * significantly. Values lower than 0.8-0.9 can result in an incorrectly estimated transformation.
+     *
+     * The function estimates an optimal 3D affine transformation between two 3D point sets using the
+     * RANSAC algorithm.
+     * @return automatically generated
+     */
+    public static int estimateAffine3D(Mat src, Mat dst, Mat out, Mat inliers, double ransacThreshold, double confidence) {
+        return estimateAffine3D_0(src.nativeObj, dst.nativeObj, out.nativeObj, inliers.nativeObj, ransacThreshold, confidence);
+    }
+
+    /**
+     * Computes an optimal affine transformation between two 3D point sets.
+     *
+     * It computes
+     * \(
+     * \begin{bmatrix}
+     * x\\
+     * y\\
+     * z\\
+     * \end{bmatrix}
+     * =
+     * \begin{bmatrix}
+     * a_{11} &amp; a_{12} &amp; a_{13}\\
+     * a_{21} &amp; a_{22} &amp; a_{23}\\
+     * a_{31} &amp; a_{32} &amp; a_{33}\\
+     * \end{bmatrix}
+     * \begin{bmatrix}
+     * X\\
+     * Y\\
+     * Z\\
+     * \end{bmatrix}
+     * +
+     * \begin{bmatrix}
+     * b_1\\
+     * b_2\\
+     * b_3\\
+     * \end{bmatrix}
+     * \)
+     *
+     * @param src First input 3D point set containing \((X,Y,Z)\).
+     * @param dst Second input 3D point set containing \((x,y,z)\).
+     * @param out Output 3D affine transformation matrix \(3 \times 4\) of the form
+     * \(
+     * \begin{bmatrix}
+     * a_{11} &amp; a_{12} &amp; a_{13} &amp; b_1\\
+     * a_{21} &amp; a_{22} &amp; a_{23} &amp; b_2\\
+     * a_{31} &amp; a_{32} &amp; a_{33} &amp; b_3\\
+     * \end{bmatrix}
+     * \)
+     * @param inliers Output vector indicating which points are inliers (1-inlier, 0-outlier).
+     * @param ransacThreshold Maximum reprojection error in the RANSAC algorithm to consider a point as
+     * an inlier.
+     * between 0.95 and 0.99 is usually good enough. Values too close to 1 can slow down the estimation
+     * significantly. Values lower than 0.8-0.9 can result in an incorrectly estimated transformation.
+     *
+     * The function estimates an optimal 3D affine transformation between two 3D point sets using the
+     * RANSAC algorithm.
+     * @return automatically generated
+     */
+    public static int estimateAffine3D(Mat src, Mat dst, Mat out, Mat inliers, double ransacThreshold) {
+        return estimateAffine3D_1(src.nativeObj, dst.nativeObj, out.nativeObj, inliers.nativeObj, ransacThreshold);
+    }
+
+    /**
+     * Computes an optimal affine transformation between two 3D point sets.
+     *
+     * It computes
+     * \(
+     * \begin{bmatrix}
+     * x\\
+     * y\\
+     * z\\
+     * \end{bmatrix}
+     * =
+     * \begin{bmatrix}
+     * a_{11} &amp; a_{12} &amp; a_{13}\\
+     * a_{21} &amp; a_{22} &amp; a_{23}\\
+     * a_{31} &amp; a_{32} &amp; a_{33}\\
+     * \end{bmatrix}
+     * \begin{bmatrix}
+     * X\\
+     * Y\\
+     * Z\\
+     * \end{bmatrix}
+     * +
+     * \begin{bmatrix}
+     * b_1\\
+     * b_2\\
+     * b_3\\
+     * \end{bmatrix}
+     * \)
+     *
+     * @param src First input 3D point set containing \((X,Y,Z)\).
+     * @param dst Second input 3D point set containing \((x,y,z)\).
+     * @param out Output 3D affine transformation matrix \(3 \times 4\) of the form
+     * \(
+     * \begin{bmatrix}
+     * a_{11} &amp; a_{12} &amp; a_{13} &amp; b_1\\
+     * a_{21} &amp; a_{22} &amp; a_{23} &amp; b_2\\
+     * a_{31} &amp; a_{32} &amp; a_{33} &amp; b_3\\
+     * \end{bmatrix}
+     * \)
+     * @param inliers Output vector indicating which points are inliers (1-inlier, 0-outlier).
+     * an inlier.
+     * between 0.95 and 0.99 is usually good enough. Values too close to 1 can slow down the estimation
+     * significantly. Values lower than 0.8-0.9 can result in an incorrectly estimated transformation.
+     *
+     * The function estimates an optimal 3D affine transformation between two 3D point sets using the
+     * RANSAC algorithm.
+     * @return automatically generated
+     */
+    public static int estimateAffine3D(Mat src, Mat dst, Mat out, Mat inliers) {
+        return estimateAffine3D_2(src.nativeObj, dst.nativeObj, out.nativeObj, inliers.nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::recoverPose(Mat E, Mat points1, Mat points2, Mat& R, Mat& t, double focal = 1.0, Point2d pp = Point2d(0, 0), Mat& mask = Mat())
+    //
+
+    /**
+     *
+     * @param E The input essential matrix.
+     * @param points1 Array of N 2D points from the first image. The point coordinates should be
+     * floating-point (single or double precision).
+     * @param points2 Array of the second image points of the same size and format as points1 .
+     * @param R Output rotation matrix. Together with the translation vector, this matrix makes up a tuple
+     * that performs a change of basis from the first camera's coordinate system to the second camera's
+     * coordinate system. Note that, in general, t can not be used for this tuple, see the parameter
+     * description below.
+     * @param t Output translation vector. This vector is obtained by REF: decomposeEssentialMat and
+     * therefore is only known up to scale, i.e. t is the direction of the translation vector and has unit
+     * length.
+     * @param focal Focal length of the camera. Note that this function assumes that points1 and points2
+     * are feature points from cameras with same focal length and principal point.
+     * @param pp principal point of the camera.
+     * @param mask Input/output mask for inliers in points1 and points2. If it is not empty, then it marks
+     * inliers in points1 and points2 for then given essential matrix E. Only these inliers will be used to
+     * recover pose. In the output mask only inliers which pass the cheirality check.
+     *
+     * This function differs from the one above that it computes camera matrix from focal length and
+     * principal point:
+     *
+     * \(A =
+     * \begin{bmatrix}
+     * f &amp; 0 &amp; x_{pp}  \\
+     * 0 &amp; f &amp; y_{pp}  \\
+     * 0 &amp; 0 &amp; 1
+     * \end{bmatrix}\)
+     * @return automatically generated
+     */
+    public static int recoverPose(Mat E, Mat points1, Mat points2, Mat R, Mat t, double focal, Point pp, Mat mask) {
+        return recoverPose_0(E.nativeObj, points1.nativeObj, points2.nativeObj, R.nativeObj, t.nativeObj, focal, pp.x, pp.y, mask.nativeObj);
+    }
+
+    /**
+     *
+     * @param E The input essential matrix.
+     * @param points1 Array of N 2D points from the first image. The point coordinates should be
+     * floating-point (single or double precision).
+     * @param points2 Array of the second image points of the same size and format as points1 .
+     * @param R Output rotation matrix. Together with the translation vector, this matrix makes up a tuple
+     * that performs a change of basis from the first camera's coordinate system to the second camera's
+     * coordinate system. Note that, in general, t can not be used for this tuple, see the parameter
+     * description below.
+     * @param t Output translation vector. This vector is obtained by REF: decomposeEssentialMat and
+     * therefore is only known up to scale, i.e. t is the direction of the translation vector and has unit
+     * length.
+     * @param focal Focal length of the camera. Note that this function assumes that points1 and points2
+     * are feature points from cameras with same focal length and principal point.
+     * @param pp principal point of the camera.
+     * inliers in points1 and points2 for then given essential matrix E. Only these inliers will be used to
+     * recover pose. In the output mask only inliers which pass the cheirality check.
+     *
+     * This function differs from the one above that it computes camera matrix from focal length and
+     * principal point:
+     *
+     * \(A =
+     * \begin{bmatrix}
+     * f &amp; 0 &amp; x_{pp}  \\
+     * 0 &amp; f &amp; y_{pp}  \\
+     * 0 &amp; 0 &amp; 1
+     * \end{bmatrix}\)
+     * @return automatically generated
+     */
+    public static int recoverPose(Mat E, Mat points1, Mat points2, Mat R, Mat t, double focal, Point pp) {
+        return recoverPose_1(E.nativeObj, points1.nativeObj, points2.nativeObj, R.nativeObj, t.nativeObj, focal, pp.x, pp.y);
+    }
+
+    /**
+     *
+     * @param E The input essential matrix.
+     * @param points1 Array of N 2D points from the first image. The point coordinates should be
+     * floating-point (single or double precision).
+     * @param points2 Array of the second image points of the same size and format as points1 .
+     * @param R Output rotation matrix. Together with the translation vector, this matrix makes up a tuple
+     * that performs a change of basis from the first camera's coordinate system to the second camera's
+     * coordinate system. Note that, in general, t can not be used for this tuple, see the parameter
+     * description below.
+     * @param t Output translation vector. This vector is obtained by REF: decomposeEssentialMat and
+     * therefore is only known up to scale, i.e. t is the direction of the translation vector and has unit
+     * length.
+     * @param focal Focal length of the camera. Note that this function assumes that points1 and points2
+     * are feature points from cameras with same focal length and principal point.
+     * inliers in points1 and points2 for then given essential matrix E. Only these inliers will be used to
+     * recover pose. In the output mask only inliers which pass the cheirality check.
+     *
+     * This function differs from the one above that it computes camera matrix from focal length and
+     * principal point:
+     *
+     * \(A =
+     * \begin{bmatrix}
+     * f &amp; 0 &amp; x_{pp}  \\
+     * 0 &amp; f &amp; y_{pp}  \\
+     * 0 &amp; 0 &amp; 1
+     * \end{bmatrix}\)
+     * @return automatically generated
+     */
+    public static int recoverPose(Mat E, Mat points1, Mat points2, Mat R, Mat t, double focal) {
+        return recoverPose_2(E.nativeObj, points1.nativeObj, points2.nativeObj, R.nativeObj, t.nativeObj, focal);
+    }
+
+    /**
+     *
+     * @param E The input essential matrix.
+     * @param points1 Array of N 2D points from the first image. The point coordinates should be
+     * floating-point (single or double precision).
+     * @param points2 Array of the second image points of the same size and format as points1 .
+     * @param R Output rotation matrix. Together with the translation vector, this matrix makes up a tuple
+     * that performs a change of basis from the first camera's coordinate system to the second camera's
+     * coordinate system. Note that, in general, t can not be used for this tuple, see the parameter
+     * description below.
+     * @param t Output translation vector. This vector is obtained by REF: decomposeEssentialMat and
+     * therefore is only known up to scale, i.e. t is the direction of the translation vector and has unit
+     * length.
+     * are feature points from cameras with same focal length and principal point.
+     * inliers in points1 and points2 for then given essential matrix E. Only these inliers will be used to
+     * recover pose. In the output mask only inliers which pass the cheirality check.
+     *
+     * This function differs from the one above that it computes camera matrix from focal length and
+     * principal point:
+     *
+     * \(A =
+     * \begin{bmatrix}
+     * f &amp; 0 &amp; x_{pp}  \\
+     * 0 &amp; f &amp; y_{pp}  \\
+     * 0 &amp; 0 &amp; 1
+     * \end{bmatrix}\)
+     * @return automatically generated
+     */
+    public static int recoverPose(Mat E, Mat points1, Mat points2, Mat R, Mat t) {
+        return recoverPose_3(E.nativeObj, points1.nativeObj, points2.nativeObj, R.nativeObj, t.nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::recoverPose(Mat E, Mat points1, Mat points2, Mat cameraMatrix, Mat& R, Mat& t, Mat& mask = Mat())
+    //
+
+    /**
+     * Recovers the relative camera rotation and the translation from an estimated essential
+     * matrix and the corresponding points in two images, using cheirality check. Returns the number of
+     * inliers that pass the check.
+     *
+     * @param E The input essential matrix.
+     * @param points1 Array of N 2D points from the first image. The point coordinates should be
+     * floating-point (single or double precision).
+     * @param points2 Array of the second image points of the same size and format as points1 .
+     * @param cameraMatrix Camera matrix \(A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}\) .
+     * Note that this function assumes that points1 and points2 are feature points from cameras with the
+     * same camera matrix.
+     * @param R Output rotation matrix. Together with the translation vector, this matrix makes up a tuple
+     * that performs a change of basis from the first camera's coordinate system to the second camera's
+     * coordinate system. Note that, in general, t can not be used for this tuple, see the parameter
+     * described below.
+     * @param t Output translation vector. This vector is obtained by REF: decomposeEssentialMat and
+     * therefore is only known up to scale, i.e. t is the direction of the translation vector and has unit
+     * length.
+     * @param mask Input/output mask for inliers in points1 and points2. If it is not empty, then it marks
+     * inliers in points1 and points2 for then given essential matrix E. Only these inliers will be used to
+     * recover pose. In the output mask only inliers which pass the cheirality check.
+     *
+     * This function decomposes an essential matrix using REF: decomposeEssentialMat and then verifies
+     * possible pose hypotheses by doing cheirality check. The cheirality check means that the
+     * triangulated 3D points should have positive depth. Some details can be found in CITE: Nister03.
+     *
+     * This function can be used to process the output E and mask from REF: findEssentialMat. In this
+     * scenario, points1 and points2 are the same input for findEssentialMat.:
+     * <code>
+     *     // Example. Estimation of fundamental matrix using the RANSAC algorithm
+     *     int point_count = 100;
+     *     vector&lt;Point2f&gt; points1(point_count);
+     *     vector&lt;Point2f&gt; points2(point_count);
+     *
+     *     // initialize the points here ...
+     *     for( int i = 0; i &lt; point_count; i++ )
+     *     {
+     *         points1[i] = ...;
+     *         points2[i] = ...;
+     *     }
+     *
+     *     // cametra matrix with both focal lengths = 1, and principal point = (0, 0)
+     *     Mat cameraMatrix = Mat::eye(3, 3, CV_64F);
+     *
+     *     Mat E, R, t, mask;
+     *
+     *     E = findEssentialMat(points1, points2, cameraMatrix, RANSAC, 0.999, 1.0, mask);
+     *     recoverPose(E, points1, points2, cameraMatrix, R, t, mask);
+     * </code>
+     * @return automatically generated
+     */
+    public static int recoverPose(Mat E, Mat points1, Mat points2, Mat cameraMatrix, Mat R, Mat t, Mat mask) {
+        return recoverPose_4(E.nativeObj, points1.nativeObj, points2.nativeObj, cameraMatrix.nativeObj, R.nativeObj, t.nativeObj, mask.nativeObj);
+    }
+
+    /**
+     * Recovers the relative camera rotation and the translation from an estimated essential
+     * matrix and the corresponding points in two images, using cheirality check. Returns the number of
+     * inliers that pass the check.
+     *
+     * @param E The input essential matrix.
+     * @param points1 Array of N 2D points from the first image. The point coordinates should be
+     * floating-point (single or double precision).
+     * @param points2 Array of the second image points of the same size and format as points1 .
+     * @param cameraMatrix Camera matrix \(A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}\) .
+     * Note that this function assumes that points1 and points2 are feature points from cameras with the
+     * same camera matrix.
+     * @param R Output rotation matrix. Together with the translation vector, this matrix makes up a tuple
+     * that performs a change of basis from the first camera's coordinate system to the second camera's
+     * coordinate system. Note that, in general, t can not be used for this tuple, see the parameter
+     * described below.
+     * @param t Output translation vector. This vector is obtained by REF: decomposeEssentialMat and
+     * therefore is only known up to scale, i.e. t is the direction of the translation vector and has unit
+     * length.
+     * inliers in points1 and points2 for then given essential matrix E. Only these inliers will be used to
+     * recover pose. In the output mask only inliers which pass the cheirality check.
+     *
+     * This function decomposes an essential matrix using REF: decomposeEssentialMat and then verifies
+     * possible pose hypotheses by doing cheirality check. The cheirality check means that the
+     * triangulated 3D points should have positive depth. Some details can be found in CITE: Nister03.
+     *
+     * This function can be used to process the output E and mask from REF: findEssentialMat. In this
+     * scenario, points1 and points2 are the same input for findEssentialMat.:
+     * <code>
+     *     // Example. Estimation of fundamental matrix using the RANSAC algorithm
+     *     int point_count = 100;
+     *     vector&lt;Point2f&gt; points1(point_count);
+     *     vector&lt;Point2f&gt; points2(point_count);
+     *
+     *     // initialize the points here ...
+     *     for( int i = 0; i &lt; point_count; i++ )
+     *     {
+     *         points1[i] = ...;
+     *         points2[i] = ...;
+     *     }
+     *
+     *     // cametra matrix with both focal lengths = 1, and principal point = (0, 0)
+     *     Mat cameraMatrix = Mat::eye(3, 3, CV_64F);
+     *
+     *     Mat E, R, t, mask;
+     *
+     *     E = findEssentialMat(points1, points2, cameraMatrix, RANSAC, 0.999, 1.0, mask);
+     *     recoverPose(E, points1, points2, cameraMatrix, R, t, mask);
+     * </code>
+     * @return automatically generated
+     */
+    public static int recoverPose(Mat E, Mat points1, Mat points2, Mat cameraMatrix, Mat R, Mat t) {
+        return recoverPose_5(E.nativeObj, points1.nativeObj, points2.nativeObj, cameraMatrix.nativeObj, R.nativeObj, t.nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::recoverPose(Mat E, Mat points1, Mat points2, Mat cameraMatrix, Mat& R, Mat& t, double distanceThresh, Mat& mask = Mat(), Mat& triangulatedPoints = Mat())
+    //
+
+    /**
+     *
+     * @param E The input essential matrix.
+     * @param points1 Array of N 2D points from the first image. The point coordinates should be
+     * floating-point (single or double precision).
+     * @param points2 Array of the second image points of the same size and format as points1.
+     * @param cameraMatrix Camera matrix \(A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}\) .
+     * Note that this function assumes that points1 and points2 are feature points from cameras with the
+     * same camera matrix.
+     * @param R Output rotation matrix. Together with the translation vector, this matrix makes up a tuple
+     * that performs a change of basis from the first camera's coordinate system to the second camera's
+     * coordinate system. Note that, in general, t can not be used for this tuple, see the parameter
+     * description below.
+     * @param t Output translation vector. This vector is obtained by REF: decomposeEssentialMat and
+     * therefore is only known up to scale, i.e. t is the direction of the translation vector and has unit
+     * length.
+     * @param distanceThresh threshold distance which is used to filter out far away points (i.e. infinite
+     * points).
+     * @param mask Input/output mask for inliers in points1 and points2. If it is not empty, then it marks
+     * inliers in points1 and points2 for then given essential matrix E. Only these inliers will be used to
+     * recover pose. In the output mask only inliers which pass the cheirality check.
+     * @param triangulatedPoints 3D points which were reconstructed by triangulation.
+     *
+     * This function differs from the one above that it outputs the triangulated 3D point that are used for
+     * the cheirality check.
+     * @return automatically generated
+     */
+    public static int recoverPose(Mat E, Mat points1, Mat points2, Mat cameraMatrix, Mat R, Mat t, double distanceThresh, Mat mask, Mat triangulatedPoints) {
+        return recoverPose_6(E.nativeObj, points1.nativeObj, points2.nativeObj, cameraMatrix.nativeObj, R.nativeObj, t.nativeObj, distanceThresh, mask.nativeObj, triangulatedPoints.nativeObj);
+    }
+
+    /**
+     *
+     * @param E The input essential matrix.
+     * @param points1 Array of N 2D points from the first image. The point coordinates should be
+     * floating-point (single or double precision).
+     * @param points2 Array of the second image points of the same size and format as points1.
+     * @param cameraMatrix Camera matrix \(A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}\) .
+     * Note that this function assumes that points1 and points2 are feature points from cameras with the
+     * same camera matrix.
+     * @param R Output rotation matrix. Together with the translation vector, this matrix makes up a tuple
+     * that performs a change of basis from the first camera's coordinate system to the second camera's
+     * coordinate system. Note that, in general, t can not be used for this tuple, see the parameter
+     * description below.
+     * @param t Output translation vector. This vector is obtained by REF: decomposeEssentialMat and
+     * therefore is only known up to scale, i.e. t is the direction of the translation vector and has unit
+     * length.
+     * @param distanceThresh threshold distance which is used to filter out far away points (i.e. infinite
+     * points).
+     * @param mask Input/output mask for inliers in points1 and points2. If it is not empty, then it marks
+     * inliers in points1 and points2 for then given essential matrix E. Only these inliers will be used to
+     * recover pose. In the output mask only inliers which pass the cheirality check.
+     *
+     * This function differs from the one above that it outputs the triangulated 3D point that are used for
+     * the cheirality check.
+     * @return automatically generated
+     */
+    public static int recoverPose(Mat E, Mat points1, Mat points2, Mat cameraMatrix, Mat R, Mat t, double distanceThresh, Mat mask) {
+        return recoverPose_7(E.nativeObj, points1.nativeObj, points2.nativeObj, cameraMatrix.nativeObj, R.nativeObj, t.nativeObj, distanceThresh, mask.nativeObj);
+    }
+
+    /**
+     *
+     * @param E The input essential matrix.
+     * @param points1 Array of N 2D points from the first image. The point coordinates should be
+     * floating-point (single or double precision).
+     * @param points2 Array of the second image points of the same size and format as points1.
+     * @param cameraMatrix Camera matrix \(A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}\) .
+     * Note that this function assumes that points1 and points2 are feature points from cameras with the
+     * same camera matrix.
+     * @param R Output rotation matrix. Together with the translation vector, this matrix makes up a tuple
+     * that performs a change of basis from the first camera's coordinate system to the second camera's
+     * coordinate system. Note that, in general, t can not be used for this tuple, see the parameter
+     * description below.
+     * @param t Output translation vector. This vector is obtained by REF: decomposeEssentialMat and
+     * therefore is only known up to scale, i.e. t is the direction of the translation vector and has unit
+     * length.
+     * @param distanceThresh threshold distance which is used to filter out far away points (i.e. infinite
+     * points).
+     * inliers in points1 and points2 for then given essential matrix E. Only these inliers will be used to
+     * recover pose. In the output mask only inliers which pass the cheirality check.
+     *
+     * This function differs from the one above that it outputs the triangulated 3D point that are used for
+     * the cheirality check.
+     * @return automatically generated
+     */
+    public static int recoverPose(Mat E, Mat points1, Mat points2, Mat cameraMatrix, Mat R, Mat t, double distanceThresh) {
+        return recoverPose_8(E.nativeObj, points1.nativeObj, points2.nativeObj, cameraMatrix.nativeObj, R.nativeObj, t.nativeObj, distanceThresh);
+    }
+
+
+    //
+    // C++:  int cv::solveP3P(Mat objectPoints, Mat imagePoints, Mat cameraMatrix, Mat distCoeffs, vector_Mat& rvecs, vector_Mat& tvecs, int flags)
+    //
+
+    /**
+     * Finds an object pose from 3 3D-2D point correspondences.
+     *
+     * @param objectPoints Array of object points in the object coordinate space, 3x3 1-channel or
+     * 1x3/3x1 3-channel. vector&lt;Point3f&gt; can be also passed here.
+     * @param imagePoints Array of corresponding image points, 3x2 1-channel or 1x3/3x1 2-channel.
+     *  vector&lt;Point2f&gt; can be also passed here.
+     * @param cameraMatrix Input camera matrix \(A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}\) .
+     * @param distCoeffs Input vector of distortion coefficients
+     * \((k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\) of
+     * 4, 5, 8, 12 or 14 elements. If the vector is NULL/empty, the zero distortion coefficients are
+     * assumed.
+     * @param rvecs Output rotation vectors (see REF: Rodrigues ) that, together with tvecs, brings points from
+     * the model coordinate system to the camera coordinate system. A P3P problem has up to 4 solutions.
+     * @param tvecs Output translation vectors.
+     * @param flags Method for solving a P3P problem:
+     * <ul>
+     *   <li>
+     *    <b>SOLVEPNP_P3P</b> Method is based on the paper of X.S. Gao, X.-R. Hou, J. Tang, H.-F. Chang
+     * "Complete Solution Classification for the Perspective-Three-Point Problem" (CITE: gao2003complete).
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_AP3P</b> Method is based on the paper of T. Ke and S. Roumeliotis.
+     * "An Efficient Algebraic Solution to the Perspective-Three-Point Problem" (CITE: Ke17).
+     *   </li>
+     * </ul>
+     *
+     * The function estimates the object pose given 3 object points, their corresponding image
+     * projections, as well as the camera matrix and the distortion coefficients.
+     *
+     * <b>Note:</b>
+     * The solutions are sorted by reprojection errors (lowest to highest).
+     * @return automatically generated
+     */
+    public static int solveP3P(Mat objectPoints, Mat imagePoints, Mat cameraMatrix, Mat distCoeffs, List<Mat> rvecs, List<Mat> tvecs, int flags) {
+        Mat rvecs_mat = new Mat();
+        Mat tvecs_mat = new Mat();
+        int retVal = solveP3P_0(objectPoints.nativeObj, imagePoints.nativeObj, cameraMatrix.nativeObj, distCoeffs.nativeObj, rvecs_mat.nativeObj, tvecs_mat.nativeObj, flags);
+        Converters.Mat_to_vector_Mat(rvecs_mat, rvecs);
+        rvecs_mat.release();
+        Converters.Mat_to_vector_Mat(tvecs_mat, tvecs);
+        tvecs_mat.release();
+        return retVal;
+    }
+
+
+    //
+    // C++:  int cv::solvePnPGeneric(Mat objectPoints, Mat imagePoints, Mat cameraMatrix, Mat distCoeffs, vector_Mat& rvecs, vector_Mat& tvecs, bool useExtrinsicGuess = false, SolvePnPMethod flags = SOLVEPNP_ITERATIVE, Mat rvec = Mat(), Mat tvec = Mat(), Mat& reprojectionError = Mat())
+    //
+
+    /**
+     * Finds an object pose from 3D-2D point correspondences.
+     * This function returns a list of all the possible solutions (a solution is a &lt;rotation vector, translation vector&gt;
+     * couple), depending on the number of input points and the chosen method:
+     * <ul>
+     *   <li>
+     *  P3P methods (REF: SOLVEPNP_P3P, REF: SOLVEPNP_AP3P): 3 or 4 input points. Number of returned solutions can be between 0 and 4 with 3 input points.
+     *   </li>
+     *   <li>
+     *  REF: SOLVEPNP_IPPE Input points must be &gt;= 4 and object points must be coplanar. Returns 2 solutions.
+     *   </li>
+     *   <li>
+     *  REF: SOLVEPNP_IPPE_SQUARE Special case suitable for marker pose estimation.
+     * Number of input points must be 4 and 2 solutions are returned. Object points must be defined in the following order:
+     *   <ul>
+     *     <li>
+     *    point 0: [-squareLength / 2,  squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *    point 1: [ squareLength / 2,  squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *    point 2: [ squareLength / 2, -squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *    point 3: [-squareLength / 2, -squareLength / 2, 0]
+     *     </li>
+     *   </ul>
+     *   <li>
+     *  for all the other flags, number of input points must be &gt;= 4 and object points can be in any configuration.
+     * Only 1 solution is returned.
+     *   </li>
+     * </ul>
+     *
+     * @param objectPoints Array of object points in the object coordinate space, Nx3 1-channel or
+     * 1xN/Nx1 3-channel, where N is the number of points. vector&lt;Point3d&gt; can be also passed here.
+     * @param imagePoints Array of corresponding image points, Nx2 1-channel or 1xN/Nx1 2-channel,
+     * where N is the number of points. vector&lt;Point2d&gt; can be also passed here.
+     * @param cameraMatrix Input camera matrix \(A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}\) .
+     * @param distCoeffs Input vector of distortion coefficients
+     * \((k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\) of
+     * 4, 5, 8, 12 or 14 elements. If the vector is NULL/empty, the zero distortion coefficients are
+     * assumed.
+     * @param rvecs Vector of output rotation vectors (see REF: Rodrigues ) that, together with tvecs, brings points from
+     * the model coordinate system to the camera coordinate system.
+     * @param tvecs Vector of output translation vectors.
+     * @param useExtrinsicGuess Parameter used for #SOLVEPNP_ITERATIVE. If true (1), the function uses
+     * the provided rvec and tvec values as initial approximations of the rotation and translation
+     * vectors, respectively, and further optimizes them.
+     * @param flags Method for solving a PnP problem:
+     * <ul>
+     *   <li>
+     *    <b>SOLVEPNP_ITERATIVE</b> Iterative method is based on a Levenberg-Marquardt optimization. In
+     * this case the function finds such a pose that minimizes reprojection error, that is the sum
+     * of squared distances between the observed projections imagePoints and the projected (using
+     * projectPoints ) objectPoints .
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_P3P</b> Method is based on the paper of X.S. Gao, X.-R. Hou, J. Tang, H.-F. Chang
+     * "Complete Solution Classification for the Perspective-Three-Point Problem" (CITE: gao2003complete).
+     * In this case the function requires exactly four object and image points.
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_AP3P</b> Method is based on the paper of T. Ke, S. Roumeliotis
+     * "An Efficient Algebraic Solution to the Perspective-Three-Point Problem" (CITE: Ke17).
+     * In this case the function requires exactly four object and image points.
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_EPNP</b> Method has been introduced by F.Moreno-Noguer, V.Lepetit and P.Fua in the
+     * paper "EPnP: Efficient Perspective-n-Point Camera Pose Estimation" (CITE: lepetit2009epnp).
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_DLS</b> Method is based on the paper of Joel A. Hesch and Stergios I. Roumeliotis.
+     * "A Direct Least-Squares (DLS) Method for PnP" (CITE: hesch2011direct).
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_UPNP</b> Method is based on the paper of A.Penate-Sanchez, J.Andrade-Cetto,
+     * F.Moreno-Noguer. "Exhaustive Linearization for Robust Camera Pose and Focal Length
+     * Estimation" (CITE: penate2013exhaustive). In this case the function also estimates the parameters \(f_x\) and \(f_y\)
+     * assuming that both have the same value. Then the cameraMatrix is updated with the estimated
+     * focal length.
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_IPPE</b> Method is based on the paper of T. Collins and A. Bartoli.
+     * "Infinitesimal Plane-Based Pose Estimation" (CITE: Collins14). This method requires coplanar object points.
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_IPPE_SQUARE</b> Method is based on the paper of Toby Collins and Adrien Bartoli.
+     * "Infinitesimal Plane-Based Pose Estimation" (CITE: Collins14). This method is suitable for marker pose estimation.
+     * It requires 4 coplanar object points defined in the following order:
+     *   <ul>
+     *     <li>
+     *    point 0: [-squareLength / 2,  squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *    point 1: [ squareLength / 2,  squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *    point 2: [ squareLength / 2, -squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *    point 3: [-squareLength / 2, -squareLength / 2, 0]
+     * @param rvec Rotation vector used to initialize an iterative PnP refinement algorithm, when flag is SOLVEPNP_ITERATIVE
+     * and useExtrinsicGuess is set to true.
+     * @param tvec Translation vector used to initialize an iterative PnP refinement algorithm, when flag is SOLVEPNP_ITERATIVE
+     * and useExtrinsicGuess is set to true.
+     * @param reprojectionError Optional vector of reprojection error, that is the RMS error
+     * (\( \text{RMSE} = \sqrt{\frac{\sum_{i}^{N} \left ( \hat{y_i} - y_i \right )^2}{N}} \)) between the input image points
+     * and the 3D object points projected with the estimated pose.
+     *     </li>
+     *   </ul>
+     *
+     * The function estimates the object pose given a set of object points, their corresponding image
+     * projections, as well as the camera matrix and the distortion coefficients, see the figure below
+     * (more precisely, the X-axis of the camera frame is pointing to the right, the Y-axis downward
+     * and the Z-axis forward).
+     *   </li>
+     * </ul>
+     *
+     * ![](pnp.jpg)
+     *
+     * Points expressed in the world frame \( \bf{X}_w \) are projected into the image plane \( \left[ u, v \right] \)
+     * using the perspective projection model \( \Pi \) and the camera intrinsic parameters matrix \( \bf{A} \):
+     *
+     * \(
+     *   \begin{align*}
+     *   \begin{bmatrix}
+     *   u \\
+     *   v \\
+     *   1
+     *   \end{bmatrix} &amp;=
+     *   \bf{A} \hspace{0.1em} \Pi \hspace{0.2em} ^{c}\bf{T}_w
+     *   \begin{bmatrix}
+     *   X_{w} \\
+     *   Y_{w} \\
+     *   Z_{w} \\
+     *   1
+     *   \end{bmatrix} \\
+     *   \begin{bmatrix}
+     *   u \\
+     *   v \\
+     *   1
+     *   \end{bmatrix} &amp;=
+     *   \begin{bmatrix}
+     *   f_x &amp; 0 &amp; c_x \\
+     *   0 &amp; f_y &amp; c_y \\
+     *   0 &amp; 0 &amp; 1
+     *   \end{bmatrix}
+     *   \begin{bmatrix}
+     *   1 &amp; 0 &amp; 0 &amp; 0 \\
+     *   0 &amp; 1 &amp; 0 &amp; 0 \\
+     *   0 &amp; 0 &amp; 1 &amp; 0
+     *   \end{bmatrix}
+     *   \begin{bmatrix}
+     *   r_{11} &amp; r_{12} &amp; r_{13} &amp; t_x \\
+     *   r_{21} &amp; r_{22} &amp; r_{23} &amp; t_y \\
+     *   r_{31} &amp; r_{32} &amp; r_{33} &amp; t_z \\
+     *   0 &amp; 0 &amp; 0 &amp; 1
+     *   \end{bmatrix}
+     *   \begin{bmatrix}
+     *   X_{w} \\
+     *   Y_{w} \\
+     *   Z_{w} \\
+     *   1
+     *   \end{bmatrix}
+     *   \end{align*}
+     * \)
+     *
+     * The estimated pose is thus the rotation ({@code rvec}) and the translation ({@code tvec}) vectors that allow transforming
+     * a 3D point expressed in the world frame into the camera frame:
+     *
+     * \(
+     *   \begin{align*}
+     *   \begin{bmatrix}
+     *   X_c \\
+     *   Y_c \\
+     *   Z_c \\
+     *   1
+     *   \end{bmatrix} &amp;=
+     *   \hspace{0.2em} ^{c}\bf{T}_w
+     *   \begin{bmatrix}
+     *   X_{w} \\
+     *   Y_{w} \\
+     *   Z_{w} \\
+     *   1
+     *   \end{bmatrix} \\
+     *   \begin{bmatrix}
+     *   X_c \\
+     *   Y_c \\
+     *   Z_c \\
+     *   1
+     *   \end{bmatrix} &amp;=
+     *   \begin{bmatrix}
+     *   r_{11} &amp; r_{12} &amp; r_{13} &amp; t_x \\
+     *   r_{21} &amp; r_{22} &amp; r_{23} &amp; t_y \\
+     *   r_{31} &amp; r_{32} &amp; r_{33} &amp; t_z \\
+     *   0 &amp; 0 &amp; 0 &amp; 1
+     *   \end{bmatrix}
+     *   \begin{bmatrix}
+     *   X_{w} \\
+     *   Y_{w} \\
+     *   Z_{w} \\
+     *   1
+     *   \end{bmatrix}
+     *   \end{align*}
+     * \)
+     *
+     * <b>Note:</b>
+     * <ul>
+     *   <li>
+     *       An example of how to use solvePnP for planar augmented reality can be found at
+     *         opencv_source_code/samples/python/plane_ar.py
+     *   </li>
+     *   <li>
+     *       If you are using Python:
+     *   <ul>
+     *     <li>
+     *          Numpy array slices won't work as input because solvePnP requires contiguous
+     *         arrays (enforced by the assertion using cv::Mat::checkVector() around line 55 of
+     *         modules/calib3d/src/solvepnp.cpp version 2.4.9)
+     *     </li>
+     *     <li>
+     *          The P3P algorithm requires image points to be in an array of shape (N,1,2) due
+     *         to its calling of cv::undistortPoints (around line 75 of modules/calib3d/src/solvepnp.cpp version 2.4.9)
+     *         which requires 2-channel information.
+     *     </li>
+     *     <li>
+     *          Thus, given some data D = np.array(...) where D.shape = (N,M), in order to use a subset of
+     *         it as, e.g., imagePoints, one must effectively copy it into a new array: imagePoints =
+     *         np.ascontiguousarray(D[:,:2]).reshape((N,1,2))
+     *     </li>
+     *   </ul>
+     *   <li>
+     *       The methods <b>SOLVEPNP_DLS</b> and <b>SOLVEPNP_UPNP</b> cannot be used as the current implementations are
+     *        unstable and sometimes give completely wrong results. If you pass one of these two
+     *        flags, <b>SOLVEPNP_EPNP</b> method will be used instead.
+     *   </li>
+     *   <li>
+     *       The minimum number of points is 4 in the general case. In the case of <b>SOLVEPNP_P3P</b> and <b>SOLVEPNP_AP3P</b>
+     *        methods, it is required to use exactly 4 points (the first 3 points are used to estimate all the solutions
+     *        of the P3P problem, the last one is used to retain the best solution that minimizes the reprojection error).
+     *   </li>
+     *   <li>
+     *       With <b>SOLVEPNP_ITERATIVE</b> method and {@code useExtrinsicGuess=true}, the minimum number of points is 3 (3 points
+     *        are sufficient to compute a pose but there are up to 4 solutions). The initial solution should be close to the
+     *        global solution to converge.
+     *   </li>
+     *   <li>
+     *       With <b>SOLVEPNP_IPPE</b> input points must be &gt;= 4 and object points must be coplanar.
+     *   </li>
+     *   <li>
+     *       With <b>SOLVEPNP_IPPE_SQUARE</b> this is a special case suitable for marker pose estimation.
+     *        Number of input points must be 4. Object points must be defined in the following order:
+     *   <ul>
+     *     <li>
+     *           point 0: [-squareLength / 2,  squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *           point 1: [ squareLength / 2,  squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *           point 2: [ squareLength / 2, -squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *           point 3: [-squareLength / 2, -squareLength / 2, 0]
+     *     </li>
+     *   </ul>
+     *   </li>
+     * </ul>
+     * @return automatically generated
+     */
+    public static int solvePnPGeneric(Mat objectPoints, Mat imagePoints, Mat cameraMatrix, Mat distCoeffs, List<Mat> rvecs, List<Mat> tvecs, boolean useExtrinsicGuess, int flags, Mat rvec, Mat tvec, Mat reprojectionError) {
+        Mat rvecs_mat = new Mat();
+        Mat tvecs_mat = new Mat();
+        int retVal = solvePnPGeneric_0(objectPoints.nativeObj, imagePoints.nativeObj, cameraMatrix.nativeObj, distCoeffs.nativeObj, rvecs_mat.nativeObj, tvecs_mat.nativeObj, useExtrinsicGuess, flags, rvec.nativeObj, tvec.nativeObj, reprojectionError.nativeObj);
+        Converters.Mat_to_vector_Mat(rvecs_mat, rvecs);
+        rvecs_mat.release();
+        Converters.Mat_to_vector_Mat(tvecs_mat, tvecs);
+        tvecs_mat.release();
+        return retVal;
+    }
+
+    /**
+     * Finds an object pose from 3D-2D point correspondences.
+     * This function returns a list of all the possible solutions (a solution is a &lt;rotation vector, translation vector&gt;
+     * couple), depending on the number of input points and the chosen method:
+     * <ul>
+     *   <li>
+     *  P3P methods (REF: SOLVEPNP_P3P, REF: SOLVEPNP_AP3P): 3 or 4 input points. Number of returned solutions can be between 0 and 4 with 3 input points.
+     *   </li>
+     *   <li>
+     *  REF: SOLVEPNP_IPPE Input points must be &gt;= 4 and object points must be coplanar. Returns 2 solutions.
+     *   </li>
+     *   <li>
+     *  REF: SOLVEPNP_IPPE_SQUARE Special case suitable for marker pose estimation.
+     * Number of input points must be 4 and 2 solutions are returned. Object points must be defined in the following order:
+     *   <ul>
+     *     <li>
+     *    point 0: [-squareLength / 2,  squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *    point 1: [ squareLength / 2,  squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *    point 2: [ squareLength / 2, -squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *    point 3: [-squareLength / 2, -squareLength / 2, 0]
+     *     </li>
+     *   </ul>
+     *   <li>
+     *  for all the other flags, number of input points must be &gt;= 4 and object points can be in any configuration.
+     * Only 1 solution is returned.
+     *   </li>
+     * </ul>
+     *
+     * @param objectPoints Array of object points in the object coordinate space, Nx3 1-channel or
+     * 1xN/Nx1 3-channel, where N is the number of points. vector&lt;Point3d&gt; can be also passed here.
+     * @param imagePoints Array of corresponding image points, Nx2 1-channel or 1xN/Nx1 2-channel,
+     * where N is the number of points. vector&lt;Point2d&gt; can be also passed here.
+     * @param cameraMatrix Input camera matrix \(A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}\) .
+     * @param distCoeffs Input vector of distortion coefficients
+     * \((k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\) of
+     * 4, 5, 8, 12 or 14 elements. If the vector is NULL/empty, the zero distortion coefficients are
+     * assumed.
+     * @param rvecs Vector of output rotation vectors (see REF: Rodrigues ) that, together with tvecs, brings points from
+     * the model coordinate system to the camera coordinate system.
+     * @param tvecs Vector of output translation vectors.
+     * @param useExtrinsicGuess Parameter used for #SOLVEPNP_ITERATIVE. If true (1), the function uses
+     * the provided rvec and tvec values as initial approximations of the rotation and translation
+     * vectors, respectively, and further optimizes them.
+     * @param flags Method for solving a PnP problem:
+     * <ul>
+     *   <li>
+     *    <b>SOLVEPNP_ITERATIVE</b> Iterative method is based on a Levenberg-Marquardt optimization. In
+     * this case the function finds such a pose that minimizes reprojection error, that is the sum
+     * of squared distances between the observed projections imagePoints and the projected (using
+     * projectPoints ) objectPoints .
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_P3P</b> Method is based on the paper of X.S. Gao, X.-R. Hou, J. Tang, H.-F. Chang
+     * "Complete Solution Classification for the Perspective-Three-Point Problem" (CITE: gao2003complete).
+     * In this case the function requires exactly four object and image points.
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_AP3P</b> Method is based on the paper of T. Ke, S. Roumeliotis
+     * "An Efficient Algebraic Solution to the Perspective-Three-Point Problem" (CITE: Ke17).
+     * In this case the function requires exactly four object and image points.
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_EPNP</b> Method has been introduced by F.Moreno-Noguer, V.Lepetit and P.Fua in the
+     * paper "EPnP: Efficient Perspective-n-Point Camera Pose Estimation" (CITE: lepetit2009epnp).
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_DLS</b> Method is based on the paper of Joel A. Hesch and Stergios I. Roumeliotis.
+     * "A Direct Least-Squares (DLS) Method for PnP" (CITE: hesch2011direct).
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_UPNP</b> Method is based on the paper of A.Penate-Sanchez, J.Andrade-Cetto,
+     * F.Moreno-Noguer. "Exhaustive Linearization for Robust Camera Pose and Focal Length
+     * Estimation" (CITE: penate2013exhaustive). In this case the function also estimates the parameters \(f_x\) and \(f_y\)
+     * assuming that both have the same value. Then the cameraMatrix is updated with the estimated
+     * focal length.
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_IPPE</b> Method is based on the paper of T. Collins and A. Bartoli.
+     * "Infinitesimal Plane-Based Pose Estimation" (CITE: Collins14). This method requires coplanar object points.
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_IPPE_SQUARE</b> Method is based on the paper of Toby Collins and Adrien Bartoli.
+     * "Infinitesimal Plane-Based Pose Estimation" (CITE: Collins14). This method is suitable for marker pose estimation.
+     * It requires 4 coplanar object points defined in the following order:
+     *   <ul>
+     *     <li>
+     *    point 0: [-squareLength / 2,  squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *    point 1: [ squareLength / 2,  squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *    point 2: [ squareLength / 2, -squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *    point 3: [-squareLength / 2, -squareLength / 2, 0]
+     * @param rvec Rotation vector used to initialize an iterative PnP refinement algorithm, when flag is SOLVEPNP_ITERATIVE
+     * and useExtrinsicGuess is set to true.
+     * @param tvec Translation vector used to initialize an iterative PnP refinement algorithm, when flag is SOLVEPNP_ITERATIVE
+     * and useExtrinsicGuess is set to true.
+     * (\( \text{RMSE} = \sqrt{\frac{\sum_{i}^{N} \left ( \hat{y_i} - y_i \right )^2}{N}} \)) between the input image points
+     * and the 3D object points projected with the estimated pose.
+     *     </li>
+     *   </ul>
+     *
+     * The function estimates the object pose given a set of object points, their corresponding image
+     * projections, as well as the camera matrix and the distortion coefficients, see the figure below
+     * (more precisely, the X-axis of the camera frame is pointing to the right, the Y-axis downward
+     * and the Z-axis forward).
+     *   </li>
+     * </ul>
+     *
+     * ![](pnp.jpg)
+     *
+     * Points expressed in the world frame \( \bf{X}_w \) are projected into the image plane \( \left[ u, v \right] \)
+     * using the perspective projection model \( \Pi \) and the camera intrinsic parameters matrix \( \bf{A} \):
+     *
+     * \(
+     *   \begin{align*}
+     *   \begin{bmatrix}
+     *   u \\
+     *   v \\
+     *   1
+     *   \end{bmatrix} &amp;=
+     *   \bf{A} \hspace{0.1em} \Pi \hspace{0.2em} ^{c}\bf{T}_w
+     *   \begin{bmatrix}
+     *   X_{w} \\
+     *   Y_{w} \\
+     *   Z_{w} \\
+     *   1
+     *   \end{bmatrix} \\
+     *   \begin{bmatrix}
+     *   u \\
+     *   v \\
+     *   1
+     *   \end{bmatrix} &amp;=
+     *   \begin{bmatrix}
+     *   f_x &amp; 0 &amp; c_x \\
+     *   0 &amp; f_y &amp; c_y \\
+     *   0 &amp; 0 &amp; 1
+     *   \end{bmatrix}
+     *   \begin{bmatrix}
+     *   1 &amp; 0 &amp; 0 &amp; 0 \\
+     *   0 &amp; 1 &amp; 0 &amp; 0 \\
+     *   0 &amp; 0 &amp; 1 &amp; 0
+     *   \end{bmatrix}
+     *   \begin{bmatrix}
+     *   r_{11} &amp; r_{12} &amp; r_{13} &amp; t_x \\
+     *   r_{21} &amp; r_{22} &amp; r_{23} &amp; t_y \\
+     *   r_{31} &amp; r_{32} &amp; r_{33} &amp; t_z \\
+     *   0 &amp; 0 &amp; 0 &amp; 1
+     *   \end{bmatrix}
+     *   \begin{bmatrix}
+     *   X_{w} \\
+     *   Y_{w} \\
+     *   Z_{w} \\
+     *   1
+     *   \end{bmatrix}
+     *   \end{align*}
+     * \)
+     *
+     * The estimated pose is thus the rotation ({@code rvec}) and the translation ({@code tvec}) vectors that allow transforming
+     * a 3D point expressed in the world frame into the camera frame:
+     *
+     * \(
+     *   \begin{align*}
+     *   \begin{bmatrix}
+     *   X_c \\
+     *   Y_c \\
+     *   Z_c \\
+     *   1
+     *   \end{bmatrix} &amp;=
+     *   \hspace{0.2em} ^{c}\bf{T}_w
+     *   \begin{bmatrix}
+     *   X_{w} \\
+     *   Y_{w} \\
+     *   Z_{w} \\
+     *   1
+     *   \end{bmatrix} \\
+     *   \begin{bmatrix}
+     *   X_c \\
+     *   Y_c \\
+     *   Z_c \\
+     *   1
+     *   \end{bmatrix} &amp;=
+     *   \begin{bmatrix}
+     *   r_{11} &amp; r_{12} &amp; r_{13} &amp; t_x \\
+     *   r_{21} &amp; r_{22} &amp; r_{23} &amp; t_y \\
+     *   r_{31} &amp; r_{32} &amp; r_{33} &amp; t_z \\
+     *   0 &amp; 0 &amp; 0 &amp; 1
+     *   \end{bmatrix}
+     *   \begin{bmatrix}
+     *   X_{w} \\
+     *   Y_{w} \\
+     *   Z_{w} \\
+     *   1
+     *   \end{bmatrix}
+     *   \end{align*}
+     * \)
+     *
+     * <b>Note:</b>
+     * <ul>
+     *   <li>
+     *       An example of how to use solvePnP for planar augmented reality can be found at
+     *         opencv_source_code/samples/python/plane_ar.py
+     *   </li>
+     *   <li>
+     *       If you are using Python:
+     *   <ul>
+     *     <li>
+     *          Numpy array slices won't work as input because solvePnP requires contiguous
+     *         arrays (enforced by the assertion using cv::Mat::checkVector() around line 55 of
+     *         modules/calib3d/src/solvepnp.cpp version 2.4.9)
+     *     </li>
+     *     <li>
+     *          The P3P algorithm requires image points to be in an array of shape (N,1,2) due
+     *         to its calling of cv::undistortPoints (around line 75 of modules/calib3d/src/solvepnp.cpp version 2.4.9)
+     *         which requires 2-channel information.
+     *     </li>
+     *     <li>
+     *          Thus, given some data D = np.array(...) where D.shape = (N,M), in order to use a subset of
+     *         it as, e.g., imagePoints, one must effectively copy it into a new array: imagePoints =
+     *         np.ascontiguousarray(D[:,:2]).reshape((N,1,2))
+     *     </li>
+     *   </ul>
+     *   <li>
+     *       The methods <b>SOLVEPNP_DLS</b> and <b>SOLVEPNP_UPNP</b> cannot be used as the current implementations are
+     *        unstable and sometimes give completely wrong results. If you pass one of these two
+     *        flags, <b>SOLVEPNP_EPNP</b> method will be used instead.
+     *   </li>
+     *   <li>
+     *       The minimum number of points is 4 in the general case. In the case of <b>SOLVEPNP_P3P</b> and <b>SOLVEPNP_AP3P</b>
+     *        methods, it is required to use exactly 4 points (the first 3 points are used to estimate all the solutions
+     *        of the P3P problem, the last one is used to retain the best solution that minimizes the reprojection error).
+     *   </li>
+     *   <li>
+     *       With <b>SOLVEPNP_ITERATIVE</b> method and {@code useExtrinsicGuess=true}, the minimum number of points is 3 (3 points
+     *        are sufficient to compute a pose but there are up to 4 solutions). The initial solution should be close to the
+     *        global solution to converge.
+     *   </li>
+     *   <li>
+     *       With <b>SOLVEPNP_IPPE</b> input points must be &gt;= 4 and object points must be coplanar.
+     *   </li>
+     *   <li>
+     *       With <b>SOLVEPNP_IPPE_SQUARE</b> this is a special case suitable for marker pose estimation.
+     *        Number of input points must be 4. Object points must be defined in the following order:
+     *   <ul>
+     *     <li>
+     *           point 0: [-squareLength / 2,  squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *           point 1: [ squareLength / 2,  squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *           point 2: [ squareLength / 2, -squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *           point 3: [-squareLength / 2, -squareLength / 2, 0]
+     *     </li>
+     *   </ul>
+     *   </li>
+     * </ul>
+     * @return automatically generated
+     */
+    public static int solvePnPGeneric(Mat objectPoints, Mat imagePoints, Mat cameraMatrix, Mat distCoeffs, List<Mat> rvecs, List<Mat> tvecs, boolean useExtrinsicGuess, int flags, Mat rvec, Mat tvec) {
+        Mat rvecs_mat = new Mat();
+        Mat tvecs_mat = new Mat();
+        int retVal = solvePnPGeneric_1(objectPoints.nativeObj, imagePoints.nativeObj, cameraMatrix.nativeObj, distCoeffs.nativeObj, rvecs_mat.nativeObj, tvecs_mat.nativeObj, useExtrinsicGuess, flags, rvec.nativeObj, tvec.nativeObj);
+        Converters.Mat_to_vector_Mat(rvecs_mat, rvecs);
+        rvecs_mat.release();
+        Converters.Mat_to_vector_Mat(tvecs_mat, tvecs);
+        tvecs_mat.release();
+        return retVal;
+    }
+
+    /**
+     * Finds an object pose from 3D-2D point correspondences.
+     * This function returns a list of all the possible solutions (a solution is a &lt;rotation vector, translation vector&gt;
+     * couple), depending on the number of input points and the chosen method:
+     * <ul>
+     *   <li>
+     *  P3P methods (REF: SOLVEPNP_P3P, REF: SOLVEPNP_AP3P): 3 or 4 input points. Number of returned solutions can be between 0 and 4 with 3 input points.
+     *   </li>
+     *   <li>
+     *  REF: SOLVEPNP_IPPE Input points must be &gt;= 4 and object points must be coplanar. Returns 2 solutions.
+     *   </li>
+     *   <li>
+     *  REF: SOLVEPNP_IPPE_SQUARE Special case suitable for marker pose estimation.
+     * Number of input points must be 4 and 2 solutions are returned. Object points must be defined in the following order:
+     *   <ul>
+     *     <li>
+     *    point 0: [-squareLength / 2,  squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *    point 1: [ squareLength / 2,  squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *    point 2: [ squareLength / 2, -squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *    point 3: [-squareLength / 2, -squareLength / 2, 0]
+     *     </li>
+     *   </ul>
+     *   <li>
+     *  for all the other flags, number of input points must be &gt;= 4 and object points can be in any configuration.
+     * Only 1 solution is returned.
+     *   </li>
+     * </ul>
+     *
+     * @param objectPoints Array of object points in the object coordinate space, Nx3 1-channel or
+     * 1xN/Nx1 3-channel, where N is the number of points. vector&lt;Point3d&gt; can be also passed here.
+     * @param imagePoints Array of corresponding image points, Nx2 1-channel or 1xN/Nx1 2-channel,
+     * where N is the number of points. vector&lt;Point2d&gt; can be also passed here.
+     * @param cameraMatrix Input camera matrix \(A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}\) .
+     * @param distCoeffs Input vector of distortion coefficients
+     * \((k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\) of
+     * 4, 5, 8, 12 or 14 elements. If the vector is NULL/empty, the zero distortion coefficients are
+     * assumed.
+     * @param rvecs Vector of output rotation vectors (see REF: Rodrigues ) that, together with tvecs, brings points from
+     * the model coordinate system to the camera coordinate system.
+     * @param tvecs Vector of output translation vectors.
+     * @param useExtrinsicGuess Parameter used for #SOLVEPNP_ITERATIVE. If true (1), the function uses
+     * the provided rvec and tvec values as initial approximations of the rotation and translation
+     * vectors, respectively, and further optimizes them.
+     * @param flags Method for solving a PnP problem:
+     * <ul>
+     *   <li>
+     *    <b>SOLVEPNP_ITERATIVE</b> Iterative method is based on a Levenberg-Marquardt optimization. In
+     * this case the function finds such a pose that minimizes reprojection error, that is the sum
+     * of squared distances between the observed projections imagePoints and the projected (using
+     * projectPoints ) objectPoints .
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_P3P</b> Method is based on the paper of X.S. Gao, X.-R. Hou, J. Tang, H.-F. Chang
+     * "Complete Solution Classification for the Perspective-Three-Point Problem" (CITE: gao2003complete).
+     * In this case the function requires exactly four object and image points.
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_AP3P</b> Method is based on the paper of T. Ke, S. Roumeliotis
+     * "An Efficient Algebraic Solution to the Perspective-Three-Point Problem" (CITE: Ke17).
+     * In this case the function requires exactly four object and image points.
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_EPNP</b> Method has been introduced by F.Moreno-Noguer, V.Lepetit and P.Fua in the
+     * paper "EPnP: Efficient Perspective-n-Point Camera Pose Estimation" (CITE: lepetit2009epnp).
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_DLS</b> Method is based on the paper of Joel A. Hesch and Stergios I. Roumeliotis.
+     * "A Direct Least-Squares (DLS) Method for PnP" (CITE: hesch2011direct).
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_UPNP</b> Method is based on the paper of A.Penate-Sanchez, J.Andrade-Cetto,
+     * F.Moreno-Noguer. "Exhaustive Linearization for Robust Camera Pose and Focal Length
+     * Estimation" (CITE: penate2013exhaustive). In this case the function also estimates the parameters \(f_x\) and \(f_y\)
+     * assuming that both have the same value. Then the cameraMatrix is updated with the estimated
+     * focal length.
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_IPPE</b> Method is based on the paper of T. Collins and A. Bartoli.
+     * "Infinitesimal Plane-Based Pose Estimation" (CITE: Collins14). This method requires coplanar object points.
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_IPPE_SQUARE</b> Method is based on the paper of Toby Collins and Adrien Bartoli.
+     * "Infinitesimal Plane-Based Pose Estimation" (CITE: Collins14). This method is suitable for marker pose estimation.
+     * It requires 4 coplanar object points defined in the following order:
+     *   <ul>
+     *     <li>
+     *    point 0: [-squareLength / 2,  squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *    point 1: [ squareLength / 2,  squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *    point 2: [ squareLength / 2, -squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *    point 3: [-squareLength / 2, -squareLength / 2, 0]
+     * @param rvec Rotation vector used to initialize an iterative PnP refinement algorithm, when flag is SOLVEPNP_ITERATIVE
+     * and useExtrinsicGuess is set to true.
+     * and useExtrinsicGuess is set to true.
+     * (\( \text{RMSE} = \sqrt{\frac{\sum_{i}^{N} \left ( \hat{y_i} - y_i \right )^2}{N}} \)) between the input image points
+     * and the 3D object points projected with the estimated pose.
+     *     </li>
+     *   </ul>
+     *
+     * The function estimates the object pose given a set of object points, their corresponding image
+     * projections, as well as the camera matrix and the distortion coefficients, see the figure below
+     * (more precisely, the X-axis of the camera frame is pointing to the right, the Y-axis downward
+     * and the Z-axis forward).
+     *   </li>
+     * </ul>
+     *
+     * ![](pnp.jpg)
+     *
+     * Points expressed in the world frame \( \bf{X}_w \) are projected into the image plane \( \left[ u, v \right] \)
+     * using the perspective projection model \( \Pi \) and the camera intrinsic parameters matrix \( \bf{A} \):
+     *
+     * \(
+     *   \begin{align*}
+     *   \begin{bmatrix}
+     *   u \\
+     *   v \\
+     *   1
+     *   \end{bmatrix} &amp;=
+     *   \bf{A} \hspace{0.1em} \Pi \hspace{0.2em} ^{c}\bf{T}_w
+     *   \begin{bmatrix}
+     *   X_{w} \\
+     *   Y_{w} \\
+     *   Z_{w} \\
+     *   1
+     *   \end{bmatrix} \\
+     *   \begin{bmatrix}
+     *   u \\
+     *   v \\
+     *   1
+     *   \end{bmatrix} &amp;=
+     *   \begin{bmatrix}
+     *   f_x &amp; 0 &amp; c_x \\
+     *   0 &amp; f_y &amp; c_y \\
+     *   0 &amp; 0 &amp; 1
+     *   \end{bmatrix}
+     *   \begin{bmatrix}
+     *   1 &amp; 0 &amp; 0 &amp; 0 \\
+     *   0 &amp; 1 &amp; 0 &amp; 0 \\
+     *   0 &amp; 0 &amp; 1 &amp; 0
+     *   \end{bmatrix}
+     *   \begin{bmatrix}
+     *   r_{11} &amp; r_{12} &amp; r_{13} &amp; t_x \\
+     *   r_{21} &amp; r_{22} &amp; r_{23} &amp; t_y \\
+     *   r_{31} &amp; r_{32} &amp; r_{33} &amp; t_z \\
+     *   0 &amp; 0 &amp; 0 &amp; 1
+     *   \end{bmatrix}
+     *   \begin{bmatrix}
+     *   X_{w} \\
+     *   Y_{w} \\
+     *   Z_{w} \\
+     *   1
+     *   \end{bmatrix}
+     *   \end{align*}
+     * \)
+     *
+     * The estimated pose is thus the rotation ({@code rvec}) and the translation ({@code tvec}) vectors that allow transforming
+     * a 3D point expressed in the world frame into the camera frame:
+     *
+     * \(
+     *   \begin{align*}
+     *   \begin{bmatrix}
+     *   X_c \\
+     *   Y_c \\
+     *   Z_c \\
+     *   1
+     *   \end{bmatrix} &amp;=
+     *   \hspace{0.2em} ^{c}\bf{T}_w
+     *   \begin{bmatrix}
+     *   X_{w} \\
+     *   Y_{w} \\
+     *   Z_{w} \\
+     *   1
+     *   \end{bmatrix} \\
+     *   \begin{bmatrix}
+     *   X_c \\
+     *   Y_c \\
+     *   Z_c \\
+     *   1
+     *   \end{bmatrix} &amp;=
+     *   \begin{bmatrix}
+     *   r_{11} &amp; r_{12} &amp; r_{13} &amp; t_x \\
+     *   r_{21} &amp; r_{22} &amp; r_{23} &amp; t_y \\
+     *   r_{31} &amp; r_{32} &amp; r_{33} &amp; t_z \\
+     *   0 &amp; 0 &amp; 0 &amp; 1
+     *   \end{bmatrix}
+     *   \begin{bmatrix}
+     *   X_{w} \\
+     *   Y_{w} \\
+     *   Z_{w} \\
+     *   1
+     *   \end{bmatrix}
+     *   \end{align*}
+     * \)
+     *
+     * <b>Note:</b>
+     * <ul>
+     *   <li>
+     *       An example of how to use solvePnP for planar augmented reality can be found at
+     *         opencv_source_code/samples/python/plane_ar.py
+     *   </li>
+     *   <li>
+     *       If you are using Python:
+     *   <ul>
+     *     <li>
+     *          Numpy array slices won't work as input because solvePnP requires contiguous
+     *         arrays (enforced by the assertion using cv::Mat::checkVector() around line 55 of
+     *         modules/calib3d/src/solvepnp.cpp version 2.4.9)
+     *     </li>
+     *     <li>
+     *          The P3P algorithm requires image points to be in an array of shape (N,1,2) due
+     *         to its calling of cv::undistortPoints (around line 75 of modules/calib3d/src/solvepnp.cpp version 2.4.9)
+     *         which requires 2-channel information.
+     *     </li>
+     *     <li>
+     *          Thus, given some data D = np.array(...) where D.shape = (N,M), in order to use a subset of
+     *         it as, e.g., imagePoints, one must effectively copy it into a new array: imagePoints =
+     *         np.ascontiguousarray(D[:,:2]).reshape((N,1,2))
+     *     </li>
+     *   </ul>
+     *   <li>
+     *       The methods <b>SOLVEPNP_DLS</b> and <b>SOLVEPNP_UPNP</b> cannot be used as the current implementations are
+     *        unstable and sometimes give completely wrong results. If you pass one of these two
+     *        flags, <b>SOLVEPNP_EPNP</b> method will be used instead.
+     *   </li>
+     *   <li>
+     *       The minimum number of points is 4 in the general case. In the case of <b>SOLVEPNP_P3P</b> and <b>SOLVEPNP_AP3P</b>
+     *        methods, it is required to use exactly 4 points (the first 3 points are used to estimate all the solutions
+     *        of the P3P problem, the last one is used to retain the best solution that minimizes the reprojection error).
+     *   </li>
+     *   <li>
+     *       With <b>SOLVEPNP_ITERATIVE</b> method and {@code useExtrinsicGuess=true}, the minimum number of points is 3 (3 points
+     *        are sufficient to compute a pose but there are up to 4 solutions). The initial solution should be close to the
+     *        global solution to converge.
+     *   </li>
+     *   <li>
+     *       With <b>SOLVEPNP_IPPE</b> input points must be &gt;= 4 and object points must be coplanar.
+     *   </li>
+     *   <li>
+     *       With <b>SOLVEPNP_IPPE_SQUARE</b> this is a special case suitable for marker pose estimation.
+     *        Number of input points must be 4. Object points must be defined in the following order:
+     *   <ul>
+     *     <li>
+     *           point 0: [-squareLength / 2,  squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *           point 1: [ squareLength / 2,  squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *           point 2: [ squareLength / 2, -squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *           point 3: [-squareLength / 2, -squareLength / 2, 0]
+     *     </li>
+     *   </ul>
+     *   </li>
+     * </ul>
+     * @return automatically generated
+     */
+    public static int solvePnPGeneric(Mat objectPoints, Mat imagePoints, Mat cameraMatrix, Mat distCoeffs, List<Mat> rvecs, List<Mat> tvecs, boolean useExtrinsicGuess, int flags, Mat rvec) {
+        Mat rvecs_mat = new Mat();
+        Mat tvecs_mat = new Mat();
+        int retVal = solvePnPGeneric_2(objectPoints.nativeObj, imagePoints.nativeObj, cameraMatrix.nativeObj, distCoeffs.nativeObj, rvecs_mat.nativeObj, tvecs_mat.nativeObj, useExtrinsicGuess, flags, rvec.nativeObj);
+        Converters.Mat_to_vector_Mat(rvecs_mat, rvecs);
+        rvecs_mat.release();
+        Converters.Mat_to_vector_Mat(tvecs_mat, tvecs);
+        tvecs_mat.release();
+        return retVal;
+    }
+
+    /**
+     * Finds an object pose from 3D-2D point correspondences.
+     * This function returns a list of all the possible solutions (a solution is a &lt;rotation vector, translation vector&gt;
+     * couple), depending on the number of input points and the chosen method:
+     * <ul>
+     *   <li>
+     *  P3P methods (REF: SOLVEPNP_P3P, REF: SOLVEPNP_AP3P): 3 or 4 input points. Number of returned solutions can be between 0 and 4 with 3 input points.
+     *   </li>
+     *   <li>
+     *  REF: SOLVEPNP_IPPE Input points must be &gt;= 4 and object points must be coplanar. Returns 2 solutions.
+     *   </li>
+     *   <li>
+     *  REF: SOLVEPNP_IPPE_SQUARE Special case suitable for marker pose estimation.
+     * Number of input points must be 4 and 2 solutions are returned. Object points must be defined in the following order:
+     *   <ul>
+     *     <li>
+     *    point 0: [-squareLength / 2,  squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *    point 1: [ squareLength / 2,  squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *    point 2: [ squareLength / 2, -squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *    point 3: [-squareLength / 2, -squareLength / 2, 0]
+     *     </li>
+     *   </ul>
+     *   <li>
+     *  for all the other flags, number of input points must be &gt;= 4 and object points can be in any configuration.
+     * Only 1 solution is returned.
+     *   </li>
+     * </ul>
+     *
+     * @param objectPoints Array of object points in the object coordinate space, Nx3 1-channel or
+     * 1xN/Nx1 3-channel, where N is the number of points. vector&lt;Point3d&gt; can be also passed here.
+     * @param imagePoints Array of corresponding image points, Nx2 1-channel or 1xN/Nx1 2-channel,
+     * where N is the number of points. vector&lt;Point2d&gt; can be also passed here.
+     * @param cameraMatrix Input camera matrix \(A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}\) .
+     * @param distCoeffs Input vector of distortion coefficients
+     * \((k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\) of
+     * 4, 5, 8, 12 or 14 elements. If the vector is NULL/empty, the zero distortion coefficients are
+     * assumed.
+     * @param rvecs Vector of output rotation vectors (see REF: Rodrigues ) that, together with tvecs, brings points from
+     * the model coordinate system to the camera coordinate system.
+     * @param tvecs Vector of output translation vectors.
+     * @param useExtrinsicGuess Parameter used for #SOLVEPNP_ITERATIVE. If true (1), the function uses
+     * the provided rvec and tvec values as initial approximations of the rotation and translation
+     * vectors, respectively, and further optimizes them.
+     * @param flags Method for solving a PnP problem:
+     * <ul>
+     *   <li>
+     *    <b>SOLVEPNP_ITERATIVE</b> Iterative method is based on a Levenberg-Marquardt optimization. In
+     * this case the function finds such a pose that minimizes reprojection error, that is the sum
+     * of squared distances between the observed projections imagePoints and the projected (using
+     * projectPoints ) objectPoints .
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_P3P</b> Method is based on the paper of X.S. Gao, X.-R. Hou, J. Tang, H.-F. Chang
+     * "Complete Solution Classification for the Perspective-Three-Point Problem" (CITE: gao2003complete).
+     * In this case the function requires exactly four object and image points.
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_AP3P</b> Method is based on the paper of T. Ke, S. Roumeliotis
+     * "An Efficient Algebraic Solution to the Perspective-Three-Point Problem" (CITE: Ke17).
+     * In this case the function requires exactly four object and image points.
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_EPNP</b> Method has been introduced by F.Moreno-Noguer, V.Lepetit and P.Fua in the
+     * paper "EPnP: Efficient Perspective-n-Point Camera Pose Estimation" (CITE: lepetit2009epnp).
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_DLS</b> Method is based on the paper of Joel A. Hesch and Stergios I. Roumeliotis.
+     * "A Direct Least-Squares (DLS) Method for PnP" (CITE: hesch2011direct).
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_UPNP</b> Method is based on the paper of A.Penate-Sanchez, J.Andrade-Cetto,
+     * F.Moreno-Noguer. "Exhaustive Linearization for Robust Camera Pose and Focal Length
+     * Estimation" (CITE: penate2013exhaustive). In this case the function also estimates the parameters \(f_x\) and \(f_y\)
+     * assuming that both have the same value. Then the cameraMatrix is updated with the estimated
+     * focal length.
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_IPPE</b> Method is based on the paper of T. Collins and A. Bartoli.
+     * "Infinitesimal Plane-Based Pose Estimation" (CITE: Collins14). This method requires coplanar object points.
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_IPPE_SQUARE</b> Method is based on the paper of Toby Collins and Adrien Bartoli.
+     * "Infinitesimal Plane-Based Pose Estimation" (CITE: Collins14). This method is suitable for marker pose estimation.
+     * It requires 4 coplanar object points defined in the following order:
+     *   <ul>
+     *     <li>
+     *    point 0: [-squareLength / 2,  squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *    point 1: [ squareLength / 2,  squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *    point 2: [ squareLength / 2, -squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *    point 3: [-squareLength / 2, -squareLength / 2, 0]
+     * and useExtrinsicGuess is set to true.
+     * and useExtrinsicGuess is set to true.
+     * (\( \text{RMSE} = \sqrt{\frac{\sum_{i}^{N} \left ( \hat{y_i} - y_i \right )^2}{N}} \)) between the input image points
+     * and the 3D object points projected with the estimated pose.
+     *     </li>
+     *   </ul>
+     *
+     * The function estimates the object pose given a set of object points, their corresponding image
+     * projections, as well as the camera matrix and the distortion coefficients, see the figure below
+     * (more precisely, the X-axis of the camera frame is pointing to the right, the Y-axis downward
+     * and the Z-axis forward).
+     *   </li>
+     * </ul>
+     *
+     * ![](pnp.jpg)
+     *
+     * Points expressed in the world frame \( \bf{X}_w \) are projected into the image plane \( \left[ u, v \right] \)
+     * using the perspective projection model \( \Pi \) and the camera intrinsic parameters matrix \( \bf{A} \):
+     *
+     * \(
+     *   \begin{align*}
+     *   \begin{bmatrix}
+     *   u \\
+     *   v \\
+     *   1
+     *   \end{bmatrix} &amp;=
+     *   \bf{A} \hspace{0.1em} \Pi \hspace{0.2em} ^{c}\bf{T}_w
+     *   \begin{bmatrix}
+     *   X_{w} \\
+     *   Y_{w} \\
+     *   Z_{w} \\
+     *   1
+     *   \end{bmatrix} \\
+     *   \begin{bmatrix}
+     *   u \\
+     *   v \\
+     *   1
+     *   \end{bmatrix} &amp;=
+     *   \begin{bmatrix}
+     *   f_x &amp; 0 &amp; c_x \\
+     *   0 &amp; f_y &amp; c_y \\
+     *   0 &amp; 0 &amp; 1
+     *   \end{bmatrix}
+     *   \begin{bmatrix}
+     *   1 &amp; 0 &amp; 0 &amp; 0 \\
+     *   0 &amp; 1 &amp; 0 &amp; 0 \\
+     *   0 &amp; 0 &amp; 1 &amp; 0
+     *   \end{bmatrix}
+     *   \begin{bmatrix}
+     *   r_{11} &amp; r_{12} &amp; r_{13} &amp; t_x \\
+     *   r_{21} &amp; r_{22} &amp; r_{23} &amp; t_y \\
+     *   r_{31} &amp; r_{32} &amp; r_{33} &amp; t_z \\
+     *   0 &amp; 0 &amp; 0 &amp; 1
+     *   \end{bmatrix}
+     *   \begin{bmatrix}
+     *   X_{w} \\
+     *   Y_{w} \\
+     *   Z_{w} \\
+     *   1
+     *   \end{bmatrix}
+     *   \end{align*}
+     * \)
+     *
+     * The estimated pose is thus the rotation ({@code rvec}) and the translation ({@code tvec}) vectors that allow transforming
+     * a 3D point expressed in the world frame into the camera frame:
+     *
+     * \(
+     *   \begin{align*}
+     *   \begin{bmatrix}
+     *   X_c \\
+     *   Y_c \\
+     *   Z_c \\
+     *   1
+     *   \end{bmatrix} &amp;=
+     *   \hspace{0.2em} ^{c}\bf{T}_w
+     *   \begin{bmatrix}
+     *   X_{w} \\
+     *   Y_{w} \\
+     *   Z_{w} \\
+     *   1
+     *   \end{bmatrix} \\
+     *   \begin{bmatrix}
+     *   X_c \\
+     *   Y_c \\
+     *   Z_c \\
+     *   1
+     *   \end{bmatrix} &amp;=
+     *   \begin{bmatrix}
+     *   r_{11} &amp; r_{12} &amp; r_{13} &amp; t_x \\
+     *   r_{21} &amp; r_{22} &amp; r_{23} &amp; t_y \\
+     *   r_{31} &amp; r_{32} &amp; r_{33} &amp; t_z \\
+     *   0 &amp; 0 &amp; 0 &amp; 1
+     *   \end{bmatrix}
+     *   \begin{bmatrix}
+     *   X_{w} \\
+     *   Y_{w} \\
+     *   Z_{w} \\
+     *   1
+     *   \end{bmatrix}
+     *   \end{align*}
+     * \)
+     *
+     * <b>Note:</b>
+     * <ul>
+     *   <li>
+     *       An example of how to use solvePnP for planar augmented reality can be found at
+     *         opencv_source_code/samples/python/plane_ar.py
+     *   </li>
+     *   <li>
+     *       If you are using Python:
+     *   <ul>
+     *     <li>
+     *          Numpy array slices won't work as input because solvePnP requires contiguous
+     *         arrays (enforced by the assertion using cv::Mat::checkVector() around line 55 of
+     *         modules/calib3d/src/solvepnp.cpp version 2.4.9)
+     *     </li>
+     *     <li>
+     *          The P3P algorithm requires image points to be in an array of shape (N,1,2) due
+     *         to its calling of cv::undistortPoints (around line 75 of modules/calib3d/src/solvepnp.cpp version 2.4.9)
+     *         which requires 2-channel information.
+     *     </li>
+     *     <li>
+     *          Thus, given some data D = np.array(...) where D.shape = (N,M), in order to use a subset of
+     *         it as, e.g., imagePoints, one must effectively copy it into a new array: imagePoints =
+     *         np.ascontiguousarray(D[:,:2]).reshape((N,1,2))
+     *     </li>
+     *   </ul>
+     *   <li>
+     *       The methods <b>SOLVEPNP_DLS</b> and <b>SOLVEPNP_UPNP</b> cannot be used as the current implementations are
+     *        unstable and sometimes give completely wrong results. If you pass one of these two
+     *        flags, <b>SOLVEPNP_EPNP</b> method will be used instead.
+     *   </li>
+     *   <li>
+     *       The minimum number of points is 4 in the general case. In the case of <b>SOLVEPNP_P3P</b> and <b>SOLVEPNP_AP3P</b>
+     *        methods, it is required to use exactly 4 points (the first 3 points are used to estimate all the solutions
+     *        of the P3P problem, the last one is used to retain the best solution that minimizes the reprojection error).
+     *   </li>
+     *   <li>
+     *       With <b>SOLVEPNP_ITERATIVE</b> method and {@code useExtrinsicGuess=true}, the minimum number of points is 3 (3 points
+     *        are sufficient to compute a pose but there are up to 4 solutions). The initial solution should be close to the
+     *        global solution to converge.
+     *   </li>
+     *   <li>
+     *       With <b>SOLVEPNP_IPPE</b> input points must be &gt;= 4 and object points must be coplanar.
+     *   </li>
+     *   <li>
+     *       With <b>SOLVEPNP_IPPE_SQUARE</b> this is a special case suitable for marker pose estimation.
+     *        Number of input points must be 4. Object points must be defined in the following order:
+     *   <ul>
+     *     <li>
+     *           point 0: [-squareLength / 2,  squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *           point 1: [ squareLength / 2,  squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *           point 2: [ squareLength / 2, -squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *           point 3: [-squareLength / 2, -squareLength / 2, 0]
+     *     </li>
+     *   </ul>
+     *   </li>
+     * </ul>
+     * @return automatically generated
+     */
+    public static int solvePnPGeneric(Mat objectPoints, Mat imagePoints, Mat cameraMatrix, Mat distCoeffs, List<Mat> rvecs, List<Mat> tvecs, boolean useExtrinsicGuess, int flags) {
+        Mat rvecs_mat = new Mat();
+        Mat tvecs_mat = new Mat();
+        int retVal = solvePnPGeneric_3(objectPoints.nativeObj, imagePoints.nativeObj, cameraMatrix.nativeObj, distCoeffs.nativeObj, rvecs_mat.nativeObj, tvecs_mat.nativeObj, useExtrinsicGuess, flags);
+        Converters.Mat_to_vector_Mat(rvecs_mat, rvecs);
+        rvecs_mat.release();
+        Converters.Mat_to_vector_Mat(tvecs_mat, tvecs);
+        tvecs_mat.release();
+        return retVal;
+    }
+
+    /**
+     * Finds an object pose from 3D-2D point correspondences.
+     * This function returns a list of all the possible solutions (a solution is a &lt;rotation vector, translation vector&gt;
+     * couple), depending on the number of input points and the chosen method:
+     * <ul>
+     *   <li>
+     *  P3P methods (REF: SOLVEPNP_P3P, REF: SOLVEPNP_AP3P): 3 or 4 input points. Number of returned solutions can be between 0 and 4 with 3 input points.
+     *   </li>
+     *   <li>
+     *  REF: SOLVEPNP_IPPE Input points must be &gt;= 4 and object points must be coplanar. Returns 2 solutions.
+     *   </li>
+     *   <li>
+     *  REF: SOLVEPNP_IPPE_SQUARE Special case suitable for marker pose estimation.
+     * Number of input points must be 4 and 2 solutions are returned. Object points must be defined in the following order:
+     *   <ul>
+     *     <li>
+     *    point 0: [-squareLength / 2,  squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *    point 1: [ squareLength / 2,  squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *    point 2: [ squareLength / 2, -squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *    point 3: [-squareLength / 2, -squareLength / 2, 0]
+     *     </li>
+     *   </ul>
+     *   <li>
+     *  for all the other flags, number of input points must be &gt;= 4 and object points can be in any configuration.
+     * Only 1 solution is returned.
+     *   </li>
+     * </ul>
+     *
+     * @param objectPoints Array of object points in the object coordinate space, Nx3 1-channel or
+     * 1xN/Nx1 3-channel, where N is the number of points. vector&lt;Point3d&gt; can be also passed here.
+     * @param imagePoints Array of corresponding image points, Nx2 1-channel or 1xN/Nx1 2-channel,
+     * where N is the number of points. vector&lt;Point2d&gt; can be also passed here.
+     * @param cameraMatrix Input camera matrix \(A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}\) .
+     * @param distCoeffs Input vector of distortion coefficients
+     * \((k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\) of
+     * 4, 5, 8, 12 or 14 elements. If the vector is NULL/empty, the zero distortion coefficients are
+     * assumed.
+     * @param rvecs Vector of output rotation vectors (see REF: Rodrigues ) that, together with tvecs, brings points from
+     * the model coordinate system to the camera coordinate system.
+     * @param tvecs Vector of output translation vectors.
+     * @param useExtrinsicGuess Parameter used for #SOLVEPNP_ITERATIVE. If true (1), the function uses
+     * the provided rvec and tvec values as initial approximations of the rotation and translation
+     * vectors, respectively, and further optimizes them.
+     * <ul>
+     *   <li>
+     *    <b>SOLVEPNP_ITERATIVE</b> Iterative method is based on a Levenberg-Marquardt optimization. In
+     * this case the function finds such a pose that minimizes reprojection error, that is the sum
+     * of squared distances between the observed projections imagePoints and the projected (using
+     * projectPoints ) objectPoints .
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_P3P</b> Method is based on the paper of X.S. Gao, X.-R. Hou, J. Tang, H.-F. Chang
+     * "Complete Solution Classification for the Perspective-Three-Point Problem" (CITE: gao2003complete).
+     * In this case the function requires exactly four object and image points.
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_AP3P</b> Method is based on the paper of T. Ke, S. Roumeliotis
+     * "An Efficient Algebraic Solution to the Perspective-Three-Point Problem" (CITE: Ke17).
+     * In this case the function requires exactly four object and image points.
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_EPNP</b> Method has been introduced by F.Moreno-Noguer, V.Lepetit and P.Fua in the
+     * paper "EPnP: Efficient Perspective-n-Point Camera Pose Estimation" (CITE: lepetit2009epnp).
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_DLS</b> Method is based on the paper of Joel A. Hesch and Stergios I. Roumeliotis.
+     * "A Direct Least-Squares (DLS) Method for PnP" (CITE: hesch2011direct).
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_UPNP</b> Method is based on the paper of A.Penate-Sanchez, J.Andrade-Cetto,
+     * F.Moreno-Noguer. "Exhaustive Linearization for Robust Camera Pose and Focal Length
+     * Estimation" (CITE: penate2013exhaustive). In this case the function also estimates the parameters \(f_x\) and \(f_y\)
+     * assuming that both have the same value. Then the cameraMatrix is updated with the estimated
+     * focal length.
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_IPPE</b> Method is based on the paper of T. Collins and A. Bartoli.
+     * "Infinitesimal Plane-Based Pose Estimation" (CITE: Collins14). This method requires coplanar object points.
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_IPPE_SQUARE</b> Method is based on the paper of Toby Collins and Adrien Bartoli.
+     * "Infinitesimal Plane-Based Pose Estimation" (CITE: Collins14). This method is suitable for marker pose estimation.
+     * It requires 4 coplanar object points defined in the following order:
+     *   <ul>
+     *     <li>
+     *    point 0: [-squareLength / 2,  squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *    point 1: [ squareLength / 2,  squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *    point 2: [ squareLength / 2, -squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *    point 3: [-squareLength / 2, -squareLength / 2, 0]
+     * and useExtrinsicGuess is set to true.
+     * and useExtrinsicGuess is set to true.
+     * (\( \text{RMSE} = \sqrt{\frac{\sum_{i}^{N} \left ( \hat{y_i} - y_i \right )^2}{N}} \)) between the input image points
+     * and the 3D object points projected with the estimated pose.
+     *     </li>
+     *   </ul>
+     *
+     * The function estimates the object pose given a set of object points, their corresponding image
+     * projections, as well as the camera matrix and the distortion coefficients, see the figure below
+     * (more precisely, the X-axis of the camera frame is pointing to the right, the Y-axis downward
+     * and the Z-axis forward).
+     *   </li>
+     * </ul>
+     *
+     * ![](pnp.jpg)
+     *
+     * Points expressed in the world frame \( \bf{X}_w \) are projected into the image plane \( \left[ u, v \right] \)
+     * using the perspective projection model \( \Pi \) and the camera intrinsic parameters matrix \( \bf{A} \):
+     *
+     * \(
+     *   \begin{align*}
+     *   \begin{bmatrix}
+     *   u \\
+     *   v \\
+     *   1
+     *   \end{bmatrix} &amp;=
+     *   \bf{A} \hspace{0.1em} \Pi \hspace{0.2em} ^{c}\bf{T}_w
+     *   \begin{bmatrix}
+     *   X_{w} \\
+     *   Y_{w} \\
+     *   Z_{w} \\
+     *   1
+     *   \end{bmatrix} \\
+     *   \begin{bmatrix}
+     *   u \\
+     *   v \\
+     *   1
+     *   \end{bmatrix} &amp;=
+     *   \begin{bmatrix}
+     *   f_x &amp; 0 &amp; c_x \\
+     *   0 &amp; f_y &amp; c_y \\
+     *   0 &amp; 0 &amp; 1
+     *   \end{bmatrix}
+     *   \begin{bmatrix}
+     *   1 &amp; 0 &amp; 0 &amp; 0 \\
+     *   0 &amp; 1 &amp; 0 &amp; 0 \\
+     *   0 &amp; 0 &amp; 1 &amp; 0
+     *   \end{bmatrix}
+     *   \begin{bmatrix}
+     *   r_{11} &amp; r_{12} &amp; r_{13} &amp; t_x \\
+     *   r_{21} &amp; r_{22} &amp; r_{23} &amp; t_y \\
+     *   r_{31} &amp; r_{32} &amp; r_{33} &amp; t_z \\
+     *   0 &amp; 0 &amp; 0 &amp; 1
+     *   \end{bmatrix}
+     *   \begin{bmatrix}
+     *   X_{w} \\
+     *   Y_{w} \\
+     *   Z_{w} \\
+     *   1
+     *   \end{bmatrix}
+     *   \end{align*}
+     * \)
+     *
+     * The estimated pose is thus the rotation ({@code rvec}) and the translation ({@code tvec}) vectors that allow transforming
+     * a 3D point expressed in the world frame into the camera frame:
+     *
+     * \(
+     *   \begin{align*}
+     *   \begin{bmatrix}
+     *   X_c \\
+     *   Y_c \\
+     *   Z_c \\
+     *   1
+     *   \end{bmatrix} &amp;=
+     *   \hspace{0.2em} ^{c}\bf{T}_w
+     *   \begin{bmatrix}
+     *   X_{w} \\
+     *   Y_{w} \\
+     *   Z_{w} \\
+     *   1
+     *   \end{bmatrix} \\
+     *   \begin{bmatrix}
+     *   X_c \\
+     *   Y_c \\
+     *   Z_c \\
+     *   1
+     *   \end{bmatrix} &amp;=
+     *   \begin{bmatrix}
+     *   r_{11} &amp; r_{12} &amp; r_{13} &amp; t_x \\
+     *   r_{21} &amp; r_{22} &amp; r_{23} &amp; t_y \\
+     *   r_{31} &amp; r_{32} &amp; r_{33} &amp; t_z \\
+     *   0 &amp; 0 &amp; 0 &amp; 1
+     *   \end{bmatrix}
+     *   \begin{bmatrix}
+     *   X_{w} \\
+     *   Y_{w} \\
+     *   Z_{w} \\
+     *   1
+     *   \end{bmatrix}
+     *   \end{align*}
+     * \)
+     *
+     * <b>Note:</b>
+     * <ul>
+     *   <li>
+     *       An example of how to use solvePnP for planar augmented reality can be found at
+     *         opencv_source_code/samples/python/plane_ar.py
+     *   </li>
+     *   <li>
+     *       If you are using Python:
+     *   <ul>
+     *     <li>
+     *          Numpy array slices won't work as input because solvePnP requires contiguous
+     *         arrays (enforced by the assertion using cv::Mat::checkVector() around line 55 of
+     *         modules/calib3d/src/solvepnp.cpp version 2.4.9)
+     *     </li>
+     *     <li>
+     *          The P3P algorithm requires image points to be in an array of shape (N,1,2) due
+     *         to its calling of cv::undistortPoints (around line 75 of modules/calib3d/src/solvepnp.cpp version 2.4.9)
+     *         which requires 2-channel information.
+     *     </li>
+     *     <li>
+     *          Thus, given some data D = np.array(...) where D.shape = (N,M), in order to use a subset of
+     *         it as, e.g., imagePoints, one must effectively copy it into a new array: imagePoints =
+     *         np.ascontiguousarray(D[:,:2]).reshape((N,1,2))
+     *     </li>
+     *   </ul>
+     *   <li>
+     *       The methods <b>SOLVEPNP_DLS</b> and <b>SOLVEPNP_UPNP</b> cannot be used as the current implementations are
+     *        unstable and sometimes give completely wrong results. If you pass one of these two
+     *        flags, <b>SOLVEPNP_EPNP</b> method will be used instead.
+     *   </li>
+     *   <li>
+     *       The minimum number of points is 4 in the general case. In the case of <b>SOLVEPNP_P3P</b> and <b>SOLVEPNP_AP3P</b>
+     *        methods, it is required to use exactly 4 points (the first 3 points are used to estimate all the solutions
+     *        of the P3P problem, the last one is used to retain the best solution that minimizes the reprojection error).
+     *   </li>
+     *   <li>
+     *       With <b>SOLVEPNP_ITERATIVE</b> method and {@code useExtrinsicGuess=true}, the minimum number of points is 3 (3 points
+     *        are sufficient to compute a pose but there are up to 4 solutions). The initial solution should be close to the
+     *        global solution to converge.
+     *   </li>
+     *   <li>
+     *       With <b>SOLVEPNP_IPPE</b> input points must be &gt;= 4 and object points must be coplanar.
+     *   </li>
+     *   <li>
+     *       With <b>SOLVEPNP_IPPE_SQUARE</b> this is a special case suitable for marker pose estimation.
+     *        Number of input points must be 4. Object points must be defined in the following order:
+     *   <ul>
+     *     <li>
+     *           point 0: [-squareLength / 2,  squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *           point 1: [ squareLength / 2,  squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *           point 2: [ squareLength / 2, -squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *           point 3: [-squareLength / 2, -squareLength / 2, 0]
+     *     </li>
+     *   </ul>
+     *   </li>
+     * </ul>
+     * @return automatically generated
+     */
+    public static int solvePnPGeneric(Mat objectPoints, Mat imagePoints, Mat cameraMatrix, Mat distCoeffs, List<Mat> rvecs, List<Mat> tvecs, boolean useExtrinsicGuess) {
+        Mat rvecs_mat = new Mat();
+        Mat tvecs_mat = new Mat();
+        int retVal = solvePnPGeneric_4(objectPoints.nativeObj, imagePoints.nativeObj, cameraMatrix.nativeObj, distCoeffs.nativeObj, rvecs_mat.nativeObj, tvecs_mat.nativeObj, useExtrinsicGuess);
+        Converters.Mat_to_vector_Mat(rvecs_mat, rvecs);
+        rvecs_mat.release();
+        Converters.Mat_to_vector_Mat(tvecs_mat, tvecs);
+        tvecs_mat.release();
+        return retVal;
+    }
+
+    /**
+     * Finds an object pose from 3D-2D point correspondences.
+     * This function returns a list of all the possible solutions (a solution is a &lt;rotation vector, translation vector&gt;
+     * couple), depending on the number of input points and the chosen method:
+     * <ul>
+     *   <li>
+     *  P3P methods (REF: SOLVEPNP_P3P, REF: SOLVEPNP_AP3P): 3 or 4 input points. Number of returned solutions can be between 0 and 4 with 3 input points.
+     *   </li>
+     *   <li>
+     *  REF: SOLVEPNP_IPPE Input points must be &gt;= 4 and object points must be coplanar. Returns 2 solutions.
+     *   </li>
+     *   <li>
+     *  REF: SOLVEPNP_IPPE_SQUARE Special case suitable for marker pose estimation.
+     * Number of input points must be 4 and 2 solutions are returned. Object points must be defined in the following order:
+     *   <ul>
+     *     <li>
+     *    point 0: [-squareLength / 2,  squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *    point 1: [ squareLength / 2,  squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *    point 2: [ squareLength / 2, -squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *    point 3: [-squareLength / 2, -squareLength / 2, 0]
+     *     </li>
+     *   </ul>
+     *   <li>
+     *  for all the other flags, number of input points must be &gt;= 4 and object points can be in any configuration.
+     * Only 1 solution is returned.
+     *   </li>
+     * </ul>
+     *
+     * @param objectPoints Array of object points in the object coordinate space, Nx3 1-channel or
+     * 1xN/Nx1 3-channel, where N is the number of points. vector&lt;Point3d&gt; can be also passed here.
+     * @param imagePoints Array of corresponding image points, Nx2 1-channel or 1xN/Nx1 2-channel,
+     * where N is the number of points. vector&lt;Point2d&gt; can be also passed here.
+     * @param cameraMatrix Input camera matrix \(A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}\) .
+     * @param distCoeffs Input vector of distortion coefficients
+     * \((k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\) of
+     * 4, 5, 8, 12 or 14 elements. If the vector is NULL/empty, the zero distortion coefficients are
+     * assumed.
+     * @param rvecs Vector of output rotation vectors (see REF: Rodrigues ) that, together with tvecs, brings points from
+     * the model coordinate system to the camera coordinate system.
+     * @param tvecs Vector of output translation vectors.
+     * the provided rvec and tvec values as initial approximations of the rotation and translation
+     * vectors, respectively, and further optimizes them.
+     * <ul>
+     *   <li>
+     *    <b>SOLVEPNP_ITERATIVE</b> Iterative method is based on a Levenberg-Marquardt optimization. In
+     * this case the function finds such a pose that minimizes reprojection error, that is the sum
+     * of squared distances between the observed projections imagePoints and the projected (using
+     * projectPoints ) objectPoints .
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_P3P</b> Method is based on the paper of X.S. Gao, X.-R. Hou, J. Tang, H.-F. Chang
+     * "Complete Solution Classification for the Perspective-Three-Point Problem" (CITE: gao2003complete).
+     * In this case the function requires exactly four object and image points.
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_AP3P</b> Method is based on the paper of T. Ke, S. Roumeliotis
+     * "An Efficient Algebraic Solution to the Perspective-Three-Point Problem" (CITE: Ke17).
+     * In this case the function requires exactly four object and image points.
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_EPNP</b> Method has been introduced by F.Moreno-Noguer, V.Lepetit and P.Fua in the
+     * paper "EPnP: Efficient Perspective-n-Point Camera Pose Estimation" (CITE: lepetit2009epnp).
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_DLS</b> Method is based on the paper of Joel A. Hesch and Stergios I. Roumeliotis.
+     * "A Direct Least-Squares (DLS) Method for PnP" (CITE: hesch2011direct).
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_UPNP</b> Method is based on the paper of A.Penate-Sanchez, J.Andrade-Cetto,
+     * F.Moreno-Noguer. "Exhaustive Linearization for Robust Camera Pose and Focal Length
+     * Estimation" (CITE: penate2013exhaustive). In this case the function also estimates the parameters \(f_x\) and \(f_y\)
+     * assuming that both have the same value. Then the cameraMatrix is updated with the estimated
+     * focal length.
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_IPPE</b> Method is based on the paper of T. Collins and A. Bartoli.
+     * "Infinitesimal Plane-Based Pose Estimation" (CITE: Collins14). This method requires coplanar object points.
+     *   </li>
+     *   <li>
+     *    <b>SOLVEPNP_IPPE_SQUARE</b> Method is based on the paper of Toby Collins and Adrien Bartoli.
+     * "Infinitesimal Plane-Based Pose Estimation" (CITE: Collins14). This method is suitable for marker pose estimation.
+     * It requires 4 coplanar object points defined in the following order:
+     *   <ul>
+     *     <li>
+     *    point 0: [-squareLength / 2,  squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *    point 1: [ squareLength / 2,  squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *    point 2: [ squareLength / 2, -squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *    point 3: [-squareLength / 2, -squareLength / 2, 0]
+     * and useExtrinsicGuess is set to true.
+     * and useExtrinsicGuess is set to true.
+     * (\( \text{RMSE} = \sqrt{\frac{\sum_{i}^{N} \left ( \hat{y_i} - y_i \right )^2}{N}} \)) between the input image points
+     * and the 3D object points projected with the estimated pose.
+     *     </li>
+     *   </ul>
+     *
+     * The function estimates the object pose given a set of object points, their corresponding image
+     * projections, as well as the camera matrix and the distortion coefficients, see the figure below
+     * (more precisely, the X-axis of the camera frame is pointing to the right, the Y-axis downward
+     * and the Z-axis forward).
+     *   </li>
+     * </ul>
+     *
+     * ![](pnp.jpg)
+     *
+     * Points expressed in the world frame \( \bf{X}_w \) are projected into the image plane \( \left[ u, v \right] \)
+     * using the perspective projection model \( \Pi \) and the camera intrinsic parameters matrix \( \bf{A} \):
+     *
+     * \(
+     *   \begin{align*}
+     *   \begin{bmatrix}
+     *   u \\
+     *   v \\
+     *   1
+     *   \end{bmatrix} &amp;=
+     *   \bf{A} \hspace{0.1em} \Pi \hspace{0.2em} ^{c}\bf{T}_w
+     *   \begin{bmatrix}
+     *   X_{w} \\
+     *   Y_{w} \\
+     *   Z_{w} \\
+     *   1
+     *   \end{bmatrix} \\
+     *   \begin{bmatrix}
+     *   u \\
+     *   v \\
+     *   1
+     *   \end{bmatrix} &amp;=
+     *   \begin{bmatrix}
+     *   f_x &amp; 0 &amp; c_x \\
+     *   0 &amp; f_y &amp; c_y \\
+     *   0 &amp; 0 &amp; 1
+     *   \end{bmatrix}
+     *   \begin{bmatrix}
+     *   1 &amp; 0 &amp; 0 &amp; 0 \\
+     *   0 &amp; 1 &amp; 0 &amp; 0 \\
+     *   0 &amp; 0 &amp; 1 &amp; 0
+     *   \end{bmatrix}
+     *   \begin{bmatrix}
+     *   r_{11} &amp; r_{12} &amp; r_{13} &amp; t_x \\
+     *   r_{21} &amp; r_{22} &amp; r_{23} &amp; t_y \\
+     *   r_{31} &amp; r_{32} &amp; r_{33} &amp; t_z \\
+     *   0 &amp; 0 &amp; 0 &amp; 1
+     *   \end{bmatrix}
+     *   \begin{bmatrix}
+     *   X_{w} \\
+     *   Y_{w} \\
+     *   Z_{w} \\
+     *   1
+     *   \end{bmatrix}
+     *   \end{align*}
+     * \)
+     *
+     * The estimated pose is thus the rotation ({@code rvec}) and the translation ({@code tvec}) vectors that allow transforming
+     * a 3D point expressed in the world frame into the camera frame:
+     *
+     * \(
+     *   \begin{align*}
+     *   \begin{bmatrix}
+     *   X_c \\
+     *   Y_c \\
+     *   Z_c \\
+     *   1
+     *   \end{bmatrix} &amp;=
+     *   \hspace{0.2em} ^{c}\bf{T}_w
+     *   \begin{bmatrix}
+     *   X_{w} \\
+     *   Y_{w} \\
+     *   Z_{w} \\
+     *   1
+     *   \end{bmatrix} \\
+     *   \begin{bmatrix}
+     *   X_c \\
+     *   Y_c \\
+     *   Z_c \\
+     *   1
+     *   \end{bmatrix} &amp;=
+     *   \begin{bmatrix}
+     *   r_{11} &amp; r_{12} &amp; r_{13} &amp; t_x \\
+     *   r_{21} &amp; r_{22} &amp; r_{23} &amp; t_y \\
+     *   r_{31} &amp; r_{32} &amp; r_{33} &amp; t_z \\
+     *   0 &amp; 0 &amp; 0 &amp; 1
+     *   \end{bmatrix}
+     *   \begin{bmatrix}
+     *   X_{w} \\
+     *   Y_{w} \\
+     *   Z_{w} \\
+     *   1
+     *   \end{bmatrix}
+     *   \end{align*}
+     * \)
+     *
+     * <b>Note:</b>
+     * <ul>
+     *   <li>
+     *       An example of how to use solvePnP for planar augmented reality can be found at
+     *         opencv_source_code/samples/python/plane_ar.py
+     *   </li>
+     *   <li>
+     *       If you are using Python:
+     *   <ul>
+     *     <li>
+     *          Numpy array slices won't work as input because solvePnP requires contiguous
+     *         arrays (enforced by the assertion using cv::Mat::checkVector() around line 55 of
+     *         modules/calib3d/src/solvepnp.cpp version 2.4.9)
+     *     </li>
+     *     <li>
+     *          The P3P algorithm requires image points to be in an array of shape (N,1,2) due
+     *         to its calling of cv::undistortPoints (around line 75 of modules/calib3d/src/solvepnp.cpp version 2.4.9)
+     *         which requires 2-channel information.
+     *     </li>
+     *     <li>
+     *          Thus, given some data D = np.array(...) where D.shape = (N,M), in order to use a subset of
+     *         it as, e.g., imagePoints, one must effectively copy it into a new array: imagePoints =
+     *         np.ascontiguousarray(D[:,:2]).reshape((N,1,2))
+     *     </li>
+     *   </ul>
+     *   <li>
+     *       The methods <b>SOLVEPNP_DLS</b> and <b>SOLVEPNP_UPNP</b> cannot be used as the current implementations are
+     *        unstable and sometimes give completely wrong results. If you pass one of these two
+     *        flags, <b>SOLVEPNP_EPNP</b> method will be used instead.
+     *   </li>
+     *   <li>
+     *       The minimum number of points is 4 in the general case. In the case of <b>SOLVEPNP_P3P</b> and <b>SOLVEPNP_AP3P</b>
+     *        methods, it is required to use exactly 4 points (the first 3 points are used to estimate all the solutions
+     *        of the P3P problem, the last one is used to retain the best solution that minimizes the reprojection error).
+     *   </li>
+     *   <li>
+     *       With <b>SOLVEPNP_ITERATIVE</b> method and {@code useExtrinsicGuess=true}, the minimum number of points is 3 (3 points
+     *        are sufficient to compute a pose but there are up to 4 solutions). The initial solution should be close to the
+     *        global solution to converge.
+     *   </li>
+     *   <li>
+     *       With <b>SOLVEPNP_IPPE</b> input points must be &gt;= 4 and object points must be coplanar.
+     *   </li>
+     *   <li>
+     *       With <b>SOLVEPNP_IPPE_SQUARE</b> this is a special case suitable for marker pose estimation.
+     *        Number of input points must be 4. Object points must be defined in the following order:
+     *   <ul>
+     *     <li>
+     *           point 0: [-squareLength / 2,  squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *           point 1: [ squareLength / 2,  squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *           point 2: [ squareLength / 2, -squareLength / 2, 0]
+     *     </li>
+     *     <li>
+     *           point 3: [-squareLength / 2, -squareLength / 2, 0]
+     *     </li>
+     *   </ul>
+     *   </li>
+     * </ul>
+     * @return automatically generated
+     */
+    public static int solvePnPGeneric(Mat objectPoints, Mat imagePoints, Mat cameraMatrix, Mat distCoeffs, List<Mat> rvecs, List<Mat> tvecs) {
+        Mat rvecs_mat = new Mat();
+        Mat tvecs_mat = new Mat();
+        int retVal = solvePnPGeneric_5(objectPoints.nativeObj, imagePoints.nativeObj, cameraMatrix.nativeObj, distCoeffs.nativeObj, rvecs_mat.nativeObj, tvecs_mat.nativeObj);
+        Converters.Mat_to_vector_Mat(rvecs_mat, rvecs);
+        rvecs_mat.release();
+        Converters.Mat_to_vector_Mat(tvecs_mat, tvecs);
+        tvecs_mat.release();
+        return retVal;
+    }
+
+
+    //
+    // C++:  void cv::Rodrigues(Mat src, Mat& dst, Mat& jacobian = Mat())
+    //
+
+    /**
+     * Converts a rotation matrix to a rotation vector or vice versa.
+     *
+     * @param src Input rotation vector (3x1 or 1x3) or rotation matrix (3x3).
+     * @param dst Output rotation matrix (3x3) or rotation vector (3x1 or 1x3), respectively.
+     * @param jacobian Optional output Jacobian matrix, 3x9 or 9x3, which is a matrix of partial
+     * derivatives of the output array components with respect to the input array components.
+     *
+     * \(\begin{array}{l} \theta \leftarrow norm(r) \\ r  \leftarrow r/ \theta \\ R =  \cos(\theta) I + (1- \cos{\theta} ) r r^T +  \sin(\theta) \vecthreethree{0}{-r_z}{r_y}{r_z}{0}{-r_x}{-r_y}{r_x}{0} \end{array}\)
+     *
+     * Inverse transformation can be also done easily, since
+     *
+     * \(\sin ( \theta ) \vecthreethree{0}{-r_z}{r_y}{r_z}{0}{-r_x}{-r_y}{r_x}{0} = \frac{R - R^T}{2}\)
+     *
+     * A rotation vector is a convenient and most compact representation of a rotation matrix (since any
+     * rotation matrix has just 3 degrees of freedom). The representation is used in the global 3D geometry
+     * optimization procedures like REF: calibrateCamera, REF: stereoCalibrate, or REF: solvePnP .
+     *
+     * <b>Note:</b> More information about the computation of the derivative of a 3D rotation matrix with respect to its exponential coordinate
+     * can be found in:
+     * <ul>
+     *   <li>
+     *      A Compact Formula for the Derivative of a 3-D Rotation in Exponential Coordinates, Guillermo Gallego, Anthony J. Yezzi CITE: Gallego2014ACF
+     *   </li>
+     * </ul>
+     *
+     * <b>Note:</b> Useful information on SE(3) and Lie Groups can be found in:
+     * <ul>
+     *   <li>
+     *      A tutorial on SE(3) transformation parameterizations and on-manifold optimization, Jose-Luis Blanco CITE: blanco2010tutorial
+     *   </li>
+     *   <li>
+     *      Lie Groups for 2D and 3D Transformation, Ethan Eade CITE: Eade17
+     *   </li>
+     *   <li>
+     *      A micro Lie theory for state estimation in robotics, Joan Sol, Jrmie Deray, Dinesh Atchuthan CITE: Sol2018AML
+     *   </li>
+     * </ul>
+     */
+    public static void Rodrigues(Mat src, Mat dst, Mat jacobian) {
+        Rodrigues_0(src.nativeObj, dst.nativeObj, jacobian.nativeObj);
+    }
+
+    /**
+     * Converts a rotation matrix to a rotation vector or vice versa.
+     *
+     * @param src Input rotation vector (3x1 or 1x3) or rotation matrix (3x3).
+     * @param dst Output rotation matrix (3x3) or rotation vector (3x1 or 1x3), respectively.
+     * derivatives of the output array components with respect to the input array components.
+     *
+     * \(\begin{array}{l} \theta \leftarrow norm(r) \\ r  \leftarrow r/ \theta \\ R =  \cos(\theta) I + (1- \cos{\theta} ) r r^T +  \sin(\theta) \vecthreethree{0}{-r_z}{r_y}{r_z}{0}{-r_x}{-r_y}{r_x}{0} \end{array}\)
+     *
+     * Inverse transformation can be also done easily, since
+     *
+     * \(\sin ( \theta ) \vecthreethree{0}{-r_z}{r_y}{r_z}{0}{-r_x}{-r_y}{r_x}{0} = \frac{R - R^T}{2}\)
+     *
+     * A rotation vector is a convenient and most compact representation of a rotation matrix (since any
+     * rotation matrix has just 3 degrees of freedom). The representation is used in the global 3D geometry
+     * optimization procedures like REF: calibrateCamera, REF: stereoCalibrate, or REF: solvePnP .
+     *
+     * <b>Note:</b> More information about the computation of the derivative of a 3D rotation matrix with respect to its exponential coordinate
+     * can be found in:
+     * <ul>
+     *   <li>
+     *      A Compact Formula for the Derivative of a 3-D Rotation in Exponential Coordinates, Guillermo Gallego, Anthony J. Yezzi CITE: Gallego2014ACF
+     *   </li>
+     * </ul>
+     *
+     * <b>Note:</b> Useful information on SE(3) and Lie Groups can be found in:
+     * <ul>
+     *   <li>
+     *      A tutorial on SE(3) transformation parameterizations and on-manifold optimization, Jose-Luis Blanco CITE: blanco2010tutorial
+     *   </li>
+     *   <li>
+     *      Lie Groups for 2D and 3D Transformation, Ethan Eade CITE: Eade17
+     *   </li>
+     *   <li>
+     *      A micro Lie theory for state estimation in robotics, Joan Sol, Jrmie Deray, Dinesh Atchuthan CITE: Sol2018AML
+     *   </li>
+     * </ul>
+     */
+    public static void Rodrigues(Mat src, Mat dst) {
+        Rodrigues_1(src.nativeObj, dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::calibrateHandEye(vector_Mat R_gripper2base, vector_Mat t_gripper2base, vector_Mat R_target2cam, vector_Mat t_target2cam, Mat& R_cam2gripper, Mat& t_cam2gripper, HandEyeCalibrationMethod method = CALIB_HAND_EYE_TSAI)
+    //
+
+    /**
+     * Computes Hand-Eye calibration: \(_{}^{g}\textrm{T}_c\)
+     *
+     * @param R_gripper2base Rotation part extracted from the homogeneous matrix that transforms a point
+     * expressed in the gripper frame to the robot base frame (\(_{}^{b}\textrm{T}_g\)).
+     * This is a vector ({@code vector&lt;Mat&gt;}) that contains the rotation matrices for all the transformations
+     * from gripper frame to robot base frame.
+     * @param t_gripper2base Translation part extracted from the homogeneous matrix that transforms a point
+     * expressed in the gripper frame to the robot base frame (\(_{}^{b}\textrm{T}_g\)).
+     * This is a vector ({@code vector&lt;Mat&gt;}) that contains the translation vectors for all the transformations
+     * from gripper frame to robot base frame.
+     * @param R_target2cam Rotation part extracted from the homogeneous matrix that transforms a point
+     * expressed in the target frame to the camera frame (\(_{}^{c}\textrm{T}_t\)).
+     * This is a vector ({@code vector&lt;Mat&gt;}) that contains the rotation matrices for all the transformations
+     * from calibration target frame to camera frame.
+     * @param t_target2cam Rotation part extracted from the homogeneous matrix that transforms a point
+     * expressed in the target frame to the camera frame (\(_{}^{c}\textrm{T}_t\)).
+     * This is a vector ({@code vector&lt;Mat&gt;}) that contains the translation vectors for all the transformations
+     * from calibration target frame to camera frame.
+     * @param R_cam2gripper Estimated rotation part extracted from the homogeneous matrix that transforms a point
+     * expressed in the camera frame to the gripper frame (\(_{}^{g}\textrm{T}_c\)).
+     * @param t_cam2gripper Estimated translation part extracted from the homogeneous matrix that transforms a point
+     * expressed in the camera frame to the gripper frame (\(_{}^{g}\textrm{T}_c\)).
+     * @param method One of the implemented Hand-Eye calibration method, see cv::HandEyeCalibrationMethod
+     *
+     * The function performs the Hand-Eye calibration using various methods. One approach consists in estimating the
+     * rotation then the translation (separable solutions) and the following methods are implemented:
+     * <ul>
+     *   <li>
+     *    R. Tsai, R. Lenz A New Technique for Fully Autonomous and Efficient 3D Robotics Hand/EyeCalibration \cite Tsai89
+     *   </li>
+     *   <li>
+     *    F. Park, B. Martin Robot Sensor Calibration: Solving AX = XB on the Euclidean Group \cite Park94
+     *   </li>
+     *   <li>
+     *    R. Horaud, F. Dornaika Hand-Eye Calibration \cite Horaud95
+     *   </li>
+     * </ul>
+     *
+     * Another approach consists in estimating simultaneously the rotation and the translation (simultaneous solutions),
+     * with the following implemented method:
+     * <ul>
+     *   <li>
+     *    N. Andreff, R. Horaud, B. Espiau On-line Hand-Eye Calibration \cite Andreff99
+     *   </li>
+     *   <li>
+     *    K. Daniilidis Hand-Eye Calibration Using Dual Quaternions \cite Daniilidis98
+     *   </li>
+     * </ul>
+     *
+     * The following picture describes the Hand-Eye calibration problem where the transformation between a camera ("eye")
+     * mounted on a robot gripper ("hand") has to be estimated.
+     *
+     * ![](pics/hand-eye_figure.png)
+     *
+     * The calibration procedure is the following:
+     * <ul>
+     *   <li>
+     *    a static calibration pattern is used to estimate the transformation between the target frame
+     *   and the camera frame
+     *   </li>
+     *   <li>
+     *    the robot gripper is moved in order to acquire several poses
+     *   </li>
+     *   <li>
+     *    for each pose, the homogeneous transformation between the gripper frame and the robot base frame is recorded using for
+     *   instance the robot kinematics
+     * \(
+     *     \begin{bmatrix}
+     *     X_b\\
+     *     Y_b\\
+     *     Z_b\\
+     *     1
+     *     \end{bmatrix}
+     *     =
+     *     \begin{bmatrix}
+     *     _{}^{b}\textrm{R}_g &amp; _{}^{b}\textrm{t}_g \\
+     *     0_{1 \times 3} &amp; 1
+     *     \end{bmatrix}
+     *     \begin{bmatrix}
+     *     X_g\\
+     *     Y_g\\
+     *     Z_g\\
+     *     1
+     *     \end{bmatrix}
+     * \)
+     *   </li>
+     *   <li>
+     *    for each pose, the homogeneous transformation between the calibration target frame and the camera frame is recorded using
+     *   for instance a pose estimation method (PnP) from 2D-3D point correspondences
+     * \(
+     *     \begin{bmatrix}
+     *     X_c\\
+     *     Y_c\\
+     *     Z_c\\
+     *     1
+     *     \end{bmatrix}
+     *     =
+     *     \begin{bmatrix}
+     *     _{}^{c}\textrm{R}_t &amp; _{}^{c}\textrm{t}_t \\
+     *     0_{1 \times 3} &amp; 1
+     *     \end{bmatrix}
+     *     \begin{bmatrix}
+     *     X_t\\
+     *     Y_t\\
+     *     Z_t\\
+     *     1
+     *     \end{bmatrix}
+     * \)
+     *   </li>
+     * </ul>
+     *
+     * The Hand-Eye calibration procedure returns the following homogeneous transformation
+     * \(
+     *     \begin{bmatrix}
+     *     X_g\\
+     *     Y_g\\
+     *     Z_g\\
+     *     1
+     *     \end{bmatrix}
+     *     =
+     *     \begin{bmatrix}
+     *     _{}^{g}\textrm{R}_c &amp; _{}^{g}\textrm{t}_c \\
+     *     0_{1 \times 3} &amp; 1
+     *     \end{bmatrix}
+     *     \begin{bmatrix}
+     *     X_c\\
+     *     Y_c\\
+     *     Z_c\\
+     *     1
+     *     \end{bmatrix}
+     * \)
+     *
+     * This problem is also known as solving the \(\mathbf{A}\mathbf{X}=\mathbf{X}\mathbf{B}\) equation:
+     * \(
+     *     \begin{align*}
+     *     ^{b}{\textrm{T}_g}^{(1)} \hspace{0.2em} ^{g}\textrm{T}_c \hspace{0.2em} ^{c}{\textrm{T}_t}^{(1)} &amp;=
+     *     \hspace{0.1em} ^{b}{\textrm{T}_g}^{(2)} \hspace{0.2em} ^{g}\textrm{T}_c \hspace{0.2em} ^{c}{\textrm{T}_t}^{(2)} \\
+     *
+     *     (^{b}{\textrm{T}_g}^{(2)})^{-1} \hspace{0.2em} ^{b}{\textrm{T}_g}^{(1)} \hspace{0.2em} ^{g}\textrm{T}_c &amp;=
+     *     \hspace{0.1em} ^{g}\textrm{T}_c \hspace{0.2em} ^{c}{\textrm{T}_t}^{(2)} (^{c}{\textrm{T}_t}^{(1)})^{-1} \\
+     *
+     *     \textrm{A}_i \textrm{X} &amp;= \textrm{X} \textrm{B}_i \\
+     *     \end{align*}
+     * \)
+     *
+     * \note
+     * Additional information can be found on this [website](http://campar.in.tum.de/Chair/HandEyeCalibration).
+     * \note
+     * A minimum of 2 motions with non parallel rotation axes are necessary to determine the hand-eye transformation.
+     * So at least 3 different poses are required, but it is strongly recommended to use many more poses.
+     */
+    public static void calibrateHandEye(List<Mat> R_gripper2base, List<Mat> t_gripper2base, List<Mat> R_target2cam, List<Mat> t_target2cam, Mat R_cam2gripper, Mat t_cam2gripper, int method) {
+        Mat R_gripper2base_mat = Converters.vector_Mat_to_Mat(R_gripper2base);
+        Mat t_gripper2base_mat = Converters.vector_Mat_to_Mat(t_gripper2base);
+        Mat R_target2cam_mat = Converters.vector_Mat_to_Mat(R_target2cam);
+        Mat t_target2cam_mat = Converters.vector_Mat_to_Mat(t_target2cam);
+        calibrateHandEye_0(R_gripper2base_mat.nativeObj, t_gripper2base_mat.nativeObj, R_target2cam_mat.nativeObj, t_target2cam_mat.nativeObj, R_cam2gripper.nativeObj, t_cam2gripper.nativeObj, method);
+    }
+
+    /**
+     * Computes Hand-Eye calibration: \(_{}^{g}\textrm{T}_c\)
+     *
+     * @param R_gripper2base Rotation part extracted from the homogeneous matrix that transforms a point
+     * expressed in the gripper frame to the robot base frame (\(_{}^{b}\textrm{T}_g\)).
+     * This is a vector ({@code vector&lt;Mat&gt;}) that contains the rotation matrices for all the transformations
+     * from gripper frame to robot base frame.
+     * @param t_gripper2base Translation part extracted from the homogeneous matrix that transforms a point
+     * expressed in the gripper frame to the robot base frame (\(_{}^{b}\textrm{T}_g\)).
+     * This is a vector ({@code vector&lt;Mat&gt;}) that contains the translation vectors for all the transformations
+     * from gripper frame to robot base frame.
+     * @param R_target2cam Rotation part extracted from the homogeneous matrix that transforms a point
+     * expressed in the target frame to the camera frame (\(_{}^{c}\textrm{T}_t\)).
+     * This is a vector ({@code vector&lt;Mat&gt;}) that contains the rotation matrices for all the transformations
+     * from calibration target frame to camera frame.
+     * @param t_target2cam Rotation part extracted from the homogeneous matrix that transforms a point
+     * expressed in the target frame to the camera frame (\(_{}^{c}\textrm{T}_t\)).
+     * This is a vector ({@code vector&lt;Mat&gt;}) that contains the translation vectors for all the transformations
+     * from calibration target frame to camera frame.
+     * @param R_cam2gripper Estimated rotation part extracted from the homogeneous matrix that transforms a point
+     * expressed in the camera frame to the gripper frame (\(_{}^{g}\textrm{T}_c\)).
+     * @param t_cam2gripper Estimated translation part extracted from the homogeneous matrix that transforms a point
+     * expressed in the camera frame to the gripper frame (\(_{}^{g}\textrm{T}_c\)).
+     *
+     * The function performs the Hand-Eye calibration using various methods. One approach consists in estimating the
+     * rotation then the translation (separable solutions) and the following methods are implemented:
+     * <ul>
+     *   <li>
+     *    R. Tsai, R. Lenz A New Technique for Fully Autonomous and Efficient 3D Robotics Hand/EyeCalibration \cite Tsai89
+     *   </li>
+     *   <li>
+     *    F. Park, B. Martin Robot Sensor Calibration: Solving AX = XB on the Euclidean Group \cite Park94
+     *   </li>
+     *   <li>
+     *    R. Horaud, F. Dornaika Hand-Eye Calibration \cite Horaud95
+     *   </li>
+     * </ul>
+     *
+     * Another approach consists in estimating simultaneously the rotation and the translation (simultaneous solutions),
+     * with the following implemented method:
+     * <ul>
+     *   <li>
+     *    N. Andreff, R. Horaud, B. Espiau On-line Hand-Eye Calibration \cite Andreff99
+     *   </li>
+     *   <li>
+     *    K. Daniilidis Hand-Eye Calibration Using Dual Quaternions \cite Daniilidis98
+     *   </li>
+     * </ul>
+     *
+     * The following picture describes the Hand-Eye calibration problem where the transformation between a camera ("eye")
+     * mounted on a robot gripper ("hand") has to be estimated.
+     *
+     * ![](pics/hand-eye_figure.png)
+     *
+     * The calibration procedure is the following:
+     * <ul>
+     *   <li>
+     *    a static calibration pattern is used to estimate the transformation between the target frame
+     *   and the camera frame
+     *   </li>
+     *   <li>
+     *    the robot gripper is moved in order to acquire several poses
+     *   </li>
+     *   <li>
+     *    for each pose, the homogeneous transformation between the gripper frame and the robot base frame is recorded using for
+     *   instance the robot kinematics
+     * \(
+     *     \begin{bmatrix}
+     *     X_b\\
+     *     Y_b\\
+     *     Z_b\\
+     *     1
+     *     \end{bmatrix}
+     *     =
+     *     \begin{bmatrix}
+     *     _{}^{b}\textrm{R}_g &amp; _{}^{b}\textrm{t}_g \\
+     *     0_{1 \times 3} &amp; 1
+     *     \end{bmatrix}
+     *     \begin{bmatrix}
+     *     X_g\\
+     *     Y_g\\
+     *     Z_g\\
+     *     1
+     *     \end{bmatrix}
+     * \)
+     *   </li>
+     *   <li>
+     *    for each pose, the homogeneous transformation between the calibration target frame and the camera frame is recorded using
+     *   for instance a pose estimation method (PnP) from 2D-3D point correspondences
+     * \(
+     *     \begin{bmatrix}
+     *     X_c\\
+     *     Y_c\\
+     *     Z_c\\
+     *     1
+     *     \end{bmatrix}
+     *     =
+     *     \begin{bmatrix}
+     *     _{}^{c}\textrm{R}_t &amp; _{}^{c}\textrm{t}_t \\
+     *     0_{1 \times 3} &amp; 1
+     *     \end{bmatrix}
+     *     \begin{bmatrix}
+     *     X_t\\
+     *     Y_t\\
+     *     Z_t\\
+     *     1
+     *     \end{bmatrix}
+     * \)
+     *   </li>
+     * </ul>
+     *
+     * The Hand-Eye calibration procedure returns the following homogeneous transformation
+     * \(
+     *     \begin{bmatrix}
+     *     X_g\\
+     *     Y_g\\
+     *     Z_g\\
+     *     1
+     *     \end{bmatrix}
+     *     =
+     *     \begin{bmatrix}
+     *     _{}^{g}\textrm{R}_c &amp; _{}^{g}\textrm{t}_c \\
+     *     0_{1 \times 3} &amp; 1
+     *     \end{bmatrix}
+     *     \begin{bmatrix}
+     *     X_c\\
+     *     Y_c\\
+     *     Z_c\\
+     *     1
+     *     \end{bmatrix}
+     * \)
+     *
+     * This problem is also known as solving the \(\mathbf{A}\mathbf{X}=\mathbf{X}\mathbf{B}\) equation:
+     * \(
+     *     \begin{align*}
+     *     ^{b}{\textrm{T}_g}^{(1)} \hspace{0.2em} ^{g}\textrm{T}_c \hspace{0.2em} ^{c}{\textrm{T}_t}^{(1)} &amp;=
+     *     \hspace{0.1em} ^{b}{\textrm{T}_g}^{(2)} \hspace{0.2em} ^{g}\textrm{T}_c \hspace{0.2em} ^{c}{\textrm{T}_t}^{(2)} \\
+     *
+     *     (^{b}{\textrm{T}_g}^{(2)})^{-1} \hspace{0.2em} ^{b}{\textrm{T}_g}^{(1)} \hspace{0.2em} ^{g}\textrm{T}_c &amp;=
+     *     \hspace{0.1em} ^{g}\textrm{T}_c \hspace{0.2em} ^{c}{\textrm{T}_t}^{(2)} (^{c}{\textrm{T}_t}^{(1)})^{-1} \\
+     *
+     *     \textrm{A}_i \textrm{X} &amp;= \textrm{X} \textrm{B}_i \\
+     *     \end{align*}
+     * \)
+     *
+     * \note
+     * Additional information can be found on this [website](http://campar.in.tum.de/Chair/HandEyeCalibration).
+     * \note
+     * A minimum of 2 motions with non parallel rotation axes are necessary to determine the hand-eye transformation.
+     * So at least 3 different poses are required, but it is strongly recommended to use many more poses.
+     */
+    public static void calibrateHandEye(List<Mat> R_gripper2base, List<Mat> t_gripper2base, List<Mat> R_target2cam, List<Mat> t_target2cam, Mat R_cam2gripper, Mat t_cam2gripper) {
+        Mat R_gripper2base_mat = Converters.vector_Mat_to_Mat(R_gripper2base);
+        Mat t_gripper2base_mat = Converters.vector_Mat_to_Mat(t_gripper2base);
+        Mat R_target2cam_mat = Converters.vector_Mat_to_Mat(R_target2cam);
+        Mat t_target2cam_mat = Converters.vector_Mat_to_Mat(t_target2cam);
+        calibrateHandEye_1(R_gripper2base_mat.nativeObj, t_gripper2base_mat.nativeObj, R_target2cam_mat.nativeObj, t_target2cam_mat.nativeObj, R_cam2gripper.nativeObj, t_cam2gripper.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::calibrationMatrixValues(Mat cameraMatrix, Size imageSize, double apertureWidth, double apertureHeight, double& fovx, double& fovy, double& focalLength, Point2d& principalPoint, double& aspectRatio)
+    //
+
+    /**
+     * Computes useful camera characteristics from the camera matrix.
+     *
+     * @param cameraMatrix Input camera matrix that can be estimated by calibrateCamera or
+     * stereoCalibrate .
+     * @param imageSize Input image size in pixels.
+     * @param apertureWidth Physical width in mm of the sensor.
+     * @param apertureHeight Physical height in mm of the sensor.
+     * @param fovx Output field of view in degrees along the horizontal sensor axis.
+     * @param fovy Output field of view in degrees along the vertical sensor axis.
+     * @param focalLength Focal length of the lens in mm.
+     * @param principalPoint Principal point in mm.
+     * @param aspectRatio \(f_y/f_x\)
+     *
+     * The function computes various useful camera characteristics from the previously estimated camera
+     * matrix.
+     *
+     * <b>Note:</b>
+     *    Do keep in mind that the unity measure 'mm' stands for whatever unit of measure one chooses for
+     *     the chessboard pitch (it can thus be any value).
+     */
+    public static void calibrationMatrixValues(Mat cameraMatrix, Size imageSize, double apertureWidth, double apertureHeight, double[] fovx, double[] fovy, double[] focalLength, Point principalPoint, double[] aspectRatio) {
+        double[] fovx_out = new double[1];
+        double[] fovy_out = new double[1];
+        double[] focalLength_out = new double[1];
+        double[] principalPoint_out = new double[2];
+        double[] aspectRatio_out = new double[1];
+        calibrationMatrixValues_0(cameraMatrix.nativeObj, imageSize.width, imageSize.height, apertureWidth, apertureHeight, fovx_out, fovy_out, focalLength_out, principalPoint_out, aspectRatio_out);
+        if(fovx!=null) fovx[0] = (double)fovx_out[0];
+        if(fovy!=null) fovy[0] = (double)fovy_out[0];
+        if(focalLength!=null) focalLength[0] = (double)focalLength_out[0];
+        if(principalPoint!=null){ principalPoint.x = principalPoint_out[0]; principalPoint.y = principalPoint_out[1]; } 
+        if(aspectRatio!=null) aspectRatio[0] = (double)aspectRatio_out[0];
+    }
+
+
+    //
+    // C++:  void cv::composeRT(Mat rvec1, Mat tvec1, Mat rvec2, Mat tvec2, Mat& rvec3, Mat& tvec3, Mat& dr3dr1 = Mat(), Mat& dr3dt1 = Mat(), Mat& dr3dr2 = Mat(), Mat& dr3dt2 = Mat(), Mat& dt3dr1 = Mat(), Mat& dt3dt1 = Mat(), Mat& dt3dr2 = Mat(), Mat& dt3dt2 = Mat())
+    //
+
+    /**
+     * Combines two rotation-and-shift transformations.
+     *
+     * @param rvec1 First rotation vector.
+     * @param tvec1 First translation vector.
+     * @param rvec2 Second rotation vector.
+     * @param tvec2 Second translation vector.
+     * @param rvec3 Output rotation vector of the superposition.
+     * @param tvec3 Output translation vector of the superposition.
+     * @param dr3dr1 Optional output derivative of rvec3 with regard to rvec1
+     * @param dr3dt1 Optional output derivative of rvec3 with regard to tvec1
+     * @param dr3dr2 Optional output derivative of rvec3 with regard to rvec2
+     * @param dr3dt2 Optional output derivative of rvec3 with regard to tvec2
+     * @param dt3dr1 Optional output derivative of tvec3 with regard to rvec1
+     * @param dt3dt1 Optional output derivative of tvec3 with regard to tvec1
+     * @param dt3dr2 Optional output derivative of tvec3 with regard to rvec2
+     * @param dt3dt2 Optional output derivative of tvec3 with regard to tvec2
+     *
+     * The functions compute:
+     *
+     * \(\begin{array}{l} \texttt{rvec3} =  \mathrm{rodrigues} ^{-1} \left ( \mathrm{rodrigues} ( \texttt{rvec2} )  \cdot \mathrm{rodrigues} ( \texttt{rvec1} ) \right )  \\ \texttt{tvec3} =  \mathrm{rodrigues} ( \texttt{rvec2} )  \cdot \texttt{tvec1} +  \texttt{tvec2} \end{array} ,\)
+     *
+     * where \(\mathrm{rodrigues}\) denotes a rotation vector to a rotation matrix transformation, and
+     * \(\mathrm{rodrigues}^{-1}\) denotes the inverse transformation. See Rodrigues for details.
+     *
+     * Also, the functions can compute the derivatives of the output vectors with regards to the input
+     * vectors (see matMulDeriv ). The functions are used inside stereoCalibrate but can also be used in
+     * your own code where Levenberg-Marquardt or another gradient-based solver is used to optimize a
+     * function that contains a matrix multiplication.
+     */
+    public static void composeRT(Mat rvec1, Mat tvec1, Mat rvec2, Mat tvec2, Mat rvec3, Mat tvec3, Mat dr3dr1, Mat dr3dt1, Mat dr3dr2, Mat dr3dt2, Mat dt3dr1, Mat dt3dt1, Mat dt3dr2, Mat dt3dt2) {
+        composeRT_0(rvec1.nativeObj, tvec1.nativeObj, rvec2.nativeObj, tvec2.nativeObj, rvec3.nativeObj, tvec3.nativeObj, dr3dr1.nativeObj, dr3dt1.nativeObj, dr3dr2.nativeObj, dr3dt2.nativeObj, dt3dr1.nativeObj, dt3dt1.nativeObj, dt3dr2.nativeObj, dt3dt2.nativeObj);
+    }
+
+    /**
+     * Combines two rotation-and-shift transformations.
+     *
+     * @param rvec1 First rotation vector.
+     * @param tvec1 First translation vector.
+     * @param rvec2 Second rotation vector.
+     * @param tvec2 Second translation vector.
+     * @param rvec3 Output rotation vector of the superposition.
+     * @param tvec3 Output translation vector of the superposition.
+     * @param dr3dr1 Optional output derivative of rvec3 with regard to rvec1
+     * @param dr3dt1 Optional output derivative of rvec3 with regard to tvec1
+     * @param dr3dr2 Optional output derivative of rvec3 with regard to rvec2
+     * @param dr3dt2 Optional output derivative of rvec3 with regard to tvec2
+     * @param dt3dr1 Optional output derivative of tvec3 with regard to rvec1
+     * @param dt3dt1 Optional output derivative of tvec3 with regard to tvec1
+     * @param dt3dr2 Optional output derivative of tvec3 with regard to rvec2
+     *
+     * The functions compute:
+     *
+     * \(\begin{array}{l} \texttt{rvec3} =  \mathrm{rodrigues} ^{-1} \left ( \mathrm{rodrigues} ( \texttt{rvec2} )  \cdot \mathrm{rodrigues} ( \texttt{rvec1} ) \right )  \\ \texttt{tvec3} =  \mathrm{rodrigues} ( \texttt{rvec2} )  \cdot \texttt{tvec1} +  \texttt{tvec2} \end{array} ,\)
+     *
+     * where \(\mathrm{rodrigues}\) denotes a rotation vector to a rotation matrix transformation, and
+     * \(\mathrm{rodrigues}^{-1}\) denotes the inverse transformation. See Rodrigues for details.
+     *
+     * Also, the functions can compute the derivatives of the output vectors with regards to the input
+     * vectors (see matMulDeriv ). The functions are used inside stereoCalibrate but can also be used in
+     * your own code where Levenberg-Marquardt or another gradient-based solver is used to optimize a
+     * function that contains a matrix multiplication.
+     */
+    public static void composeRT(Mat rvec1, Mat tvec1, Mat rvec2, Mat tvec2, Mat rvec3, Mat tvec3, Mat dr3dr1, Mat dr3dt1, Mat dr3dr2, Mat dr3dt2, Mat dt3dr1, Mat dt3dt1, Mat dt3dr2) {
+        composeRT_1(rvec1.nativeObj, tvec1.nativeObj, rvec2.nativeObj, tvec2.nativeObj, rvec3.nativeObj, tvec3.nativeObj, dr3dr1.nativeObj, dr3dt1.nativeObj, dr3dr2.nativeObj, dr3dt2.nativeObj, dt3dr1.nativeObj, dt3dt1.nativeObj, dt3dr2.nativeObj);
+    }
+
+    /**
+     * Combines two rotation-and-shift transformations.
+     *
+     * @param rvec1 First rotation vector.
+     * @param tvec1 First translation vector.
+     * @param rvec2 Second rotation vector.
+     * @param tvec2 Second translation vector.
+     * @param rvec3 Output rotation vector of the superposition.
+     * @param tvec3 Output translation vector of the superposition.
+     * @param dr3dr1 Optional output derivative of rvec3 with regard to rvec1
+     * @param dr3dt1 Optional output derivative of rvec3 with regard to tvec1
+     * @param dr3dr2 Optional output derivative of rvec3 with regard to rvec2
+     * @param dr3dt2 Optional output derivative of rvec3 with regard to tvec2
+     * @param dt3dr1 Optional output derivative of tvec3 with regard to rvec1
+     * @param dt3dt1 Optional output derivative of tvec3 with regard to tvec1
+     *
+     * The functions compute:
+     *
+     * \(\begin{array}{l} \texttt{rvec3} =  \mathrm{rodrigues} ^{-1} \left ( \mathrm{rodrigues} ( \texttt{rvec2} )  \cdot \mathrm{rodrigues} ( \texttt{rvec1} ) \right )  \\ \texttt{tvec3} =  \mathrm{rodrigues} ( \texttt{rvec2} )  \cdot \texttt{tvec1} +  \texttt{tvec2} \end{array} ,\)
+     *
+     * where \(\mathrm{rodrigues}\) denotes a rotation vector to a rotation matrix transformation, and
+     * \(\mathrm{rodrigues}^{-1}\) denotes the inverse transformation. See Rodrigues for details.
+     *
+     * Also, the functions can compute the derivatives of the output vectors with regards to the input
+     * vectors (see matMulDeriv ). The functions are used inside stereoCalibrate but can also be used in
+     * your own code where Levenberg-Marquardt or another gradient-based solver is used to optimize a
+     * function that contains a matrix multiplication.
+     */
+    public static void composeRT(Mat rvec1, Mat tvec1, Mat rvec2, Mat tvec2, Mat rvec3, Mat tvec3, Mat dr3dr1, Mat dr3dt1, Mat dr3dr2, Mat dr3dt2, Mat dt3dr1, Mat dt3dt1) {
+        composeRT_2(rvec1.nativeObj, tvec1.nativeObj, rvec2.nativeObj, tvec2.nativeObj, rvec3.nativeObj, tvec3.nativeObj, dr3dr1.nativeObj, dr3dt1.nativeObj, dr3dr2.nativeObj, dr3dt2.nativeObj, dt3dr1.nativeObj, dt3dt1.nativeObj);
+    }
+
+    /**
+     * Combines two rotation-and-shift transformations.
+     *
+     * @param rvec1 First rotation vector.
+     * @param tvec1 First translation vector.
+     * @param rvec2 Second rotation vector.
+     * @param tvec2 Second translation vector.
+     * @param rvec3 Output rotation vector of the superposition.
+     * @param tvec3 Output translation vector of the superposition.
+     * @param dr3dr1 Optional output derivative of rvec3 with regard to rvec1
+     * @param dr3dt1 Optional output derivative of rvec3 with regard to tvec1
+     * @param dr3dr2 Optional output derivative of rvec3 with regard to rvec2
+     * @param dr3dt2 Optional output derivative of rvec3 with regard to tvec2
+     * @param dt3dr1 Optional output derivative of tvec3 with regard to rvec1
+     *
+     * The functions compute:
+     *
+     * \(\begin{array}{l} \texttt{rvec3} =  \mathrm{rodrigues} ^{-1} \left ( \mathrm{rodrigues} ( \texttt{rvec2} )  \cdot \mathrm{rodrigues} ( \texttt{rvec1} ) \right )  \\ \texttt{tvec3} =  \mathrm{rodrigues} ( \texttt{rvec2} )  \cdot \texttt{tvec1} +  \texttt{tvec2} \end{array} ,\)
+     *
+     * where \(\mathrm{rodrigues}\) denotes a rotation vector to a rotation matrix transformation, and
+     * \(\mathrm{rodrigues}^{-1}\) denotes the inverse transformation. See Rodrigues for details.
+     *
+     * Also, the functions can compute the derivatives of the output vectors with regards to the input
+     * vectors (see matMulDeriv ). The functions are used inside stereoCalibrate but can also be used in
+     * your own code where Levenberg-Marquardt or another gradient-based solver is used to optimize a
+     * function that contains a matrix multiplication.
+     */
+    public static void composeRT(Mat rvec1, Mat tvec1, Mat rvec2, Mat tvec2, Mat rvec3, Mat tvec3, Mat dr3dr1, Mat dr3dt1, Mat dr3dr2, Mat dr3dt2, Mat dt3dr1) {
+        composeRT_3(rvec1.nativeObj, tvec1.nativeObj, rvec2.nativeObj, tvec2.nativeObj, rvec3.nativeObj, tvec3.nativeObj, dr3dr1.nativeObj, dr3dt1.nativeObj, dr3dr2.nativeObj, dr3dt2.nativeObj, dt3dr1.nativeObj);
+    }
+
+    /**
+     * Combines two rotation-and-shift transformations.
+     *
+     * @param rvec1 First rotation vector.
+     * @param tvec1 First translation vector.
+     * @param rvec2 Second rotation vector.
+     * @param tvec2 Second translation vector.
+     * @param rvec3 Output rotation vector of the superposition.
+     * @param tvec3 Output translation vector of the superposition.
+     * @param dr3dr1 Optional output derivative of rvec3 with regard to rvec1
+     * @param dr3dt1 Optional output derivative of rvec3 with regard to tvec1
+     * @param dr3dr2 Optional output derivative of rvec3 with regard to rvec2
+     * @param dr3dt2 Optional output derivative of rvec3 with regard to tvec2
+     *
+     * The functions compute:
+     *
+     * \(\begin{array}{l} \texttt{rvec3} =  \mathrm{rodrigues} ^{-1} \left ( \mathrm{rodrigues} ( \texttt{rvec2} )  \cdot \mathrm{rodrigues} ( \texttt{rvec1} ) \right )  \\ \texttt{tvec3} =  \mathrm{rodrigues} ( \texttt{rvec2} )  \cdot \texttt{tvec1} +  \texttt{tvec2} \end{array} ,\)
+     *
+     * where \(\mathrm{rodrigues}\) denotes a rotation vector to a rotation matrix transformation, and
+     * \(\mathrm{rodrigues}^{-1}\) denotes the inverse transformation. See Rodrigues for details.
+     *
+     * Also, the functions can compute the derivatives of the output vectors with regards to the input
+     * vectors (see matMulDeriv ). The functions are used inside stereoCalibrate but can also be used in
+     * your own code where Levenberg-Marquardt or another gradient-based solver is used to optimize a
+     * function that contains a matrix multiplication.
+     */
+    public static void composeRT(Mat rvec1, Mat tvec1, Mat rvec2, Mat tvec2, Mat rvec3, Mat tvec3, Mat dr3dr1, Mat dr3dt1, Mat dr3dr2, Mat dr3dt2) {
+        composeRT_4(rvec1.nativeObj, tvec1.nativeObj, rvec2.nativeObj, tvec2.nativeObj, rvec3.nativeObj, tvec3.nativeObj, dr3dr1.nativeObj, dr3dt1.nativeObj, dr3dr2.nativeObj, dr3dt2.nativeObj);
+    }
+
+    /**
+     * Combines two rotation-and-shift transformations.
+     *
+     * @param rvec1 First rotation vector.
+     * @param tvec1 First translation vector.
+     * @param rvec2 Second rotation vector.
+     * @param tvec2 Second translation vector.
+     * @param rvec3 Output rotation vector of the superposition.
+     * @param tvec3 Output translation vector of the superposition.
+     * @param dr3dr1 Optional output derivative of rvec3 with regard to rvec1
+     * @param dr3dt1 Optional output derivative of rvec3 with regard to tvec1
+     * @param dr3dr2 Optional output derivative of rvec3 with regard to rvec2
+     *
+     * The functions compute:
+     *
+     * \(\begin{array}{l} \texttt{rvec3} =  \mathrm{rodrigues} ^{-1} \left ( \mathrm{rodrigues} ( \texttt{rvec2} )  \cdot \mathrm{rodrigues} ( \texttt{rvec1} ) \right )  \\ \texttt{tvec3} =  \mathrm{rodrigues} ( \texttt{rvec2} )  \cdot \texttt{tvec1} +  \texttt{tvec2} \end{array} ,\)
+     *
+     * where \(\mathrm{rodrigues}\) denotes a rotation vector to a rotation matrix transformation, and
+     * \(\mathrm{rodrigues}^{-1}\) denotes the inverse transformation. See Rodrigues for details.
+     *
+     * Also, the functions can compute the derivatives of the output vectors with regards to the input
+     * vectors (see matMulDeriv ). The functions are used inside stereoCalibrate but can also be used in
+     * your own code where Levenberg-Marquardt or another gradient-based solver is used to optimize a
+     * function that contains a matrix multiplication.
+     */
+    public static void composeRT(Mat rvec1, Mat tvec1, Mat rvec2, Mat tvec2, Mat rvec3, Mat tvec3, Mat dr3dr1, Mat dr3dt1, Mat dr3dr2) {
+        composeRT_5(rvec1.nativeObj, tvec1.nativeObj, rvec2.nativeObj, tvec2.nativeObj, rvec3.nativeObj, tvec3.nativeObj, dr3dr1.nativeObj, dr3dt1.nativeObj, dr3dr2.nativeObj);
+    }
+
+    /**
+     * Combines two rotation-and-shift transformations.
+     *
+     * @param rvec1 First rotation vector.
+     * @param tvec1 First translation vector.
+     * @param rvec2 Second rotation vector.
+     * @param tvec2 Second translation vector.
+     * @param rvec3 Output rotation vector of the superposition.
+     * @param tvec3 Output translation vector of the superposition.
+     * @param dr3dr1 Optional output derivative of rvec3 with regard to rvec1
+     * @param dr3dt1 Optional output derivative of rvec3 with regard to tvec1
+     *
+     * The functions compute:
+     *
+     * \(\begin{array}{l} \texttt{rvec3} =  \mathrm{rodrigues} ^{-1} \left ( \mathrm{rodrigues} ( \texttt{rvec2} )  \cdot \mathrm{rodrigues} ( \texttt{rvec1} ) \right )  \\ \texttt{tvec3} =  \mathrm{rodrigues} ( \texttt{rvec2} )  \cdot \texttt{tvec1} +  \texttt{tvec2} \end{array} ,\)
+     *
+     * where \(\mathrm{rodrigues}\) denotes a rotation vector to a rotation matrix transformation, and
+     * \(\mathrm{rodrigues}^{-1}\) denotes the inverse transformation. See Rodrigues for details.
+     *
+     * Also, the functions can compute the derivatives of the output vectors with regards to the input
+     * vectors (see matMulDeriv ). The functions are used inside stereoCalibrate but can also be used in
+     * your own code where Levenberg-Marquardt or another gradient-based solver is used to optimize a
+     * function that contains a matrix multiplication.
+     */
+    public static void composeRT(Mat rvec1, Mat tvec1, Mat rvec2, Mat tvec2, Mat rvec3, Mat tvec3, Mat dr3dr1, Mat dr3dt1) {
+        composeRT_6(rvec1.nativeObj, tvec1.nativeObj, rvec2.nativeObj, tvec2.nativeObj, rvec3.nativeObj, tvec3.nativeObj, dr3dr1.nativeObj, dr3dt1.nativeObj);
+    }
+
+    /**
+     * Combines two rotation-and-shift transformations.
+     *
+     * @param rvec1 First rotation vector.
+     * @param tvec1 First translation vector.
+     * @param rvec2 Second rotation vector.
+     * @param tvec2 Second translation vector.
+     * @param rvec3 Output rotation vector of the superposition.
+     * @param tvec3 Output translation vector of the superposition.
+     * @param dr3dr1 Optional output derivative of rvec3 with regard to rvec1
+     *
+     * The functions compute:
+     *
+     * \(\begin{array}{l} \texttt{rvec3} =  \mathrm{rodrigues} ^{-1} \left ( \mathrm{rodrigues} ( \texttt{rvec2} )  \cdot \mathrm{rodrigues} ( \texttt{rvec1} ) \right )  \\ \texttt{tvec3} =  \mathrm{rodrigues} ( \texttt{rvec2} )  \cdot \texttt{tvec1} +  \texttt{tvec2} \end{array} ,\)
+     *
+     * where \(\mathrm{rodrigues}\) denotes a rotation vector to a rotation matrix transformation, and
+     * \(\mathrm{rodrigues}^{-1}\) denotes the inverse transformation. See Rodrigues for details.
+     *
+     * Also, the functions can compute the derivatives of the output vectors with regards to the input
+     * vectors (see matMulDeriv ). The functions are used inside stereoCalibrate but can also be used in
+     * your own code where Levenberg-Marquardt or another gradient-based solver is used to optimize a
+     * function that contains a matrix multiplication.
+     */
+    public static void composeRT(Mat rvec1, Mat tvec1, Mat rvec2, Mat tvec2, Mat rvec3, Mat tvec3, Mat dr3dr1) {
+        composeRT_7(rvec1.nativeObj, tvec1.nativeObj, rvec2.nativeObj, tvec2.nativeObj, rvec3.nativeObj, tvec3.nativeObj, dr3dr1.nativeObj);
+    }
+
+    /**
+     * Combines two rotation-and-shift transformations.
+     *
+     * @param rvec1 First rotation vector.
+     * @param tvec1 First translation vector.
+     * @param rvec2 Second rotation vector.
+     * @param tvec2 Second translation vector.
+     * @param rvec3 Output rotation vector of the superposition.
+     * @param tvec3 Output translation vector of the superposition.
+     *
+     * The functions compute:
+     *
+     * \(\begin{array}{l} \texttt{rvec3} =  \mathrm{rodrigues} ^{-1} \left ( \mathrm{rodrigues} ( \texttt{rvec2} )  \cdot \mathrm{rodrigues} ( \texttt{rvec1} ) \right )  \\ \texttt{tvec3} =  \mathrm{rodrigues} ( \texttt{rvec2} )  \cdot \texttt{tvec1} +  \texttt{tvec2} \end{array} ,\)
+     *
+     * where \(\mathrm{rodrigues}\) denotes a rotation vector to a rotation matrix transformation, and
+     * \(\mathrm{rodrigues}^{-1}\) denotes the inverse transformation. See Rodrigues for details.
+     *
+     * Also, the functions can compute the derivatives of the output vectors with regards to the input
+     * vectors (see matMulDeriv ). The functions are used inside stereoCalibrate but can also be used in
+     * your own code where Levenberg-Marquardt or another gradient-based solver is used to optimize a
+     * function that contains a matrix multiplication.
+     */
+    public static void composeRT(Mat rvec1, Mat tvec1, Mat rvec2, Mat tvec2, Mat rvec3, Mat tvec3) {
+        composeRT_8(rvec1.nativeObj, tvec1.nativeObj, rvec2.nativeObj, tvec2.nativeObj, rvec3.nativeObj, tvec3.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::computeCorrespondEpilines(Mat points, int whichImage, Mat F, Mat& lines)
+    //
+
+    /**
+     * For points in an image of a stereo pair, computes the corresponding epilines in the other image.
+     *
+     * @param points Input points. \(N \times 1\) or \(1 \times N\) matrix of type CV_32FC2 or
+     * vector&lt;Point2f&gt; .
+     * @param whichImage Index of the image (1 or 2) that contains the points .
+     * @param F Fundamental matrix that can be estimated using findFundamentalMat or stereoRectify .
+     * @param lines Output vector of the epipolar lines corresponding to the points in the other image.
+     * Each line \(ax + by + c=0\) is encoded by 3 numbers \((a, b, c)\) .
+     *
+     * For every point in one of the two images of a stereo pair, the function finds the equation of the
+     * corresponding epipolar line in the other image.
+     *
+     * From the fundamental matrix definition (see findFundamentalMat ), line \(l^{(2)}_i\) in the second
+     * image for the point \(p^{(1)}_i\) in the first image (when whichImage=1 ) is computed as:
+     *
+     * \(l^{(2)}_i = F p^{(1)}_i\)
+     *
+     * And vice versa, when whichImage=2, \(l^{(1)}_i\) is computed from \(p^{(2)}_i\) as:
+     *
+     * \(l^{(1)}_i = F^T p^{(2)}_i\)
+     *
+     * Line coefficients are defined up to a scale. They are normalized so that \(a_i^2+b_i^2=1\) .
+     */
+    public static void computeCorrespondEpilines(Mat points, int whichImage, Mat F, Mat lines) {
+        computeCorrespondEpilines_0(points.nativeObj, whichImage, F.nativeObj, lines.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::convertPointsFromHomogeneous(Mat src, Mat& dst)
+    //
+
+    /**
+     * Converts points from homogeneous to Euclidean space.
+     *
+     * @param src Input vector of N-dimensional points.
+     * @param dst Output vector of N-1-dimensional points.
+     *
+     * The function converts points homogeneous to Euclidean space using perspective projection. That is,
+     * each point (x1, x2, ... x(n-1), xn) is converted to (x1/xn, x2/xn, ..., x(n-1)/xn). When xn=0, the
+     * output point coordinates will be (0,0,0,...).
+     */
+    public static void convertPointsFromHomogeneous(Mat src, Mat dst) {
+        convertPointsFromHomogeneous_0(src.nativeObj, dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::convertPointsToHomogeneous(Mat src, Mat& dst)
+    //
+
+    /**
+     * Converts points from Euclidean to homogeneous space.
+     *
+     * @param src Input vector of N-dimensional points.
+     * @param dst Output vector of N+1-dimensional points.
+     *
+     * The function converts points from Euclidean to homogeneous space by appending 1's to the tuple of
+     * point coordinates. That is, each point (x1, x2, ..., xn) is converted to (x1, x2, ..., xn, 1).
+     */
+    public static void convertPointsToHomogeneous(Mat src, Mat dst) {
+        convertPointsToHomogeneous_0(src.nativeObj, dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::correctMatches(Mat F, Mat points1, Mat points2, Mat& newPoints1, Mat& newPoints2)
+    //
+
+    /**
+     * Refines coordinates of corresponding points.
+     *
+     * @param F 3x3 fundamental matrix.
+     * @param points1 1xN array containing the first set of points.
+     * @param points2 1xN array containing the second set of points.
+     * @param newPoints1 The optimized points1.
+     * @param newPoints2 The optimized points2.
+     *
+     * The function implements the Optimal Triangulation Method (see Multiple View Geometry for details).
+     * For each given point correspondence points1[i] &lt;-&gt; points2[i], and a fundamental matrix F, it
+     * computes the corrected correspondences newPoints1[i] &lt;-&gt; newPoints2[i] that minimize the geometric
+     * error \(d(points1[i], newPoints1[i])^2 + d(points2[i],newPoints2[i])^2\) (where \(d(a,b)\) is the
+     * geometric distance between points \(a\) and \(b\) ) subject to the epipolar constraint
+     * \(newPoints2^T * F * newPoints1 = 0\) .
+     */
+    public static void correctMatches(Mat F, Mat points1, Mat points2, Mat newPoints1, Mat newPoints2) {
+        correctMatches_0(F.nativeObj, points1.nativeObj, points2.nativeObj, newPoints1.nativeObj, newPoints2.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::decomposeEssentialMat(Mat E, Mat& R1, Mat& R2, Mat& t)
+    //
+
+    /**
+     * Decompose an essential matrix to possible rotations and translation.
+     *
+     * @param E The input essential matrix.
+     * @param R1 One possible rotation matrix.
+     * @param R2 Another possible rotation matrix.
+     * @param t One possible translation.
+     *
+     * This function decomposes the essential matrix E using svd decomposition CITE: HartleyZ00. In
+     * general, four possible poses exist for the decomposition of E. They are \([R_1, t]\),
+     * \([R_1, -t]\), \([R_2, t]\), \([R_2, -t]\).
+     *
+     * If E gives the epipolar constraint \([p_2; 1]^T A^{-T} E A^{-1} [p_1; 1] = 0\) between the image
+     * points \(p_1\) in the first image and \(p_2\) in second image, then any of the tuples
+     * \([R_1, t]\), \([R_1, -t]\), \([R_2, t]\), \([R_2, -t]\) is a change of basis from the first
+     * camera's coordinate system to the second camera's coordinate system. However, by decomposing E, one
+     * can only get the direction of the translation. For this reason, the translation t is returned with
+     * unit length.
+     */
+    public static void decomposeEssentialMat(Mat E, Mat R1, Mat R2, Mat t) {
+        decomposeEssentialMat_0(E.nativeObj, R1.nativeObj, R2.nativeObj, t.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::decomposeProjectionMatrix(Mat projMatrix, Mat& cameraMatrix, Mat& rotMatrix, Mat& transVect, Mat& rotMatrixX = Mat(), Mat& rotMatrixY = Mat(), Mat& rotMatrixZ = Mat(), Mat& eulerAngles = Mat())
+    //
+
+    /**
+     * Decomposes a projection matrix into a rotation matrix and a camera matrix.
+     *
+     * @param projMatrix 3x4 input projection matrix P.
+     * @param cameraMatrix Output 3x3 camera matrix K.
+     * @param rotMatrix Output 3x3 external rotation matrix R.
+     * @param transVect Output 4x1 translation vector T.
+     * @param rotMatrixX Optional 3x3 rotation matrix around x-axis.
+     * @param rotMatrixY Optional 3x3 rotation matrix around y-axis.
+     * @param rotMatrixZ Optional 3x3 rotation matrix around z-axis.
+     * @param eulerAngles Optional three-element vector containing three Euler angles of rotation in
+     * degrees.
+     *
+     * The function computes a decomposition of a projection matrix into a calibration and a rotation
+     * matrix and the position of a camera.
+     *
+     * It optionally returns three rotation matrices, one for each axis, and three Euler angles that could
+     * be used in OpenGL. Note, there is always more than one sequence of rotations about the three
+     * principal axes that results in the same orientation of an object, e.g. see CITE: Slabaugh . Returned
+     * tree rotation matrices and corresponding three Euler angles are only one of the possible solutions.
+     *
+     * The function is based on RQDecomp3x3 .
+     */
+    public static void decomposeProjectionMatrix(Mat projMatrix, Mat cameraMatrix, Mat rotMatrix, Mat transVect, Mat rotMatrixX, Mat rotMatrixY, Mat rotMatrixZ, Mat eulerAngles) {
+        decomposeProjectionMatrix_0(projMatrix.nativeObj, cameraMatrix.nativeObj, rotMatrix.nativeObj, transVect.nativeObj, rotMatrixX.nativeObj, rotMatrixY.nativeObj, rotMatrixZ.nativeObj, eulerAngles.nativeObj);
+    }
+
+    /**
+     * Decomposes a projection matrix into a rotation matrix and a camera matrix.
+     *
+     * @param projMatrix 3x4 input projection matrix P.
+     * @param cameraMatrix Output 3x3 camera matrix K.
+     * @param rotMatrix Output 3x3 external rotation matrix R.
+     * @param transVect Output 4x1 translation vector T.
+     * @param rotMatrixX Optional 3x3 rotation matrix around x-axis.
+     * @param rotMatrixY Optional 3x3 rotation matrix around y-axis.
+     * @param rotMatrixZ Optional 3x3 rotation matrix around z-axis.
+     * degrees.
+     *
+     * The function computes a decomposition of a projection matrix into a calibration and a rotation
+     * matrix and the position of a camera.
+     *
+     * It optionally returns three rotation matrices, one for each axis, and three Euler angles that could
+     * be used in OpenGL. Note, there is always more than one sequence of rotations about the three
+     * principal axes that results in the same orientation of an object, e.g. see CITE: Slabaugh . Returned
+     * tree rotation matrices and corresponding three Euler angles are only one of the possible solutions.
+     *
+     * The function is based on RQDecomp3x3 .
+     */
+    public static void decomposeProjectionMatrix(Mat projMatrix, Mat cameraMatrix, Mat rotMatrix, Mat transVect, Mat rotMatrixX, Mat rotMatrixY, Mat rotMatrixZ) {
+        decomposeProjectionMatrix_1(projMatrix.nativeObj, cameraMatrix.nativeObj, rotMatrix.nativeObj, transVect.nativeObj, rotMatrixX.nativeObj, rotMatrixY.nativeObj, rotMatrixZ.nativeObj);
+    }
+
+    /**
+     * Decomposes a projection matrix into a rotation matrix and a camera matrix.
+     *
+     * @param projMatrix 3x4 input projection matrix P.
+     * @param cameraMatrix Output 3x3 camera matrix K.
+     * @param rotMatrix Output 3x3 external rotation matrix R.
+     * @param transVect Output 4x1 translation vector T.
+     * @param rotMatrixX Optional 3x3 rotation matrix around x-axis.
+     * @param rotMatrixY Optional 3x3 rotation matrix around y-axis.
+     * degrees.
+     *
+     * The function computes a decomposition of a projection matrix into a calibration and a rotation
+     * matrix and the position of a camera.
+     *
+     * It optionally returns three rotation matrices, one for each axis, and three Euler angles that could
+     * be used in OpenGL. Note, there is always more than one sequence of rotations about the three
+     * principal axes that results in the same orientation of an object, e.g. see CITE: Slabaugh . Returned
+     * tree rotation matrices and corresponding three Euler angles are only one of the possible solutions.
+     *
+     * The function is based on RQDecomp3x3 .
+     */
+    public static void decomposeProjectionMatrix(Mat projMatrix, Mat cameraMatrix, Mat rotMatrix, Mat transVect, Mat rotMatrixX, Mat rotMatrixY) {
+        decomposeProjectionMatrix_2(projMatrix.nativeObj, cameraMatrix.nativeObj, rotMatrix.nativeObj, transVect.nativeObj, rotMatrixX.nativeObj, rotMatrixY.nativeObj);
+    }
+
+    /**
+     * Decomposes a projection matrix into a rotation matrix and a camera matrix.
+     *
+     * @param projMatrix 3x4 input projection matrix P.
+     * @param cameraMatrix Output 3x3 camera matrix K.
+     * @param rotMatrix Output 3x3 external rotation matrix R.
+     * @param transVect Output 4x1 translation vector T.
+     * @param rotMatrixX Optional 3x3 rotation matrix around x-axis.
+     * degrees.
+     *
+     * The function computes a decomposition of a projection matrix into a calibration and a rotation
+     * matrix and the position of a camera.
+     *
+     * It optionally returns three rotation matrices, one for each axis, and three Euler angles that could
+     * be used in OpenGL. Note, there is always more than one sequence of rotations about the three
+     * principal axes that results in the same orientation of an object, e.g. see CITE: Slabaugh . Returned
+     * tree rotation matrices and corresponding three Euler angles are only one of the possible solutions.
+     *
+     * The function is based on RQDecomp3x3 .
+     */
+    public static void decomposeProjectionMatrix(Mat projMatrix, Mat cameraMatrix, Mat rotMatrix, Mat transVect, Mat rotMatrixX) {
+        decomposeProjectionMatrix_3(projMatrix.nativeObj, cameraMatrix.nativeObj, rotMatrix.nativeObj, transVect.nativeObj, rotMatrixX.nativeObj);
+    }
+
+    /**
+     * Decomposes a projection matrix into a rotation matrix and a camera matrix.
+     *
+     * @param projMatrix 3x4 input projection matrix P.
+     * @param cameraMatrix Output 3x3 camera matrix K.
+     * @param rotMatrix Output 3x3 external rotation matrix R.
+     * @param transVect Output 4x1 translation vector T.
+     * degrees.
+     *
+     * The function computes a decomposition of a projection matrix into a calibration and a rotation
+     * matrix and the position of a camera.
+     *
+     * It optionally returns three rotation matrices, one for each axis, and three Euler angles that could
+     * be used in OpenGL. Note, there is always more than one sequence of rotations about the three
+     * principal axes that results in the same orientation of an object, e.g. see CITE: Slabaugh . Returned
+     * tree rotation matrices and corresponding three Euler angles are only one of the possible solutions.
+     *
+     * The function is based on RQDecomp3x3 .
+     */
+    public static void decomposeProjectionMatrix(Mat projMatrix, Mat cameraMatrix, Mat rotMatrix, Mat transVect) {
+        decomposeProjectionMatrix_4(projMatrix.nativeObj, cameraMatrix.nativeObj, rotMatrix.nativeObj, transVect.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::drawChessboardCorners(Mat& image, Size patternSize, vector_Point2f corners, bool patternWasFound)
+    //
+
+    /**
+     * Renders the detected chessboard corners.
+     *
+     * @param image Destination image. It must be an 8-bit color image.
+     * @param patternSize Number of inner corners per a chessboard row and column
+     * (patternSize = cv::Size(points_per_row,points_per_column)).
+     * @param corners Array of detected corners, the output of findChessboardCorners.
+     * @param patternWasFound Parameter indicating whether the complete board was found or not. The
+     * return value of findChessboardCorners should be passed here.
+     *
+     * The function draws individual chessboard corners detected either as red circles if the board was not
+     * found, or as colored corners connected with lines if the board was found.
+     */
+    public static void drawChessboardCorners(Mat image, Size patternSize, MatOfPoint2f corners, boolean patternWasFound) {
+        Mat corners_mat = corners;
+        drawChessboardCorners_0(image.nativeObj, patternSize.width, patternSize.height, corners_mat.nativeObj, patternWasFound);
+    }
+
+
+    //
+    // C++:  void cv::drawFrameAxes(Mat& image, Mat cameraMatrix, Mat distCoeffs, Mat rvec, Mat tvec, float length, int thickness = 3)
+    //
+
+    /**
+     * Draw axes of the world/object coordinate system from pose estimation. SEE: solvePnP
+     *
+     * @param image Input/output image. It must have 1 or 3 channels. The number of channels is not altered.
+     * @param cameraMatrix Input 3x3 floating-point matrix of camera intrinsic parameters.
+     * \(A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}\)
+     * @param distCoeffs Input vector of distortion coefficients
+     * \((k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\) of
+     * 4, 5, 8, 12 or 14 elements. If the vector is empty, the zero distortion coefficients are assumed.
+     * @param rvec Rotation vector (see REF: Rodrigues ) that, together with tvec, brings points from
+     * the model coordinate system to the camera coordinate system.
+     * @param tvec Translation vector.
+     * @param length Length of the painted axes in the same unit than tvec (usually in meters).
+     * @param thickness Line thickness of the painted axes.
+     *
+     * This function draws the axes of the world/object coordinate system w.r.t. to the camera frame.
+     * OX is drawn in red, OY in green and OZ in blue.
+     */
+    public static void drawFrameAxes(Mat image, Mat cameraMatrix, Mat distCoeffs, Mat rvec, Mat tvec, float length, int thickness) {
+        drawFrameAxes_0(image.nativeObj, cameraMatrix.nativeObj, distCoeffs.nativeObj, rvec.nativeObj, tvec.nativeObj, length, thickness);
+    }
+
+    /**
+     * Draw axes of the world/object coordinate system from pose estimation. SEE: solvePnP
+     *
+     * @param image Input/output image. It must have 1 or 3 channels. The number of channels is not altered.
+     * @param cameraMatrix Input 3x3 floating-point matrix of camera intrinsic parameters.
+     * \(A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}\)
+     * @param distCoeffs Input vector of distortion coefficients
+     * \((k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\) of
+     * 4, 5, 8, 12 or 14 elements. If the vector is empty, the zero distortion coefficients are assumed.
+     * @param rvec Rotation vector (see REF: Rodrigues ) that, together with tvec, brings points from
+     * the model coordinate system to the camera coordinate system.
+     * @param tvec Translation vector.
+     * @param length Length of the painted axes in the same unit than tvec (usually in meters).
+     *
+     * This function draws the axes of the world/object coordinate system w.r.t. to the camera frame.
+     * OX is drawn in red, OY in green and OZ in blue.
+     */
+    public static void drawFrameAxes(Mat image, Mat cameraMatrix, Mat distCoeffs, Mat rvec, Mat tvec, float length) {
+        drawFrameAxes_1(image.nativeObj, cameraMatrix.nativeObj, distCoeffs.nativeObj, rvec.nativeObj, tvec.nativeObj, length);
+    }
+
+
+    //
+    // C++:  void cv::filterHomographyDecompByVisibleRefpoints(vector_Mat rotations, vector_Mat normals, Mat beforePoints, Mat afterPoints, Mat& possibleSolutions, Mat pointsMask = Mat())
+    //
+
+    /**
+     * Filters homography decompositions based on additional information.
+     *
+     * @param rotations Vector of rotation matrices.
+     * @param normals Vector of plane normal matrices.
+     * @param beforePoints Vector of (rectified) visible reference points before the homography is applied
+     * @param afterPoints Vector of (rectified) visible reference points after the homography is applied
+     * @param possibleSolutions Vector of int indices representing the viable solution set after filtering
+     * @param pointsMask optional Mat/Vector of 8u type representing the mask for the inliers as given by the findHomography function
+     *
+     * This function is intended to filter the output of the decomposeHomographyMat based on additional
+     * information as described in CITE: Malis . The summary of the method: the decomposeHomographyMat function
+     * returns 2 unique solutions and their "opposites" for a total of 4 solutions. If we have access to the
+     * sets of points visible in the camera frame before and after the homography transformation is applied,
+     * we can determine which are the true potential solutions and which are the opposites by verifying which
+     * homographies are consistent with all visible reference points being in front of the camera. The inputs
+     * are left unchanged; the filtered solution set is returned as indices into the existing one.
+     */
+    public static void filterHomographyDecompByVisibleRefpoints(List<Mat> rotations, List<Mat> normals, Mat beforePoints, Mat afterPoints, Mat possibleSolutions, Mat pointsMask) {
+        Mat rotations_mat = Converters.vector_Mat_to_Mat(rotations);
+        Mat normals_mat = Converters.vector_Mat_to_Mat(normals);
+        filterHomographyDecompByVisibleRefpoints_0(rotations_mat.nativeObj, normals_mat.nativeObj, beforePoints.nativeObj, afterPoints.nativeObj, possibleSolutions.nativeObj, pointsMask.nativeObj);
+    }
+
+    /**
+     * Filters homography decompositions based on additional information.
+     *
+     * @param rotations Vector of rotation matrices.
+     * @param normals Vector of plane normal matrices.
+     * @param beforePoints Vector of (rectified) visible reference points before the homography is applied
+     * @param afterPoints Vector of (rectified) visible reference points after the homography is applied
+     * @param possibleSolutions Vector of int indices representing the viable solution set after filtering
+     *
+     * This function is intended to filter the output of the decomposeHomographyMat based on additional
+     * information as described in CITE: Malis . The summary of the method: the decomposeHomographyMat function
+     * returns 2 unique solutions and their "opposites" for a total of 4 solutions. If we have access to the
+     * sets of points visible in the camera frame before and after the homography transformation is applied,
+     * we can determine which are the true potential solutions and which are the opposites by verifying which
+     * homographies are consistent with all visible reference points being in front of the camera. The inputs
+     * are left unchanged; the filtered solution set is returned as indices into the existing one.
+     */
+    public static void filterHomographyDecompByVisibleRefpoints(List<Mat> rotations, List<Mat> normals, Mat beforePoints, Mat afterPoints, Mat possibleSolutions) {
+        Mat rotations_mat = Converters.vector_Mat_to_Mat(rotations);
+        Mat normals_mat = Converters.vector_Mat_to_Mat(normals);
+        filterHomographyDecompByVisibleRefpoints_1(rotations_mat.nativeObj, normals_mat.nativeObj, beforePoints.nativeObj, afterPoints.nativeObj, possibleSolutions.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::filterSpeckles(Mat& img, double newVal, int maxSpeckleSize, double maxDiff, Mat& buf = Mat())
+    //
+
+    /**
+     * Filters off small noise blobs (speckles) in the disparity map
+     *
+     * @param img The input 16-bit signed disparity image
+     * @param newVal The disparity value used to paint-off the speckles
+     * @param maxSpeckleSize The maximum speckle size to consider it a speckle. Larger blobs are not
+     * affected by the algorithm
+     * @param maxDiff Maximum difference between neighbor disparity pixels to put them into the same
+     * blob. Note that since StereoBM, StereoSGBM and may be other algorithms return a fixed-point
+     * disparity map, where disparity values are multiplied by 16, this scale factor should be taken into
+     * account when specifying this parameter value.
+     * @param buf The optional temporary buffer to avoid memory allocation within the function.
+     */
+    public static void filterSpeckles(Mat img, double newVal, int maxSpeckleSize, double maxDiff, Mat buf) {
+        filterSpeckles_0(img.nativeObj, newVal, maxSpeckleSize, maxDiff, buf.nativeObj);
+    }
+
+    /**
+     * Filters off small noise blobs (speckles) in the disparity map
+     *
+     * @param img The input 16-bit signed disparity image
+     * @param newVal The disparity value used to paint-off the speckles
+     * @param maxSpeckleSize The maximum speckle size to consider it a speckle. Larger blobs are not
+     * affected by the algorithm
+     * @param maxDiff Maximum difference between neighbor disparity pixels to put them into the same
+     * blob. Note that since StereoBM, StereoSGBM and may be other algorithms return a fixed-point
+     * disparity map, where disparity values are multiplied by 16, this scale factor should be taken into
+     * account when specifying this parameter value.
+     */
+    public static void filterSpeckles(Mat img, double newVal, int maxSpeckleSize, double maxDiff) {
+        filterSpeckles_1(img.nativeObj, newVal, maxSpeckleSize, maxDiff);
+    }
+
+
+    //
+    // C++:  void cv::matMulDeriv(Mat A, Mat B, Mat& dABdA, Mat& dABdB)
+    //
+
+    /**
+     * Computes partial derivatives of the matrix product for each multiplied matrix.
+     *
+     * @param A First multiplied matrix.
+     * @param B Second multiplied matrix.
+     * @param dABdA First output derivative matrix d(A\*B)/dA of size
+     * \(\texttt{A.rows*B.cols} \times {A.rows*A.cols}\) .
+     * @param dABdB Second output derivative matrix d(A\*B)/dB of size
+     * \(\texttt{A.rows*B.cols} \times {B.rows*B.cols}\) .
+     *
+     * The function computes partial derivatives of the elements of the matrix product \(A*B\) with regard to
+     * the elements of each of the two input matrices. The function is used to compute the Jacobian
+     * matrices in stereoCalibrate but can also be used in any other similar optimization function.
+     */
+    public static void matMulDeriv(Mat A, Mat B, Mat dABdA, Mat dABdB) {
+        matMulDeriv_0(A.nativeObj, B.nativeObj, dABdA.nativeObj, dABdB.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::projectPoints(vector_Point3f objectPoints, Mat rvec, Mat tvec, Mat cameraMatrix, vector_double distCoeffs, vector_Point2f& imagePoints, Mat& jacobian = Mat(), double aspectRatio = 0)
+    //
+
+    /**
+     * Projects 3D points to an image plane.
+     *
+     * @param objectPoints Array of object points expressed wrt. the world coordinate frame. A 3xN/Nx3
+     * 1-channel or 1xN/Nx1 3-channel (or vector&lt;Point3f&gt; ), where N is the number of points in the view.
+     * @param rvec The rotation vector (REF: Rodrigues) that, together with tvec, performs a change of
+     * basis from world to camera coordinate system, see REF: calibrateCamera for details.
+     * @param tvec The translation vector, see parameter description above.
+     * @param cameraMatrix Camera matrix \(A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{_1}\) .
+     * @param distCoeffs Input vector of distortion coefficients
+     * \((k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\) of
+     * 4, 5, 8, 12 or 14 elements. If the vector is empty, the zero distortion coefficients are assumed.
+     * @param imagePoints Output array of image points, 1xN/Nx1 2-channel, or
+     * vector&lt;Point2f&gt; .
+     * @param jacobian Optional output 2Nx(10+&lt;numDistCoeffs&gt;) jacobian matrix of derivatives of image
+     * points with respect to components of the rotation vector, translation vector, focal lengths,
+     * coordinates of the principal point and the distortion coefficients. In the old interface different
+     * components of the jacobian are returned via different output parameters.
+     * @param aspectRatio Optional "fixed aspect ratio" parameter. If the parameter is not 0, the
+     * function assumes that the aspect ratio (\(f_x / f_y\)) is fixed and correspondingly adjusts the
+     * jacobian matrix.
+     *
+     * The function computes the 2D projections of 3D points to the image plane, given intrinsic and
+     * extrinsic camera parameters. Optionally, the function computes Jacobians -matrices of partial
+     * derivatives of image points coordinates (as functions of all the input parameters) with respect to
+     * the particular parameters, intrinsic and/or extrinsic. The Jacobians are used during the global
+     * optimization in REF: calibrateCamera, REF: solvePnP, and REF: stereoCalibrate. The function itself
+     * can also be used to compute a re-projection error, given the current intrinsic and extrinsic
+     * parameters.
+     *
+     * <b>Note:</b> By setting rvec = tvec = \([0, 0, 0]\), or by setting cameraMatrix to a 3x3 identity matrix,
+     * or by passing zero distortion coefficients, one can get various useful partial cases of the
+     * function. This means, one can compute the distorted coordinates for a sparse set of points or apply
+     * a perspective transformation (and also compute the derivatives) in the ideal zero-distortion setup.
+     */
+    public static void projectPoints(MatOfPoint3f objectPoints, Mat rvec, Mat tvec, Mat cameraMatrix, MatOfDouble distCoeffs, MatOfPoint2f imagePoints, Mat jacobian, double aspectRatio) {
+        Mat objectPoints_mat = objectPoints;
+        Mat distCoeffs_mat = distCoeffs;
+        Mat imagePoints_mat = imagePoints;
+        projectPoints_0(objectPoints_mat.nativeObj, rvec.nativeObj, tvec.nativeObj, cameraMatrix.nativeObj, distCoeffs_mat.nativeObj, imagePoints_mat.nativeObj, jacobian.nativeObj, aspectRatio);
+    }
+
+    /**
+     * Projects 3D points to an image plane.
+     *
+     * @param objectPoints Array of object points expressed wrt. the world coordinate frame. A 3xN/Nx3
+     * 1-channel or 1xN/Nx1 3-channel (or vector&lt;Point3f&gt; ), where N is the number of points in the view.
+     * @param rvec The rotation vector (REF: Rodrigues) that, together with tvec, performs a change of
+     * basis from world to camera coordinate system, see REF: calibrateCamera for details.
+     * @param tvec The translation vector, see parameter description above.
+     * @param cameraMatrix Camera matrix \(A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{_1}\) .
+     * @param distCoeffs Input vector of distortion coefficients
+     * \((k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\) of
+     * 4, 5, 8, 12 or 14 elements. If the vector is empty, the zero distortion coefficients are assumed.
+     * @param imagePoints Output array of image points, 1xN/Nx1 2-channel, or
+     * vector&lt;Point2f&gt; .
+     * @param jacobian Optional output 2Nx(10+&lt;numDistCoeffs&gt;) jacobian matrix of derivatives of image
+     * points with respect to components of the rotation vector, translation vector, focal lengths,
+     * coordinates of the principal point and the distortion coefficients. In the old interface different
+     * components of the jacobian are returned via different output parameters.
+     * function assumes that the aspect ratio (\(f_x / f_y\)) is fixed and correspondingly adjusts the
+     * jacobian matrix.
+     *
+     * The function computes the 2D projections of 3D points to the image plane, given intrinsic and
+     * extrinsic camera parameters. Optionally, the function computes Jacobians -matrices of partial
+     * derivatives of image points coordinates (as functions of all the input parameters) with respect to
+     * the particular parameters, intrinsic and/or extrinsic. The Jacobians are used during the global
+     * optimization in REF: calibrateCamera, REF: solvePnP, and REF: stereoCalibrate. The function itself
+     * can also be used to compute a re-projection error, given the current intrinsic and extrinsic
+     * parameters.
+     *
+     * <b>Note:</b> By setting rvec = tvec = \([0, 0, 0]\), or by setting cameraMatrix to a 3x3 identity matrix,
+     * or by passing zero distortion coefficients, one can get various useful partial cases of the
+     * function. This means, one can compute the distorted coordinates for a sparse set of points or apply
+     * a perspective transformation (and also compute the derivatives) in the ideal zero-distortion setup.
+     */
+    public static void projectPoints(MatOfPoint3f objectPoints, Mat rvec, Mat tvec, Mat cameraMatrix, MatOfDouble distCoeffs, MatOfPoint2f imagePoints, Mat jacobian) {
+        Mat objectPoints_mat = objectPoints;
+        Mat distCoeffs_mat = distCoeffs;
+        Mat imagePoints_mat = imagePoints;
+        projectPoints_1(objectPoints_mat.nativeObj, rvec.nativeObj, tvec.nativeObj, cameraMatrix.nativeObj, distCoeffs_mat.nativeObj, imagePoints_mat.nativeObj, jacobian.nativeObj);
+    }
+
+    /**
+     * Projects 3D points to an image plane.
+     *
+     * @param objectPoints Array of object points expressed wrt. the world coordinate frame. A 3xN/Nx3
+     * 1-channel or 1xN/Nx1 3-channel (or vector&lt;Point3f&gt; ), where N is the number of points in the view.
+     * @param rvec The rotation vector (REF: Rodrigues) that, together with tvec, performs a change of
+     * basis from world to camera coordinate system, see REF: calibrateCamera for details.
+     * @param tvec The translation vector, see parameter description above.
+     * @param cameraMatrix Camera matrix \(A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{_1}\) .
+     * @param distCoeffs Input vector of distortion coefficients
+     * \((k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\) of
+     * 4, 5, 8, 12 or 14 elements. If the vector is empty, the zero distortion coefficients are assumed.
+     * @param imagePoints Output array of image points, 1xN/Nx1 2-channel, or
+     * vector&lt;Point2f&gt; .
+     * points with respect to components of the rotation vector, translation vector, focal lengths,
+     * coordinates of the principal point and the distortion coefficients. In the old interface different
+     * components of the jacobian are returned via different output parameters.
+     * function assumes that the aspect ratio (\(f_x / f_y\)) is fixed and correspondingly adjusts the
+     * jacobian matrix.
+     *
+     * The function computes the 2D projections of 3D points to the image plane, given intrinsic and
+     * extrinsic camera parameters. Optionally, the function computes Jacobians -matrices of partial
+     * derivatives of image points coordinates (as functions of all the input parameters) with respect to
+     * the particular parameters, intrinsic and/or extrinsic. The Jacobians are used during the global
+     * optimization in REF: calibrateCamera, REF: solvePnP, and REF: stereoCalibrate. The function itself
+     * can also be used to compute a re-projection error, given the current intrinsic and extrinsic
+     * parameters.
+     *
+     * <b>Note:</b> By setting rvec = tvec = \([0, 0, 0]\), or by setting cameraMatrix to a 3x3 identity matrix,
+     * or by passing zero distortion coefficients, one can get various useful partial cases of the
+     * function. This means, one can compute the distorted coordinates for a sparse set of points or apply
+     * a perspective transformation (and also compute the derivatives) in the ideal zero-distortion setup.
+     */
+    public static void projectPoints(MatOfPoint3f objectPoints, Mat rvec, Mat tvec, Mat cameraMatrix, MatOfDouble distCoeffs, MatOfPoint2f imagePoints) {
+        Mat objectPoints_mat = objectPoints;
+        Mat distCoeffs_mat = distCoeffs;
+        Mat imagePoints_mat = imagePoints;
+        projectPoints_2(objectPoints_mat.nativeObj, rvec.nativeObj, tvec.nativeObj, cameraMatrix.nativeObj, distCoeffs_mat.nativeObj, imagePoints_mat.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::reprojectImageTo3D(Mat disparity, Mat& _3dImage, Mat Q, bool handleMissingValues = false, int ddepth = -1)
+    //
+
+    /**
+     * Reprojects a disparity image to 3D space.
+     *
+     * @param disparity Input single-channel 8-bit unsigned, 16-bit signed, 32-bit signed or 32-bit
+     * floating-point disparity image. The values of 8-bit / 16-bit signed formats are assumed to have no
+     * fractional bits. If the disparity is 16-bit signed format, as computed by REF: StereoBM or
+     * REF: StereoSGBM and maybe other algorithms, it should be divided by 16 (and scaled to float) before
+     * being used here.
+     * @param _3dImage Output 3-channel floating-point image of the same size as disparity. Each element of
+     * _3dImage(x,y) contains 3D coordinates of the point (x,y) computed from the disparity map. If one
+     * uses Q obtained by REF: stereoRectify, then the returned points are represented in the first
+     * camera's rectified coordinate system.
+     * @param Q \(4 \times 4\) perspective transformation matrix that can be obtained with
+     * REF: stereoRectify.
+     * @param handleMissingValues Indicates, whether the function should handle missing values (i.e.
+     * points where the disparity was not computed). If handleMissingValues=true, then pixels with the
+     * minimal disparity that corresponds to the outliers (see StereoMatcher::compute ) are transformed
+     * to 3D points with a very large Z value (currently set to 10000).
+     * @param ddepth The optional output array depth. If it is -1, the output image will have CV_32F
+     * depth. ddepth can also be set to CV_16S, CV_32S or CV_32F.
+     *
+     * The function transforms a single-channel disparity map to a 3-channel image representing a 3D
+     * surface. That is, for each pixel (x,y) and the corresponding disparity d=disparity(x,y) , it
+     * computes:
+     *
+     * \(\begin{bmatrix}
+     * X \\
+     * Y \\
+     * Z \\
+     * W
+     * \end{bmatrix} = Q \begin{bmatrix}
+     * x \\
+     * y \\
+     * \texttt{disparity} (x,y) \\
+     * z
+     * \end{bmatrix}.\)
+     *
+     * SEE:
+     *    To reproject a sparse set of points {(x,y,d),...} to 3D space, use perspectiveTransform.
+     */
+    public static void reprojectImageTo3D(Mat disparity, Mat _3dImage, Mat Q, boolean handleMissingValues, int ddepth) {
+        reprojectImageTo3D_0(disparity.nativeObj, _3dImage.nativeObj, Q.nativeObj, handleMissingValues, ddepth);
+    }
+
+    /**
+     * Reprojects a disparity image to 3D space.
+     *
+     * @param disparity Input single-channel 8-bit unsigned, 16-bit signed, 32-bit signed or 32-bit
+     * floating-point disparity image. The values of 8-bit / 16-bit signed formats are assumed to have no
+     * fractional bits. If the disparity is 16-bit signed format, as computed by REF: StereoBM or
+     * REF: StereoSGBM and maybe other algorithms, it should be divided by 16 (and scaled to float) before
+     * being used here.
+     * @param _3dImage Output 3-channel floating-point image of the same size as disparity. Each element of
+     * _3dImage(x,y) contains 3D coordinates of the point (x,y) computed from the disparity map. If one
+     * uses Q obtained by REF: stereoRectify, then the returned points are represented in the first
+     * camera's rectified coordinate system.
+     * @param Q \(4 \times 4\) perspective transformation matrix that can be obtained with
+     * REF: stereoRectify.
+     * @param handleMissingValues Indicates, whether the function should handle missing values (i.e.
+     * points where the disparity was not computed). If handleMissingValues=true, then pixels with the
+     * minimal disparity that corresponds to the outliers (see StereoMatcher::compute ) are transformed
+     * to 3D points with a very large Z value (currently set to 10000).
+     * depth. ddepth can also be set to CV_16S, CV_32S or CV_32F.
+     *
+     * The function transforms a single-channel disparity map to a 3-channel image representing a 3D
+     * surface. That is, for each pixel (x,y) and the corresponding disparity d=disparity(x,y) , it
+     * computes:
+     *
+     * \(\begin{bmatrix}
+     * X \\
+     * Y \\
+     * Z \\
+     * W
+     * \end{bmatrix} = Q \begin{bmatrix}
+     * x \\
+     * y \\
+     * \texttt{disparity} (x,y) \\
+     * z
+     * \end{bmatrix}.\)
+     *
+     * SEE:
+     *    To reproject a sparse set of points {(x,y,d),...} to 3D space, use perspectiveTransform.
+     */
+    public static void reprojectImageTo3D(Mat disparity, Mat _3dImage, Mat Q, boolean handleMissingValues) {
+        reprojectImageTo3D_1(disparity.nativeObj, _3dImage.nativeObj, Q.nativeObj, handleMissingValues);
+    }
+
+    /**
+     * Reprojects a disparity image to 3D space.
+     *
+     * @param disparity Input single-channel 8-bit unsigned, 16-bit signed, 32-bit signed or 32-bit
+     * floating-point disparity image. The values of 8-bit / 16-bit signed formats are assumed to have no
+     * fractional bits. If the disparity is 16-bit signed format, as computed by REF: StereoBM or
+     * REF: StereoSGBM and maybe other algorithms, it should be divided by 16 (and scaled to float) before
+     * being used here.
+     * @param _3dImage Output 3-channel floating-point image of the same size as disparity. Each element of
+     * _3dImage(x,y) contains 3D coordinates of the point (x,y) computed from the disparity map. If one
+     * uses Q obtained by REF: stereoRectify, then the returned points are represented in the first
+     * camera's rectified coordinate system.
+     * @param Q \(4 \times 4\) perspective transformation matrix that can be obtained with
+     * REF: stereoRectify.
+     * points where the disparity was not computed). If handleMissingValues=true, then pixels with the
+     * minimal disparity that corresponds to the outliers (see StereoMatcher::compute ) are transformed
+     * to 3D points with a very large Z value (currently set to 10000).
+     * depth. ddepth can also be set to CV_16S, CV_32S or CV_32F.
+     *
+     * The function transforms a single-channel disparity map to a 3-channel image representing a 3D
+     * surface. That is, for each pixel (x,y) and the corresponding disparity d=disparity(x,y) , it
+     * computes:
+     *
+     * \(\begin{bmatrix}
+     * X \\
+     * Y \\
+     * Z \\
+     * W
+     * \end{bmatrix} = Q \begin{bmatrix}
+     * x \\
+     * y \\
+     * \texttt{disparity} (x,y) \\
+     * z
+     * \end{bmatrix}.\)
+     *
+     * SEE:
+     *    To reproject a sparse set of points {(x,y,d),...} to 3D space, use perspectiveTransform.
+     */
+    public static void reprojectImageTo3D(Mat disparity, Mat _3dImage, Mat Q) {
+        reprojectImageTo3D_2(disparity.nativeObj, _3dImage.nativeObj, Q.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::solvePnPRefineLM(Mat objectPoints, Mat imagePoints, Mat cameraMatrix, Mat distCoeffs, Mat& rvec, Mat& tvec, TermCriteria criteria = TermCriteria(TermCriteria::EPS + TermCriteria::COUNT, 20, FLT_EPSILON))
+    //
+
+    /**
+     * Refine a pose (the translation and the rotation that transform a 3D point expressed in the object coordinate frame
+     * to the camera coordinate frame) from a 3D-2D point correspondences and starting from an initial solution.
+     *
+     * @param objectPoints Array of object points in the object coordinate space, Nx3 1-channel or 1xN/Nx1 3-channel,
+     * where N is the number of points. vector&lt;Point3d&gt; can also be passed here.
+     * @param imagePoints Array of corresponding image points, Nx2 1-channel or 1xN/Nx1 2-channel,
+     * where N is the number of points. vector&lt;Point2d&gt; can also be passed here.
+     * @param cameraMatrix Input camera matrix \(A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}\) .
+     * @param distCoeffs Input vector of distortion coefficients
+     * \((k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\) of
+     * 4, 5, 8, 12 or 14 elements. If the vector is NULL/empty, the zero distortion coefficients are
+     * assumed.
+     * @param rvec Input/Output rotation vector (see REF: Rodrigues ) that, together with tvec, brings points from
+     * the model coordinate system to the camera coordinate system. Input values are used as an initial solution.
+     * @param tvec Input/Output translation vector. Input values are used as an initial solution.
+     * @param criteria Criteria when to stop the Levenberg-Marquard iterative algorithm.
+     *
+     * The function refines the object pose given at least 3 object points, their corresponding image
+     * projections, an initial solution for the rotation and translation vector,
+     * as well as the camera matrix and the distortion coefficients.
+     * The function minimizes the projection error with respect to the rotation and the translation vectors, according
+     * to a Levenberg-Marquardt iterative minimization CITE: Madsen04 CITE: Eade13 process.
+     */
+    public static void solvePnPRefineLM(Mat objectPoints, Mat imagePoints, Mat cameraMatrix, Mat distCoeffs, Mat rvec, Mat tvec, TermCriteria criteria) {
+        solvePnPRefineLM_0(objectPoints.nativeObj, imagePoints.nativeObj, cameraMatrix.nativeObj, distCoeffs.nativeObj, rvec.nativeObj, tvec.nativeObj, criteria.type, criteria.maxCount, criteria.epsilon);
+    }
+
+    /**
+     * Refine a pose (the translation and the rotation that transform a 3D point expressed in the object coordinate frame
+     * to the camera coordinate frame) from a 3D-2D point correspondences and starting from an initial solution.
+     *
+     * @param objectPoints Array of object points in the object coordinate space, Nx3 1-channel or 1xN/Nx1 3-channel,
+     * where N is the number of points. vector&lt;Point3d&gt; can also be passed here.
+     * @param imagePoints Array of corresponding image points, Nx2 1-channel or 1xN/Nx1 2-channel,
+     * where N is the number of points. vector&lt;Point2d&gt; can also be passed here.
+     * @param cameraMatrix Input camera matrix \(A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}\) .
+     * @param distCoeffs Input vector of distortion coefficients
+     * \((k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\) of
+     * 4, 5, 8, 12 or 14 elements. If the vector is NULL/empty, the zero distortion coefficients are
+     * assumed.
+     * @param rvec Input/Output rotation vector (see REF: Rodrigues ) that, together with tvec, brings points from
+     * the model coordinate system to the camera coordinate system. Input values are used as an initial solution.
+     * @param tvec Input/Output translation vector. Input values are used as an initial solution.
+     *
+     * The function refines the object pose given at least 3 object points, their corresponding image
+     * projections, an initial solution for the rotation and translation vector,
+     * as well as the camera matrix and the distortion coefficients.
+     * The function minimizes the projection error with respect to the rotation and the translation vectors, according
+     * to a Levenberg-Marquardt iterative minimization CITE: Madsen04 CITE: Eade13 process.
+     */
+    public static void solvePnPRefineLM(Mat objectPoints, Mat imagePoints, Mat cameraMatrix, Mat distCoeffs, Mat rvec, Mat tvec) {
+        solvePnPRefineLM_1(objectPoints.nativeObj, imagePoints.nativeObj, cameraMatrix.nativeObj, distCoeffs.nativeObj, rvec.nativeObj, tvec.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::solvePnPRefineVVS(Mat objectPoints, Mat imagePoints, Mat cameraMatrix, Mat distCoeffs, Mat& rvec, Mat& tvec, TermCriteria criteria = TermCriteria(TermCriteria::EPS + TermCriteria::COUNT, 20, FLT_EPSILON), double VVSlambda = 1)
+    //
+
+    /**
+     * Refine a pose (the translation and the rotation that transform a 3D point expressed in the object coordinate frame
+     * to the camera coordinate frame) from a 3D-2D point correspondences and starting from an initial solution.
+     *
+     * @param objectPoints Array of object points in the object coordinate space, Nx3 1-channel or 1xN/Nx1 3-channel,
+     * where N is the number of points. vector&lt;Point3d&gt; can also be passed here.
+     * @param imagePoints Array of corresponding image points, Nx2 1-channel or 1xN/Nx1 2-channel,
+     * where N is the number of points. vector&lt;Point2d&gt; can also be passed here.
+     * @param cameraMatrix Input camera matrix \(A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}\) .
+     * @param distCoeffs Input vector of distortion coefficients
+     * \((k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\) of
+     * 4, 5, 8, 12 or 14 elements. If the vector is NULL/empty, the zero distortion coefficients are
+     * assumed.
+     * @param rvec Input/Output rotation vector (see REF: Rodrigues ) that, together with tvec, brings points from
+     * the model coordinate system to the camera coordinate system. Input values are used as an initial solution.
+     * @param tvec Input/Output translation vector. Input values are used as an initial solution.
+     * @param criteria Criteria when to stop the Levenberg-Marquard iterative algorithm.
+     * @param VVSlambda Gain for the virtual visual servoing control law, equivalent to the \(\alpha\)
+     * gain in the Damped Gauss-Newton formulation.
+     *
+     * The function refines the object pose given at least 3 object points, their corresponding image
+     * projections, an initial solution for the rotation and translation vector,
+     * as well as the camera matrix and the distortion coefficients.
+     * The function minimizes the projection error with respect to the rotation and the translation vectors, using a
+     * virtual visual servoing (VVS) CITE: Chaumette06 CITE: Marchand16 scheme.
+     */
+    public static void solvePnPRefineVVS(Mat objectPoints, Mat imagePoints, Mat cameraMatrix, Mat distCoeffs, Mat rvec, Mat tvec, TermCriteria criteria, double VVSlambda) {
+        solvePnPRefineVVS_0(objectPoints.nativeObj, imagePoints.nativeObj, cameraMatrix.nativeObj, distCoeffs.nativeObj, rvec.nativeObj, tvec.nativeObj, criteria.type, criteria.maxCount, criteria.epsilon, VVSlambda);
+    }
+
+    /**
+     * Refine a pose (the translation and the rotation that transform a 3D point expressed in the object coordinate frame
+     * to the camera coordinate frame) from a 3D-2D point correspondences and starting from an initial solution.
+     *
+     * @param objectPoints Array of object points in the object coordinate space, Nx3 1-channel or 1xN/Nx1 3-channel,
+     * where N is the number of points. vector&lt;Point3d&gt; can also be passed here.
+     * @param imagePoints Array of corresponding image points, Nx2 1-channel or 1xN/Nx1 2-channel,
+     * where N is the number of points. vector&lt;Point2d&gt; can also be passed here.
+     * @param cameraMatrix Input camera matrix \(A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}\) .
+     * @param distCoeffs Input vector of distortion coefficients
+     * \((k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\) of
+     * 4, 5, 8, 12 or 14 elements. If the vector is NULL/empty, the zero distortion coefficients are
+     * assumed.
+     * @param rvec Input/Output rotation vector (see REF: Rodrigues ) that, together with tvec, brings points from
+     * the model coordinate system to the camera coordinate system. Input values are used as an initial solution.
+     * @param tvec Input/Output translation vector. Input values are used as an initial solution.
+     * @param criteria Criteria when to stop the Levenberg-Marquard iterative algorithm.
+     * gain in the Damped Gauss-Newton formulation.
+     *
+     * The function refines the object pose given at least 3 object points, their corresponding image
+     * projections, an initial solution for the rotation and translation vector,
+     * as well as the camera matrix and the distortion coefficients.
+     * The function minimizes the projection error with respect to the rotation and the translation vectors, using a
+     * virtual visual servoing (VVS) CITE: Chaumette06 CITE: Marchand16 scheme.
+     */
+    public static void solvePnPRefineVVS(Mat objectPoints, Mat imagePoints, Mat cameraMatrix, Mat distCoeffs, Mat rvec, Mat tvec, TermCriteria criteria) {
+        solvePnPRefineVVS_1(objectPoints.nativeObj, imagePoints.nativeObj, cameraMatrix.nativeObj, distCoeffs.nativeObj, rvec.nativeObj, tvec.nativeObj, criteria.type, criteria.maxCount, criteria.epsilon);
+    }
+
+    /**
+     * Refine a pose (the translation and the rotation that transform a 3D point expressed in the object coordinate frame
+     * to the camera coordinate frame) from a 3D-2D point correspondences and starting from an initial solution.
+     *
+     * @param objectPoints Array of object points in the object coordinate space, Nx3 1-channel or 1xN/Nx1 3-channel,
+     * where N is the number of points. vector&lt;Point3d&gt; can also be passed here.
+     * @param imagePoints Array of corresponding image points, Nx2 1-channel or 1xN/Nx1 2-channel,
+     * where N is the number of points. vector&lt;Point2d&gt; can also be passed here.
+     * @param cameraMatrix Input camera matrix \(A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}\) .
+     * @param distCoeffs Input vector of distortion coefficients
+     * \((k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\) of
+     * 4, 5, 8, 12 or 14 elements. If the vector is NULL/empty, the zero distortion coefficients are
+     * assumed.
+     * @param rvec Input/Output rotation vector (see REF: Rodrigues ) that, together with tvec, brings points from
+     * the model coordinate system to the camera coordinate system. Input values are used as an initial solution.
+     * @param tvec Input/Output translation vector. Input values are used as an initial solution.
+     * gain in the Damped Gauss-Newton formulation.
+     *
+     * The function refines the object pose given at least 3 object points, their corresponding image
+     * projections, an initial solution for the rotation and translation vector,
+     * as well as the camera matrix and the distortion coefficients.
+     * The function minimizes the projection error with respect to the rotation and the translation vectors, using a
+     * virtual visual servoing (VVS) CITE: Chaumette06 CITE: Marchand16 scheme.
+     */
+    public static void solvePnPRefineVVS(Mat objectPoints, Mat imagePoints, Mat cameraMatrix, Mat distCoeffs, Mat rvec, Mat tvec) {
+        solvePnPRefineVVS_2(objectPoints.nativeObj, imagePoints.nativeObj, cameraMatrix.nativeObj, distCoeffs.nativeObj, rvec.nativeObj, tvec.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::stereoRectify(Mat cameraMatrix1, Mat distCoeffs1, Mat cameraMatrix2, Mat distCoeffs2, Size imageSize, Mat R, Mat T, Mat& R1, Mat& R2, Mat& P1, Mat& P2, Mat& Q, int flags = CALIB_ZERO_DISPARITY, double alpha = -1, Size newImageSize = Size(), Rect* validPixROI1 = 0, Rect* validPixROI2 = 0)
+    //
+
+    /**
+     * Computes rectification transforms for each head of a calibrated stereo camera.
+     *
+     * @param cameraMatrix1 First camera matrix.
+     * @param distCoeffs1 First camera distortion parameters.
+     * @param cameraMatrix2 Second camera matrix.
+     * @param distCoeffs2 Second camera distortion parameters.
+     * @param imageSize Size of the image used for stereo calibration.
+     * @param R Rotation matrix from the coordinate system of the first camera to the second camera,
+     * see REF: stereoCalibrate.
+     * @param T Translation vector from the coordinate system of the first camera to the second camera,
+     * see REF: stereoCalibrate.
+     * @param R1 Output 3x3 rectification transform (rotation matrix) for the first camera. This matrix
+     * brings points given in the unrectified first camera's coordinate system to points in the rectified
+     * first camera's coordinate system. In more technical terms, it performs a change of basis from the
+     * unrectified first camera's coordinate system to the rectified first camera's coordinate system.
+     * @param R2 Output 3x3 rectification transform (rotation matrix) for the second camera. This matrix
+     * brings points given in the unrectified second camera's coordinate system to points in the rectified
+     * second camera's coordinate system. In more technical terms, it performs a change of basis from the
+     * unrectified second camera's coordinate system to the rectified second camera's coordinate system.
+     * @param P1 Output 3x4 projection matrix in the new (rectified) coordinate systems for the first
+     * camera, i.e. it projects points given in the rectified first camera coordinate system into the
+     * rectified first camera's image.
+     * @param P2 Output 3x4 projection matrix in the new (rectified) coordinate systems for the second
+     * camera, i.e. it projects points given in the rectified first camera coordinate system into the
+     * rectified second camera's image.
+     * @param Q Output \(4 \times 4\) disparity-to-depth mapping matrix (see REF: reprojectImageTo3D).
+     * @param flags Operation flags that may be zero or CALIB_ZERO_DISPARITY . If the flag is set,
+     * the function makes the principal points of each camera have the same pixel coordinates in the
+     * rectified views. And if the flag is not set, the function may still shift the images in the
+     * horizontal or vertical direction (depending on the orientation of epipolar lines) to maximize the
+     * useful image area.
+     * @param alpha Free scaling parameter. If it is -1 or absent, the function performs the default
+     * scaling. Otherwise, the parameter should be between 0 and 1. alpha=0 means that the rectified
+     * images are zoomed and shifted so that only valid pixels are visible (no black areas after
+     * rectification). alpha=1 means that the rectified image is decimated and shifted so that all the
+     * pixels from the original images from the cameras are retained in the rectified images (no source
+     * image pixels are lost). Any intermediate value yields an intermediate result between
+     * those two extreme cases.
+     * @param newImageSize New image resolution after rectification. The same size should be passed to
+     * initUndistortRectifyMap (see the stereo_calib.cpp sample in OpenCV samples directory). When (0,0)
+     * is passed (default), it is set to the original imageSize . Setting it to a larger value can help you
+     * preserve details in the original image, especially when there is a big radial distortion.
+     * @param validPixROI1 Optional output rectangles inside the rectified images where all the pixels
+     * are valid. If alpha=0 , the ROIs cover the whole images. Otherwise, they are likely to be smaller
+     * (see the picture below).
+     * @param validPixROI2 Optional output rectangles inside the rectified images where all the pixels
+     * are valid. If alpha=0 , the ROIs cover the whole images. Otherwise, they are likely to be smaller
+     * (see the picture below).
+     *
+     * The function computes the rotation matrices for each camera that (virtually) make both camera image
+     * planes the same plane. Consequently, this makes all the epipolar lines parallel and thus simplifies
+     * the dense stereo correspondence problem. The function takes the matrices computed by stereoCalibrate
+     * as input. As output, it provides two rotation matrices and also two projection matrices in the new
+     * coordinates. The function distinguishes the following two cases:
+     *
+     * <ul>
+     *   <li>
+     *    <b>Horizontal stereo</b>: the first and the second camera views are shifted relative to each other
+     *     mainly along the x-axis (with possible small vertical shift). In the rectified images, the
+     *     corresponding epipolar lines in the left and right cameras are horizontal and have the same
+     *     y-coordinate. P1 and P2 look like:
+     *   </li>
+     * </ul>
+     *
+     *     \(\texttt{P1} = \begin{bmatrix}
+     *                         f &amp; 0 &amp; cx_1 &amp; 0 \\
+     *                         0 &amp; f &amp; cy &amp; 0 \\
+     *                         0 &amp; 0 &amp; 1 &amp; 0
+     *                      \end{bmatrix}\)
+     *
+     *     \(\texttt{P2} = \begin{bmatrix}
+     *                         f &amp; 0 &amp; cx_2 &amp; T_x*f \\
+     *                         0 &amp; f &amp; cy &amp; 0 \\
+     *                         0 &amp; 0 &amp; 1 &amp; 0
+     *                      \end{bmatrix} ,\)
+     *
+     *     where \(T_x\) is a horizontal shift between the cameras and \(cx_1=cx_2\) if
+     *     CALIB_ZERO_DISPARITY is set.
+     *
+     * <ul>
+     *   <li>
+     *    <b>Vertical stereo</b>: the first and the second camera views are shifted relative to each other
+     *     mainly in the vertical direction (and probably a bit in the horizontal direction too). The epipolar
+     *     lines in the rectified images are vertical and have the same x-coordinate. P1 and P2 look like:
+     *   </li>
+     * </ul>
+     *
+     *     \(\texttt{P1} = \begin{bmatrix}
+     *                         f &amp; 0 &amp; cx &amp; 0 \\
+     *                         0 &amp; f &amp; cy_1 &amp; 0 \\
+     *                         0 &amp; 0 &amp; 1 &amp; 0
+     *                      \end{bmatrix}\)
+     *
+     *     \(\texttt{P2} = \begin{bmatrix}
+     *                         f &amp; 0 &amp; cx &amp; 0 \\
+     *                         0 &amp; f &amp; cy_2 &amp; T_y*f \\
+     *                         0 &amp; 0 &amp; 1 &amp; 0
+     *                      \end{bmatrix},\)
+     *
+     *     where \(T_y\) is a vertical shift between the cameras and \(cy_1=cy_2\) if
+     *     CALIB_ZERO_DISPARITY is set.
+     *
+     * As you can see, the first three columns of P1 and P2 will effectively be the new "rectified" camera
+     * matrices. The matrices, together with R1 and R2 , can then be passed to initUndistortRectifyMap to
+     * initialize the rectification map for each camera.
+     *
+     * See below the screenshot from the stereo_calib.cpp sample. Some red horizontal lines pass through
+     * the corresponding image regions. This means that the images are well rectified, which is what most
+     * stereo correspondence algorithms rely on. The green rectangles are roi1 and roi2 . You see that
+     * their interiors are all valid pixels.
+     *
+     * ![image](pics/stereo_undistort.jpg)
+     */
+    public static void stereoRectify(Mat cameraMatrix1, Mat distCoeffs1, Mat cameraMatrix2, Mat distCoeffs2, Size imageSize, Mat R, Mat T, Mat R1, Mat R2, Mat P1, Mat P2, Mat Q, int flags, double alpha, Size newImageSize, Rect validPixROI1, Rect validPixROI2) {
+        double[] validPixROI1_out = new double[4];
+        double[] validPixROI2_out = new double[4];
+        stereoRectify_0(cameraMatrix1.nativeObj, distCoeffs1.nativeObj, cameraMatrix2.nativeObj, distCoeffs2.nativeObj, imageSize.width, imageSize.height, R.nativeObj, T.nativeObj, R1.nativeObj, R2.nativeObj, P1.nativeObj, P2.nativeObj, Q.nativeObj, flags, alpha, newImageSize.width, newImageSize.height, validPixROI1_out, validPixROI2_out);
+        if(validPixROI1!=null){ validPixROI1.x = (int)validPixROI1_out[0]; validPixROI1.y = (int)validPixROI1_out[1]; validPixROI1.width = (int)validPixROI1_out[2]; validPixROI1.height = (int)validPixROI1_out[3]; } 
+        if(validPixROI2!=null){ validPixROI2.x = (int)validPixROI2_out[0]; validPixROI2.y = (int)validPixROI2_out[1]; validPixROI2.width = (int)validPixROI2_out[2]; validPixROI2.height = (int)validPixROI2_out[3]; } 
+    }
+
+    /**
+     * Computes rectification transforms for each head of a calibrated stereo camera.
+     *
+     * @param cameraMatrix1 First camera matrix.
+     * @param distCoeffs1 First camera distortion parameters.
+     * @param cameraMatrix2 Second camera matrix.
+     * @param distCoeffs2 Second camera distortion parameters.
+     * @param imageSize Size of the image used for stereo calibration.
+     * @param R Rotation matrix from the coordinate system of the first camera to the second camera,
+     * see REF: stereoCalibrate.
+     * @param T Translation vector from the coordinate system of the first camera to the second camera,
+     * see REF: stereoCalibrate.
+     * @param R1 Output 3x3 rectification transform (rotation matrix) for the first camera. This matrix
+     * brings points given in the unrectified first camera's coordinate system to points in the rectified
+     * first camera's coordinate system. In more technical terms, it performs a change of basis from the
+     * unrectified first camera's coordinate system to the rectified first camera's coordinate system.
+     * @param R2 Output 3x3 rectification transform (rotation matrix) for the second camera. This matrix
+     * brings points given in the unrectified second camera's coordinate system to points in the rectified
+     * second camera's coordinate system. In more technical terms, it performs a change of basis from the
+     * unrectified second camera's coordinate system to the rectified second camera's coordinate system.
+     * @param P1 Output 3x4 projection matrix in the new (rectified) coordinate systems for the first
+     * camera, i.e. it projects points given in the rectified first camera coordinate system into the
+     * rectified first camera's image.
+     * @param P2 Output 3x4 projection matrix in the new (rectified) coordinate systems for the second
+     * camera, i.e. it projects points given in the rectified first camera coordinate system into the
+     * rectified second camera's image.
+     * @param Q Output \(4 \times 4\) disparity-to-depth mapping matrix (see REF: reprojectImageTo3D).
+     * @param flags Operation flags that may be zero or CALIB_ZERO_DISPARITY . If the flag is set,
+     * the function makes the principal points of each camera have the same pixel coordinates in the
+     * rectified views. And if the flag is not set, the function may still shift the images in the
+     * horizontal or vertical direction (depending on the orientation of epipolar lines) to maximize the
+     * useful image area.
+     * @param alpha Free scaling parameter. If it is -1 or absent, the function performs the default
+     * scaling. Otherwise, the parameter should be between 0 and 1. alpha=0 means that the rectified
+     * images are zoomed and shifted so that only valid pixels are visible (no black areas after
+     * rectification). alpha=1 means that the rectified image is decimated and shifted so that all the
+     * pixels from the original images from the cameras are retained in the rectified images (no source
+     * image pixels are lost). Any intermediate value yields an intermediate result between
+     * those two extreme cases.
+     * @param newImageSize New image resolution after rectification. The same size should be passed to
+     * initUndistortRectifyMap (see the stereo_calib.cpp sample in OpenCV samples directory). When (0,0)
+     * is passed (default), it is set to the original imageSize . Setting it to a larger value can help you
+     * preserve details in the original image, especially when there is a big radial distortion.
+     * @param validPixROI1 Optional output rectangles inside the rectified images where all the pixels
+     * are valid. If alpha=0 , the ROIs cover the whole images. Otherwise, they are likely to be smaller
+     * (see the picture below).
+     * are valid. If alpha=0 , the ROIs cover the whole images. Otherwise, they are likely to be smaller
+     * (see the picture below).
+     *
+     * The function computes the rotation matrices for each camera that (virtually) make both camera image
+     * planes the same plane. Consequently, this makes all the epipolar lines parallel and thus simplifies
+     * the dense stereo correspondence problem. The function takes the matrices computed by stereoCalibrate
+     * as input. As output, it provides two rotation matrices and also two projection matrices in the new
+     * coordinates. The function distinguishes the following two cases:
+     *
+     * <ul>
+     *   <li>
+     *    <b>Horizontal stereo</b>: the first and the second camera views are shifted relative to each other
+     *     mainly along the x-axis (with possible small vertical shift). In the rectified images, the
+     *     corresponding epipolar lines in the left and right cameras are horizontal and have the same
+     *     y-coordinate. P1 and P2 look like:
+     *   </li>
+     * </ul>
+     *
+     *     \(\texttt{P1} = \begin{bmatrix}
+     *                         f &amp; 0 &amp; cx_1 &amp; 0 \\
+     *                         0 &amp; f &amp; cy &amp; 0 \\
+     *                         0 &amp; 0 &amp; 1 &amp; 0
+     *                      \end{bmatrix}\)
+     *
+     *     \(\texttt{P2} = \begin{bmatrix}
+     *                         f &amp; 0 &amp; cx_2 &amp; T_x*f \\
+     *                         0 &amp; f &amp; cy &amp; 0 \\
+     *                         0 &amp; 0 &amp; 1 &amp; 0
+     *                      \end{bmatrix} ,\)
+     *
+     *     where \(T_x\) is a horizontal shift between the cameras and \(cx_1=cx_2\) if
+     *     CALIB_ZERO_DISPARITY is set.
+     *
+     * <ul>
+     *   <li>
+     *    <b>Vertical stereo</b>: the first and the second camera views are shifted relative to each other
+     *     mainly in the vertical direction (and probably a bit in the horizontal direction too). The epipolar
+     *     lines in the rectified images are vertical and have the same x-coordinate. P1 and P2 look like:
+     *   </li>
+     * </ul>
+     *
+     *     \(\texttt{P1} = \begin{bmatrix}
+     *                         f &amp; 0 &amp; cx &amp; 0 \\
+     *                         0 &amp; f &amp; cy_1 &amp; 0 \\
+     *                         0 &amp; 0 &amp; 1 &amp; 0
+     *                      \end{bmatrix}\)
+     *
+     *     \(\texttt{P2} = \begin{bmatrix}
+     *                         f &amp; 0 &amp; cx &amp; 0 \\
+     *                         0 &amp; f &amp; cy_2 &amp; T_y*f \\
+     *                         0 &amp; 0 &amp; 1 &amp; 0
+     *                      \end{bmatrix},\)
+     *
+     *     where \(T_y\) is a vertical shift between the cameras and \(cy_1=cy_2\) if
+     *     CALIB_ZERO_DISPARITY is set.
+     *
+     * As you can see, the first three columns of P1 and P2 will effectively be the new "rectified" camera
+     * matrices. The matrices, together with R1 and R2 , can then be passed to initUndistortRectifyMap to
+     * initialize the rectification map for each camera.
+     *
+     * See below the screenshot from the stereo_calib.cpp sample. Some red horizontal lines pass through
+     * the corresponding image regions. This means that the images are well rectified, which is what most
+     * stereo correspondence algorithms rely on. The green rectangles are roi1 and roi2 . You see that
+     * their interiors are all valid pixels.
+     *
+     * ![image](pics/stereo_undistort.jpg)
+     */
+    public static void stereoRectify(Mat cameraMatrix1, Mat distCoeffs1, Mat cameraMatrix2, Mat distCoeffs2, Size imageSize, Mat R, Mat T, Mat R1, Mat R2, Mat P1, Mat P2, Mat Q, int flags, double alpha, Size newImageSize, Rect validPixROI1) {
+        double[] validPixROI1_out = new double[4];
+        stereoRectify_1(cameraMatrix1.nativeObj, distCoeffs1.nativeObj, cameraMatrix2.nativeObj, distCoeffs2.nativeObj, imageSize.width, imageSize.height, R.nativeObj, T.nativeObj, R1.nativeObj, R2.nativeObj, P1.nativeObj, P2.nativeObj, Q.nativeObj, flags, alpha, newImageSize.width, newImageSize.height, validPixROI1_out);
+        if(validPixROI1!=null){ validPixROI1.x = (int)validPixROI1_out[0]; validPixROI1.y = (int)validPixROI1_out[1]; validPixROI1.width = (int)validPixROI1_out[2]; validPixROI1.height = (int)validPixROI1_out[3]; } 
+    }
+
+    /**
+     * Computes rectification transforms for each head of a calibrated stereo camera.
+     *
+     * @param cameraMatrix1 First camera matrix.
+     * @param distCoeffs1 First camera distortion parameters.
+     * @param cameraMatrix2 Second camera matrix.
+     * @param distCoeffs2 Second camera distortion parameters.
+     * @param imageSize Size of the image used for stereo calibration.
+     * @param R Rotation matrix from the coordinate system of the first camera to the second camera,
+     * see REF: stereoCalibrate.
+     * @param T Translation vector from the coordinate system of the first camera to the second camera,
+     * see REF: stereoCalibrate.
+     * @param R1 Output 3x3 rectification transform (rotation matrix) for the first camera. This matrix
+     * brings points given in the unrectified first camera's coordinate system to points in the rectified
+     * first camera's coordinate system. In more technical terms, it performs a change of basis from the
+     * unrectified first camera's coordinate system to the rectified first camera's coordinate system.
+     * @param R2 Output 3x3 rectification transform (rotation matrix) for the second camera. This matrix
+     * brings points given in the unrectified second camera's coordinate system to points in the rectified
+     * second camera's coordinate system. In more technical terms, it performs a change of basis from the
+     * unrectified second camera's coordinate system to the rectified second camera's coordinate system.
+     * @param P1 Output 3x4 projection matrix in the new (rectified) coordinate systems for the first
+     * camera, i.e. it projects points given in the rectified first camera coordinate system into the
+     * rectified first camera's image.
+     * @param P2 Output 3x4 projection matrix in the new (rectified) coordinate systems for the second
+     * camera, i.e. it projects points given in the rectified first camera coordinate system into the
+     * rectified second camera's image.
+     * @param Q Output \(4 \times 4\) disparity-to-depth mapping matrix (see REF: reprojectImageTo3D).
+     * @param flags Operation flags that may be zero or CALIB_ZERO_DISPARITY . If the flag is set,
+     * the function makes the principal points of each camera have the same pixel coordinates in the
+     * rectified views. And if the flag is not set, the function may still shift the images in the
+     * horizontal or vertical direction (depending on the orientation of epipolar lines) to maximize the
+     * useful image area.
+     * @param alpha Free scaling parameter. If it is -1 or absent, the function performs the default
+     * scaling. Otherwise, the parameter should be between 0 and 1. alpha=0 means that the rectified
+     * images are zoomed and shifted so that only valid pixels are visible (no black areas after
+     * rectification). alpha=1 means that the rectified image is decimated and shifted so that all the
+     * pixels from the original images from the cameras are retained in the rectified images (no source
+     * image pixels are lost). Any intermediate value yields an intermediate result between
+     * those two extreme cases.
+     * @param newImageSize New image resolution after rectification. The same size should be passed to
+     * initUndistortRectifyMap (see the stereo_calib.cpp sample in OpenCV samples directory). When (0,0)
+     * is passed (default), it is set to the original imageSize . Setting it to a larger value can help you
+     * preserve details in the original image, especially when there is a big radial distortion.
+     * are valid. If alpha=0 , the ROIs cover the whole images. Otherwise, they are likely to be smaller
+     * (see the picture below).
+     * are valid. If alpha=0 , the ROIs cover the whole images. Otherwise, they are likely to be smaller
+     * (see the picture below).
+     *
+     * The function computes the rotation matrices for each camera that (virtually) make both camera image
+     * planes the same plane. Consequently, this makes all the epipolar lines parallel and thus simplifies
+     * the dense stereo correspondence problem. The function takes the matrices computed by stereoCalibrate
+     * as input. As output, it provides two rotation matrices and also two projection matrices in the new
+     * coordinates. The function distinguishes the following two cases:
+     *
+     * <ul>
+     *   <li>
+     *    <b>Horizontal stereo</b>: the first and the second camera views are shifted relative to each other
+     *     mainly along the x-axis (with possible small vertical shift). In the rectified images, the
+     *     corresponding epipolar lines in the left and right cameras are horizontal and have the same
+     *     y-coordinate. P1 and P2 look like:
+     *   </li>
+     * </ul>
+     *
+     *     \(\texttt{P1} = \begin{bmatrix}
+     *                         f &amp; 0 &amp; cx_1 &amp; 0 \\
+     *                         0 &amp; f &amp; cy &amp; 0 \\
+     *                         0 &amp; 0 &amp; 1 &amp; 0
+     *                      \end{bmatrix}\)
+     *
+     *     \(\texttt{P2} = \begin{bmatrix}
+     *                         f &amp; 0 &amp; cx_2 &amp; T_x*f \\
+     *                         0 &amp; f &amp; cy &amp; 0 \\
+     *                         0 &amp; 0 &amp; 1 &amp; 0
+     *                      \end{bmatrix} ,\)
+     *
+     *     where \(T_x\) is a horizontal shift between the cameras and \(cx_1=cx_2\) if
+     *     CALIB_ZERO_DISPARITY is set.
+     *
+     * <ul>
+     *   <li>
+     *    <b>Vertical stereo</b>: the first and the second camera views are shifted relative to each other
+     *     mainly in the vertical direction (and probably a bit in the horizontal direction too). The epipolar
+     *     lines in the rectified images are vertical and have the same x-coordinate. P1 and P2 look like:
+     *   </li>
+     * </ul>
+     *
+     *     \(\texttt{P1} = \begin{bmatrix}
+     *                         f &amp; 0 &amp; cx &amp; 0 \\
+     *                         0 &amp; f &amp; cy_1 &amp; 0 \\
+     *                         0 &amp; 0 &amp; 1 &amp; 0
+     *                      \end{bmatrix}\)
+     *
+     *     \(\texttt{P2} = \begin{bmatrix}
+     *                         f &amp; 0 &amp; cx &amp; 0 \\
+     *                         0 &amp; f &amp; cy_2 &amp; T_y*f \\
+     *                         0 &amp; 0 &amp; 1 &amp; 0
+     *                      \end{bmatrix},\)
+     *
+     *     where \(T_y\) is a vertical shift between the cameras and \(cy_1=cy_2\) if
+     *     CALIB_ZERO_DISPARITY is set.
+     *
+     * As you can see, the first three columns of P1 and P2 will effectively be the new "rectified" camera
+     * matrices. The matrices, together with R1 and R2 , can then be passed to initUndistortRectifyMap to
+     * initialize the rectification map for each camera.
+     *
+     * See below the screenshot from the stereo_calib.cpp sample. Some red horizontal lines pass through
+     * the corresponding image regions. This means that the images are well rectified, which is what most
+     * stereo correspondence algorithms rely on. The green rectangles are roi1 and roi2 . You see that
+     * their interiors are all valid pixels.
+     *
+     * ![image](pics/stereo_undistort.jpg)
+     */
+    public static void stereoRectify(Mat cameraMatrix1, Mat distCoeffs1, Mat cameraMatrix2, Mat distCoeffs2, Size imageSize, Mat R, Mat T, Mat R1, Mat R2, Mat P1, Mat P2, Mat Q, int flags, double alpha, Size newImageSize) {
+        stereoRectify_2(cameraMatrix1.nativeObj, distCoeffs1.nativeObj, cameraMatrix2.nativeObj, distCoeffs2.nativeObj, imageSize.width, imageSize.height, R.nativeObj, T.nativeObj, R1.nativeObj, R2.nativeObj, P1.nativeObj, P2.nativeObj, Q.nativeObj, flags, alpha, newImageSize.width, newImageSize.height);
+    }
+
+    /**
+     * Computes rectification transforms for each head of a calibrated stereo camera.
+     *
+     * @param cameraMatrix1 First camera matrix.
+     * @param distCoeffs1 First camera distortion parameters.
+     * @param cameraMatrix2 Second camera matrix.
+     * @param distCoeffs2 Second camera distortion parameters.
+     * @param imageSize Size of the image used for stereo calibration.
+     * @param R Rotation matrix from the coordinate system of the first camera to the second camera,
+     * see REF: stereoCalibrate.
+     * @param T Translation vector from the coordinate system of the first camera to the second camera,
+     * see REF: stereoCalibrate.
+     * @param R1 Output 3x3 rectification transform (rotation matrix) for the first camera. This matrix
+     * brings points given in the unrectified first camera's coordinate system to points in the rectified
+     * first camera's coordinate system. In more technical terms, it performs a change of basis from the
+     * unrectified first camera's coordinate system to the rectified first camera's coordinate system.
+     * @param R2 Output 3x3 rectification transform (rotation matrix) for the second camera. This matrix
+     * brings points given in the unrectified second camera's coordinate system to points in the rectified
+     * second camera's coordinate system. In more technical terms, it performs a change of basis from the
+     * unrectified second camera's coordinate system to the rectified second camera's coordinate system.
+     * @param P1 Output 3x4 projection matrix in the new (rectified) coordinate systems for the first
+     * camera, i.e. it projects points given in the rectified first camera coordinate system into the
+     * rectified first camera's image.
+     * @param P2 Output 3x4 projection matrix in the new (rectified) coordinate systems for the second
+     * camera, i.e. it projects points given in the rectified first camera coordinate system into the
+     * rectified second camera's image.
+     * @param Q Output \(4 \times 4\) disparity-to-depth mapping matrix (see REF: reprojectImageTo3D).
+     * @param flags Operation flags that may be zero or CALIB_ZERO_DISPARITY . If the flag is set,
+     * the function makes the principal points of each camera have the same pixel coordinates in the
+     * rectified views. And if the flag is not set, the function may still shift the images in the
+     * horizontal or vertical direction (depending on the orientation of epipolar lines) to maximize the
+     * useful image area.
+     * @param alpha Free scaling parameter. If it is -1 or absent, the function performs the default
+     * scaling. Otherwise, the parameter should be between 0 and 1. alpha=0 means that the rectified
+     * images are zoomed and shifted so that only valid pixels are visible (no black areas after
+     * rectification). alpha=1 means that the rectified image is decimated and shifted so that all the
+     * pixels from the original images from the cameras are retained in the rectified images (no source
+     * image pixels are lost). Any intermediate value yields an intermediate result between
+     * those two extreme cases.
+     * initUndistortRectifyMap (see the stereo_calib.cpp sample in OpenCV samples directory). When (0,0)
+     * is passed (default), it is set to the original imageSize . Setting it to a larger value can help you
+     * preserve details in the original image, especially when there is a big radial distortion.
+     * are valid. If alpha=0 , the ROIs cover the whole images. Otherwise, they are likely to be smaller
+     * (see the picture below).
+     * are valid. If alpha=0 , the ROIs cover the whole images. Otherwise, they are likely to be smaller
+     * (see the picture below).
+     *
+     * The function computes the rotation matrices for each camera that (virtually) make both camera image
+     * planes the same plane. Consequently, this makes all the epipolar lines parallel and thus simplifies
+     * the dense stereo correspondence problem. The function takes the matrices computed by stereoCalibrate
+     * as input. As output, it provides two rotation matrices and also two projection matrices in the new
+     * coordinates. The function distinguishes the following two cases:
+     *
+     * <ul>
+     *   <li>
+     *    <b>Horizontal stereo</b>: the first and the second camera views are shifted relative to each other
+     *     mainly along the x-axis (with possible small vertical shift). In the rectified images, the
+     *     corresponding epipolar lines in the left and right cameras are horizontal and have the same
+     *     y-coordinate. P1 and P2 look like:
+     *   </li>
+     * </ul>
+     *
+     *     \(\texttt{P1} = \begin{bmatrix}
+     *                         f &amp; 0 &amp; cx_1 &amp; 0 \\
+     *                         0 &amp; f &amp; cy &amp; 0 \\
+     *                         0 &amp; 0 &amp; 1 &amp; 0
+     *                      \end{bmatrix}\)
+     *
+     *     \(\texttt{P2} = \begin{bmatrix}
+     *                         f &amp; 0 &amp; cx_2 &amp; T_x*f \\
+     *                         0 &amp; f &amp; cy &amp; 0 \\
+     *                         0 &amp; 0 &amp; 1 &amp; 0
+     *                      \end{bmatrix} ,\)
+     *
+     *     where \(T_x\) is a horizontal shift between the cameras and \(cx_1=cx_2\) if
+     *     CALIB_ZERO_DISPARITY is set.
+     *
+     * <ul>
+     *   <li>
+     *    <b>Vertical stereo</b>: the first and the second camera views are shifted relative to each other
+     *     mainly in the vertical direction (and probably a bit in the horizontal direction too). The epipolar
+     *     lines in the rectified images are vertical and have the same x-coordinate. P1 and P2 look like:
+     *   </li>
+     * </ul>
+     *
+     *     \(\texttt{P1} = \begin{bmatrix}
+     *                         f &amp; 0 &amp; cx &amp; 0 \\
+     *                         0 &amp; f &amp; cy_1 &amp; 0 \\
+     *                         0 &amp; 0 &amp; 1 &amp; 0
+     *                      \end{bmatrix}\)
+     *
+     *     \(\texttt{P2} = \begin{bmatrix}
+     *                         f &amp; 0 &amp; cx &amp; 0 \\
+     *                         0 &amp; f &amp; cy_2 &amp; T_y*f \\
+     *                         0 &amp; 0 &amp; 1 &amp; 0
+     *                      \end{bmatrix},\)
+     *
+     *     where \(T_y\) is a vertical shift between the cameras and \(cy_1=cy_2\) if
+     *     CALIB_ZERO_DISPARITY is set.
+     *
+     * As you can see, the first three columns of P1 and P2 will effectively be the new "rectified" camera
+     * matrices. The matrices, together with R1 and R2 , can then be passed to initUndistortRectifyMap to
+     * initialize the rectification map for each camera.
+     *
+     * See below the screenshot from the stereo_calib.cpp sample. Some red horizontal lines pass through
+     * the corresponding image regions. This means that the images are well rectified, which is what most
+     * stereo correspondence algorithms rely on. The green rectangles are roi1 and roi2 . You see that
+     * their interiors are all valid pixels.
+     *
+     * ![image](pics/stereo_undistort.jpg)
+     */
+    public static void stereoRectify(Mat cameraMatrix1, Mat distCoeffs1, Mat cameraMatrix2, Mat distCoeffs2, Size imageSize, Mat R, Mat T, Mat R1, Mat R2, Mat P1, Mat P2, Mat Q, int flags, double alpha) {
+        stereoRectify_3(cameraMatrix1.nativeObj, distCoeffs1.nativeObj, cameraMatrix2.nativeObj, distCoeffs2.nativeObj, imageSize.width, imageSize.height, R.nativeObj, T.nativeObj, R1.nativeObj, R2.nativeObj, P1.nativeObj, P2.nativeObj, Q.nativeObj, flags, alpha);
+    }
+
+    /**
+     * Computes rectification transforms for each head of a calibrated stereo camera.
+     *
+     * @param cameraMatrix1 First camera matrix.
+     * @param distCoeffs1 First camera distortion parameters.
+     * @param cameraMatrix2 Second camera matrix.
+     * @param distCoeffs2 Second camera distortion parameters.
+     * @param imageSize Size of the image used for stereo calibration.
+     * @param R Rotation matrix from the coordinate system of the first camera to the second camera,
+     * see REF: stereoCalibrate.
+     * @param T Translation vector from the coordinate system of the first camera to the second camera,
+     * see REF: stereoCalibrate.
+     * @param R1 Output 3x3 rectification transform (rotation matrix) for the first camera. This matrix
+     * brings points given in the unrectified first camera's coordinate system to points in the rectified
+     * first camera's coordinate system. In more technical terms, it performs a change of basis from the
+     * unrectified first camera's coordinate system to the rectified first camera's coordinate system.
+     * @param R2 Output 3x3 rectification transform (rotation matrix) for the second camera. This matrix
+     * brings points given in the unrectified second camera's coordinate system to points in the rectified
+     * second camera's coordinate system. In more technical terms, it performs a change of basis from the
+     * unrectified second camera's coordinate system to the rectified second camera's coordinate system.
+     * @param P1 Output 3x4 projection matrix in the new (rectified) coordinate systems for the first
+     * camera, i.e. it projects points given in the rectified first camera coordinate system into the
+     * rectified first camera's image.
+     * @param P2 Output 3x4 projection matrix in the new (rectified) coordinate systems for the second
+     * camera, i.e. it projects points given in the rectified first camera coordinate system into the
+     * rectified second camera's image.
+     * @param Q Output \(4 \times 4\) disparity-to-depth mapping matrix (see REF: reprojectImageTo3D).
+     * @param flags Operation flags that may be zero or CALIB_ZERO_DISPARITY . If the flag is set,
+     * the function makes the principal points of each camera have the same pixel coordinates in the
+     * rectified views. And if the flag is not set, the function may still shift the images in the
+     * horizontal or vertical direction (depending on the orientation of epipolar lines) to maximize the
+     * useful image area.
+     * scaling. Otherwise, the parameter should be between 0 and 1. alpha=0 means that the rectified
+     * images are zoomed and shifted so that only valid pixels are visible (no black areas after
+     * rectification). alpha=1 means that the rectified image is decimated and shifted so that all the
+     * pixels from the original images from the cameras are retained in the rectified images (no source
+     * image pixels are lost). Any intermediate value yields an intermediate result between
+     * those two extreme cases.
+     * initUndistortRectifyMap (see the stereo_calib.cpp sample in OpenCV samples directory). When (0,0)
+     * is passed (default), it is set to the original imageSize . Setting it to a larger value can help you
+     * preserve details in the original image, especially when there is a big radial distortion.
+     * are valid. If alpha=0 , the ROIs cover the whole images. Otherwise, they are likely to be smaller
+     * (see the picture below).
+     * are valid. If alpha=0 , the ROIs cover the whole images. Otherwise, they are likely to be smaller
+     * (see the picture below).
+     *
+     * The function computes the rotation matrices for each camera that (virtually) make both camera image
+     * planes the same plane. Consequently, this makes all the epipolar lines parallel and thus simplifies
+     * the dense stereo correspondence problem. The function takes the matrices computed by stereoCalibrate
+     * as input. As output, it provides two rotation matrices and also two projection matrices in the new
+     * coordinates. The function distinguishes the following two cases:
+     *
+     * <ul>
+     *   <li>
+     *    <b>Horizontal stereo</b>: the first and the second camera views are shifted relative to each other
+     *     mainly along the x-axis (with possible small vertical shift). In the rectified images, the
+     *     corresponding epipolar lines in the left and right cameras are horizontal and have the same
+     *     y-coordinate. P1 and P2 look like:
+     *   </li>
+     * </ul>
+     *
+     *     \(\texttt{P1} = \begin{bmatrix}
+     *                         f &amp; 0 &amp; cx_1 &amp; 0 \\
+     *                         0 &amp; f &amp; cy &amp; 0 \\
+     *                         0 &amp; 0 &amp; 1 &amp; 0
+     *                      \end{bmatrix}\)
+     *
+     *     \(\texttt{P2} = \begin{bmatrix}
+     *                         f &amp; 0 &amp; cx_2 &amp; T_x*f \\
+     *                         0 &amp; f &amp; cy &amp; 0 \\
+     *                         0 &amp; 0 &amp; 1 &amp; 0
+     *                      \end{bmatrix} ,\)
+     *
+     *     where \(T_x\) is a horizontal shift between the cameras and \(cx_1=cx_2\) if
+     *     CALIB_ZERO_DISPARITY is set.
+     *
+     * <ul>
+     *   <li>
+     *    <b>Vertical stereo</b>: the first and the second camera views are shifted relative to each other
+     *     mainly in the vertical direction (and probably a bit in the horizontal direction too). The epipolar
+     *     lines in the rectified images are vertical and have the same x-coordinate. P1 and P2 look like:
+     *   </li>
+     * </ul>
+     *
+     *     \(\texttt{P1} = \begin{bmatrix}
+     *                         f &amp; 0 &amp; cx &amp; 0 \\
+     *                         0 &amp; f &amp; cy_1 &amp; 0 \\
+     *                         0 &amp; 0 &amp; 1 &amp; 0
+     *                      \end{bmatrix}\)
+     *
+     *     \(\texttt{P2} = \begin{bmatrix}
+     *                         f &amp; 0 &amp; cx &amp; 0 \\
+     *                         0 &amp; f &amp; cy_2 &amp; T_y*f \\
+     *                         0 &amp; 0 &amp; 1 &amp; 0
+     *                      \end{bmatrix},\)
+     *
+     *     where \(T_y\) is a vertical shift between the cameras and \(cy_1=cy_2\) if
+     *     CALIB_ZERO_DISPARITY is set.
+     *
+     * As you can see, the first three columns of P1 and P2 will effectively be the new "rectified" camera
+     * matrices. The matrices, together with R1 and R2 , can then be passed to initUndistortRectifyMap to
+     * initialize the rectification map for each camera.
+     *
+     * See below the screenshot from the stereo_calib.cpp sample. Some red horizontal lines pass through
+     * the corresponding image regions. This means that the images are well rectified, which is what most
+     * stereo correspondence algorithms rely on. The green rectangles are roi1 and roi2 . You see that
+     * their interiors are all valid pixels.
+     *
+     * ![image](pics/stereo_undistort.jpg)
+     */
+    public static void stereoRectify(Mat cameraMatrix1, Mat distCoeffs1, Mat cameraMatrix2, Mat distCoeffs2, Size imageSize, Mat R, Mat T, Mat R1, Mat R2, Mat P1, Mat P2, Mat Q, int flags) {
+        stereoRectify_4(cameraMatrix1.nativeObj, distCoeffs1.nativeObj, cameraMatrix2.nativeObj, distCoeffs2.nativeObj, imageSize.width, imageSize.height, R.nativeObj, T.nativeObj, R1.nativeObj, R2.nativeObj, P1.nativeObj, P2.nativeObj, Q.nativeObj, flags);
+    }
+
+    /**
+     * Computes rectification transforms for each head of a calibrated stereo camera.
+     *
+     * @param cameraMatrix1 First camera matrix.
+     * @param distCoeffs1 First camera distortion parameters.
+     * @param cameraMatrix2 Second camera matrix.
+     * @param distCoeffs2 Second camera distortion parameters.
+     * @param imageSize Size of the image used for stereo calibration.
+     * @param R Rotation matrix from the coordinate system of the first camera to the second camera,
+     * see REF: stereoCalibrate.
+     * @param T Translation vector from the coordinate system of the first camera to the second camera,
+     * see REF: stereoCalibrate.
+     * @param R1 Output 3x3 rectification transform (rotation matrix) for the first camera. This matrix
+     * brings points given in the unrectified first camera's coordinate system to points in the rectified
+     * first camera's coordinate system. In more technical terms, it performs a change of basis from the
+     * unrectified first camera's coordinate system to the rectified first camera's coordinate system.
+     * @param R2 Output 3x3 rectification transform (rotation matrix) for the second camera. This matrix
+     * brings points given in the unrectified second camera's coordinate system to points in the rectified
+     * second camera's coordinate system. In more technical terms, it performs a change of basis from the
+     * unrectified second camera's coordinate system to the rectified second camera's coordinate system.
+     * @param P1 Output 3x4 projection matrix in the new (rectified) coordinate systems for the first
+     * camera, i.e. it projects points given in the rectified first camera coordinate system into the
+     * rectified first camera's image.
+     * @param P2 Output 3x4 projection matrix in the new (rectified) coordinate systems for the second
+     * camera, i.e. it projects points given in the rectified first camera coordinate system into the
+     * rectified second camera's image.
+     * @param Q Output \(4 \times 4\) disparity-to-depth mapping matrix (see REF: reprojectImageTo3D).
+     * the function makes the principal points of each camera have the same pixel coordinates in the
+     * rectified views. And if the flag is not set, the function may still shift the images in the
+     * horizontal or vertical direction (depending on the orientation of epipolar lines) to maximize the
+     * useful image area.
+     * scaling. Otherwise, the parameter should be between 0 and 1. alpha=0 means that the rectified
+     * images are zoomed and shifted so that only valid pixels are visible (no black areas after
+     * rectification). alpha=1 means that the rectified image is decimated and shifted so that all the
+     * pixels from the original images from the cameras are retained in the rectified images (no source
+     * image pixels are lost). Any intermediate value yields an intermediate result between
+     * those two extreme cases.
+     * initUndistortRectifyMap (see the stereo_calib.cpp sample in OpenCV samples directory). When (0,0)
+     * is passed (default), it is set to the original imageSize . Setting it to a larger value can help you
+     * preserve details in the original image, especially when there is a big radial distortion.
+     * are valid. If alpha=0 , the ROIs cover the whole images. Otherwise, they are likely to be smaller
+     * (see the picture below).
+     * are valid. If alpha=0 , the ROIs cover the whole images. Otherwise, they are likely to be smaller
+     * (see the picture below).
+     *
+     * The function computes the rotation matrices for each camera that (virtually) make both camera image
+     * planes the same plane. Consequently, this makes all the epipolar lines parallel and thus simplifies
+     * the dense stereo correspondence problem. The function takes the matrices computed by stereoCalibrate
+     * as input. As output, it provides two rotation matrices and also two projection matrices in the new
+     * coordinates. The function distinguishes the following two cases:
+     *
+     * <ul>
+     *   <li>
+     *    <b>Horizontal stereo</b>: the first and the second camera views are shifted relative to each other
+     *     mainly along the x-axis (with possible small vertical shift). In the rectified images, the
+     *     corresponding epipolar lines in the left and right cameras are horizontal and have the same
+     *     y-coordinate. P1 and P2 look like:
+     *   </li>
+     * </ul>
+     *
+     *     \(\texttt{P1} = \begin{bmatrix}
+     *                         f &amp; 0 &amp; cx_1 &amp; 0 \\
+     *                         0 &amp; f &amp; cy &amp; 0 \\
+     *                         0 &amp; 0 &amp; 1 &amp; 0
+     *                      \end{bmatrix}\)
+     *
+     *     \(\texttt{P2} = \begin{bmatrix}
+     *                         f &amp; 0 &amp; cx_2 &amp; T_x*f \\
+     *                         0 &amp; f &amp; cy &amp; 0 \\
+     *                         0 &amp; 0 &amp; 1 &amp; 0
+     *                      \end{bmatrix} ,\)
+     *
+     *     where \(T_x\) is a horizontal shift between the cameras and \(cx_1=cx_2\) if
+     *     CALIB_ZERO_DISPARITY is set.
+     *
+     * <ul>
+     *   <li>
+     *    <b>Vertical stereo</b>: the first and the second camera views are shifted relative to each other
+     *     mainly in the vertical direction (and probably a bit in the horizontal direction too). The epipolar
+     *     lines in the rectified images are vertical and have the same x-coordinate. P1 and P2 look like:
+     *   </li>
+     * </ul>
+     *
+     *     \(\texttt{P1} = \begin{bmatrix}
+     *                         f &amp; 0 &amp; cx &amp; 0 \\
+     *                         0 &amp; f &amp; cy_1 &amp; 0 \\
+     *                         0 &amp; 0 &amp; 1 &amp; 0
+     *                      \end{bmatrix}\)
+     *
+     *     \(\texttt{P2} = \begin{bmatrix}
+     *                         f &amp; 0 &amp; cx &amp; 0 \\
+     *                         0 &amp; f &amp; cy_2 &amp; T_y*f \\
+     *                         0 &amp; 0 &amp; 1 &amp; 0
+     *                      \end{bmatrix},\)
+     *
+     *     where \(T_y\) is a vertical shift between the cameras and \(cy_1=cy_2\) if
+     *     CALIB_ZERO_DISPARITY is set.
+     *
+     * As you can see, the first three columns of P1 and P2 will effectively be the new "rectified" camera
+     * matrices. The matrices, together with R1 and R2 , can then be passed to initUndistortRectifyMap to
+     * initialize the rectification map for each camera.
+     *
+     * See below the screenshot from the stereo_calib.cpp sample. Some red horizontal lines pass through
+     * the corresponding image regions. This means that the images are well rectified, which is what most
+     * stereo correspondence algorithms rely on. The green rectangles are roi1 and roi2 . You see that
+     * their interiors are all valid pixels.
+     *
+     * ![image](pics/stereo_undistort.jpg)
+     */
+    public static void stereoRectify(Mat cameraMatrix1, Mat distCoeffs1, Mat cameraMatrix2, Mat distCoeffs2, Size imageSize, Mat R, Mat T, Mat R1, Mat R2, Mat P1, Mat P2, Mat Q) {
+        stereoRectify_5(cameraMatrix1.nativeObj, distCoeffs1.nativeObj, cameraMatrix2.nativeObj, distCoeffs2.nativeObj, imageSize.width, imageSize.height, R.nativeObj, T.nativeObj, R1.nativeObj, R2.nativeObj, P1.nativeObj, P2.nativeObj, Q.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::triangulatePoints(Mat projMatr1, Mat projMatr2, Mat projPoints1, Mat projPoints2, Mat& points4D)
+    //
+
+    /**
+     * This function reconstructs 3-dimensional points (in homogeneous coordinates) by using
+     * their observations with a stereo camera.
+     *
+     * @param projMatr1 3x4 projection matrix of the first camera, i.e. this matrix projects 3D points
+     * given in the world's coordinate system into the first image.
+     * @param projMatr2 3x4 projection matrix of the second camera, i.e. this matrix projects 3D points
+     * given in the world's coordinate system into the second image.
+     * @param projPoints1 2xN array of feature points in the first image. In the case of the c++ version,
+     * it can be also a vector of feature points or two-channel matrix of size 1xN or Nx1.
+     * @param projPoints2 2xN array of corresponding points in the second image. In the case of the c++
+     * version, it can be also a vector of feature points or two-channel matrix of size 1xN or Nx1.
+     * @param points4D 4xN array of reconstructed points in homogeneous coordinates. These points are
+     * returned in the world's coordinate system.
+     *
+     * <b>Note:</b>
+     *    Keep in mind that all input data should be of float type in order for this function to work.
+     *
+     * <b>Note:</b>
+     *    If the projection matrices from REF: stereoRectify are used, then the returned points are
+     *    represented in the first camera's rectified coordinate system.
+     *
+     * SEE:
+     *    reprojectImageTo3D
+     */
+    public static void triangulatePoints(Mat projMatr1, Mat projMatr2, Mat projPoints1, Mat projPoints2, Mat points4D) {
+        triangulatePoints_0(projMatr1.nativeObj, projMatr2.nativeObj, projPoints1.nativeObj, projPoints2.nativeObj, points4D.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::validateDisparity(Mat& disparity, Mat cost, int minDisparity, int numberOfDisparities, int disp12MaxDisp = 1)
+    //
+
+    public static void validateDisparity(Mat disparity, Mat cost, int minDisparity, int numberOfDisparities, int disp12MaxDisp) {
+        validateDisparity_0(disparity.nativeObj, cost.nativeObj, minDisparity, numberOfDisparities, disp12MaxDisp);
+    }
+
+    public static void validateDisparity(Mat disparity, Mat cost, int minDisparity, int numberOfDisparities) {
+        validateDisparity_1(disparity.nativeObj, cost.nativeObj, minDisparity, numberOfDisparities);
+    }
+
+
+    //
+    // C++:  void cv::fisheye::distortPoints(Mat undistorted, Mat& distorted, Mat K, Mat D, double alpha = 0)
+    //
+
+    /**
+     * Distorts 2D points using fisheye model.
+     *
+     *     @param undistorted Array of object points, 1xN/Nx1 2-channel (or vector&lt;Point2f&gt; ), where N is
+     *     the number of points in the view.
+     *     @param K Camera matrix \(K = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{_1}\).
+     *     @param D Input vector of distortion coefficients \((k_1, k_2, k_3, k_4)\).
+     *     @param alpha The skew coefficient.
+     *     @param distorted Output array of image points, 1xN/Nx1 2-channel, or vector&lt;Point2f&gt; .
+     *
+     *     Note that the function assumes the camera matrix of the undistorted points to be identity.
+     *     This means if you want to transform back points undistorted with undistortPoints() you have to
+     *     multiply them with \(P^{-1}\).
+     */
+    public static void fisheye_distortPoints(Mat undistorted, Mat distorted, Mat K, Mat D, double alpha) {
+        fisheye_distortPoints_0(undistorted.nativeObj, distorted.nativeObj, K.nativeObj, D.nativeObj, alpha);
+    }
+
+    /**
+     * Distorts 2D points using fisheye model.
+     *
+     *     @param undistorted Array of object points, 1xN/Nx1 2-channel (or vector&lt;Point2f&gt; ), where N is
+     *     the number of points in the view.
+     *     @param K Camera matrix \(K = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{_1}\).
+     *     @param D Input vector of distortion coefficients \((k_1, k_2, k_3, k_4)\).
+     *     @param distorted Output array of image points, 1xN/Nx1 2-channel, or vector&lt;Point2f&gt; .
+     *
+     *     Note that the function assumes the camera matrix of the undistorted points to be identity.
+     *     This means if you want to transform back points undistorted with undistortPoints() you have to
+     *     multiply them with \(P^{-1}\).
+     */
+    public static void fisheye_distortPoints(Mat undistorted, Mat distorted, Mat K, Mat D) {
+        fisheye_distortPoints_1(undistorted.nativeObj, distorted.nativeObj, K.nativeObj, D.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::fisheye::estimateNewCameraMatrixForUndistortRectify(Mat K, Mat D, Size image_size, Mat R, Mat& P, double balance = 0.0, Size new_size = Size(), double fov_scale = 1.0)
+    //
+
+    /**
+     * Estimates new camera matrix for undistortion or rectification.
+     *
+     *     @param K Camera matrix \(K = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{_1}\).
+     *     @param image_size Size of the image
+     *     @param D Input vector of distortion coefficients \((k_1, k_2, k_3, k_4)\).
+     *     @param R Rectification transformation in the object space: 3x3 1-channel, or vector: 3x1/1x3
+     *     1-channel or 1x1 3-channel
+     *     @param P New camera matrix (3x3) or new projection matrix (3x4)
+     *     @param balance Sets the new focal length in range between the min focal length and the max focal
+     *     length. Balance is in range of [0, 1].
+     *     @param new_size the new size
+     *     @param fov_scale Divisor for new focal length.
+     */
+    public static void fisheye_estimateNewCameraMatrixForUndistortRectify(Mat K, Mat D, Size image_size, Mat R, Mat P, double balance, Size new_size, double fov_scale) {
+        fisheye_estimateNewCameraMatrixForUndistortRectify_0(K.nativeObj, D.nativeObj, image_size.width, image_size.height, R.nativeObj, P.nativeObj, balance, new_size.width, new_size.height, fov_scale);
+    }
+
+    /**
+     * Estimates new camera matrix for undistortion or rectification.
+     *
+     *     @param K Camera matrix \(K = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{_1}\).
+     *     @param image_size Size of the image
+     *     @param D Input vector of distortion coefficients \((k_1, k_2, k_3, k_4)\).
+     *     @param R Rectification transformation in the object space: 3x3 1-channel, or vector: 3x1/1x3
+     *     1-channel or 1x1 3-channel
+     *     @param P New camera matrix (3x3) or new projection matrix (3x4)
+     *     @param balance Sets the new focal length in range between the min focal length and the max focal
+     *     length. Balance is in range of [0, 1].
+     *     @param new_size the new size
+     */
+    public static void fisheye_estimateNewCameraMatrixForUndistortRectify(Mat K, Mat D, Size image_size, Mat R, Mat P, double balance, Size new_size) {
+        fisheye_estimateNewCameraMatrixForUndistortRectify_1(K.nativeObj, D.nativeObj, image_size.width, image_size.height, R.nativeObj, P.nativeObj, balance, new_size.width, new_size.height);
+    }
+
+    /**
+     * Estimates new camera matrix for undistortion or rectification.
+     *
+     *     @param K Camera matrix \(K = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{_1}\).
+     *     @param image_size Size of the image
+     *     @param D Input vector of distortion coefficients \((k_1, k_2, k_3, k_4)\).
+     *     @param R Rectification transformation in the object space: 3x3 1-channel, or vector: 3x1/1x3
+     *     1-channel or 1x1 3-channel
+     *     @param P New camera matrix (3x3) or new projection matrix (3x4)
+     *     @param balance Sets the new focal length in range between the min focal length and the max focal
+     *     length. Balance is in range of [0, 1].
+     */
+    public static void fisheye_estimateNewCameraMatrixForUndistortRectify(Mat K, Mat D, Size image_size, Mat R, Mat P, double balance) {
+        fisheye_estimateNewCameraMatrixForUndistortRectify_2(K.nativeObj, D.nativeObj, image_size.width, image_size.height, R.nativeObj, P.nativeObj, balance);
+    }
+
+    /**
+     * Estimates new camera matrix for undistortion or rectification.
+     *
+     *     @param K Camera matrix \(K = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{_1}\).
+     *     @param image_size Size of the image
+     *     @param D Input vector of distortion coefficients \((k_1, k_2, k_3, k_4)\).
+     *     @param R Rectification transformation in the object space: 3x3 1-channel, or vector: 3x1/1x3
+     *     1-channel or 1x1 3-channel
+     *     @param P New camera matrix (3x3) or new projection matrix (3x4)
+     *     length. Balance is in range of [0, 1].
+     */
+    public static void fisheye_estimateNewCameraMatrixForUndistortRectify(Mat K, Mat D, Size image_size, Mat R, Mat P) {
+        fisheye_estimateNewCameraMatrixForUndistortRectify_3(K.nativeObj, D.nativeObj, image_size.width, image_size.height, R.nativeObj, P.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::fisheye::initUndistortRectifyMap(Mat K, Mat D, Mat R, Mat P, Size size, int m1type, Mat& map1, Mat& map2)
+    //
+
+    /**
+     * Computes undistortion and rectification maps for image transform by cv::remap(). If D is empty zero
+     *     distortion is used, if R or P is empty identity matrixes are used.
+     *
+     *     @param K Camera matrix \(K = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{_1}\).
+     *     @param D Input vector of distortion coefficients \((k_1, k_2, k_3, k_4)\).
+     *     @param R Rectification transformation in the object space: 3x3 1-channel, or vector: 3x1/1x3
+     *     1-channel or 1x1 3-channel
+     *     @param P New camera matrix (3x3) or new projection matrix (3x4)
+     *     @param size Undistorted image size.
+     *     @param m1type Type of the first output map that can be CV_32FC1 or CV_16SC2 . See convertMaps()
+     *     for details.
+     *     @param map1 The first output map.
+     *     @param map2 The second output map.
+     */
+    public static void fisheye_initUndistortRectifyMap(Mat K, Mat D, Mat R, Mat P, Size size, int m1type, Mat map1, Mat map2) {
+        fisheye_initUndistortRectifyMap_0(K.nativeObj, D.nativeObj, R.nativeObj, P.nativeObj, size.width, size.height, m1type, map1.nativeObj, map2.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::fisheye::projectPoints(Mat objectPoints, Mat& imagePoints, Mat rvec, Mat tvec, Mat K, Mat D, double alpha = 0, Mat& jacobian = Mat())
+    //
+
+    public static void fisheye_projectPoints(Mat objectPoints, Mat imagePoints, Mat rvec, Mat tvec, Mat K, Mat D, double alpha, Mat jacobian) {
+        fisheye_projectPoints_0(objectPoints.nativeObj, imagePoints.nativeObj, rvec.nativeObj, tvec.nativeObj, K.nativeObj, D.nativeObj, alpha, jacobian.nativeObj);
+    }
+
+    public static void fisheye_projectPoints(Mat objectPoints, Mat imagePoints, Mat rvec, Mat tvec, Mat K, Mat D, double alpha) {
+        fisheye_projectPoints_1(objectPoints.nativeObj, imagePoints.nativeObj, rvec.nativeObj, tvec.nativeObj, K.nativeObj, D.nativeObj, alpha);
+    }
+
+    public static void fisheye_projectPoints(Mat objectPoints, Mat imagePoints, Mat rvec, Mat tvec, Mat K, Mat D) {
+        fisheye_projectPoints_2(objectPoints.nativeObj, imagePoints.nativeObj, rvec.nativeObj, tvec.nativeObj, K.nativeObj, D.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::fisheye::stereoRectify(Mat K1, Mat D1, Mat K2, Mat D2, Size imageSize, Mat R, Mat tvec, Mat& R1, Mat& R2, Mat& P1, Mat& P2, Mat& Q, int flags, Size newImageSize = Size(), double balance = 0.0, double fov_scale = 1.0)
+    //
+
+    /**
+     * Stereo rectification for fisheye camera model
+     *
+     *     @param K1 First camera matrix.
+     *     @param D1 First camera distortion parameters.
+     *     @param K2 Second camera matrix.
+     *     @param D2 Second camera distortion parameters.
+     *     @param imageSize Size of the image used for stereo calibration.
+     *     @param R Rotation matrix between the coordinate systems of the first and the second
+     *     cameras.
+     *     @param tvec Translation vector between coordinate systems of the cameras.
+     *     @param R1 Output 3x3 rectification transform (rotation matrix) for the first camera.
+     *     @param R2 Output 3x3 rectification transform (rotation matrix) for the second camera.
+     *     @param P1 Output 3x4 projection matrix in the new (rectified) coordinate systems for the first
+     *     camera.
+     *     @param P2 Output 3x4 projection matrix in the new (rectified) coordinate systems for the second
+     *     camera.
+     *     @param Q Output \(4 \times 4\) disparity-to-depth mapping matrix (see reprojectImageTo3D ).
+     *     @param flags Operation flags that may be zero or CALIB_ZERO_DISPARITY . If the flag is set,
+     *     the function makes the principal points of each camera have the same pixel coordinates in the
+     *     rectified views. And if the flag is not set, the function may still shift the images in the
+     *     horizontal or vertical direction (depending on the orientation of epipolar lines) to maximize the
+     *     useful image area.
+     *     @param newImageSize New image resolution after rectification. The same size should be passed to
+     *     initUndistortRectifyMap (see the stereo_calib.cpp sample in OpenCV samples directory). When (0,0)
+     *     is passed (default), it is set to the original imageSize . Setting it to larger value can help you
+     *     preserve details in the original image, especially when there is a big radial distortion.
+     *     @param balance Sets the new focal length in range between the min focal length and the max focal
+     *     length. Balance is in range of [0, 1].
+     *     @param fov_scale Divisor for new focal length.
+     */
+    public static void fisheye_stereoRectify(Mat K1, Mat D1, Mat K2, Mat D2, Size imageSize, Mat R, Mat tvec, Mat R1, Mat R2, Mat P1, Mat P2, Mat Q, int flags, Size newImageSize, double balance, double fov_scale) {
+        fisheye_stereoRectify_0(K1.nativeObj, D1.nativeObj, K2.nativeObj, D2.nativeObj, imageSize.width, imageSize.height, R.nativeObj, tvec.nativeObj, R1.nativeObj, R2.nativeObj, P1.nativeObj, P2.nativeObj, Q.nativeObj, flags, newImageSize.width, newImageSize.height, balance, fov_scale);
+    }
+
+    /**
+     * Stereo rectification for fisheye camera model
+     *
+     *     @param K1 First camera matrix.
+     *     @param D1 First camera distortion parameters.
+     *     @param K2 Second camera matrix.
+     *     @param D2 Second camera distortion parameters.
+     *     @param imageSize Size of the image used for stereo calibration.
+     *     @param R Rotation matrix between the coordinate systems of the first and the second
+     *     cameras.
+     *     @param tvec Translation vector between coordinate systems of the cameras.
+     *     @param R1 Output 3x3 rectification transform (rotation matrix) for the first camera.
+     *     @param R2 Output 3x3 rectification transform (rotation matrix) for the second camera.
+     *     @param P1 Output 3x4 projection matrix in the new (rectified) coordinate systems for the first
+     *     camera.
+     *     @param P2 Output 3x4 projection matrix in the new (rectified) coordinate systems for the second
+     *     camera.
+     *     @param Q Output \(4 \times 4\) disparity-to-depth mapping matrix (see reprojectImageTo3D ).
+     *     @param flags Operation flags that may be zero or CALIB_ZERO_DISPARITY . If the flag is set,
+     *     the function makes the principal points of each camera have the same pixel coordinates in the
+     *     rectified views. And if the flag is not set, the function may still shift the images in the
+     *     horizontal or vertical direction (depending on the orientation of epipolar lines) to maximize the
+     *     useful image area.
+     *     @param newImageSize New image resolution after rectification. The same size should be passed to
+     *     initUndistortRectifyMap (see the stereo_calib.cpp sample in OpenCV samples directory). When (0,0)
+     *     is passed (default), it is set to the original imageSize . Setting it to larger value can help you
+     *     preserve details in the original image, especially when there is a big radial distortion.
+     *     @param balance Sets the new focal length in range between the min focal length and the max focal
+     *     length. Balance is in range of [0, 1].
+     */
+    public static void fisheye_stereoRectify(Mat K1, Mat D1, Mat K2, Mat D2, Size imageSize, Mat R, Mat tvec, Mat R1, Mat R2, Mat P1, Mat P2, Mat Q, int flags, Size newImageSize, double balance) {
+        fisheye_stereoRectify_1(K1.nativeObj, D1.nativeObj, K2.nativeObj, D2.nativeObj, imageSize.width, imageSize.height, R.nativeObj, tvec.nativeObj, R1.nativeObj, R2.nativeObj, P1.nativeObj, P2.nativeObj, Q.nativeObj, flags, newImageSize.width, newImageSize.height, balance);
+    }
+
+    /**
+     * Stereo rectification for fisheye camera model
+     *
+     *     @param K1 First camera matrix.
+     *     @param D1 First camera distortion parameters.
+     *     @param K2 Second camera matrix.
+     *     @param D2 Second camera distortion parameters.
+     *     @param imageSize Size of the image used for stereo calibration.
+     *     @param R Rotation matrix between the coordinate systems of the first and the second
+     *     cameras.
+     *     @param tvec Translation vector between coordinate systems of the cameras.
+     *     @param R1 Output 3x3 rectification transform (rotation matrix) for the first camera.
+     *     @param R2 Output 3x3 rectification transform (rotation matrix) for the second camera.
+     *     @param P1 Output 3x4 projection matrix in the new (rectified) coordinate systems for the first
+     *     camera.
+     *     @param P2 Output 3x4 projection matrix in the new (rectified) coordinate systems for the second
+     *     camera.
+     *     @param Q Output \(4 \times 4\) disparity-to-depth mapping matrix (see reprojectImageTo3D ).
+     *     @param flags Operation flags that may be zero or CALIB_ZERO_DISPARITY . If the flag is set,
+     *     the function makes the principal points of each camera have the same pixel coordinates in the
+     *     rectified views. And if the flag is not set, the function may still shift the images in the
+     *     horizontal or vertical direction (depending on the orientation of epipolar lines) to maximize the
+     *     useful image area.
+     *     @param newImageSize New image resolution after rectification. The same size should be passed to
+     *     initUndistortRectifyMap (see the stereo_calib.cpp sample in OpenCV samples directory). When (0,0)
+     *     is passed (default), it is set to the original imageSize . Setting it to larger value can help you
+     *     preserve details in the original image, especially when there is a big radial distortion.
+     *     length. Balance is in range of [0, 1].
+     */
+    public static void fisheye_stereoRectify(Mat K1, Mat D1, Mat K2, Mat D2, Size imageSize, Mat R, Mat tvec, Mat R1, Mat R2, Mat P1, Mat P2, Mat Q, int flags, Size newImageSize) {
+        fisheye_stereoRectify_2(K1.nativeObj, D1.nativeObj, K2.nativeObj, D2.nativeObj, imageSize.width, imageSize.height, R.nativeObj, tvec.nativeObj, R1.nativeObj, R2.nativeObj, P1.nativeObj, P2.nativeObj, Q.nativeObj, flags, newImageSize.width, newImageSize.height);
+    }
+
+    /**
+     * Stereo rectification for fisheye camera model
+     *
+     *     @param K1 First camera matrix.
+     *     @param D1 First camera distortion parameters.
+     *     @param K2 Second camera matrix.
+     *     @param D2 Second camera distortion parameters.
+     *     @param imageSize Size of the image used for stereo calibration.
+     *     @param R Rotation matrix between the coordinate systems of the first and the second
+     *     cameras.
+     *     @param tvec Translation vector between coordinate systems of the cameras.
+     *     @param R1 Output 3x3 rectification transform (rotation matrix) for the first camera.
+     *     @param R2 Output 3x3 rectification transform (rotation matrix) for the second camera.
+     *     @param P1 Output 3x4 projection matrix in the new (rectified) coordinate systems for the first
+     *     camera.
+     *     @param P2 Output 3x4 projection matrix in the new (rectified) coordinate systems for the second
+     *     camera.
+     *     @param Q Output \(4 \times 4\) disparity-to-depth mapping matrix (see reprojectImageTo3D ).
+     *     @param flags Operation flags that may be zero or CALIB_ZERO_DISPARITY . If the flag is set,
+     *     the function makes the principal points of each camera have the same pixel coordinates in the
+     *     rectified views. And if the flag is not set, the function may still shift the images in the
+     *     horizontal or vertical direction (depending on the orientation of epipolar lines) to maximize the
+     *     useful image area.
+     *     initUndistortRectifyMap (see the stereo_calib.cpp sample in OpenCV samples directory). When (0,0)
+     *     is passed (default), it is set to the original imageSize . Setting it to larger value can help you
+     *     preserve details in the original image, especially when there is a big radial distortion.
+     *     length. Balance is in range of [0, 1].
+     */
+    public static void fisheye_stereoRectify(Mat K1, Mat D1, Mat K2, Mat D2, Size imageSize, Mat R, Mat tvec, Mat R1, Mat R2, Mat P1, Mat P2, Mat Q, int flags) {
+        fisheye_stereoRectify_3(K1.nativeObj, D1.nativeObj, K2.nativeObj, D2.nativeObj, imageSize.width, imageSize.height, R.nativeObj, tvec.nativeObj, R1.nativeObj, R2.nativeObj, P1.nativeObj, P2.nativeObj, Q.nativeObj, flags);
+    }
+
+
+    //
+    // C++:  void cv::fisheye::undistortImage(Mat distorted, Mat& undistorted, Mat K, Mat D, Mat Knew = cv::Mat(), Size new_size = Size())
+    //
+
+    /**
+     * Transforms an image to compensate for fisheye lens distortion.
+     *
+     *     @param distorted image with fisheye lens distortion.
+     *     @param undistorted Output image with compensated fisheye lens distortion.
+     *     @param K Camera matrix \(K = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{_1}\).
+     *     @param D Input vector of distortion coefficients \((k_1, k_2, k_3, k_4)\).
+     *     @param Knew Camera matrix of the distorted image. By default, it is the identity matrix but you
+     *     may additionally scale and shift the result by using a different matrix.
+     *     @param new_size the new size
+     *
+     *     The function transforms an image to compensate radial and tangential lens distortion.
+     *
+     *     The function is simply a combination of fisheye::initUndistortRectifyMap (with unity R ) and remap
+     *     (with bilinear interpolation). See the former function for details of the transformation being
+     *     performed.
+     *
+     *     See below the results of undistortImage.
+     * <ul>
+     *   <li>
+     *           a\) result of undistort of perspective camera model (all possible coefficients (k_1, k_2, k_3,
+     *             k_4, k_5, k_6) of distortion were optimized under calibration)
+     *   <ul>
+     *     <li>
+     *            b\) result of fisheye::undistortImage of fisheye camera model (all possible coefficients (k_1, k_2,
+     *             k_3, k_4) of fisheye distortion were optimized under calibration)
+     *     </li>
+     *     <li>
+     *            c\) original image was captured with fisheye lens
+     *     </li>
+     *   </ul>
+     *
+     *     Pictures a) and b) almost the same. But if we consider points of image located far from the center
+     *     of image, we can notice that on image a) these points are distorted.
+     *   </li>
+     * </ul>
+     *
+     *     ![image](pics/fisheye_undistorted.jpg)
+     */
+    public static void fisheye_undistortImage(Mat distorted, Mat undistorted, Mat K, Mat D, Mat Knew, Size new_size) {
+        fisheye_undistortImage_0(distorted.nativeObj, undistorted.nativeObj, K.nativeObj, D.nativeObj, Knew.nativeObj, new_size.width, new_size.height);
+    }
+
+    /**
+     * Transforms an image to compensate for fisheye lens distortion.
+     *
+     *     @param distorted image with fisheye lens distortion.
+     *     @param undistorted Output image with compensated fisheye lens distortion.
+     *     @param K Camera matrix \(K = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{_1}\).
+     *     @param D Input vector of distortion coefficients \((k_1, k_2, k_3, k_4)\).
+     *     @param Knew Camera matrix of the distorted image. By default, it is the identity matrix but you
+     *     may additionally scale and shift the result by using a different matrix.
+     *
+     *     The function transforms an image to compensate radial and tangential lens distortion.
+     *
+     *     The function is simply a combination of fisheye::initUndistortRectifyMap (with unity R ) and remap
+     *     (with bilinear interpolation). See the former function for details of the transformation being
+     *     performed.
+     *
+     *     See below the results of undistortImage.
+     * <ul>
+     *   <li>
+     *           a\) result of undistort of perspective camera model (all possible coefficients (k_1, k_2, k_3,
+     *             k_4, k_5, k_6) of distortion were optimized under calibration)
+     *   <ul>
+     *     <li>
+     *            b\) result of fisheye::undistortImage of fisheye camera model (all possible coefficients (k_1, k_2,
+     *             k_3, k_4) of fisheye distortion were optimized under calibration)
+     *     </li>
+     *     <li>
+     *            c\) original image was captured with fisheye lens
+     *     </li>
+     *   </ul>
+     *
+     *     Pictures a) and b) almost the same. But if we consider points of image located far from the center
+     *     of image, we can notice that on image a) these points are distorted.
+     *   </li>
+     * </ul>
+     *
+     *     ![image](pics/fisheye_undistorted.jpg)
+     */
+    public static void fisheye_undistortImage(Mat distorted, Mat undistorted, Mat K, Mat D, Mat Knew) {
+        fisheye_undistortImage_1(distorted.nativeObj, undistorted.nativeObj, K.nativeObj, D.nativeObj, Knew.nativeObj);
+    }
+
+    /**
+     * Transforms an image to compensate for fisheye lens distortion.
+     *
+     *     @param distorted image with fisheye lens distortion.
+     *     @param undistorted Output image with compensated fisheye lens distortion.
+     *     @param K Camera matrix \(K = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{_1}\).
+     *     @param D Input vector of distortion coefficients \((k_1, k_2, k_3, k_4)\).
+     *     may additionally scale and shift the result by using a different matrix.
+     *
+     *     The function transforms an image to compensate radial and tangential lens distortion.
+     *
+     *     The function is simply a combination of fisheye::initUndistortRectifyMap (with unity R ) and remap
+     *     (with bilinear interpolation). See the former function for details of the transformation being
+     *     performed.
+     *
+     *     See below the results of undistortImage.
+     * <ul>
+     *   <li>
+     *           a\) result of undistort of perspective camera model (all possible coefficients (k_1, k_2, k_3,
+     *             k_4, k_5, k_6) of distortion were optimized under calibration)
+     *   <ul>
+     *     <li>
+     *            b\) result of fisheye::undistortImage of fisheye camera model (all possible coefficients (k_1, k_2,
+     *             k_3, k_4) of fisheye distortion were optimized under calibration)
+     *     </li>
+     *     <li>
+     *            c\) original image was captured with fisheye lens
+     *     </li>
+     *   </ul>
+     *
+     *     Pictures a) and b) almost the same. But if we consider points of image located far from the center
+     *     of image, we can notice that on image a) these points are distorted.
+     *   </li>
+     * </ul>
+     *
+     *     ![image](pics/fisheye_undistorted.jpg)
+     */
+    public static void fisheye_undistortImage(Mat distorted, Mat undistorted, Mat K, Mat D) {
+        fisheye_undistortImage_2(distorted.nativeObj, undistorted.nativeObj, K.nativeObj, D.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::fisheye::undistortPoints(Mat distorted, Mat& undistorted, Mat K, Mat D, Mat R = Mat(), Mat P = Mat())
+    //
+
+    /**
+     * Undistorts 2D points using fisheye model
+     *
+     *     @param distorted Array of object points, 1xN/Nx1 2-channel (or vector&lt;Point2f&gt; ), where N is the
+     *     number of points in the view.
+     *     @param K Camera matrix \(K = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{_1}\).
+     *     @param D Input vector of distortion coefficients \((k_1, k_2, k_3, k_4)\).
+     *     @param R Rectification transformation in the object space: 3x3 1-channel, or vector: 3x1/1x3
+     *     1-channel or 1x1 3-channel
+     *     @param P New camera matrix (3x3) or new projection matrix (3x4)
+     *     @param undistorted Output array of image points, 1xN/Nx1 2-channel, or vector&lt;Point2f&gt; .
+     */
+    public static void fisheye_undistortPoints(Mat distorted, Mat undistorted, Mat K, Mat D, Mat R, Mat P) {
+        fisheye_undistortPoints_0(distorted.nativeObj, undistorted.nativeObj, K.nativeObj, D.nativeObj, R.nativeObj, P.nativeObj);
+    }
+
+    /**
+     * Undistorts 2D points using fisheye model
+     *
+     *     @param distorted Array of object points, 1xN/Nx1 2-channel (or vector&lt;Point2f&gt; ), where N is the
+     *     number of points in the view.
+     *     @param K Camera matrix \(K = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{_1}\).
+     *     @param D Input vector of distortion coefficients \((k_1, k_2, k_3, k_4)\).
+     *     @param R Rectification transformation in the object space: 3x3 1-channel, or vector: 3x1/1x3
+     *     1-channel or 1x1 3-channel
+     *     @param undistorted Output array of image points, 1xN/Nx1 2-channel, or vector&lt;Point2f&gt; .
+     */
+    public static void fisheye_undistortPoints(Mat distorted, Mat undistorted, Mat K, Mat D, Mat R) {
+        fisheye_undistortPoints_1(distorted.nativeObj, undistorted.nativeObj, K.nativeObj, D.nativeObj, R.nativeObj);
+    }
+
+    /**
+     * Undistorts 2D points using fisheye model
+     *
+     *     @param distorted Array of object points, 1xN/Nx1 2-channel (or vector&lt;Point2f&gt; ), where N is the
+     *     number of points in the view.
+     *     @param K Camera matrix \(K = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{_1}\).
+     *     @param D Input vector of distortion coefficients \((k_1, k_2, k_3, k_4)\).
+     *     1-channel or 1x1 3-channel
+     *     @param undistorted Output array of image points, 1xN/Nx1 2-channel, or vector&lt;Point2f&gt; .
+     */
+    public static void fisheye_undistortPoints(Mat distorted, Mat undistorted, Mat K, Mat D) {
+        fisheye_undistortPoints_2(distorted.nativeObj, undistorted.nativeObj, K.nativeObj, D.nativeObj);
+    }
+
+
+
+
+    // C++:  Mat cv::estimateAffine2D(Mat from, Mat to, Mat& inliers = Mat(), int method = RANSAC, double ransacReprojThreshold = 3, size_t maxIters = 2000, double confidence = 0.99, size_t refineIters = 10)
+    private static native long estimateAffine2D_0(long from_nativeObj, long to_nativeObj, long inliers_nativeObj, int method, double ransacReprojThreshold, long maxIters, double confidence, long refineIters);
+    private static native long estimateAffine2D_1(long from_nativeObj, long to_nativeObj, long inliers_nativeObj, int method, double ransacReprojThreshold, long maxIters, double confidence);
+    private static native long estimateAffine2D_2(long from_nativeObj, long to_nativeObj, long inliers_nativeObj, int method, double ransacReprojThreshold, long maxIters);
+    private static native long estimateAffine2D_3(long from_nativeObj, long to_nativeObj, long inliers_nativeObj, int method, double ransacReprojThreshold);
+    private static native long estimateAffine2D_4(long from_nativeObj, long to_nativeObj, long inliers_nativeObj, int method);
+    private static native long estimateAffine2D_5(long from_nativeObj, long to_nativeObj, long inliers_nativeObj);
+    private static native long estimateAffine2D_6(long from_nativeObj, long to_nativeObj);
+
+    // C++:  Mat cv::estimateAffinePartial2D(Mat from, Mat to, Mat& inliers = Mat(), int method = RANSAC, double ransacReprojThreshold = 3, size_t maxIters = 2000, double confidence = 0.99, size_t refineIters = 10)
+    private static native long estimateAffinePartial2D_0(long from_nativeObj, long to_nativeObj, long inliers_nativeObj, int method, double ransacReprojThreshold, long maxIters, double confidence, long refineIters);
+    private static native long estimateAffinePartial2D_1(long from_nativeObj, long to_nativeObj, long inliers_nativeObj, int method, double ransacReprojThreshold, long maxIters, double confidence);
+    private static native long estimateAffinePartial2D_2(long from_nativeObj, long to_nativeObj, long inliers_nativeObj, int method, double ransacReprojThreshold, long maxIters);
+    private static native long estimateAffinePartial2D_3(long from_nativeObj, long to_nativeObj, long inliers_nativeObj, int method, double ransacReprojThreshold);
+    private static native long estimateAffinePartial2D_4(long from_nativeObj, long to_nativeObj, long inliers_nativeObj, int method);
+    private static native long estimateAffinePartial2D_5(long from_nativeObj, long to_nativeObj, long inliers_nativeObj);
+    private static native long estimateAffinePartial2D_6(long from_nativeObj, long to_nativeObj);
+
+    // C++:  Mat cv::findEssentialMat(Mat points1, Mat points2, Mat cameraMatrix, int method = RANSAC, double prob = 0.999, double threshold = 1.0, Mat& mask = Mat())
+    private static native long findEssentialMat_0(long points1_nativeObj, long points2_nativeObj, long cameraMatrix_nativeObj, int method, double prob, double threshold, long mask_nativeObj);
+    private static native long findEssentialMat_1(long points1_nativeObj, long points2_nativeObj, long cameraMatrix_nativeObj, int method, double prob, double threshold);
+    private static native long findEssentialMat_2(long points1_nativeObj, long points2_nativeObj, long cameraMatrix_nativeObj, int method, double prob);
+    private static native long findEssentialMat_3(long points1_nativeObj, long points2_nativeObj, long cameraMatrix_nativeObj, int method);
+    private static native long findEssentialMat_4(long points1_nativeObj, long points2_nativeObj, long cameraMatrix_nativeObj);
+
+    // C++:  Mat cv::findEssentialMat(Mat points1, Mat points2, double focal = 1.0, Point2d pp = Point2d(0, 0), int method = RANSAC, double prob = 0.999, double threshold = 1.0, Mat& mask = Mat())
+    private static native long findEssentialMat_5(long points1_nativeObj, long points2_nativeObj, double focal, double pp_x, double pp_y, int method, double prob, double threshold, long mask_nativeObj);
+    private static native long findEssentialMat_6(long points1_nativeObj, long points2_nativeObj, double focal, double pp_x, double pp_y, int method, double prob, double threshold);
+    private static native long findEssentialMat_7(long points1_nativeObj, long points2_nativeObj, double focal, double pp_x, double pp_y, int method, double prob);
+    private static native long findEssentialMat_8(long points1_nativeObj, long points2_nativeObj, double focal, double pp_x, double pp_y, int method);
+    private static native long findEssentialMat_9(long points1_nativeObj, long points2_nativeObj, double focal, double pp_x, double pp_y);
+    private static native long findEssentialMat_10(long points1_nativeObj, long points2_nativeObj, double focal);
+    private static native long findEssentialMat_11(long points1_nativeObj, long points2_nativeObj);
+
+    // C++:  Mat cv::findFundamentalMat(vector_Point2f points1, vector_Point2f points2, int method, double ransacReprojThreshold, double confidence, int maxIters, Mat& mask = Mat())
+    private static native long findFundamentalMat_0(long points1_mat_nativeObj, long points2_mat_nativeObj, int method, double ransacReprojThreshold, double confidence, int maxIters, long mask_nativeObj);
+    private static native long findFundamentalMat_1(long points1_mat_nativeObj, long points2_mat_nativeObj, int method, double ransacReprojThreshold, double confidence, int maxIters);
+
+    // C++:  Mat cv::findFundamentalMat(vector_Point2f points1, vector_Point2f points2, int method = FM_RANSAC, double ransacReprojThreshold = 3., double confidence = 0.99, Mat& mask = Mat())
+    private static native long findFundamentalMat_2(long points1_mat_nativeObj, long points2_mat_nativeObj, int method, double ransacReprojThreshold, double confidence, long mask_nativeObj);
+    private static native long findFundamentalMat_3(long points1_mat_nativeObj, long points2_mat_nativeObj, int method, double ransacReprojThreshold, double confidence);
+    private static native long findFundamentalMat_4(long points1_mat_nativeObj, long points2_mat_nativeObj, int method, double ransacReprojThreshold);
+    private static native long findFundamentalMat_5(long points1_mat_nativeObj, long points2_mat_nativeObj, int method);
+    private static native long findFundamentalMat_6(long points1_mat_nativeObj, long points2_mat_nativeObj);
+
+    // C++:  Mat cv::findHomography(vector_Point2f srcPoints, vector_Point2f dstPoints, int method = 0, double ransacReprojThreshold = 3, Mat& mask = Mat(), int maxIters = 2000, double confidence = 0.995)
+    private static native long findHomography_0(long srcPoints_mat_nativeObj, long dstPoints_mat_nativeObj, int method, double ransacReprojThreshold, long mask_nativeObj, int maxIters, double confidence);
+    private static native long findHomography_1(long srcPoints_mat_nativeObj, long dstPoints_mat_nativeObj, int method, double ransacReprojThreshold, long mask_nativeObj, int maxIters);
+    private static native long findHomography_2(long srcPoints_mat_nativeObj, long dstPoints_mat_nativeObj, int method, double ransacReprojThreshold, long mask_nativeObj);
+    private static native long findHomography_3(long srcPoints_mat_nativeObj, long dstPoints_mat_nativeObj, int method, double ransacReprojThreshold);
+    private static native long findHomography_4(long srcPoints_mat_nativeObj, long dstPoints_mat_nativeObj, int method);
+    private static native long findHomography_5(long srcPoints_mat_nativeObj, long dstPoints_mat_nativeObj);
+
+    // C++:  Mat cv::getOptimalNewCameraMatrix(Mat cameraMatrix, Mat distCoeffs, Size imageSize, double alpha, Size newImgSize = Size(), Rect* validPixROI = 0, bool centerPrincipalPoint = false)
+    private static native long getOptimalNewCameraMatrix_0(long cameraMatrix_nativeObj, long distCoeffs_nativeObj, double imageSize_width, double imageSize_height, double alpha, double newImgSize_width, double newImgSize_height, double[] validPixROI_out, boolean centerPrincipalPoint);
+    private static native long getOptimalNewCameraMatrix_1(long cameraMatrix_nativeObj, long distCoeffs_nativeObj, double imageSize_width, double imageSize_height, double alpha, double newImgSize_width, double newImgSize_height, double[] validPixROI_out);
+    private static native long getOptimalNewCameraMatrix_2(long cameraMatrix_nativeObj, long distCoeffs_nativeObj, double imageSize_width, double imageSize_height, double alpha, double newImgSize_width, double newImgSize_height);
+    private static native long getOptimalNewCameraMatrix_3(long cameraMatrix_nativeObj, long distCoeffs_nativeObj, double imageSize_width, double imageSize_height, double alpha);
+
+    // C++:  Mat cv::initCameraMatrix2D(vector_vector_Point3f objectPoints, vector_vector_Point2f imagePoints, Size imageSize, double aspectRatio = 1.0)
+    private static native long initCameraMatrix2D_0(long objectPoints_mat_nativeObj, long imagePoints_mat_nativeObj, double imageSize_width, double imageSize_height, double aspectRatio);
+    private static native long initCameraMatrix2D_1(long objectPoints_mat_nativeObj, long imagePoints_mat_nativeObj, double imageSize_width, double imageSize_height);
+
+    // C++:  Rect cv::getValidDisparityROI(Rect roi1, Rect roi2, int minDisparity, int numberOfDisparities, int blockSize)
+    private static native double[] getValidDisparityROI_0(int roi1_x, int roi1_y, int roi1_width, int roi1_height, int roi2_x, int roi2_y, int roi2_width, int roi2_height, int minDisparity, int numberOfDisparities, int blockSize);
+
+    // C++:  Vec3d cv::RQDecomp3x3(Mat src, Mat& mtxR, Mat& mtxQ, Mat& Qx = Mat(), Mat& Qy = Mat(), Mat& Qz = Mat())
+    private static native double[] RQDecomp3x3_0(long src_nativeObj, long mtxR_nativeObj, long mtxQ_nativeObj, long Qx_nativeObj, long Qy_nativeObj, long Qz_nativeObj);
+    private static native double[] RQDecomp3x3_1(long src_nativeObj, long mtxR_nativeObj, long mtxQ_nativeObj, long Qx_nativeObj, long Qy_nativeObj);
+    private static native double[] RQDecomp3x3_2(long src_nativeObj, long mtxR_nativeObj, long mtxQ_nativeObj, long Qx_nativeObj);
+    private static native double[] RQDecomp3x3_3(long src_nativeObj, long mtxR_nativeObj, long mtxQ_nativeObj);
+
+    // C++:  bool cv::find4QuadCornerSubpix(Mat img, Mat& corners, Size region_size)
+    private static native boolean find4QuadCornerSubpix_0(long img_nativeObj, long corners_nativeObj, double region_size_width, double region_size_height);
+
+    // C++:  bool cv::findChessboardCorners(Mat image, Size patternSize, vector_Point2f& corners, int flags = CALIB_CB_ADAPTIVE_THRESH + CALIB_CB_NORMALIZE_IMAGE)
+    private static native boolean findChessboardCorners_0(long image_nativeObj, double patternSize_width, double patternSize_height, long corners_mat_nativeObj, int flags);
+    private static native boolean findChessboardCorners_1(long image_nativeObj, double patternSize_width, double patternSize_height, long corners_mat_nativeObj);
+
+    // C++:  bool cv::findCirclesGrid(Mat image, Size patternSize, Mat& centers, int flags = CALIB_CB_SYMMETRIC_GRID, Ptr_FeatureDetector blobDetector = SimpleBlobDetector::create())
+    private static native boolean findCirclesGrid_0(long image_nativeObj, double patternSize_width, double patternSize_height, long centers_nativeObj, int flags);
+    private static native boolean findCirclesGrid_2(long image_nativeObj, double patternSize_width, double patternSize_height, long centers_nativeObj);
+
+    // C++:  bool cv::solvePnP(vector_Point3f objectPoints, vector_Point2f imagePoints, Mat cameraMatrix, vector_double distCoeffs, Mat& rvec, Mat& tvec, bool useExtrinsicGuess = false, int flags = SOLVEPNP_ITERATIVE)
+    private static native boolean solvePnP_0(long objectPoints_mat_nativeObj, long imagePoints_mat_nativeObj, long cameraMatrix_nativeObj, long distCoeffs_mat_nativeObj, long rvec_nativeObj, long tvec_nativeObj, boolean useExtrinsicGuess, int flags);
+    private static native boolean solvePnP_1(long objectPoints_mat_nativeObj, long imagePoints_mat_nativeObj, long cameraMatrix_nativeObj, long distCoeffs_mat_nativeObj, long rvec_nativeObj, long tvec_nativeObj, boolean useExtrinsicGuess);
+    private static native boolean solvePnP_2(long objectPoints_mat_nativeObj, long imagePoints_mat_nativeObj, long cameraMatrix_nativeObj, long distCoeffs_mat_nativeObj, long rvec_nativeObj, long tvec_nativeObj);
+
+    // C++:  bool cv::solvePnPRansac(vector_Point3f objectPoints, vector_Point2f imagePoints, Mat cameraMatrix, vector_double distCoeffs, Mat& rvec, Mat& tvec, bool useExtrinsicGuess = false, int iterationsCount = 100, float reprojectionError = 8.0, double confidence = 0.99, Mat& inliers = Mat(), int flags = SOLVEPNP_ITERATIVE)
+    private static native boolean solvePnPRansac_0(long objectPoints_mat_nativeObj, long imagePoints_mat_nativeObj, long cameraMatrix_nativeObj, long distCoeffs_mat_nativeObj, long rvec_nativeObj, long tvec_nativeObj, boolean useExtrinsicGuess, int iterationsCount, float reprojectionError, double confidence, long inliers_nativeObj, int flags);
+    private static native boolean solvePnPRansac_1(long objectPoints_mat_nativeObj, long imagePoints_mat_nativeObj, long cameraMatrix_nativeObj, long distCoeffs_mat_nativeObj, long rvec_nativeObj, long tvec_nativeObj, boolean useExtrinsicGuess, int iterationsCount, float reprojectionError, double confidence, long inliers_nativeObj);
+    private static native boolean solvePnPRansac_2(long objectPoints_mat_nativeObj, long imagePoints_mat_nativeObj, long cameraMatrix_nativeObj, long distCoeffs_mat_nativeObj, long rvec_nativeObj, long tvec_nativeObj, boolean useExtrinsicGuess, int iterationsCount, float reprojectionError, double confidence);
+    private static native boolean solvePnPRansac_3(long objectPoints_mat_nativeObj, long imagePoints_mat_nativeObj, long cameraMatrix_nativeObj, long distCoeffs_mat_nativeObj, long rvec_nativeObj, long tvec_nativeObj, boolean useExtrinsicGuess, int iterationsCount, float reprojectionError);
+    private static native boolean solvePnPRansac_4(long objectPoints_mat_nativeObj, long imagePoints_mat_nativeObj, long cameraMatrix_nativeObj, long distCoeffs_mat_nativeObj, long rvec_nativeObj, long tvec_nativeObj, boolean useExtrinsicGuess, int iterationsCount);
+    private static native boolean solvePnPRansac_5(long objectPoints_mat_nativeObj, long imagePoints_mat_nativeObj, long cameraMatrix_nativeObj, long distCoeffs_mat_nativeObj, long rvec_nativeObj, long tvec_nativeObj, boolean useExtrinsicGuess);
+    private static native boolean solvePnPRansac_6(long objectPoints_mat_nativeObj, long imagePoints_mat_nativeObj, long cameraMatrix_nativeObj, long distCoeffs_mat_nativeObj, long rvec_nativeObj, long tvec_nativeObj);
+
+    // C++:  bool cv::stereoRectifyUncalibrated(Mat points1, Mat points2, Mat F, Size imgSize, Mat& H1, Mat& H2, double threshold = 5)
+    private static native boolean stereoRectifyUncalibrated_0(long points1_nativeObj, long points2_nativeObj, long F_nativeObj, double imgSize_width, double imgSize_height, long H1_nativeObj, long H2_nativeObj, double threshold);
+    private static native boolean stereoRectifyUncalibrated_1(long points1_nativeObj, long points2_nativeObj, long F_nativeObj, double imgSize_width, double imgSize_height, long H1_nativeObj, long H2_nativeObj);
+
+    // C++:  double cv::calibrateCamera(vector_Mat objectPoints, vector_Mat imagePoints, Size imageSize, Mat& cameraMatrix, Mat& distCoeffs, vector_Mat& rvecs, vector_Mat& tvecs, Mat& stdDeviationsIntrinsics, Mat& stdDeviationsExtrinsics, Mat& perViewErrors, int flags = 0, TermCriteria criteria = TermCriteria( TermCriteria::COUNT + TermCriteria::EPS, 30, DBL_EPSILON))
+    private static native double calibrateCameraExtended_0(long objectPoints_mat_nativeObj, long imagePoints_mat_nativeObj, double imageSize_width, double imageSize_height, long cameraMatrix_nativeObj, long distCoeffs_nativeObj, long rvecs_mat_nativeObj, long tvecs_mat_nativeObj, long stdDeviationsIntrinsics_nativeObj, long stdDeviationsExtrinsics_nativeObj, long perViewErrors_nativeObj, int flags, int criteria_type, int criteria_maxCount, double criteria_epsilon);
+    private static native double calibrateCameraExtended_1(long objectPoints_mat_nativeObj, long imagePoints_mat_nativeObj, double imageSize_width, double imageSize_height, long cameraMatrix_nativeObj, long distCoeffs_nativeObj, long rvecs_mat_nativeObj, long tvecs_mat_nativeObj, long stdDeviationsIntrinsics_nativeObj, long stdDeviationsExtrinsics_nativeObj, long perViewErrors_nativeObj, int flags);
+    private static native double calibrateCameraExtended_2(long objectPoints_mat_nativeObj, long imagePoints_mat_nativeObj, double imageSize_width, double imageSize_height, long cameraMatrix_nativeObj, long distCoeffs_nativeObj, long rvecs_mat_nativeObj, long tvecs_mat_nativeObj, long stdDeviationsIntrinsics_nativeObj, long stdDeviationsExtrinsics_nativeObj, long perViewErrors_nativeObj);
+
+    // C++:  double cv::calibrateCamera(vector_Mat objectPoints, vector_Mat imagePoints, Size imageSize, Mat& cameraMatrix, Mat& distCoeffs, vector_Mat& rvecs, vector_Mat& tvecs, int flags = 0, TermCriteria criteria = TermCriteria( TermCriteria::COUNT + TermCriteria::EPS, 30, DBL_EPSILON))
+    private static native double calibrateCamera_0(long objectPoints_mat_nativeObj, long imagePoints_mat_nativeObj, double imageSize_width, double imageSize_height, long cameraMatrix_nativeObj, long distCoeffs_nativeObj, long rvecs_mat_nativeObj, long tvecs_mat_nativeObj, int flags, int criteria_type, int criteria_maxCount, double criteria_epsilon);
+    private static native double calibrateCamera_1(long objectPoints_mat_nativeObj, long imagePoints_mat_nativeObj, double imageSize_width, double imageSize_height, long cameraMatrix_nativeObj, long distCoeffs_nativeObj, long rvecs_mat_nativeObj, long tvecs_mat_nativeObj, int flags);
+    private static native double calibrateCamera_2(long objectPoints_mat_nativeObj, long imagePoints_mat_nativeObj, double imageSize_width, double imageSize_height, long cameraMatrix_nativeObj, long distCoeffs_nativeObj, long rvecs_mat_nativeObj, long tvecs_mat_nativeObj);
+
+    // C++:  double cv::sampsonDistance(Mat pt1, Mat pt2, Mat F)
+    private static native double sampsonDistance_0(long pt1_nativeObj, long pt2_nativeObj, long F_nativeObj);
+
+    // C++:  double cv::stereoCalibrate(vector_Mat objectPoints, vector_Mat imagePoints1, vector_Mat imagePoints2, Mat& cameraMatrix1, Mat& distCoeffs1, Mat& cameraMatrix2, Mat& distCoeffs2, Size imageSize, Mat& R, Mat& T, Mat& E, Mat& F, Mat& perViewErrors, int flags = CALIB_FIX_INTRINSIC, TermCriteria criteria = TermCriteria(TermCriteria::COUNT+TermCriteria::EPS, 30, 1e-6))
+    private static native double stereoCalibrateExtended_0(long objectPoints_mat_nativeObj, long imagePoints1_mat_nativeObj, long imagePoints2_mat_nativeObj, long cameraMatrix1_nativeObj, long distCoeffs1_nativeObj, long cameraMatrix2_nativeObj, long distCoeffs2_nativeObj, double imageSize_width, double imageSize_height, long R_nativeObj, long T_nativeObj, long E_nativeObj, long F_nativeObj, long perViewErrors_nativeObj, int flags, int criteria_type, int criteria_maxCount, double criteria_epsilon);
+    private static native double stereoCalibrateExtended_1(long objectPoints_mat_nativeObj, long imagePoints1_mat_nativeObj, long imagePoints2_mat_nativeObj, long cameraMatrix1_nativeObj, long distCoeffs1_nativeObj, long cameraMatrix2_nativeObj, long distCoeffs2_nativeObj, double imageSize_width, double imageSize_height, long R_nativeObj, long T_nativeObj, long E_nativeObj, long F_nativeObj, long perViewErrors_nativeObj, int flags);
+    private static native double stereoCalibrateExtended_2(long objectPoints_mat_nativeObj, long imagePoints1_mat_nativeObj, long imagePoints2_mat_nativeObj, long cameraMatrix1_nativeObj, long distCoeffs1_nativeObj, long cameraMatrix2_nativeObj, long distCoeffs2_nativeObj, double imageSize_width, double imageSize_height, long R_nativeObj, long T_nativeObj, long E_nativeObj, long F_nativeObj, long perViewErrors_nativeObj);
+
+    // C++:  double cv::stereoCalibrate(vector_Mat objectPoints, vector_Mat imagePoints1, vector_Mat imagePoints2, Mat& cameraMatrix1, Mat& distCoeffs1, Mat& cameraMatrix2, Mat& distCoeffs2, Size imageSize, Mat& R, Mat& T, Mat& E, Mat& F, int flags = CALIB_FIX_INTRINSIC, TermCriteria criteria = TermCriteria(TermCriteria::COUNT+TermCriteria::EPS, 30, 1e-6))
+    private static native double stereoCalibrate_0(long objectPoints_mat_nativeObj, long imagePoints1_mat_nativeObj, long imagePoints2_mat_nativeObj, long cameraMatrix1_nativeObj, long distCoeffs1_nativeObj, long cameraMatrix2_nativeObj, long distCoeffs2_nativeObj, double imageSize_width, double imageSize_height, long R_nativeObj, long T_nativeObj, long E_nativeObj, long F_nativeObj, int flags, int criteria_type, int criteria_maxCount, double criteria_epsilon);
+    private static native double stereoCalibrate_1(long objectPoints_mat_nativeObj, long imagePoints1_mat_nativeObj, long imagePoints2_mat_nativeObj, long cameraMatrix1_nativeObj, long distCoeffs1_nativeObj, long cameraMatrix2_nativeObj, long distCoeffs2_nativeObj, double imageSize_width, double imageSize_height, long R_nativeObj, long T_nativeObj, long E_nativeObj, long F_nativeObj, int flags);
+    private static native double stereoCalibrate_2(long objectPoints_mat_nativeObj, long imagePoints1_mat_nativeObj, long imagePoints2_mat_nativeObj, long cameraMatrix1_nativeObj, long distCoeffs1_nativeObj, long cameraMatrix2_nativeObj, long distCoeffs2_nativeObj, double imageSize_width, double imageSize_height, long R_nativeObj, long T_nativeObj, long E_nativeObj, long F_nativeObj);
+
+    // C++:  double cv::fisheye::calibrate(vector_Mat objectPoints, vector_Mat imagePoints, Size image_size, Mat& K, Mat& D, vector_Mat& rvecs, vector_Mat& tvecs, int flags = 0, TermCriteria criteria = TermCriteria(TermCriteria::COUNT + TermCriteria::EPS, 100, DBL_EPSILON))
+    private static native double fisheye_calibrate_0(long objectPoints_mat_nativeObj, long imagePoints_mat_nativeObj, double image_size_width, double image_size_height, long K_nativeObj, long D_nativeObj, long rvecs_mat_nativeObj, long tvecs_mat_nativeObj, int flags, int criteria_type, int criteria_maxCount, double criteria_epsilon);
+    private static native double fisheye_calibrate_1(long objectPoints_mat_nativeObj, long imagePoints_mat_nativeObj, double image_size_width, double image_size_height, long K_nativeObj, long D_nativeObj, long rvecs_mat_nativeObj, long tvecs_mat_nativeObj, int flags);
+    private static native double fisheye_calibrate_2(long objectPoints_mat_nativeObj, long imagePoints_mat_nativeObj, double image_size_width, double image_size_height, long K_nativeObj, long D_nativeObj, long rvecs_mat_nativeObj, long tvecs_mat_nativeObj);
+
+    // C++:  double cv::fisheye::stereoCalibrate(vector_Mat objectPoints, vector_Mat imagePoints1, vector_Mat imagePoints2, Mat& K1, Mat& D1, Mat& K2, Mat& D2, Size imageSize, Mat& R, Mat& T, int flags = fisheye::CALIB_FIX_INTRINSIC, TermCriteria criteria = TermCriteria(TermCriteria::COUNT + TermCriteria::EPS, 100, DBL_EPSILON))
+    private static native double fisheye_stereoCalibrate_0(long objectPoints_mat_nativeObj, long imagePoints1_mat_nativeObj, long imagePoints2_mat_nativeObj, long K1_nativeObj, long D1_nativeObj, long K2_nativeObj, long D2_nativeObj, double imageSize_width, double imageSize_height, long R_nativeObj, long T_nativeObj, int flags, int criteria_type, int criteria_maxCount, double criteria_epsilon);
+    private static native double fisheye_stereoCalibrate_1(long objectPoints_mat_nativeObj, long imagePoints1_mat_nativeObj, long imagePoints2_mat_nativeObj, long K1_nativeObj, long D1_nativeObj, long K2_nativeObj, long D2_nativeObj, double imageSize_width, double imageSize_height, long R_nativeObj, long T_nativeObj, int flags);
+    private static native double fisheye_stereoCalibrate_2(long objectPoints_mat_nativeObj, long imagePoints1_mat_nativeObj, long imagePoints2_mat_nativeObj, long K1_nativeObj, long D1_nativeObj, long K2_nativeObj, long D2_nativeObj, double imageSize_width, double imageSize_height, long R_nativeObj, long T_nativeObj);
+
+    // C++:  float cv::rectify3Collinear(Mat cameraMatrix1, Mat distCoeffs1, Mat cameraMatrix2, Mat distCoeffs2, Mat cameraMatrix3, Mat distCoeffs3, vector_Mat imgpt1, vector_Mat imgpt3, Size imageSize, Mat R12, Mat T12, Mat R13, Mat T13, Mat& R1, Mat& R2, Mat& R3, Mat& P1, Mat& P2, Mat& P3, Mat& Q, double alpha, Size newImgSize, Rect* roi1, Rect* roi2, int flags)
+    private static native float rectify3Collinear_0(long cameraMatrix1_nativeObj, long distCoeffs1_nativeObj, long cameraMatrix2_nativeObj, long distCoeffs2_nativeObj, long cameraMatrix3_nativeObj, long distCoeffs3_nativeObj, long imgpt1_mat_nativeObj, long imgpt3_mat_nativeObj, double imageSize_width, double imageSize_height, long R12_nativeObj, long T12_nativeObj, long R13_nativeObj, long T13_nativeObj, long R1_nativeObj, long R2_nativeObj, long R3_nativeObj, long P1_nativeObj, long P2_nativeObj, long P3_nativeObj, long Q_nativeObj, double alpha, double newImgSize_width, double newImgSize_height, double[] roi1_out, double[] roi2_out, int flags);
+
+    // C++:  int cv::decomposeHomographyMat(Mat H, Mat K, vector_Mat& rotations, vector_Mat& translations, vector_Mat& normals)
+    private static native int decomposeHomographyMat_0(long H_nativeObj, long K_nativeObj, long rotations_mat_nativeObj, long translations_mat_nativeObj, long normals_mat_nativeObj);
+
+    // C++:  int cv::estimateAffine3D(Mat src, Mat dst, Mat& out, Mat& inliers, double ransacThreshold = 3, double confidence = 0.99)
+    private static native int estimateAffine3D_0(long src_nativeObj, long dst_nativeObj, long out_nativeObj, long inliers_nativeObj, double ransacThreshold, double confidence);
+    private static native int estimateAffine3D_1(long src_nativeObj, long dst_nativeObj, long out_nativeObj, long inliers_nativeObj, double ransacThreshold);
+    private static native int estimateAffine3D_2(long src_nativeObj, long dst_nativeObj, long out_nativeObj, long inliers_nativeObj);
+
+    // C++:  int cv::recoverPose(Mat E, Mat points1, Mat points2, Mat& R, Mat& t, double focal = 1.0, Point2d pp = Point2d(0, 0), Mat& mask = Mat())
+    private static native int recoverPose_0(long E_nativeObj, long points1_nativeObj, long points2_nativeObj, long R_nativeObj, long t_nativeObj, double focal, double pp_x, double pp_y, long mask_nativeObj);
+    private static native int recoverPose_1(long E_nativeObj, long points1_nativeObj, long points2_nativeObj, long R_nativeObj, long t_nativeObj, double focal, double pp_x, double pp_y);
+    private static native int recoverPose_2(long E_nativeObj, long points1_nativeObj, long points2_nativeObj, long R_nativeObj, long t_nativeObj, double focal);
+    private static native int recoverPose_3(long E_nativeObj, long points1_nativeObj, long points2_nativeObj, long R_nativeObj, long t_nativeObj);
+
+    // C++:  int cv::recoverPose(Mat E, Mat points1, Mat points2, Mat cameraMatrix, Mat& R, Mat& t, Mat& mask = Mat())
+    private static native int recoverPose_4(long E_nativeObj, long points1_nativeObj, long points2_nativeObj, long cameraMatrix_nativeObj, long R_nativeObj, long t_nativeObj, long mask_nativeObj);
+    private static native int recoverPose_5(long E_nativeObj, long points1_nativeObj, long points2_nativeObj, long cameraMatrix_nativeObj, long R_nativeObj, long t_nativeObj);
+
+    // C++:  int cv::recoverPose(Mat E, Mat points1, Mat points2, Mat cameraMatrix, Mat& R, Mat& t, double distanceThresh, Mat& mask = Mat(), Mat& triangulatedPoints = Mat())
+    private static native int recoverPose_6(long E_nativeObj, long points1_nativeObj, long points2_nativeObj, long cameraMatrix_nativeObj, long R_nativeObj, long t_nativeObj, double distanceThresh, long mask_nativeObj, long triangulatedPoints_nativeObj);
+    private static native int recoverPose_7(long E_nativeObj, long points1_nativeObj, long points2_nativeObj, long cameraMatrix_nativeObj, long R_nativeObj, long t_nativeObj, double distanceThresh, long mask_nativeObj);
+    private static native int recoverPose_8(long E_nativeObj, long points1_nativeObj, long points2_nativeObj, long cameraMatrix_nativeObj, long R_nativeObj, long t_nativeObj, double distanceThresh);
+
+    // C++:  int cv::solveP3P(Mat objectPoints, Mat imagePoints, Mat cameraMatrix, Mat distCoeffs, vector_Mat& rvecs, vector_Mat& tvecs, int flags)
+    private static native int solveP3P_0(long objectPoints_nativeObj, long imagePoints_nativeObj, long cameraMatrix_nativeObj, long distCoeffs_nativeObj, long rvecs_mat_nativeObj, long tvecs_mat_nativeObj, int flags);
+
+    // C++:  int cv::solvePnPGeneric(Mat objectPoints, Mat imagePoints, Mat cameraMatrix, Mat distCoeffs, vector_Mat& rvecs, vector_Mat& tvecs, bool useExtrinsicGuess = false, SolvePnPMethod flags = SOLVEPNP_ITERATIVE, Mat rvec = Mat(), Mat tvec = Mat(), Mat& reprojectionError = Mat())
+    private static native int solvePnPGeneric_0(long objectPoints_nativeObj, long imagePoints_nativeObj, long cameraMatrix_nativeObj, long distCoeffs_nativeObj, long rvecs_mat_nativeObj, long tvecs_mat_nativeObj, boolean useExtrinsicGuess, int flags, long rvec_nativeObj, long tvec_nativeObj, long reprojectionError_nativeObj);
+    private static native int solvePnPGeneric_1(long objectPoints_nativeObj, long imagePoints_nativeObj, long cameraMatrix_nativeObj, long distCoeffs_nativeObj, long rvecs_mat_nativeObj, long tvecs_mat_nativeObj, boolean useExtrinsicGuess, int flags, long rvec_nativeObj, long tvec_nativeObj);
+    private static native int solvePnPGeneric_2(long objectPoints_nativeObj, long imagePoints_nativeObj, long cameraMatrix_nativeObj, long distCoeffs_nativeObj, long rvecs_mat_nativeObj, long tvecs_mat_nativeObj, boolean useExtrinsicGuess, int flags, long rvec_nativeObj);
+    private static native int solvePnPGeneric_3(long objectPoints_nativeObj, long imagePoints_nativeObj, long cameraMatrix_nativeObj, long distCoeffs_nativeObj, long rvecs_mat_nativeObj, long tvecs_mat_nativeObj, boolean useExtrinsicGuess, int flags);
+    private static native int solvePnPGeneric_4(long objectPoints_nativeObj, long imagePoints_nativeObj, long cameraMatrix_nativeObj, long distCoeffs_nativeObj, long rvecs_mat_nativeObj, long tvecs_mat_nativeObj, boolean useExtrinsicGuess);
+    private static native int solvePnPGeneric_5(long objectPoints_nativeObj, long imagePoints_nativeObj, long cameraMatrix_nativeObj, long distCoeffs_nativeObj, long rvecs_mat_nativeObj, long tvecs_mat_nativeObj);
+
+    // C++:  void cv::Rodrigues(Mat src, Mat& dst, Mat& jacobian = Mat())
+    private static native void Rodrigues_0(long src_nativeObj, long dst_nativeObj, long jacobian_nativeObj);
+    private static native void Rodrigues_1(long src_nativeObj, long dst_nativeObj);
+
+    // C++:  void cv::calibrateHandEye(vector_Mat R_gripper2base, vector_Mat t_gripper2base, vector_Mat R_target2cam, vector_Mat t_target2cam, Mat& R_cam2gripper, Mat& t_cam2gripper, HandEyeCalibrationMethod method = CALIB_HAND_EYE_TSAI)
+    private static native void calibrateHandEye_0(long R_gripper2base_mat_nativeObj, long t_gripper2base_mat_nativeObj, long R_target2cam_mat_nativeObj, long t_target2cam_mat_nativeObj, long R_cam2gripper_nativeObj, long t_cam2gripper_nativeObj, int method);
+    private static native void calibrateHandEye_1(long R_gripper2base_mat_nativeObj, long t_gripper2base_mat_nativeObj, long R_target2cam_mat_nativeObj, long t_target2cam_mat_nativeObj, long R_cam2gripper_nativeObj, long t_cam2gripper_nativeObj);
+
+    // C++:  void cv::calibrationMatrixValues(Mat cameraMatrix, Size imageSize, double apertureWidth, double apertureHeight, double& fovx, double& fovy, double& focalLength, Point2d& principalPoint, double& aspectRatio)
+    private static native void calibrationMatrixValues_0(long cameraMatrix_nativeObj, double imageSize_width, double imageSize_height, double apertureWidth, double apertureHeight, double[] fovx_out, double[] fovy_out, double[] focalLength_out, double[] principalPoint_out, double[] aspectRatio_out);
+
+    // C++:  void cv::composeRT(Mat rvec1, Mat tvec1, Mat rvec2, Mat tvec2, Mat& rvec3, Mat& tvec3, Mat& dr3dr1 = Mat(), Mat& dr3dt1 = Mat(), Mat& dr3dr2 = Mat(), Mat& dr3dt2 = Mat(), Mat& dt3dr1 = Mat(), Mat& dt3dt1 = Mat(), Mat& dt3dr2 = Mat(), Mat& dt3dt2 = Mat())
+    private static native void composeRT_0(long rvec1_nativeObj, long tvec1_nativeObj, long rvec2_nativeObj, long tvec2_nativeObj, long rvec3_nativeObj, long tvec3_nativeObj, long dr3dr1_nativeObj, long dr3dt1_nativeObj, long dr3dr2_nativeObj, long dr3dt2_nativeObj, long dt3dr1_nativeObj, long dt3dt1_nativeObj, long dt3dr2_nativeObj, long dt3dt2_nativeObj);
+    private static native void composeRT_1(long rvec1_nativeObj, long tvec1_nativeObj, long rvec2_nativeObj, long tvec2_nativeObj, long rvec3_nativeObj, long tvec3_nativeObj, long dr3dr1_nativeObj, long dr3dt1_nativeObj, long dr3dr2_nativeObj, long dr3dt2_nativeObj, long dt3dr1_nativeObj, long dt3dt1_nativeObj, long dt3dr2_nativeObj);
+    private static native void composeRT_2(long rvec1_nativeObj, long tvec1_nativeObj, long rvec2_nativeObj, long tvec2_nativeObj, long rvec3_nativeObj, long tvec3_nativeObj, long dr3dr1_nativeObj, long dr3dt1_nativeObj, long dr3dr2_nativeObj, long dr3dt2_nativeObj, long dt3dr1_nativeObj, long dt3dt1_nativeObj);
+    private static native void composeRT_3(long rvec1_nativeObj, long tvec1_nativeObj, long rvec2_nativeObj, long tvec2_nativeObj, long rvec3_nativeObj, long tvec3_nativeObj, long dr3dr1_nativeObj, long dr3dt1_nativeObj, long dr3dr2_nativeObj, long dr3dt2_nativeObj, long dt3dr1_nativeObj);
+    private static native void composeRT_4(long rvec1_nativeObj, long tvec1_nativeObj, long rvec2_nativeObj, long tvec2_nativeObj, long rvec3_nativeObj, long tvec3_nativeObj, long dr3dr1_nativeObj, long dr3dt1_nativeObj, long dr3dr2_nativeObj, long dr3dt2_nativeObj);
+    private static native void composeRT_5(long rvec1_nativeObj, long tvec1_nativeObj, long rvec2_nativeObj, long tvec2_nativeObj, long rvec3_nativeObj, long tvec3_nativeObj, long dr3dr1_nativeObj, long dr3dt1_nativeObj, long dr3dr2_nativeObj);
+    private static native void composeRT_6(long rvec1_nativeObj, long tvec1_nativeObj, long rvec2_nativeObj, long tvec2_nativeObj, long rvec3_nativeObj, long tvec3_nativeObj, long dr3dr1_nativeObj, long dr3dt1_nativeObj);
+    private static native void composeRT_7(long rvec1_nativeObj, long tvec1_nativeObj, long rvec2_nativeObj, long tvec2_nativeObj, long rvec3_nativeObj, long tvec3_nativeObj, long dr3dr1_nativeObj);
+    private static native void composeRT_8(long rvec1_nativeObj, long tvec1_nativeObj, long rvec2_nativeObj, long tvec2_nativeObj, long rvec3_nativeObj, long tvec3_nativeObj);
+
+    // C++:  void cv::computeCorrespondEpilines(Mat points, int whichImage, Mat F, Mat& lines)
+    private static native void computeCorrespondEpilines_0(long points_nativeObj, int whichImage, long F_nativeObj, long lines_nativeObj);
+
+    // C++:  void cv::convertPointsFromHomogeneous(Mat src, Mat& dst)
+    private static native void convertPointsFromHomogeneous_0(long src_nativeObj, long dst_nativeObj);
+
+    // C++:  void cv::convertPointsToHomogeneous(Mat src, Mat& dst)
+    private static native void convertPointsToHomogeneous_0(long src_nativeObj, long dst_nativeObj);
+
+    // C++:  void cv::correctMatches(Mat F, Mat points1, Mat points2, Mat& newPoints1, Mat& newPoints2)
+    private static native void correctMatches_0(long F_nativeObj, long points1_nativeObj, long points2_nativeObj, long newPoints1_nativeObj, long newPoints2_nativeObj);
+
+    // C++:  void cv::decomposeEssentialMat(Mat E, Mat& R1, Mat& R2, Mat& t)
+    private static native void decomposeEssentialMat_0(long E_nativeObj, long R1_nativeObj, long R2_nativeObj, long t_nativeObj);
+
+    // C++:  void cv::decomposeProjectionMatrix(Mat projMatrix, Mat& cameraMatrix, Mat& rotMatrix, Mat& transVect, Mat& rotMatrixX = Mat(), Mat& rotMatrixY = Mat(), Mat& rotMatrixZ = Mat(), Mat& eulerAngles = Mat())
+    private static native void decomposeProjectionMatrix_0(long projMatrix_nativeObj, long cameraMatrix_nativeObj, long rotMatrix_nativeObj, long transVect_nativeObj, long rotMatrixX_nativeObj, long rotMatrixY_nativeObj, long rotMatrixZ_nativeObj, long eulerAngles_nativeObj);
+    private static native void decomposeProjectionMatrix_1(long projMatrix_nativeObj, long cameraMatrix_nativeObj, long rotMatrix_nativeObj, long transVect_nativeObj, long rotMatrixX_nativeObj, long rotMatrixY_nativeObj, long rotMatrixZ_nativeObj);
+    private static native void decomposeProjectionMatrix_2(long projMatrix_nativeObj, long cameraMatrix_nativeObj, long rotMatrix_nativeObj, long transVect_nativeObj, long rotMatrixX_nativeObj, long rotMatrixY_nativeObj);
+    private static native void decomposeProjectionMatrix_3(long projMatrix_nativeObj, long cameraMatrix_nativeObj, long rotMatrix_nativeObj, long transVect_nativeObj, long rotMatrixX_nativeObj);
+    private static native void decomposeProjectionMatrix_4(long projMatrix_nativeObj, long cameraMatrix_nativeObj, long rotMatrix_nativeObj, long transVect_nativeObj);
+
+    // C++:  void cv::drawChessboardCorners(Mat& image, Size patternSize, vector_Point2f corners, bool patternWasFound)
+    private static native void drawChessboardCorners_0(long image_nativeObj, double patternSize_width, double patternSize_height, long corners_mat_nativeObj, boolean patternWasFound);
+
+    // C++:  void cv::drawFrameAxes(Mat& image, Mat cameraMatrix, Mat distCoeffs, Mat rvec, Mat tvec, float length, int thickness = 3)
+    private static native void drawFrameAxes_0(long image_nativeObj, long cameraMatrix_nativeObj, long distCoeffs_nativeObj, long rvec_nativeObj, long tvec_nativeObj, float length, int thickness);
+    private static native void drawFrameAxes_1(long image_nativeObj, long cameraMatrix_nativeObj, long distCoeffs_nativeObj, long rvec_nativeObj, long tvec_nativeObj, float length);
+
+    // C++:  void cv::filterHomographyDecompByVisibleRefpoints(vector_Mat rotations, vector_Mat normals, Mat beforePoints, Mat afterPoints, Mat& possibleSolutions, Mat pointsMask = Mat())
+    private static native void filterHomographyDecompByVisibleRefpoints_0(long rotations_mat_nativeObj, long normals_mat_nativeObj, long beforePoints_nativeObj, long afterPoints_nativeObj, long possibleSolutions_nativeObj, long pointsMask_nativeObj);
+    private static native void filterHomographyDecompByVisibleRefpoints_1(long rotations_mat_nativeObj, long normals_mat_nativeObj, long beforePoints_nativeObj, long afterPoints_nativeObj, long possibleSolutions_nativeObj);
+
+    // C++:  void cv::filterSpeckles(Mat& img, double newVal, int maxSpeckleSize, double maxDiff, Mat& buf = Mat())
+    private static native void filterSpeckles_0(long img_nativeObj, double newVal, int maxSpeckleSize, double maxDiff, long buf_nativeObj);
+    private static native void filterSpeckles_1(long img_nativeObj, double newVal, int maxSpeckleSize, double maxDiff);
+
+    // C++:  void cv::matMulDeriv(Mat A, Mat B, Mat& dABdA, Mat& dABdB)
+    private static native void matMulDeriv_0(long A_nativeObj, long B_nativeObj, long dABdA_nativeObj, long dABdB_nativeObj);
+
+    // C++:  void cv::projectPoints(vector_Point3f objectPoints, Mat rvec, Mat tvec, Mat cameraMatrix, vector_double distCoeffs, vector_Point2f& imagePoints, Mat& jacobian = Mat(), double aspectRatio = 0)
+    private static native void projectPoints_0(long objectPoints_mat_nativeObj, long rvec_nativeObj, long tvec_nativeObj, long cameraMatrix_nativeObj, long distCoeffs_mat_nativeObj, long imagePoints_mat_nativeObj, long jacobian_nativeObj, double aspectRatio);
+    private static native void projectPoints_1(long objectPoints_mat_nativeObj, long rvec_nativeObj, long tvec_nativeObj, long cameraMatrix_nativeObj, long distCoeffs_mat_nativeObj, long imagePoints_mat_nativeObj, long jacobian_nativeObj);
+    private static native void projectPoints_2(long objectPoints_mat_nativeObj, long rvec_nativeObj, long tvec_nativeObj, long cameraMatrix_nativeObj, long distCoeffs_mat_nativeObj, long imagePoints_mat_nativeObj);
+
+    // C++:  void cv::reprojectImageTo3D(Mat disparity, Mat& _3dImage, Mat Q, bool handleMissingValues = false, int ddepth = -1)
+    private static native void reprojectImageTo3D_0(long disparity_nativeObj, long _3dImage_nativeObj, long Q_nativeObj, boolean handleMissingValues, int ddepth);
+    private static native void reprojectImageTo3D_1(long disparity_nativeObj, long _3dImage_nativeObj, long Q_nativeObj, boolean handleMissingValues);
+    private static native void reprojectImageTo3D_2(long disparity_nativeObj, long _3dImage_nativeObj, long Q_nativeObj);
+
+    // C++:  void cv::solvePnPRefineLM(Mat objectPoints, Mat imagePoints, Mat cameraMatrix, Mat distCoeffs, Mat& rvec, Mat& tvec, TermCriteria criteria = TermCriteria(TermCriteria::EPS + TermCriteria::COUNT, 20, FLT_EPSILON))
+    private static native void solvePnPRefineLM_0(long objectPoints_nativeObj, long imagePoints_nativeObj, long cameraMatrix_nativeObj, long distCoeffs_nativeObj, long rvec_nativeObj, long tvec_nativeObj, int criteria_type, int criteria_maxCount, double criteria_epsilon);
+    private static native void solvePnPRefineLM_1(long objectPoints_nativeObj, long imagePoints_nativeObj, long cameraMatrix_nativeObj, long distCoeffs_nativeObj, long rvec_nativeObj, long tvec_nativeObj);
+
+    // C++:  void cv::solvePnPRefineVVS(Mat objectPoints, Mat imagePoints, Mat cameraMatrix, Mat distCoeffs, Mat& rvec, Mat& tvec, TermCriteria criteria = TermCriteria(TermCriteria::EPS + TermCriteria::COUNT, 20, FLT_EPSILON), double VVSlambda = 1)
+    private static native void solvePnPRefineVVS_0(long objectPoints_nativeObj, long imagePoints_nativeObj, long cameraMatrix_nativeObj, long distCoeffs_nativeObj, long rvec_nativeObj, long tvec_nativeObj, int criteria_type, int criteria_maxCount, double criteria_epsilon, double VVSlambda);
+    private static native void solvePnPRefineVVS_1(long objectPoints_nativeObj, long imagePoints_nativeObj, long cameraMatrix_nativeObj, long distCoeffs_nativeObj, long rvec_nativeObj, long tvec_nativeObj, int criteria_type, int criteria_maxCount, double criteria_epsilon);
+    private static native void solvePnPRefineVVS_2(long objectPoints_nativeObj, long imagePoints_nativeObj, long cameraMatrix_nativeObj, long distCoeffs_nativeObj, long rvec_nativeObj, long tvec_nativeObj);
+
+    // C++:  void cv::stereoRectify(Mat cameraMatrix1, Mat distCoeffs1, Mat cameraMatrix2, Mat distCoeffs2, Size imageSize, Mat R, Mat T, Mat& R1, Mat& R2, Mat& P1, Mat& P2, Mat& Q, int flags = CALIB_ZERO_DISPARITY, double alpha = -1, Size newImageSize = Size(), Rect* validPixROI1 = 0, Rect* validPixROI2 = 0)
+    private static native void stereoRectify_0(long cameraMatrix1_nativeObj, long distCoeffs1_nativeObj, long cameraMatrix2_nativeObj, long distCoeffs2_nativeObj, double imageSize_width, double imageSize_height, long R_nativeObj, long T_nativeObj, long R1_nativeObj, long R2_nativeObj, long P1_nativeObj, long P2_nativeObj, long Q_nativeObj, int flags, double alpha, double newImageSize_width, double newImageSize_height, double[] validPixROI1_out, double[] validPixROI2_out);
+    private static native void stereoRectify_1(long cameraMatrix1_nativeObj, long distCoeffs1_nativeObj, long cameraMatrix2_nativeObj, long distCoeffs2_nativeObj, double imageSize_width, double imageSize_height, long R_nativeObj, long T_nativeObj, long R1_nativeObj, long R2_nativeObj, long P1_nativeObj, long P2_nativeObj, long Q_nativeObj, int flags, double alpha, double newImageSize_width, double newImageSize_height, double[] validPixROI1_out);
+    private static native void stereoRectify_2(long cameraMatrix1_nativeObj, long distCoeffs1_nativeObj, long cameraMatrix2_nativeObj, long distCoeffs2_nativeObj, double imageSize_width, double imageSize_height, long R_nativeObj, long T_nativeObj, long R1_nativeObj, long R2_nativeObj, long P1_nativeObj, long P2_nativeObj, long Q_nativeObj, int flags, double alpha, double newImageSize_width, double newImageSize_height);
+    private static native void stereoRectify_3(long cameraMatrix1_nativeObj, long distCoeffs1_nativeObj, long cameraMatrix2_nativeObj, long distCoeffs2_nativeObj, double imageSize_width, double imageSize_height, long R_nativeObj, long T_nativeObj, long R1_nativeObj, long R2_nativeObj, long P1_nativeObj, long P2_nativeObj, long Q_nativeObj, int flags, double alpha);
+    private static native void stereoRectify_4(long cameraMatrix1_nativeObj, long distCoeffs1_nativeObj, long cameraMatrix2_nativeObj, long distCoeffs2_nativeObj, double imageSize_width, double imageSize_height, long R_nativeObj, long T_nativeObj, long R1_nativeObj, long R2_nativeObj, long P1_nativeObj, long P2_nativeObj, long Q_nativeObj, int flags);
+    private static native void stereoRectify_5(long cameraMatrix1_nativeObj, long distCoeffs1_nativeObj, long cameraMatrix2_nativeObj, long distCoeffs2_nativeObj, double imageSize_width, double imageSize_height, long R_nativeObj, long T_nativeObj, long R1_nativeObj, long R2_nativeObj, long P1_nativeObj, long P2_nativeObj, long Q_nativeObj);
+
+    // C++:  void cv::triangulatePoints(Mat projMatr1, Mat projMatr2, Mat projPoints1, Mat projPoints2, Mat& points4D)
+    private static native void triangulatePoints_0(long projMatr1_nativeObj, long projMatr2_nativeObj, long projPoints1_nativeObj, long projPoints2_nativeObj, long points4D_nativeObj);
+
+    // C++:  void cv::validateDisparity(Mat& disparity, Mat cost, int minDisparity, int numberOfDisparities, int disp12MaxDisp = 1)
+    private static native void validateDisparity_0(long disparity_nativeObj, long cost_nativeObj, int minDisparity, int numberOfDisparities, int disp12MaxDisp);
+    private static native void validateDisparity_1(long disparity_nativeObj, long cost_nativeObj, int minDisparity, int numberOfDisparities);
+
+    // C++:  void cv::fisheye::distortPoints(Mat undistorted, Mat& distorted, Mat K, Mat D, double alpha = 0)
+    private static native void fisheye_distortPoints_0(long undistorted_nativeObj, long distorted_nativeObj, long K_nativeObj, long D_nativeObj, double alpha);
+    private static native void fisheye_distortPoints_1(long undistorted_nativeObj, long distorted_nativeObj, long K_nativeObj, long D_nativeObj);
+
+    // C++:  void cv::fisheye::estimateNewCameraMatrixForUndistortRectify(Mat K, Mat D, Size image_size, Mat R, Mat& P, double balance = 0.0, Size new_size = Size(), double fov_scale = 1.0)
+    private static native void fisheye_estimateNewCameraMatrixForUndistortRectify_0(long K_nativeObj, long D_nativeObj, double image_size_width, double image_size_height, long R_nativeObj, long P_nativeObj, double balance, double new_size_width, double new_size_height, double fov_scale);
+    private static native void fisheye_estimateNewCameraMatrixForUndistortRectify_1(long K_nativeObj, long D_nativeObj, double image_size_width, double image_size_height, long R_nativeObj, long P_nativeObj, double balance, double new_size_width, double new_size_height);
+    private static native void fisheye_estimateNewCameraMatrixForUndistortRectify_2(long K_nativeObj, long D_nativeObj, double image_size_width, double image_size_height, long R_nativeObj, long P_nativeObj, double balance);
+    private static native void fisheye_estimateNewCameraMatrixForUndistortRectify_3(long K_nativeObj, long D_nativeObj, double image_size_width, double image_size_height, long R_nativeObj, long P_nativeObj);
+
+    // C++:  void cv::fisheye::initUndistortRectifyMap(Mat K, Mat D, Mat R, Mat P, Size size, int m1type, Mat& map1, Mat& map2)
+    private static native void fisheye_initUndistortRectifyMap_0(long K_nativeObj, long D_nativeObj, long R_nativeObj, long P_nativeObj, double size_width, double size_height, int m1type, long map1_nativeObj, long map2_nativeObj);
+
+    // C++:  void cv::fisheye::projectPoints(Mat objectPoints, Mat& imagePoints, Mat rvec, Mat tvec, Mat K, Mat D, double alpha = 0, Mat& jacobian = Mat())
+    private static native void fisheye_projectPoints_0(long objectPoints_nativeObj, long imagePoints_nativeObj, long rvec_nativeObj, long tvec_nativeObj, long K_nativeObj, long D_nativeObj, double alpha, long jacobian_nativeObj);
+    private static native void fisheye_projectPoints_1(long objectPoints_nativeObj, long imagePoints_nativeObj, long rvec_nativeObj, long tvec_nativeObj, long K_nativeObj, long D_nativeObj, double alpha);
+    private static native void fisheye_projectPoints_2(long objectPoints_nativeObj, long imagePoints_nativeObj, long rvec_nativeObj, long tvec_nativeObj, long K_nativeObj, long D_nativeObj);
+
+    // C++:  void cv::fisheye::stereoRectify(Mat K1, Mat D1, Mat K2, Mat D2, Size imageSize, Mat R, Mat tvec, Mat& R1, Mat& R2, Mat& P1, Mat& P2, Mat& Q, int flags, Size newImageSize = Size(), double balance = 0.0, double fov_scale = 1.0)
+    private static native void fisheye_stereoRectify_0(long K1_nativeObj, long D1_nativeObj, long K2_nativeObj, long D2_nativeObj, double imageSize_width, double imageSize_height, long R_nativeObj, long tvec_nativeObj, long R1_nativeObj, long R2_nativeObj, long P1_nativeObj, long P2_nativeObj, long Q_nativeObj, int flags, double newImageSize_width, double newImageSize_height, double balance, double fov_scale);
+    private static native void fisheye_stereoRectify_1(long K1_nativeObj, long D1_nativeObj, long K2_nativeObj, long D2_nativeObj, double imageSize_width, double imageSize_height, long R_nativeObj, long tvec_nativeObj, long R1_nativeObj, long R2_nativeObj, long P1_nativeObj, long P2_nativeObj, long Q_nativeObj, int flags, double newImageSize_width, double newImageSize_height, double balance);
+    private static native void fisheye_stereoRectify_2(long K1_nativeObj, long D1_nativeObj, long K2_nativeObj, long D2_nativeObj, double imageSize_width, double imageSize_height, long R_nativeObj, long tvec_nativeObj, long R1_nativeObj, long R2_nativeObj, long P1_nativeObj, long P2_nativeObj, long Q_nativeObj, int flags, double newImageSize_width, double newImageSize_height);
+    private static native void fisheye_stereoRectify_3(long K1_nativeObj, long D1_nativeObj, long K2_nativeObj, long D2_nativeObj, double imageSize_width, double imageSize_height, long R_nativeObj, long tvec_nativeObj, long R1_nativeObj, long R2_nativeObj, long P1_nativeObj, long P2_nativeObj, long Q_nativeObj, int flags);
+
+    // C++:  void cv::fisheye::undistortImage(Mat distorted, Mat& undistorted, Mat K, Mat D, Mat Knew = cv::Mat(), Size new_size = Size())
+    private static native void fisheye_undistortImage_0(long distorted_nativeObj, long undistorted_nativeObj, long K_nativeObj, long D_nativeObj, long Knew_nativeObj, double new_size_width, double new_size_height);
+    private static native void fisheye_undistortImage_1(long distorted_nativeObj, long undistorted_nativeObj, long K_nativeObj, long D_nativeObj, long Knew_nativeObj);
+    private static native void fisheye_undistortImage_2(long distorted_nativeObj, long undistorted_nativeObj, long K_nativeObj, long D_nativeObj);
+
+    // C++:  void cv::fisheye::undistortPoints(Mat distorted, Mat& undistorted, Mat K, Mat D, Mat R = Mat(), Mat P = Mat())
+    private static native void fisheye_undistortPoints_0(long distorted_nativeObj, long undistorted_nativeObj, long K_nativeObj, long D_nativeObj, long R_nativeObj, long P_nativeObj);
+    private static native void fisheye_undistortPoints_1(long distorted_nativeObj, long undistorted_nativeObj, long K_nativeObj, long D_nativeObj, long R_nativeObj);
+    private static native void fisheye_undistortPoints_2(long distorted_nativeObj, long undistorted_nativeObj, long K_nativeObj, long D_nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/calib3d/StereoBM.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/calib3d/StereoBM.java	(date 1605830247226)
+++ openCVLibrary3411/src/main/java/org/opencv/calib3d/StereoBM.java	(date 1605830247226)
@@ -0,0 +1,294 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.calib3d;
+
+import org.opencv.calib3d.StereoBM;
+import org.opencv.calib3d.StereoMatcher;
+import org.opencv.core.Rect;
+
+// C++: class StereoBM
+/**
+ * Class for computing stereo correspondence using the block matching algorithm, introduced and
+ * contributed to OpenCV by K. Konolige.
+ */
+public class StereoBM extends StereoMatcher {
+
+    protected StereoBM(long addr) { super(addr); }
+
+    // internal usage only
+    public static StereoBM __fromPtr__(long addr) { return new StereoBM(addr); }
+
+    // C++: enum <unnamed>
+    public static final int
+            PREFILTER_NORMALIZED_RESPONSE = 0,
+            PREFILTER_XSOBEL = 1;
+
+
+    //
+    // C++: static Ptr_StereoBM cv::StereoBM::create(int numDisparities = 0, int blockSize = 21)
+    //
+
+    /**
+     * Creates StereoBM object
+     *
+     *     @param numDisparities the disparity search range. For each pixel algorithm will find the best
+     *     disparity from 0 (default minimum disparity) to numDisparities. The search range can then be
+     *     shifted by changing the minimum disparity.
+     *     @param blockSize the linear size of the blocks compared by the algorithm. The size should be odd
+     *     (as the block is centered at the current pixel). Larger block size implies smoother, though less
+     *     accurate disparity map. Smaller block size gives more detailed disparity map, but there is higher
+     *     chance for algorithm to find a wrong correspondence.
+     *
+     *     The function create StereoBM object. You can then call StereoBM::compute() to compute disparity for
+     *     a specific stereo pair.
+     * @return automatically generated
+     */
+    public static StereoBM create(int numDisparities, int blockSize) {
+        return StereoBM.__fromPtr__(create_0(numDisparities, blockSize));
+    }
+
+    /**
+     * Creates StereoBM object
+     *
+     *     @param numDisparities the disparity search range. For each pixel algorithm will find the best
+     *     disparity from 0 (default minimum disparity) to numDisparities. The search range can then be
+     *     shifted by changing the minimum disparity.
+     *     (as the block is centered at the current pixel). Larger block size implies smoother, though less
+     *     accurate disparity map. Smaller block size gives more detailed disparity map, but there is higher
+     *     chance for algorithm to find a wrong correspondence.
+     *
+     *     The function create StereoBM object. You can then call StereoBM::compute() to compute disparity for
+     *     a specific stereo pair.
+     * @return automatically generated
+     */
+    public static StereoBM create(int numDisparities) {
+        return StereoBM.__fromPtr__(create_1(numDisparities));
+    }
+
+    /**
+     * Creates StereoBM object
+     *
+     *     disparity from 0 (default minimum disparity) to numDisparities. The search range can then be
+     *     shifted by changing the minimum disparity.
+     *     (as the block is centered at the current pixel). Larger block size implies smoother, though less
+     *     accurate disparity map. Smaller block size gives more detailed disparity map, but there is higher
+     *     chance for algorithm to find a wrong correspondence.
+     *
+     *     The function create StereoBM object. You can then call StereoBM::compute() to compute disparity for
+     *     a specific stereo pair.
+     * @return automatically generated
+     */
+    public static StereoBM create() {
+        return StereoBM.__fromPtr__(create_2());
+    }
+
+
+    //
+    // C++:  Rect cv::StereoBM::getROI1()
+    //
+
+    public Rect getROI1() {
+        return new Rect(getROI1_0(nativeObj));
+    }
+
+
+    //
+    // C++:  Rect cv::StereoBM::getROI2()
+    //
+
+    public Rect getROI2() {
+        return new Rect(getROI2_0(nativeObj));
+    }
+
+
+    //
+    // C++:  int cv::StereoBM::getPreFilterCap()
+    //
+
+    public int getPreFilterCap() {
+        return getPreFilterCap_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::StereoBM::getPreFilterSize()
+    //
+
+    public int getPreFilterSize() {
+        return getPreFilterSize_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::StereoBM::getPreFilterType()
+    //
+
+    public int getPreFilterType() {
+        return getPreFilterType_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::StereoBM::getSmallerBlockSize()
+    //
+
+    public int getSmallerBlockSize() {
+        return getSmallerBlockSize_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::StereoBM::getTextureThreshold()
+    //
+
+    public int getTextureThreshold() {
+        return getTextureThreshold_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::StereoBM::getUniquenessRatio()
+    //
+
+    public int getUniquenessRatio() {
+        return getUniquenessRatio_0(nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::StereoBM::setPreFilterCap(int preFilterCap)
+    //
+
+    public void setPreFilterCap(int preFilterCap) {
+        setPreFilterCap_0(nativeObj, preFilterCap);
+    }
+
+
+    //
+    // C++:  void cv::StereoBM::setPreFilterSize(int preFilterSize)
+    //
+
+    public void setPreFilterSize(int preFilterSize) {
+        setPreFilterSize_0(nativeObj, preFilterSize);
+    }
+
+
+    //
+    // C++:  void cv::StereoBM::setPreFilterType(int preFilterType)
+    //
+
+    public void setPreFilterType(int preFilterType) {
+        setPreFilterType_0(nativeObj, preFilterType);
+    }
+
+
+    //
+    // C++:  void cv::StereoBM::setROI1(Rect roi1)
+    //
+
+    public void setROI1(Rect roi1) {
+        setROI1_0(nativeObj, roi1.x, roi1.y, roi1.width, roi1.height);
+    }
+
+
+    //
+    // C++:  void cv::StereoBM::setROI2(Rect roi2)
+    //
+
+    public void setROI2(Rect roi2) {
+        setROI2_0(nativeObj, roi2.x, roi2.y, roi2.width, roi2.height);
+    }
+
+
+    //
+    // C++:  void cv::StereoBM::setSmallerBlockSize(int blockSize)
+    //
+
+    public void setSmallerBlockSize(int blockSize) {
+        setSmallerBlockSize_0(nativeObj, blockSize);
+    }
+
+
+    //
+    // C++:  void cv::StereoBM::setTextureThreshold(int textureThreshold)
+    //
+
+    public void setTextureThreshold(int textureThreshold) {
+        setTextureThreshold_0(nativeObj, textureThreshold);
+    }
+
+
+    //
+    // C++:  void cv::StereoBM::setUniquenessRatio(int uniquenessRatio)
+    //
+
+    public void setUniquenessRatio(int uniquenessRatio) {
+        setUniquenessRatio_0(nativeObj, uniquenessRatio);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++: static Ptr_StereoBM cv::StereoBM::create(int numDisparities = 0, int blockSize = 21)
+    private static native long create_0(int numDisparities, int blockSize);
+    private static native long create_1(int numDisparities);
+    private static native long create_2();
+
+    // C++:  Rect cv::StereoBM::getROI1()
+    private static native double[] getROI1_0(long nativeObj);
+
+    // C++:  Rect cv::StereoBM::getROI2()
+    private static native double[] getROI2_0(long nativeObj);
+
+    // C++:  int cv::StereoBM::getPreFilterCap()
+    private static native int getPreFilterCap_0(long nativeObj);
+
+    // C++:  int cv::StereoBM::getPreFilterSize()
+    private static native int getPreFilterSize_0(long nativeObj);
+
+    // C++:  int cv::StereoBM::getPreFilterType()
+    private static native int getPreFilterType_0(long nativeObj);
+
+    // C++:  int cv::StereoBM::getSmallerBlockSize()
+    private static native int getSmallerBlockSize_0(long nativeObj);
+
+    // C++:  int cv::StereoBM::getTextureThreshold()
+    private static native int getTextureThreshold_0(long nativeObj);
+
+    // C++:  int cv::StereoBM::getUniquenessRatio()
+    private static native int getUniquenessRatio_0(long nativeObj);
+
+    // C++:  void cv::StereoBM::setPreFilterCap(int preFilterCap)
+    private static native void setPreFilterCap_0(long nativeObj, int preFilterCap);
+
+    // C++:  void cv::StereoBM::setPreFilterSize(int preFilterSize)
+    private static native void setPreFilterSize_0(long nativeObj, int preFilterSize);
+
+    // C++:  void cv::StereoBM::setPreFilterType(int preFilterType)
+    private static native void setPreFilterType_0(long nativeObj, int preFilterType);
+
+    // C++:  void cv::StereoBM::setROI1(Rect roi1)
+    private static native void setROI1_0(long nativeObj, int roi1_x, int roi1_y, int roi1_width, int roi1_height);
+
+    // C++:  void cv::StereoBM::setROI2(Rect roi2)
+    private static native void setROI2_0(long nativeObj, int roi2_x, int roi2_y, int roi2_width, int roi2_height);
+
+    // C++:  void cv::StereoBM::setSmallerBlockSize(int blockSize)
+    private static native void setSmallerBlockSize_0(long nativeObj, int blockSize);
+
+    // C++:  void cv::StereoBM::setTextureThreshold(int textureThreshold)
+    private static native void setTextureThreshold_0(long nativeObj, int textureThreshold);
+
+    // C++:  void cv::StereoBM::setUniquenessRatio(int uniquenessRatio)
+    private static native void setUniquenessRatio_0(long nativeObj, int uniquenessRatio);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/calib3d/StereoMatcher.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/calib3d/StereoMatcher.java	(date 1605830247239)
+++ openCVLibrary3411/src/main/java/org/opencv/calib3d/StereoMatcher.java	(date 1605830247239)
@@ -0,0 +1,201 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.calib3d;
+
+import org.opencv.core.Algorithm;
+import org.opencv.core.Mat;
+
+// C++: class StereoMatcher
+/**
+ * The base class for stereo correspondence algorithms.
+ */
+public class StereoMatcher extends Algorithm {
+
+    protected StereoMatcher(long addr) { super(addr); }
+
+    // internal usage only
+    public static StereoMatcher __fromPtr__(long addr) { return new StereoMatcher(addr); }
+
+    // C++: enum <unnamed>
+    public static final int
+            DISP_SHIFT = 4,
+            DISP_SCALE = (1 << DISP_SHIFT);
+
+
+    //
+    // C++:  int cv::StereoMatcher::getBlockSize()
+    //
+
+    public int getBlockSize() {
+        return getBlockSize_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::StereoMatcher::getDisp12MaxDiff()
+    //
+
+    public int getDisp12MaxDiff() {
+        return getDisp12MaxDiff_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::StereoMatcher::getMinDisparity()
+    //
+
+    public int getMinDisparity() {
+        return getMinDisparity_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::StereoMatcher::getNumDisparities()
+    //
+
+    public int getNumDisparities() {
+        return getNumDisparities_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::StereoMatcher::getSpeckleRange()
+    //
+
+    public int getSpeckleRange() {
+        return getSpeckleRange_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::StereoMatcher::getSpeckleWindowSize()
+    //
+
+    public int getSpeckleWindowSize() {
+        return getSpeckleWindowSize_0(nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::StereoMatcher::compute(Mat left, Mat right, Mat& disparity)
+    //
+
+    /**
+     * Computes disparity map for the specified stereo pair
+     *
+     *     @param left Left 8-bit single-channel image.
+     *     @param right Right image of the same size and the same type as the left one.
+     *     @param disparity Output disparity map. It has the same size as the input images. Some algorithms,
+     *     like StereoBM or StereoSGBM compute 16-bit fixed-point disparity map (where each disparity value
+     *     has 4 fractional bits), whereas other algorithms output 32-bit floating-point disparity map.
+     */
+    public void compute(Mat left, Mat right, Mat disparity) {
+        compute_0(nativeObj, left.nativeObj, right.nativeObj, disparity.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::StereoMatcher::setBlockSize(int blockSize)
+    //
+
+    public void setBlockSize(int blockSize) {
+        setBlockSize_0(nativeObj, blockSize);
+    }
+
+
+    //
+    // C++:  void cv::StereoMatcher::setDisp12MaxDiff(int disp12MaxDiff)
+    //
+
+    public void setDisp12MaxDiff(int disp12MaxDiff) {
+        setDisp12MaxDiff_0(nativeObj, disp12MaxDiff);
+    }
+
+
+    //
+    // C++:  void cv::StereoMatcher::setMinDisparity(int minDisparity)
+    //
+
+    public void setMinDisparity(int minDisparity) {
+        setMinDisparity_0(nativeObj, minDisparity);
+    }
+
+
+    //
+    // C++:  void cv::StereoMatcher::setNumDisparities(int numDisparities)
+    //
+
+    public void setNumDisparities(int numDisparities) {
+        setNumDisparities_0(nativeObj, numDisparities);
+    }
+
+
+    //
+    // C++:  void cv::StereoMatcher::setSpeckleRange(int speckleRange)
+    //
+
+    public void setSpeckleRange(int speckleRange) {
+        setSpeckleRange_0(nativeObj, speckleRange);
+    }
+
+
+    //
+    // C++:  void cv::StereoMatcher::setSpeckleWindowSize(int speckleWindowSize)
+    //
+
+    public void setSpeckleWindowSize(int speckleWindowSize) {
+        setSpeckleWindowSize_0(nativeObj, speckleWindowSize);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++:  int cv::StereoMatcher::getBlockSize()
+    private static native int getBlockSize_0(long nativeObj);
+
+    // C++:  int cv::StereoMatcher::getDisp12MaxDiff()
+    private static native int getDisp12MaxDiff_0(long nativeObj);
+
+    // C++:  int cv::StereoMatcher::getMinDisparity()
+    private static native int getMinDisparity_0(long nativeObj);
+
+    // C++:  int cv::StereoMatcher::getNumDisparities()
+    private static native int getNumDisparities_0(long nativeObj);
+
+    // C++:  int cv::StereoMatcher::getSpeckleRange()
+    private static native int getSpeckleRange_0(long nativeObj);
+
+    // C++:  int cv::StereoMatcher::getSpeckleWindowSize()
+    private static native int getSpeckleWindowSize_0(long nativeObj);
+
+    // C++:  void cv::StereoMatcher::compute(Mat left, Mat right, Mat& disparity)
+    private static native void compute_0(long nativeObj, long left_nativeObj, long right_nativeObj, long disparity_nativeObj);
+
+    // C++:  void cv::StereoMatcher::setBlockSize(int blockSize)
+    private static native void setBlockSize_0(long nativeObj, int blockSize);
+
+    // C++:  void cv::StereoMatcher::setDisp12MaxDiff(int disp12MaxDiff)
+    private static native void setDisp12MaxDiff_0(long nativeObj, int disp12MaxDiff);
+
+    // C++:  void cv::StereoMatcher::setMinDisparity(int minDisparity)
+    private static native void setMinDisparity_0(long nativeObj, int minDisparity);
+
+    // C++:  void cv::StereoMatcher::setNumDisparities(int numDisparities)
+    private static native void setNumDisparities_0(long nativeObj, int numDisparities);
+
+    // C++:  void cv::StereoMatcher::setSpeckleRange(int speckleRange)
+    private static native void setSpeckleRange_0(long nativeObj, int speckleRange);
+
+    // C++:  void cv::StereoMatcher::setSpeckleWindowSize(int speckleWindowSize)
+    private static native void setSpeckleWindowSize_0(long nativeObj, int speckleWindowSize);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/calib3d/StereoSGBM.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/calib3d/StereoSGBM.java	(date 1605830247252)
+++ openCVLibrary3411/src/main/java/org/opencv/calib3d/StereoSGBM.java	(date 1605830247252)
@@ -0,0 +1,657 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.calib3d;
+
+import org.opencv.calib3d.StereoMatcher;
+import org.opencv.calib3d.StereoSGBM;
+
+// C++: class StereoSGBM
+/**
+ * The class implements the modified H. Hirschmuller algorithm CITE: HH08 that differs from the original
+ * one as follows:
+ *
+ * <ul>
+ *   <li>
+ *    By default, the algorithm is single-pass, which means that you consider only 5 directions
+ * instead of 8. Set mode=StereoSGBM::MODE_HH in createStereoSGBM to run the full variant of the
+ * algorithm but beware that it may consume a lot of memory.
+ *   </li>
+ *   <li>
+ *    The algorithm matches blocks, not individual pixels. Though, setting blockSize=1 reduces the
+ * blocks to single pixels.
+ *   </li>
+ *   <li>
+ *    Mutual information cost function is not implemented. Instead, a simpler Birchfield-Tomasi
+ * sub-pixel metric from CITE: BT98 is used. Though, the color images are supported as well.
+ *   </li>
+ *   <li>
+ *    Some pre- and post- processing steps from K. Konolige algorithm StereoBM are included, for
+ * example: pre-filtering (StereoBM::PREFILTER_XSOBEL type) and post-filtering (uniqueness
+ * check, quadratic interpolation and speckle filtering).
+ *   </li>
+ * </ul>
+ *
+ * <b>Note:</b>
+ * <ul>
+ *   <li>
+ *       (Python) An example illustrating the use of the StereoSGBM matching algorithm can be found
+ *         at opencv_source_code/samples/python/stereo_match.py
+ *   </li>
+ * </ul>
+ */
+public class StereoSGBM extends StereoMatcher {
+
+    protected StereoSGBM(long addr) { super(addr); }
+
+    // internal usage only
+    public static StereoSGBM __fromPtr__(long addr) { return new StereoSGBM(addr); }
+
+    // C++: enum <unnamed>
+    public static final int
+            MODE_SGBM = 0,
+            MODE_HH = 1,
+            MODE_SGBM_3WAY = 2,
+            MODE_HH4 = 3;
+
+
+    //
+    // C++: static Ptr_StereoSGBM cv::StereoSGBM::create(int minDisparity = 0, int numDisparities = 16, int blockSize = 3, int P1 = 0, int P2 = 0, int disp12MaxDiff = 0, int preFilterCap = 0, int uniquenessRatio = 0, int speckleWindowSize = 0, int speckleRange = 0, int mode = StereoSGBM::MODE_SGBM)
+    //
+
+    /**
+     * Creates StereoSGBM object
+     *
+     *     @param minDisparity Minimum possible disparity value. Normally, it is zero but sometimes
+     *     rectification algorithms can shift images, so this parameter needs to be adjusted accordingly.
+     *     @param numDisparities Maximum disparity minus minimum disparity. The value is always greater than
+     *     zero. In the current implementation, this parameter must be divisible by 16.
+     *     @param blockSize Matched block size. It must be an odd number &gt;=1 . Normally, it should be
+     *     somewhere in the 3..11 range.
+     *     @param P1 The first parameter controlling the disparity smoothness. See below.
+     *     @param P2 The second parameter controlling the disparity smoothness. The larger the values are,
+     *     the smoother the disparity is. P1 is the penalty on the disparity change by plus or minus 1
+     *     between neighbor pixels. P2 is the penalty on the disparity change by more than 1 between neighbor
+     *     pixels. The algorithm requires P2 &gt; P1 . See stereo_match.cpp sample where some reasonably good
+     *     P1 and P2 values are shown (like 8\*number_of_image_channels\*blockSize\*blockSize and
+     *     32\*number_of_image_channels\*blockSize\*blockSize , respectively).
+     *     @param disp12MaxDiff Maximum allowed difference (in integer pixel units) in the left-right
+     *     disparity check. Set it to a non-positive value to disable the check.
+     *     @param preFilterCap Truncation value for the prefiltered image pixels. The algorithm first
+     *     computes x-derivative at each pixel and clips its value by [-preFilterCap, preFilterCap] interval.
+     *     The result values are passed to the Birchfield-Tomasi pixel cost function.
+     *     @param uniquenessRatio Margin in percentage by which the best (minimum) computed cost function
+     *     value should "win" the second best value to consider the found match correct. Normally, a value
+     *     within the 5-15 range is good enough.
+     *     @param speckleWindowSize Maximum size of smooth disparity regions to consider their noise speckles
+     *     and invalidate. Set it to 0 to disable speckle filtering. Otherwise, set it somewhere in the
+     *     50-200 range.
+     *     @param speckleRange Maximum disparity variation within each connected component. If you do speckle
+     *     filtering, set the parameter to a positive value, it will be implicitly multiplied by 16.
+     *     Normally, 1 or 2 is good enough.
+     *     @param mode Set it to StereoSGBM::MODE_HH to run the full-scale two-pass dynamic programming
+     *     algorithm. It will consume O(W\*H\*numDisparities) bytes, which is large for 640x480 stereo and
+     *     huge for HD-size pictures. By default, it is set to false .
+     *
+     *     The first constructor initializes StereoSGBM with all the default parameters. So, you only have to
+     *     set StereoSGBM::numDisparities at minimum. The second constructor enables you to set each parameter
+     *     to a custom value.
+     * @return automatically generated
+     */
+    public static StereoSGBM create(int minDisparity, int numDisparities, int blockSize, int P1, int P2, int disp12MaxDiff, int preFilterCap, int uniquenessRatio, int speckleWindowSize, int speckleRange, int mode) {
+        return StereoSGBM.__fromPtr__(create_0(minDisparity, numDisparities, blockSize, P1, P2, disp12MaxDiff, preFilterCap, uniquenessRatio, speckleWindowSize, speckleRange, mode));
+    }
+
+    /**
+     * Creates StereoSGBM object
+     *
+     *     @param minDisparity Minimum possible disparity value. Normally, it is zero but sometimes
+     *     rectification algorithms can shift images, so this parameter needs to be adjusted accordingly.
+     *     @param numDisparities Maximum disparity minus minimum disparity. The value is always greater than
+     *     zero. In the current implementation, this parameter must be divisible by 16.
+     *     @param blockSize Matched block size. It must be an odd number &gt;=1 . Normally, it should be
+     *     somewhere in the 3..11 range.
+     *     @param P1 The first parameter controlling the disparity smoothness. See below.
+     *     @param P2 The second parameter controlling the disparity smoothness. The larger the values are,
+     *     the smoother the disparity is. P1 is the penalty on the disparity change by plus or minus 1
+     *     between neighbor pixels. P2 is the penalty on the disparity change by more than 1 between neighbor
+     *     pixels. The algorithm requires P2 &gt; P1 . See stereo_match.cpp sample where some reasonably good
+     *     P1 and P2 values are shown (like 8\*number_of_image_channels\*blockSize\*blockSize and
+     *     32\*number_of_image_channels\*blockSize\*blockSize , respectively).
+     *     @param disp12MaxDiff Maximum allowed difference (in integer pixel units) in the left-right
+     *     disparity check. Set it to a non-positive value to disable the check.
+     *     @param preFilterCap Truncation value for the prefiltered image pixels. The algorithm first
+     *     computes x-derivative at each pixel and clips its value by [-preFilterCap, preFilterCap] interval.
+     *     The result values are passed to the Birchfield-Tomasi pixel cost function.
+     *     @param uniquenessRatio Margin in percentage by which the best (minimum) computed cost function
+     *     value should "win" the second best value to consider the found match correct. Normally, a value
+     *     within the 5-15 range is good enough.
+     *     @param speckleWindowSize Maximum size of smooth disparity regions to consider their noise speckles
+     *     and invalidate. Set it to 0 to disable speckle filtering. Otherwise, set it somewhere in the
+     *     50-200 range.
+     *     @param speckleRange Maximum disparity variation within each connected component. If you do speckle
+     *     filtering, set the parameter to a positive value, it will be implicitly multiplied by 16.
+     *     Normally, 1 or 2 is good enough.
+     *     algorithm. It will consume O(W\*H\*numDisparities) bytes, which is large for 640x480 stereo and
+     *     huge for HD-size pictures. By default, it is set to false .
+     *
+     *     The first constructor initializes StereoSGBM with all the default parameters. So, you only have to
+     *     set StereoSGBM::numDisparities at minimum. The second constructor enables you to set each parameter
+     *     to a custom value.
+     * @return automatically generated
+     */
+    public static StereoSGBM create(int minDisparity, int numDisparities, int blockSize, int P1, int P2, int disp12MaxDiff, int preFilterCap, int uniquenessRatio, int speckleWindowSize, int speckleRange) {
+        return StereoSGBM.__fromPtr__(create_1(minDisparity, numDisparities, blockSize, P1, P2, disp12MaxDiff, preFilterCap, uniquenessRatio, speckleWindowSize, speckleRange));
+    }
+
+    /**
+     * Creates StereoSGBM object
+     *
+     *     @param minDisparity Minimum possible disparity value. Normally, it is zero but sometimes
+     *     rectification algorithms can shift images, so this parameter needs to be adjusted accordingly.
+     *     @param numDisparities Maximum disparity minus minimum disparity. The value is always greater than
+     *     zero. In the current implementation, this parameter must be divisible by 16.
+     *     @param blockSize Matched block size. It must be an odd number &gt;=1 . Normally, it should be
+     *     somewhere in the 3..11 range.
+     *     @param P1 The first parameter controlling the disparity smoothness. See below.
+     *     @param P2 The second parameter controlling the disparity smoothness. The larger the values are,
+     *     the smoother the disparity is. P1 is the penalty on the disparity change by plus or minus 1
+     *     between neighbor pixels. P2 is the penalty on the disparity change by more than 1 between neighbor
+     *     pixels. The algorithm requires P2 &gt; P1 . See stereo_match.cpp sample where some reasonably good
+     *     P1 and P2 values are shown (like 8\*number_of_image_channels\*blockSize\*blockSize and
+     *     32\*number_of_image_channels\*blockSize\*blockSize , respectively).
+     *     @param disp12MaxDiff Maximum allowed difference (in integer pixel units) in the left-right
+     *     disparity check. Set it to a non-positive value to disable the check.
+     *     @param preFilterCap Truncation value for the prefiltered image pixels. The algorithm first
+     *     computes x-derivative at each pixel and clips its value by [-preFilterCap, preFilterCap] interval.
+     *     The result values are passed to the Birchfield-Tomasi pixel cost function.
+     *     @param uniquenessRatio Margin in percentage by which the best (minimum) computed cost function
+     *     value should "win" the second best value to consider the found match correct. Normally, a value
+     *     within the 5-15 range is good enough.
+     *     @param speckleWindowSize Maximum size of smooth disparity regions to consider their noise speckles
+     *     and invalidate. Set it to 0 to disable speckle filtering. Otherwise, set it somewhere in the
+     *     50-200 range.
+     *     filtering, set the parameter to a positive value, it will be implicitly multiplied by 16.
+     *     Normally, 1 or 2 is good enough.
+     *     algorithm. It will consume O(W\*H\*numDisparities) bytes, which is large for 640x480 stereo and
+     *     huge for HD-size pictures. By default, it is set to false .
+     *
+     *     The first constructor initializes StereoSGBM with all the default parameters. So, you only have to
+     *     set StereoSGBM::numDisparities at minimum. The second constructor enables you to set each parameter
+     *     to a custom value.
+     * @return automatically generated
+     */
+    public static StereoSGBM create(int minDisparity, int numDisparities, int blockSize, int P1, int P2, int disp12MaxDiff, int preFilterCap, int uniquenessRatio, int speckleWindowSize) {
+        return StereoSGBM.__fromPtr__(create_2(minDisparity, numDisparities, blockSize, P1, P2, disp12MaxDiff, preFilterCap, uniquenessRatio, speckleWindowSize));
+    }
+
+    /**
+     * Creates StereoSGBM object
+     *
+     *     @param minDisparity Minimum possible disparity value. Normally, it is zero but sometimes
+     *     rectification algorithms can shift images, so this parameter needs to be adjusted accordingly.
+     *     @param numDisparities Maximum disparity minus minimum disparity. The value is always greater than
+     *     zero. In the current implementation, this parameter must be divisible by 16.
+     *     @param blockSize Matched block size. It must be an odd number &gt;=1 . Normally, it should be
+     *     somewhere in the 3..11 range.
+     *     @param P1 The first parameter controlling the disparity smoothness. See below.
+     *     @param P2 The second parameter controlling the disparity smoothness. The larger the values are,
+     *     the smoother the disparity is. P1 is the penalty on the disparity change by plus or minus 1
+     *     between neighbor pixels. P2 is the penalty on the disparity change by more than 1 between neighbor
+     *     pixels. The algorithm requires P2 &gt; P1 . See stereo_match.cpp sample where some reasonably good
+     *     P1 and P2 values are shown (like 8\*number_of_image_channels\*blockSize\*blockSize and
+     *     32\*number_of_image_channels\*blockSize\*blockSize , respectively).
+     *     @param disp12MaxDiff Maximum allowed difference (in integer pixel units) in the left-right
+     *     disparity check. Set it to a non-positive value to disable the check.
+     *     @param preFilterCap Truncation value for the prefiltered image pixels. The algorithm first
+     *     computes x-derivative at each pixel and clips its value by [-preFilterCap, preFilterCap] interval.
+     *     The result values are passed to the Birchfield-Tomasi pixel cost function.
+     *     @param uniquenessRatio Margin in percentage by which the best (minimum) computed cost function
+     *     value should "win" the second best value to consider the found match correct. Normally, a value
+     *     within the 5-15 range is good enough.
+     *     and invalidate. Set it to 0 to disable speckle filtering. Otherwise, set it somewhere in the
+     *     50-200 range.
+     *     filtering, set the parameter to a positive value, it will be implicitly multiplied by 16.
+     *     Normally, 1 or 2 is good enough.
+     *     algorithm. It will consume O(W\*H\*numDisparities) bytes, which is large for 640x480 stereo and
+     *     huge for HD-size pictures. By default, it is set to false .
+     *
+     *     The first constructor initializes StereoSGBM with all the default parameters. So, you only have to
+     *     set StereoSGBM::numDisparities at minimum. The second constructor enables you to set each parameter
+     *     to a custom value.
+     * @return automatically generated
+     */
+    public static StereoSGBM create(int minDisparity, int numDisparities, int blockSize, int P1, int P2, int disp12MaxDiff, int preFilterCap, int uniquenessRatio) {
+        return StereoSGBM.__fromPtr__(create_3(minDisparity, numDisparities, blockSize, P1, P2, disp12MaxDiff, preFilterCap, uniquenessRatio));
+    }
+
+    /**
+     * Creates StereoSGBM object
+     *
+     *     @param minDisparity Minimum possible disparity value. Normally, it is zero but sometimes
+     *     rectification algorithms can shift images, so this parameter needs to be adjusted accordingly.
+     *     @param numDisparities Maximum disparity minus minimum disparity. The value is always greater than
+     *     zero. In the current implementation, this parameter must be divisible by 16.
+     *     @param blockSize Matched block size. It must be an odd number &gt;=1 . Normally, it should be
+     *     somewhere in the 3..11 range.
+     *     @param P1 The first parameter controlling the disparity smoothness. See below.
+     *     @param P2 The second parameter controlling the disparity smoothness. The larger the values are,
+     *     the smoother the disparity is. P1 is the penalty on the disparity change by plus or minus 1
+     *     between neighbor pixels. P2 is the penalty on the disparity change by more than 1 between neighbor
+     *     pixels. The algorithm requires P2 &gt; P1 . See stereo_match.cpp sample where some reasonably good
+     *     P1 and P2 values are shown (like 8\*number_of_image_channels\*blockSize\*blockSize and
+     *     32\*number_of_image_channels\*blockSize\*blockSize , respectively).
+     *     @param disp12MaxDiff Maximum allowed difference (in integer pixel units) in the left-right
+     *     disparity check. Set it to a non-positive value to disable the check.
+     *     @param preFilterCap Truncation value for the prefiltered image pixels. The algorithm first
+     *     computes x-derivative at each pixel and clips its value by [-preFilterCap, preFilterCap] interval.
+     *     The result values are passed to the Birchfield-Tomasi pixel cost function.
+     *     value should "win" the second best value to consider the found match correct. Normally, a value
+     *     within the 5-15 range is good enough.
+     *     and invalidate. Set it to 0 to disable speckle filtering. Otherwise, set it somewhere in the
+     *     50-200 range.
+     *     filtering, set the parameter to a positive value, it will be implicitly multiplied by 16.
+     *     Normally, 1 or 2 is good enough.
+     *     algorithm. It will consume O(W\*H\*numDisparities) bytes, which is large for 640x480 stereo and
+     *     huge for HD-size pictures. By default, it is set to false .
+     *
+     *     The first constructor initializes StereoSGBM with all the default parameters. So, you only have to
+     *     set StereoSGBM::numDisparities at minimum. The second constructor enables you to set each parameter
+     *     to a custom value.
+     * @return automatically generated
+     */
+    public static StereoSGBM create(int minDisparity, int numDisparities, int blockSize, int P1, int P2, int disp12MaxDiff, int preFilterCap) {
+        return StereoSGBM.__fromPtr__(create_4(minDisparity, numDisparities, blockSize, P1, P2, disp12MaxDiff, preFilterCap));
+    }
+
+    /**
+     * Creates StereoSGBM object
+     *
+     *     @param minDisparity Minimum possible disparity value. Normally, it is zero but sometimes
+     *     rectification algorithms can shift images, so this parameter needs to be adjusted accordingly.
+     *     @param numDisparities Maximum disparity minus minimum disparity. The value is always greater than
+     *     zero. In the current implementation, this parameter must be divisible by 16.
+     *     @param blockSize Matched block size. It must be an odd number &gt;=1 . Normally, it should be
+     *     somewhere in the 3..11 range.
+     *     @param P1 The first parameter controlling the disparity smoothness. See below.
+     *     @param P2 The second parameter controlling the disparity smoothness. The larger the values are,
+     *     the smoother the disparity is. P1 is the penalty on the disparity change by plus or minus 1
+     *     between neighbor pixels. P2 is the penalty on the disparity change by more than 1 between neighbor
+     *     pixels. The algorithm requires P2 &gt; P1 . See stereo_match.cpp sample where some reasonably good
+     *     P1 and P2 values are shown (like 8\*number_of_image_channels\*blockSize\*blockSize and
+     *     32\*number_of_image_channels\*blockSize\*blockSize , respectively).
+     *     @param disp12MaxDiff Maximum allowed difference (in integer pixel units) in the left-right
+     *     disparity check. Set it to a non-positive value to disable the check.
+     *     computes x-derivative at each pixel and clips its value by [-preFilterCap, preFilterCap] interval.
+     *     The result values are passed to the Birchfield-Tomasi pixel cost function.
+     *     value should "win" the second best value to consider the found match correct. Normally, a value
+     *     within the 5-15 range is good enough.
+     *     and invalidate. Set it to 0 to disable speckle filtering. Otherwise, set it somewhere in the
+     *     50-200 range.
+     *     filtering, set the parameter to a positive value, it will be implicitly multiplied by 16.
+     *     Normally, 1 or 2 is good enough.
+     *     algorithm. It will consume O(W\*H\*numDisparities) bytes, which is large for 640x480 stereo and
+     *     huge for HD-size pictures. By default, it is set to false .
+     *
+     *     The first constructor initializes StereoSGBM with all the default parameters. So, you only have to
+     *     set StereoSGBM::numDisparities at minimum. The second constructor enables you to set each parameter
+     *     to a custom value.
+     * @return automatically generated
+     */
+    public static StereoSGBM create(int minDisparity, int numDisparities, int blockSize, int P1, int P2, int disp12MaxDiff) {
+        return StereoSGBM.__fromPtr__(create_5(minDisparity, numDisparities, blockSize, P1, P2, disp12MaxDiff));
+    }
+
+    /**
+     * Creates StereoSGBM object
+     *
+     *     @param minDisparity Minimum possible disparity value. Normally, it is zero but sometimes
+     *     rectification algorithms can shift images, so this parameter needs to be adjusted accordingly.
+     *     @param numDisparities Maximum disparity minus minimum disparity. The value is always greater than
+     *     zero. In the current implementation, this parameter must be divisible by 16.
+     *     @param blockSize Matched block size. It must be an odd number &gt;=1 . Normally, it should be
+     *     somewhere in the 3..11 range.
+     *     @param P1 The first parameter controlling the disparity smoothness. See below.
+     *     @param P2 The second parameter controlling the disparity smoothness. The larger the values are,
+     *     the smoother the disparity is. P1 is the penalty on the disparity change by plus or minus 1
+     *     between neighbor pixels. P2 is the penalty on the disparity change by more than 1 between neighbor
+     *     pixels. The algorithm requires P2 &gt; P1 . See stereo_match.cpp sample where some reasonably good
+     *     P1 and P2 values are shown (like 8\*number_of_image_channels\*blockSize\*blockSize and
+     *     32\*number_of_image_channels\*blockSize\*blockSize , respectively).
+     *     disparity check. Set it to a non-positive value to disable the check.
+     *     computes x-derivative at each pixel and clips its value by [-preFilterCap, preFilterCap] interval.
+     *     The result values are passed to the Birchfield-Tomasi pixel cost function.
+     *     value should "win" the second best value to consider the found match correct. Normally, a value
+     *     within the 5-15 range is good enough.
+     *     and invalidate. Set it to 0 to disable speckle filtering. Otherwise, set it somewhere in the
+     *     50-200 range.
+     *     filtering, set the parameter to a positive value, it will be implicitly multiplied by 16.
+     *     Normally, 1 or 2 is good enough.
+     *     algorithm. It will consume O(W\*H\*numDisparities) bytes, which is large for 640x480 stereo and
+     *     huge for HD-size pictures. By default, it is set to false .
+     *
+     *     The first constructor initializes StereoSGBM with all the default parameters. So, you only have to
+     *     set StereoSGBM::numDisparities at minimum. The second constructor enables you to set each parameter
+     *     to a custom value.
+     * @return automatically generated
+     */
+    public static StereoSGBM create(int minDisparity, int numDisparities, int blockSize, int P1, int P2) {
+        return StereoSGBM.__fromPtr__(create_6(minDisparity, numDisparities, blockSize, P1, P2));
+    }
+
+    /**
+     * Creates StereoSGBM object
+     *
+     *     @param minDisparity Minimum possible disparity value. Normally, it is zero but sometimes
+     *     rectification algorithms can shift images, so this parameter needs to be adjusted accordingly.
+     *     @param numDisparities Maximum disparity minus minimum disparity. The value is always greater than
+     *     zero. In the current implementation, this parameter must be divisible by 16.
+     *     @param blockSize Matched block size. It must be an odd number &gt;=1 . Normally, it should be
+     *     somewhere in the 3..11 range.
+     *     @param P1 The first parameter controlling the disparity smoothness. See below.
+     *     the smoother the disparity is. P1 is the penalty on the disparity change by plus or minus 1
+     *     between neighbor pixels. P2 is the penalty on the disparity change by more than 1 between neighbor
+     *     pixels. The algorithm requires P2 &gt; P1 . See stereo_match.cpp sample where some reasonably good
+     *     P1 and P2 values are shown (like 8\*number_of_image_channels\*blockSize\*blockSize and
+     *     32\*number_of_image_channels\*blockSize\*blockSize , respectively).
+     *     disparity check. Set it to a non-positive value to disable the check.
+     *     computes x-derivative at each pixel and clips its value by [-preFilterCap, preFilterCap] interval.
+     *     The result values are passed to the Birchfield-Tomasi pixel cost function.
+     *     value should "win" the second best value to consider the found match correct. Normally, a value
+     *     within the 5-15 range is good enough.
+     *     and invalidate. Set it to 0 to disable speckle filtering. Otherwise, set it somewhere in the
+     *     50-200 range.
+     *     filtering, set the parameter to a positive value, it will be implicitly multiplied by 16.
+     *     Normally, 1 or 2 is good enough.
+     *     algorithm. It will consume O(W\*H\*numDisparities) bytes, which is large for 640x480 stereo and
+     *     huge for HD-size pictures. By default, it is set to false .
+     *
+     *     The first constructor initializes StereoSGBM with all the default parameters. So, you only have to
+     *     set StereoSGBM::numDisparities at minimum. The second constructor enables you to set each parameter
+     *     to a custom value.
+     * @return automatically generated
+     */
+    public static StereoSGBM create(int minDisparity, int numDisparities, int blockSize, int P1) {
+        return StereoSGBM.__fromPtr__(create_7(minDisparity, numDisparities, blockSize, P1));
+    }
+
+    /**
+     * Creates StereoSGBM object
+     *
+     *     @param minDisparity Minimum possible disparity value. Normally, it is zero but sometimes
+     *     rectification algorithms can shift images, so this parameter needs to be adjusted accordingly.
+     *     @param numDisparities Maximum disparity minus minimum disparity. The value is always greater than
+     *     zero. In the current implementation, this parameter must be divisible by 16.
+     *     @param blockSize Matched block size. It must be an odd number &gt;=1 . Normally, it should be
+     *     somewhere in the 3..11 range.
+     *     the smoother the disparity is. P1 is the penalty on the disparity change by plus or minus 1
+     *     between neighbor pixels. P2 is the penalty on the disparity change by more than 1 between neighbor
+     *     pixels. The algorithm requires P2 &gt; P1 . See stereo_match.cpp sample where some reasonably good
+     *     P1 and P2 values are shown (like 8\*number_of_image_channels\*blockSize\*blockSize and
+     *     32\*number_of_image_channels\*blockSize\*blockSize , respectively).
+     *     disparity check. Set it to a non-positive value to disable the check.
+     *     computes x-derivative at each pixel and clips its value by [-preFilterCap, preFilterCap] interval.
+     *     The result values are passed to the Birchfield-Tomasi pixel cost function.
+     *     value should "win" the second best value to consider the found match correct. Normally, a value
+     *     within the 5-15 range is good enough.
+     *     and invalidate. Set it to 0 to disable speckle filtering. Otherwise, set it somewhere in the
+     *     50-200 range.
+     *     filtering, set the parameter to a positive value, it will be implicitly multiplied by 16.
+     *     Normally, 1 or 2 is good enough.
+     *     algorithm. It will consume O(W\*H\*numDisparities) bytes, which is large for 640x480 stereo and
+     *     huge for HD-size pictures. By default, it is set to false .
+     *
+     *     The first constructor initializes StereoSGBM with all the default parameters. So, you only have to
+     *     set StereoSGBM::numDisparities at minimum. The second constructor enables you to set each parameter
+     *     to a custom value.
+     * @return automatically generated
+     */
+    public static StereoSGBM create(int minDisparity, int numDisparities, int blockSize) {
+        return StereoSGBM.__fromPtr__(create_8(minDisparity, numDisparities, blockSize));
+    }
+
+    /**
+     * Creates StereoSGBM object
+     *
+     *     @param minDisparity Minimum possible disparity value. Normally, it is zero but sometimes
+     *     rectification algorithms can shift images, so this parameter needs to be adjusted accordingly.
+     *     @param numDisparities Maximum disparity minus minimum disparity. The value is always greater than
+     *     zero. In the current implementation, this parameter must be divisible by 16.
+     *     somewhere in the 3..11 range.
+     *     the smoother the disparity is. P1 is the penalty on the disparity change by plus or minus 1
+     *     between neighbor pixels. P2 is the penalty on the disparity change by more than 1 between neighbor
+     *     pixels. The algorithm requires P2 &gt; P1 . See stereo_match.cpp sample where some reasonably good
+     *     P1 and P2 values are shown (like 8\*number_of_image_channels\*blockSize\*blockSize and
+     *     32\*number_of_image_channels\*blockSize\*blockSize , respectively).
+     *     disparity check. Set it to a non-positive value to disable the check.
+     *     computes x-derivative at each pixel and clips its value by [-preFilterCap, preFilterCap] interval.
+     *     The result values are passed to the Birchfield-Tomasi pixel cost function.
+     *     value should "win" the second best value to consider the found match correct. Normally, a value
+     *     within the 5-15 range is good enough.
+     *     and invalidate. Set it to 0 to disable speckle filtering. Otherwise, set it somewhere in the
+     *     50-200 range.
+     *     filtering, set the parameter to a positive value, it will be implicitly multiplied by 16.
+     *     Normally, 1 or 2 is good enough.
+     *     algorithm. It will consume O(W\*H\*numDisparities) bytes, which is large for 640x480 stereo and
+     *     huge for HD-size pictures. By default, it is set to false .
+     *
+     *     The first constructor initializes StereoSGBM with all the default parameters. So, you only have to
+     *     set StereoSGBM::numDisparities at minimum. The second constructor enables you to set each parameter
+     *     to a custom value.
+     * @return automatically generated
+     */
+    public static StereoSGBM create(int minDisparity, int numDisparities) {
+        return StereoSGBM.__fromPtr__(create_9(minDisparity, numDisparities));
+    }
+
+    /**
+     * Creates StereoSGBM object
+     *
+     *     @param minDisparity Minimum possible disparity value. Normally, it is zero but sometimes
+     *     rectification algorithms can shift images, so this parameter needs to be adjusted accordingly.
+     *     zero. In the current implementation, this parameter must be divisible by 16.
+     *     somewhere in the 3..11 range.
+     *     the smoother the disparity is. P1 is the penalty on the disparity change by plus or minus 1
+     *     between neighbor pixels. P2 is the penalty on the disparity change by more than 1 between neighbor
+     *     pixels. The algorithm requires P2 &gt; P1 . See stereo_match.cpp sample where some reasonably good
+     *     P1 and P2 values are shown (like 8\*number_of_image_channels\*blockSize\*blockSize and
+     *     32\*number_of_image_channels\*blockSize\*blockSize , respectively).
+     *     disparity check. Set it to a non-positive value to disable the check.
+     *     computes x-derivative at each pixel and clips its value by [-preFilterCap, preFilterCap] interval.
+     *     The result values are passed to the Birchfield-Tomasi pixel cost function.
+     *     value should "win" the second best value to consider the found match correct. Normally, a value
+     *     within the 5-15 range is good enough.
+     *     and invalidate. Set it to 0 to disable speckle filtering. Otherwise, set it somewhere in the
+     *     50-200 range.
+     *     filtering, set the parameter to a positive value, it will be implicitly multiplied by 16.
+     *     Normally, 1 or 2 is good enough.
+     *     algorithm. It will consume O(W\*H\*numDisparities) bytes, which is large for 640x480 stereo and
+     *     huge for HD-size pictures. By default, it is set to false .
+     *
+     *     The first constructor initializes StereoSGBM with all the default parameters. So, you only have to
+     *     set StereoSGBM::numDisparities at minimum. The second constructor enables you to set each parameter
+     *     to a custom value.
+     * @return automatically generated
+     */
+    public static StereoSGBM create(int minDisparity) {
+        return StereoSGBM.__fromPtr__(create_10(minDisparity));
+    }
+
+    /**
+     * Creates StereoSGBM object
+     *
+     *     rectification algorithms can shift images, so this parameter needs to be adjusted accordingly.
+     *     zero. In the current implementation, this parameter must be divisible by 16.
+     *     somewhere in the 3..11 range.
+     *     the smoother the disparity is. P1 is the penalty on the disparity change by plus or minus 1
+     *     between neighbor pixels. P2 is the penalty on the disparity change by more than 1 between neighbor
+     *     pixels. The algorithm requires P2 &gt; P1 . See stereo_match.cpp sample where some reasonably good
+     *     P1 and P2 values are shown (like 8\*number_of_image_channels\*blockSize\*blockSize and
+     *     32\*number_of_image_channels\*blockSize\*blockSize , respectively).
+     *     disparity check. Set it to a non-positive value to disable the check.
+     *     computes x-derivative at each pixel and clips its value by [-preFilterCap, preFilterCap] interval.
+     *     The result values are passed to the Birchfield-Tomasi pixel cost function.
+     *     value should "win" the second best value to consider the found match correct. Normally, a value
+     *     within the 5-15 range is good enough.
+     *     and invalidate. Set it to 0 to disable speckle filtering. Otherwise, set it somewhere in the
+     *     50-200 range.
+     *     filtering, set the parameter to a positive value, it will be implicitly multiplied by 16.
+     *     Normally, 1 or 2 is good enough.
+     *     algorithm. It will consume O(W\*H\*numDisparities) bytes, which is large for 640x480 stereo and
+     *     huge for HD-size pictures. By default, it is set to false .
+     *
+     *     The first constructor initializes StereoSGBM with all the default parameters. So, you only have to
+     *     set StereoSGBM::numDisparities at minimum. The second constructor enables you to set each parameter
+     *     to a custom value.
+     * @return automatically generated
+     */
+    public static StereoSGBM create() {
+        return StereoSGBM.__fromPtr__(create_11());
+    }
+
+
+    //
+    // C++:  int cv::StereoSGBM::getMode()
+    //
+
+    public int getMode() {
+        return getMode_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::StereoSGBM::getP1()
+    //
+
+    public int getP1() {
+        return getP1_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::StereoSGBM::getP2()
+    //
+
+    public int getP2() {
+        return getP2_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::StereoSGBM::getPreFilterCap()
+    //
+
+    public int getPreFilterCap() {
+        return getPreFilterCap_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::StereoSGBM::getUniquenessRatio()
+    //
+
+    public int getUniquenessRatio() {
+        return getUniquenessRatio_0(nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::StereoSGBM::setMode(int mode)
+    //
+
+    public void setMode(int mode) {
+        setMode_0(nativeObj, mode);
+    }
+
+
+    //
+    // C++:  void cv::StereoSGBM::setP1(int P1)
+    //
+
+    public void setP1(int P1) {
+        setP1_0(nativeObj, P1);
+    }
+
+
+    //
+    // C++:  void cv::StereoSGBM::setP2(int P2)
+    //
+
+    public void setP2(int P2) {
+        setP2_0(nativeObj, P2);
+    }
+
+
+    //
+    // C++:  void cv::StereoSGBM::setPreFilterCap(int preFilterCap)
+    //
+
+    public void setPreFilterCap(int preFilterCap) {
+        setPreFilterCap_0(nativeObj, preFilterCap);
+    }
+
+
+    //
+    // C++:  void cv::StereoSGBM::setUniquenessRatio(int uniquenessRatio)
+    //
+
+    public void setUniquenessRatio(int uniquenessRatio) {
+        setUniquenessRatio_0(nativeObj, uniquenessRatio);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++: static Ptr_StereoSGBM cv::StereoSGBM::create(int minDisparity = 0, int numDisparities = 16, int blockSize = 3, int P1 = 0, int P2 = 0, int disp12MaxDiff = 0, int preFilterCap = 0, int uniquenessRatio = 0, int speckleWindowSize = 0, int speckleRange = 0, int mode = StereoSGBM::MODE_SGBM)
+    private static native long create_0(int minDisparity, int numDisparities, int blockSize, int P1, int P2, int disp12MaxDiff, int preFilterCap, int uniquenessRatio, int speckleWindowSize, int speckleRange, int mode);
+    private static native long create_1(int minDisparity, int numDisparities, int blockSize, int P1, int P2, int disp12MaxDiff, int preFilterCap, int uniquenessRatio, int speckleWindowSize, int speckleRange);
+    private static native long create_2(int minDisparity, int numDisparities, int blockSize, int P1, int P2, int disp12MaxDiff, int preFilterCap, int uniquenessRatio, int speckleWindowSize);
+    private static native long create_3(int minDisparity, int numDisparities, int blockSize, int P1, int P2, int disp12MaxDiff, int preFilterCap, int uniquenessRatio);
+    private static native long create_4(int minDisparity, int numDisparities, int blockSize, int P1, int P2, int disp12MaxDiff, int preFilterCap);
+    private static native long create_5(int minDisparity, int numDisparities, int blockSize, int P1, int P2, int disp12MaxDiff);
+    private static native long create_6(int minDisparity, int numDisparities, int blockSize, int P1, int P2);
+    private static native long create_7(int minDisparity, int numDisparities, int blockSize, int P1);
+    private static native long create_8(int minDisparity, int numDisparities, int blockSize);
+    private static native long create_9(int minDisparity, int numDisparities);
+    private static native long create_10(int minDisparity);
+    private static native long create_11();
+
+    // C++:  int cv::StereoSGBM::getMode()
+    private static native int getMode_0(long nativeObj);
+
+    // C++:  int cv::StereoSGBM::getP1()
+    private static native int getP1_0(long nativeObj);
+
+    // C++:  int cv::StereoSGBM::getP2()
+    private static native int getP2_0(long nativeObj);
+
+    // C++:  int cv::StereoSGBM::getPreFilterCap()
+    private static native int getPreFilterCap_0(long nativeObj);
+
+    // C++:  int cv::StereoSGBM::getUniquenessRatio()
+    private static native int getUniquenessRatio_0(long nativeObj);
+
+    // C++:  void cv::StereoSGBM::setMode(int mode)
+    private static native void setMode_0(long nativeObj, int mode);
+
+    // C++:  void cv::StereoSGBM::setP1(int P1)
+    private static native void setP1_0(long nativeObj, int P1);
+
+    // C++:  void cv::StereoSGBM::setP2(int P2)
+    private static native void setP2_0(long nativeObj, int P2);
+
+    // C++:  void cv::StereoSGBM::setPreFilterCap(int preFilterCap)
+    private static native void setPreFilterCap_0(long nativeObj, int preFilterCap);
+
+    // C++:  void cv::StereoSGBM::setUniquenessRatio(int uniquenessRatio)
+    private static native void setUniquenessRatio_0(long nativeObj, int uniquenessRatio);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/android/LoaderCallbackInterface.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/android/LoaderCallbackInterface.java	(date 1605830247168)
+++ openCVLibrary3411/src/main/java/org/opencv/android/LoaderCallbackInterface.java	(date 1605830247168)
@@ -0,0 +1,40 @@
+package org.opencv.android;
+
+/**
+ * Interface for callback object in case of asynchronous initialization of OpenCV.
+ */
+public interface LoaderCallbackInterface
+{
+    /**
+     * OpenCV initialization finished successfully.
+     */
+    static final int SUCCESS = 0;
+    /**
+     * Google Play Market cannot be invoked.
+     */
+    static final int MARKET_ERROR = 2;
+    /**
+     * OpenCV library installation has been canceled by the user.
+     */
+    static final int INSTALL_CANCELED = 3;
+    /**
+     * This version of OpenCV Manager Service is incompatible with the app. Possibly, a service update is required.
+     */
+    static final int INCOMPATIBLE_MANAGER_VERSION = 4;
+    /**
+     * OpenCV library initialization has failed.
+     */
+    static final int INIT_FAILED = 0xff;
+
+    /**
+     * Callback method, called after OpenCV library initialization.
+     * @param status status of initialization (see initialization status constants).
+     */
+    public void onManagerConnected(int status);
+
+    /**
+     * Callback method, called in case the package installation is needed.
+     * @param callback answer object with approve and cancel methods and the package description.
+     */
+    public void onPackageInstall(final int operation, InstallCallbackInterface callback);
+};
Index: openCVLibrary3411/src/main/java/org/opencv/android/OpenCVLoader.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/android/OpenCVLoader.java	(date 1605830247170)
+++ openCVLibrary3411/src/main/java/org/opencv/android/OpenCVLoader.java	(date 1605830247170)
@@ -0,0 +1,132 @@
+package org.opencv.android;
+
+import android.content.Context;
+
+/**
+ * Helper class provides common initialization methods for OpenCV library.
+ */
+public class OpenCVLoader
+{
+    /**
+     * OpenCV Library version 2.4.2.
+     */
+    public static final String OPENCV_VERSION_2_4_2 = "2.4.2";
+
+    /**
+     * OpenCV Library version 2.4.3.
+     */
+    public static final String OPENCV_VERSION_2_4_3 = "2.4.3";
+
+    /**
+     * OpenCV Library version 2.4.4.
+     */
+    public static final String OPENCV_VERSION_2_4_4 = "2.4.4";
+
+    /**
+     * OpenCV Library version 2.4.5.
+     */
+    public static final String OPENCV_VERSION_2_4_5 = "2.4.5";
+
+    /**
+     * OpenCV Library version 2.4.6.
+     */
+    public static final String OPENCV_VERSION_2_4_6 = "2.4.6";
+
+    /**
+     * OpenCV Library version 2.4.7.
+     */
+    public static final String OPENCV_VERSION_2_4_7 = "2.4.7";
+
+    /**
+     * OpenCV Library version 2.4.8.
+     */
+    public static final String OPENCV_VERSION_2_4_8 = "2.4.8";
+
+    /**
+     * OpenCV Library version 2.4.9.
+     */
+    public static final String OPENCV_VERSION_2_4_9 = "2.4.9";
+
+    /**
+     * OpenCV Library version 2.4.10.
+     */
+    public static final String OPENCV_VERSION_2_4_10 = "2.4.10";
+
+    /**
+     * OpenCV Library version 2.4.11.
+     */
+    public static final String OPENCV_VERSION_2_4_11 = "2.4.11";
+
+    /**
+     * OpenCV Library version 2.4.12.
+     */
+    public static final String OPENCV_VERSION_2_4_12 = "2.4.12";
+
+    /**
+     * OpenCV Library version 2.4.13.
+     */
+    public static final String OPENCV_VERSION_2_4_13 = "2.4.13";
+
+    /**
+     * OpenCV Library version 3.0.0.
+     */
+    public static final String OPENCV_VERSION_3_0_0 = "3.0.0";
+
+    /**
+     * OpenCV Library version 3.1.0.
+     */
+    public static final String OPENCV_VERSION_3_1_0 = "3.1.0";
+
+    /**
+     * OpenCV Library version 3.2.0.
+     */
+    public static final String OPENCV_VERSION_3_2_0 = "3.2.0";
+
+    /**
+     * OpenCV Library version 3.3.0.
+     */
+    public static final String OPENCV_VERSION_3_3_0 = "3.3.0";
+
+    /**
+     * OpenCV Library version 3.4.0.
+     */
+    public static final String OPENCV_VERSION_3_4_0 = "3.4.0";
+
+    /**
+     * Current OpenCV Library version
+     */
+    public static final String OPENCV_VERSION = "3.4.11";
+
+
+    /**
+     * Loads and initializes OpenCV library from current application package. Roughly, it's an analog of system.loadLibrary("opencv_java").
+     * @return Returns true is initialization of OpenCV was successful.
+     */
+    public static boolean initDebug()
+    {
+        return StaticHelper.initOpenCV(false);
+    }
+
+    /**
+     * Loads and initializes OpenCV library from current application package. Roughly, it's an analog of system.loadLibrary("opencv_java").
+     * @param InitCuda load and initialize CUDA runtime libraries.
+     * @return Returns true is initialization of OpenCV was successful.
+     */
+    public static boolean initDebug(boolean InitCuda)
+    {
+        return StaticHelper.initOpenCV(InitCuda);
+    }
+
+    /**
+     * Loads and initializes OpenCV library using OpenCV Engine service.
+     * @param Version OpenCV library version.
+     * @param AppContext application context for connecting to the service.
+     * @param Callback object, that implements LoaderCallbackInterface for handling the connection status.
+     * @return Returns true if initialization of OpenCV is successful.
+     */
+    public static boolean initAsync(String Version, Context AppContext,
+            LoaderCallbackInterface Callback)
+    {
+        return AsyncServiceHelper.initOpenCV(Version, AppContext, Callback);
+    }
+}
Index: openCVLibrary3411/src/main/java/org/opencv/android/StaticHelper.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/android/StaticHelper.java	(date 1605830247171)
+++ openCVLibrary3411/src/main/java/org/opencv/android/StaticHelper.java	(date 1605830247171)
@@ -0,0 +1,104 @@
+package org.opencv.android;
+
+import org.opencv.core.Core;
+
+import java.util.StringTokenizer;
+import android.util.Log;
+
+class StaticHelper {
+
+    public static boolean initOpenCV(boolean InitCuda)
+    {
+        boolean result;
+        String libs = "";
+
+        if(InitCuda)
+        {
+            loadLibrary("cudart");
+            loadLibrary("nppc");
+            loadLibrary("nppi");
+            loadLibrary("npps");
+            loadLibrary("cufft");
+            loadLibrary("cublas");
+        }
+
+        Log.d(TAG, "Trying to get library list");
+
+        try
+        {
+            System.loadLibrary("opencv_info");
+            libs = getLibraryList();
+        }
+        catch(UnsatisfiedLinkError e)
+        {
+            Log.e(TAG, "OpenCV error: Cannot load info library for OpenCV");
+        }
+
+        Log.d(TAG, "Library list: \"" + libs + "\"");
+        Log.d(TAG, "First attempt to load libs");
+        if (initOpenCVLibs(libs))
+        {
+            Log.d(TAG, "First attempt to load libs is OK");
+            String eol = System.getProperty("line.separator");
+            for (String str : Core.getBuildInformation().split(eol))
+                Log.i(TAG, str);
+
+            result = true;
+        }
+        else
+        {
+            Log.d(TAG, "First attempt to load libs fails");
+            result = false;
+        }
+
+        return result;
+    }
+
+    private static boolean loadLibrary(String Name)
+    {
+        boolean result = true;
+
+        Log.d(TAG, "Trying to load library " + Name);
+        try
+        {
+            System.loadLibrary(Name);
+            Log.d(TAG, "Library " + Name + " loaded");
+        }
+        catch(UnsatisfiedLinkError e)
+        {
+            Log.d(TAG, "Cannot load library \"" + Name + "\"");
+            e.printStackTrace();
+            result = false;
+        }
+
+        return result;
+    }
+
+    private static boolean initOpenCVLibs(String Libs)
+    {
+        Log.d(TAG, "Trying to init OpenCV libs");
+
+        boolean result = true;
+
+        if ((null != Libs) && (Libs.length() != 0))
+        {
+            Log.d(TAG, "Trying to load libs by dependency list");
+            StringTokenizer splitter = new StringTokenizer(Libs, ";");
+            while(splitter.hasMoreTokens())
+            {
+                result &= loadLibrary(splitter.nextToken());
+            }
+        }
+        else
+        {
+            // If dependencies list is not defined or empty.
+            result = loadLibrary("opencv_java3");
+        }
+
+        return result;
+    }
+
+    private static final String TAG = "OpenCV/StaticHelper";
+
+    private static native String getLibraryList();
+}
Index: openCVLibrary3411/src/main/java/org/opencv/android/Utils.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/android/Utils.java	(date 1605830247188)
+++ openCVLibrary3411/src/main/java/org/opencv/android/Utils.java	(date 1605830247188)
@@ -0,0 +1,139 @@
+package org.opencv.android;
+
+import android.content.Context;
+import android.graphics.Bitmap;
+
+import org.opencv.core.CvException;
+import org.opencv.core.CvType;
+import org.opencv.core.Mat;
+import org.opencv.imgcodecs.Imgcodecs;
+
+import java.io.ByteArrayOutputStream;
+import java.io.File;
+import java.io.FileOutputStream;
+import java.io.IOException;
+import java.io.InputStream;
+
+public class Utils {
+
+    public static String exportResource(Context context, int resourceId) {
+        return exportResource(context, resourceId, "OpenCV_data");
+    }
+
+    public static String exportResource(Context context, int resourceId, String dirname) {
+        String fullname = context.getResources().getString(resourceId);
+        String resName = fullname.substring(fullname.lastIndexOf("/") + 1);
+        try {
+            InputStream is = context.getResources().openRawResource(resourceId);
+            File resDir = context.getDir(dirname, Context.MODE_PRIVATE);
+            File resFile = new File(resDir, resName);
+
+            FileOutputStream os = new FileOutputStream(resFile);
+
+            byte[] buffer = new byte[4096];
+            int bytesRead;
+            while ((bytesRead = is.read(buffer)) != -1) {
+                os.write(buffer, 0, bytesRead);
+            }
+            is.close();
+            os.close();
+
+            return resFile.getAbsolutePath();
+        } catch (IOException e) {
+            e.printStackTrace();
+            throw new CvException("Failed to export resource " + resName
+                    + ". Exception thrown: " + e);
+        }
+    }
+
+    public static Mat loadResource(Context context, int resourceId) throws IOException
+    {
+        return loadResource(context, resourceId, -1);
+    }
+
+    public static Mat loadResource(Context context, int resourceId, int flags) throws IOException
+    {
+        InputStream is = context.getResources().openRawResource(resourceId);
+        ByteArrayOutputStream os = new ByteArrayOutputStream(is.available());
+
+        byte[] buffer = new byte[4096];
+        int bytesRead;
+        while ((bytesRead = is.read(buffer)) != -1) {
+            os.write(buffer, 0, bytesRead);
+        }
+        is.close();
+
+        Mat encoded = new Mat(1, os.size(), CvType.CV_8U);
+        encoded.put(0, 0, os.toByteArray());
+        os.close();
+
+        Mat decoded = Imgcodecs.imdecode(encoded, flags);
+        encoded.release();
+
+        return decoded;
+    }
+
+    /**
+     * Converts Android Bitmap to OpenCV Mat.
+     * <p>
+     * This function converts an Android Bitmap image to the OpenCV Mat.
+     * <br>'ARGB_8888' and 'RGB_565' input Bitmap formats are supported.
+     * <br>The output Mat is always created of the same size as the input Bitmap and of the 'CV_8UC4' type,
+     * it keeps the image in RGBA format.
+     * <br>This function throws an exception if the conversion fails.
+     * @param bmp is a valid input Bitmap object of the type 'ARGB_8888' or 'RGB_565'.
+     * @param mat is a valid output Mat object, it will be reallocated if needed, so it may be empty.
+     * @param unPremultiplyAlpha is a flag, that determines, whether the bitmap needs to be converted from alpha premultiplied format (like Android keeps 'ARGB_8888' ones) to regular one; this flag is ignored for 'RGB_565' bitmaps.
+     */
+    public static void bitmapToMat(Bitmap bmp, Mat mat, boolean unPremultiplyAlpha) {
+        if (bmp == null)
+            throw new IllegalArgumentException("bmp == null");
+        if (mat == null)
+            throw new IllegalArgumentException("mat == null");
+        nBitmapToMat2(bmp, mat.nativeObj, unPremultiplyAlpha);
+    }
+
+    /**
+     * Short form of the bitmapToMat(bmp, mat, unPremultiplyAlpha=false).
+     * @param bmp is a valid input Bitmap object of the type 'ARGB_8888' or 'RGB_565'.
+     * @param mat is a valid output Mat object, it will be reallocated if needed, so Mat may be empty.
+     */
+    public static void bitmapToMat(Bitmap bmp, Mat mat) {
+        bitmapToMat(bmp, mat, false);
+    }
+
+
+    /**
+     * Converts OpenCV Mat to Android Bitmap.
+     * <p>
+     * <br>This function converts an image in the OpenCV Mat representation to the Android Bitmap.
+     * <br>The input Mat object has to be of the types 'CV_8UC1' (gray-scale), 'CV_8UC3' (RGB) or 'CV_8UC4' (RGBA).
+     * <br>The output Bitmap object has to be of the same size as the input Mat and of the types 'ARGB_8888' or 'RGB_565'.
+     * <br>This function throws an exception if the conversion fails.
+     *
+     * @param mat is a valid input Mat object of types 'CV_8UC1', 'CV_8UC3' or 'CV_8UC4'.
+     * @param bmp is a valid Bitmap object of the same size as the Mat and of type 'ARGB_8888' or 'RGB_565'.
+     * @param premultiplyAlpha is a flag, that determines, whether the Mat needs to be converted to alpha premultiplied format (like Android keeps 'ARGB_8888' bitmaps); the flag is ignored for 'RGB_565' bitmaps.
+     */
+    public static void matToBitmap(Mat mat, Bitmap bmp, boolean premultiplyAlpha) {
+        if (mat == null)
+            throw new IllegalArgumentException("mat == null");
+        if (bmp == null)
+            throw new IllegalArgumentException("bmp == null");
+        nMatToBitmap2(mat.nativeObj, bmp, premultiplyAlpha);
+    }
+
+    /**
+     * Short form of the <b>matToBitmap(mat, bmp, premultiplyAlpha=false)</b>
+     * @param mat is a valid input Mat object of the types 'CV_8UC1', 'CV_8UC3' or 'CV_8UC4'.
+     * @param bmp is a valid Bitmap object of the same size as the Mat and of type 'ARGB_8888' or 'RGB_565'.
+     */
+    public static void matToBitmap(Mat mat, Bitmap bmp) {
+        matToBitmap(mat, bmp, false);
+    }
+
+
+    private static native void nBitmapToMat2(Bitmap b, long m_addr, boolean unPremultiplyAlpha);
+
+    private static native void nMatToBitmap2(long m_addr, Bitmap b, boolean premultiplyAlpha);
+}
Index: openCVLibrary3411/src/main/java/org/opencv/android/FpsMeter.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/android/FpsMeter.java	(date 1605830247114)
+++ openCVLibrary3411/src/main/java/org/opencv/android/FpsMeter.java	(date 1605830247114)
@@ -0,0 +1,66 @@
+package org.opencv.android;
+
+import java.text.DecimalFormat;
+
+import org.opencv.core.Core;
+
+import android.graphics.Canvas;
+import android.graphics.Color;
+import android.graphics.Paint;
+import android.util.Log;
+
+public class FpsMeter {
+    private static final String TAG               = "FpsMeter";
+    private static final int    STEP              = 20;
+    private static final DecimalFormat FPS_FORMAT = new DecimalFormat("0.00");
+
+    private int                 mFramesCounter;
+    private double              mFrequency;
+    private long                mprevFrameTime;
+    private String              mStrfps;
+    Paint                       mPaint;
+    boolean                     mIsInitialized = false;
+    int                         mWidth = 0;
+    int                         mHeight = 0;
+
+    public void init() {
+        mFramesCounter = 0;
+        mFrequency = Core.getTickFrequency();
+        mprevFrameTime = Core.getTickCount();
+        mStrfps = "";
+
+        mPaint = new Paint();
+        mPaint.setColor(Color.BLUE);
+        mPaint.setTextSize(20);
+    }
+
+    public void measure() {
+        if (!mIsInitialized) {
+            init();
+            mIsInitialized = true;
+        } else {
+            mFramesCounter++;
+            if (mFramesCounter % STEP == 0) {
+                long time = Core.getTickCount();
+                double fps = STEP * mFrequency / (time - mprevFrameTime);
+                mprevFrameTime = time;
+                if (mWidth != 0 && mHeight != 0)
+                    mStrfps = FPS_FORMAT.format(fps) + " FPS@" + Integer.valueOf(mWidth) + "x" + Integer.valueOf(mHeight);
+                else
+                    mStrfps = FPS_FORMAT.format(fps) + " FPS";
+                Log.i(TAG, mStrfps);
+            }
+        }
+    }
+
+    public void setResolution(int width, int height) {
+        mWidth = width;
+        mHeight = height;
+    }
+
+    public void draw(Canvas canvas, float offsetx, float offsety) {
+        Log.d(TAG, mStrfps);
+        canvas.drawText(mStrfps, offsetx, offsety, mPaint);
+    }
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/android/InstallCallbackInterface.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/android/InstallCallbackInterface.java	(date 1605830247116)
+++ openCVLibrary3411/src/main/java/org/opencv/android/InstallCallbackInterface.java	(date 1605830247116)
@@ -0,0 +1,34 @@
+package org.opencv.android;
+
+/**
+ * Installation callback interface.
+ */
+public interface InstallCallbackInterface
+{
+    /**
+     * New package installation is required.
+     */
+    static final int NEW_INSTALLATION = 0;
+    /**
+     * Current package installation is in progress.
+     */
+    static final int INSTALLATION_PROGRESS = 1;
+
+    /**
+     * Target package name.
+     * @return Return target package name.
+     */
+    public String getPackageName();
+    /**
+     * Installation is approved.
+     */
+    public void install();
+    /**
+     * Installation is canceled.
+     */
+    public void cancel();
+    /**
+     * Wait for package installation.
+     */
+    public void wait_install();
+};
Index: openCVLibrary3411/src/main/java/org/opencv/android/JavaCamera2View.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/android/JavaCamera2View.java	(date 1605830247119)
+++ openCVLibrary3411/src/main/java/org/opencv/android/JavaCamera2View.java	(date 1605830247119)
@@ -0,0 +1,447 @@
+package org.opencv.android;
+
+import java.nio.ByteBuffer;
+import java.util.Arrays;
+import java.util.List;
+
+import android.annotation.TargetApi;
+import android.content.Context;
+import android.graphics.ImageFormat;
+import android.hardware.camera2.CameraAccessException;
+import android.hardware.camera2.CameraCaptureSession;
+import android.hardware.camera2.CameraCharacteristics;
+import android.hardware.camera2.CameraDevice;
+import android.hardware.camera2.CameraManager;
+import android.hardware.camera2.CaptureRequest;
+import android.hardware.camera2.params.StreamConfigurationMap;
+import android.media.Image;
+import android.media.ImageReader;
+import android.os.Handler;
+import android.os.HandlerThread;
+import android.util.AttributeSet;
+import android.util.Log;
+import android.view.Surface;
+import android.view.ViewGroup.LayoutParams;
+
+import org.opencv.core.CvType;
+import org.opencv.core.Mat;
+import org.opencv.core.Size;
+import org.opencv.imgproc.Imgproc;
+
+/**
+ * This class is an implementation of the Bridge View between OpenCV and Java Camera.
+ * This class relays on the functionality available in base class and only implements
+ * required functions:
+ * connectCamera - opens Java camera and sets the PreviewCallback to be delivered.
+ * disconnectCamera - closes the camera and stops preview.
+ * When frame is delivered via callback from Camera - it processed via OpenCV to be
+ * converted to RGBA32 and then passed to the external callback for modifications if required.
+ */
+
+@TargetApi(21)
+public class JavaCamera2View extends CameraBridgeViewBase {
+
+    private static final String LOGTAG = "JavaCamera2View";
+
+    private ImageReader mImageReader;
+    private int mPreviewFormat = ImageFormat.YUV_420_888;
+
+    private CameraDevice mCameraDevice;
+    private CameraCaptureSession mCaptureSession;
+    private CaptureRequest.Builder mPreviewRequestBuilder;
+    private String mCameraID;
+    private android.util.Size mPreviewSize = new android.util.Size(-1, -1);
+
+    private HandlerThread mBackgroundThread;
+    private Handler mBackgroundHandler;
+
+    public JavaCamera2View(Context context, int cameraId) {
+        super(context, cameraId);
+    }
+
+    public JavaCamera2View(Context context, AttributeSet attrs) {
+        super(context, attrs);
+    }
+
+    private void startBackgroundThread() {
+        Log.i(LOGTAG, "startBackgroundThread");
+        stopBackgroundThread();
+        mBackgroundThread = new HandlerThread("OpenCVCameraBackground");
+        mBackgroundThread.start();
+        mBackgroundHandler = new Handler(mBackgroundThread.getLooper());
+    }
+
+    private void stopBackgroundThread() {
+        Log.i(LOGTAG, "stopBackgroundThread");
+        if (mBackgroundThread == null)
+            return;
+        mBackgroundThread.quitSafely();
+        try {
+            mBackgroundThread.join();
+            mBackgroundThread = null;
+            mBackgroundHandler = null;
+        } catch (InterruptedException e) {
+            Log.e(LOGTAG, "stopBackgroundThread", e);
+        }
+    }
+
+    protected boolean initializeCamera() {
+        Log.i(LOGTAG, "initializeCamera");
+        CameraManager manager = (CameraManager) getContext().getSystemService(Context.CAMERA_SERVICE);
+        try {
+            String camList[] = manager.getCameraIdList();
+            if (camList.length == 0) {
+                Log.e(LOGTAG, "Error: camera isn't detected.");
+                return false;
+            }
+            if (mCameraIndex == CameraBridgeViewBase.CAMERA_ID_ANY) {
+                mCameraID = camList[0];
+            } else {
+                for (String cameraID : camList) {
+                    CameraCharacteristics characteristics = manager.getCameraCharacteristics(cameraID);
+                    if ((mCameraIndex == CameraBridgeViewBase.CAMERA_ID_BACK &&
+                            characteristics.get(CameraCharacteristics.LENS_FACING) == CameraCharacteristics.LENS_FACING_BACK) ||
+                        (mCameraIndex == CameraBridgeViewBase.CAMERA_ID_FRONT &&
+                            characteristics.get(CameraCharacteristics.LENS_FACING) == CameraCharacteristics.LENS_FACING_FRONT)
+                    ) {
+                        mCameraID = cameraID;
+                        break;
+                    }
+                }
+            }
+            if (mCameraID != null) {
+                Log.i(LOGTAG, "Opening camera: " + mCameraID);
+                manager.openCamera(mCameraID, mStateCallback, mBackgroundHandler);
+            } else { // make JavaCamera2View behaves in the same way as JavaCameraView
+                Log.i(LOGTAG, "Trying to open camera with the value (" + mCameraIndex + ")");
+                if (mCameraIndex < camList.length) {
+                    mCameraID = camList[mCameraIndex];
+                    manager.openCamera(mCameraID, mStateCallback, mBackgroundHandler);
+                } else {
+                    // CAMERA_DISCONNECTED is used when the camera id is no longer valid
+                    throw new CameraAccessException(CameraAccessException.CAMERA_DISCONNECTED);
+                }
+            }
+            return true;
+        } catch (CameraAccessException e) {
+            Log.e(LOGTAG, "OpenCamera - Camera Access Exception", e);
+        } catch (IllegalArgumentException e) {
+            Log.e(LOGTAG, "OpenCamera - Illegal Argument Exception", e);
+        } catch (SecurityException e) {
+            Log.e(LOGTAG, "OpenCamera - Security Exception", e);
+        }
+        return false;
+    }
+
+    private final CameraDevice.StateCallback mStateCallback = new CameraDevice.StateCallback() {
+
+        @Override
+        public void onOpened(CameraDevice cameraDevice) {
+            mCameraDevice = cameraDevice;
+            createCameraPreviewSession();
+        }
+
+        @Override
+        public void onDisconnected(CameraDevice cameraDevice) {
+            cameraDevice.close();
+            mCameraDevice = null;
+        }
+
+        @Override
+        public void onError(CameraDevice cameraDevice, int error) {
+            cameraDevice.close();
+            mCameraDevice = null;
+        }
+
+    };
+
+    private void createCameraPreviewSession() {
+        final int w = mPreviewSize.getWidth(), h = mPreviewSize.getHeight();
+        Log.i(LOGTAG, "createCameraPreviewSession(" + w + "x" + h + ")");
+        if (w < 0 || h < 0)
+            return;
+        try {
+            if (null == mCameraDevice) {
+                Log.e(LOGTAG, "createCameraPreviewSession: camera isn't opened");
+                return;
+            }
+            if (null != mCaptureSession) {
+                Log.e(LOGTAG, "createCameraPreviewSession: mCaptureSession is already started");
+                return;
+            }
+
+            mImageReader = ImageReader.newInstance(w, h, mPreviewFormat, 2);
+            mImageReader.setOnImageAvailableListener(new ImageReader.OnImageAvailableListener() {
+                @Override
+                public void onImageAvailable(ImageReader reader) {
+                    Image image = reader.acquireLatestImage();
+                    if (image == null)
+                        return;
+
+                    // sanity checks - 3 planes
+                    Image.Plane[] planes = image.getPlanes();
+                    assert (planes.length == 3);
+                    assert (image.getFormat() == mPreviewFormat);
+
+                    JavaCamera2Frame tempFrame = new JavaCamera2Frame(image);
+                    deliverAndDrawFrame(tempFrame);
+                    tempFrame.release();
+                    image.close();
+                }
+            }, mBackgroundHandler);
+            Surface surface = mImageReader.getSurface();
+
+            mPreviewRequestBuilder = mCameraDevice.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
+            mPreviewRequestBuilder.addTarget(surface);
+
+            mCameraDevice.createCaptureSession(Arrays.asList(surface),
+                new CameraCaptureSession.StateCallback() {
+                    @Override
+                    public void onConfigured(CameraCaptureSession cameraCaptureSession) {
+                        Log.i(LOGTAG, "createCaptureSession::onConfigured");
+                        if (null == mCameraDevice) {
+                            return; // camera is already closed
+                        }
+                        mCaptureSession = cameraCaptureSession;
+                        try {
+                            mPreviewRequestBuilder.set(CaptureRequest.CONTROL_AF_MODE,
+                                    CaptureRequest.CONTROL_AF_MODE_CONTINUOUS_PICTURE);
+                            mPreviewRequestBuilder.set(CaptureRequest.CONTROL_AE_MODE,
+                                    CaptureRequest.CONTROL_AE_MODE_ON_AUTO_FLASH);
+
+                            mCaptureSession.setRepeatingRequest(mPreviewRequestBuilder.build(), null, mBackgroundHandler);
+                            Log.i(LOGTAG, "CameraPreviewSession has been started");
+                        } catch (Exception e) {
+                            Log.e(LOGTAG, "createCaptureSession failed", e);
+                        }
+                    }
+
+                    @Override
+                    public void onConfigureFailed(CameraCaptureSession cameraCaptureSession) {
+                        Log.e(LOGTAG, "createCameraPreviewSession failed");
+                    }
+                },
+                null
+            );
+        } catch (CameraAccessException e) {
+            Log.e(LOGTAG, "createCameraPreviewSession", e);
+        }
+    }
+
+    @Override
+    protected void disconnectCamera() {
+        Log.i(LOGTAG, "close camera");
+        try {
+            CameraDevice c = mCameraDevice;
+            mCameraDevice = null;
+            if (null != mCaptureSession) {
+                mCaptureSession.close();
+                mCaptureSession = null;
+            }
+            if (null != c) {
+                c.close();
+            }
+        } finally {
+            stopBackgroundThread();
+            if (null != mImageReader) {
+                mImageReader.close();
+                mImageReader = null;
+            }
+        }
+        Log.i(LOGTAG, "camera closed!");
+    }
+
+    public static class JavaCameraSizeAccessor implements ListItemAccessor {
+        @Override
+        public int getWidth(Object obj) {
+            android.util.Size size = (android.util.Size)obj;
+            return size.getWidth();
+        }
+
+        @Override
+        public int getHeight(Object obj) {
+            android.util.Size size = (android.util.Size)obj;
+            return size.getHeight();
+        }
+    }
+
+    boolean calcPreviewSize(final int width, final int height) {
+        Log.i(LOGTAG, "calcPreviewSize: " + width + "x" + height);
+        if (mCameraID == null) {
+            Log.e(LOGTAG, "Camera isn't initialized!");
+            return false;
+        }
+        CameraManager manager = (CameraManager) getContext().getSystemService(Context.CAMERA_SERVICE);
+        try {
+            CameraCharacteristics characteristics = manager.getCameraCharacteristics(mCameraID);
+            StreamConfigurationMap map = characteristics.get(CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
+            android.util.Size[] sizes = map.getOutputSizes(ImageReader.class);
+            List<android.util.Size> sizes_list = Arrays.asList(sizes);
+            Size frameSize = calculateCameraFrameSize(sizes_list, new JavaCameraSizeAccessor(), width, height);
+            Log.i(LOGTAG, "Selected preview size to " + Integer.valueOf((int)frameSize.width) + "x" + Integer.valueOf((int)frameSize.height));
+            assert(!(frameSize.width == 0 || frameSize.height == 0));
+            if (mPreviewSize.getWidth() == frameSize.width && mPreviewSize.getHeight() == frameSize.height)
+                return false;
+            else {
+                mPreviewSize = new android.util.Size((int)frameSize.width, (int)frameSize.height);
+                return true;
+            }
+        } catch (CameraAccessException e) {
+            Log.e(LOGTAG, "calcPreviewSize - Camera Access Exception", e);
+        } catch (IllegalArgumentException e) {
+            Log.e(LOGTAG, "calcPreviewSize - Illegal Argument Exception", e);
+        } catch (SecurityException e) {
+            Log.e(LOGTAG, "calcPreviewSize - Security Exception", e);
+        }
+        return false;
+    }
+
+    @Override
+    protected boolean connectCamera(int width, int height) {
+        Log.i(LOGTAG, "setCameraPreviewSize(" + width + "x" + height + ")");
+        startBackgroundThread();
+        initializeCamera();
+        try {
+            boolean needReconfig = calcPreviewSize(width, height);
+            mFrameWidth = mPreviewSize.getWidth();
+            mFrameHeight = mPreviewSize.getHeight();
+
+            if ((getLayoutParams().width == LayoutParams.MATCH_PARENT) && (getLayoutParams().height == LayoutParams.MATCH_PARENT))
+                mScale = Math.min(((float)height)/mFrameHeight, ((float)width)/mFrameWidth);
+            else
+                mScale = 0;
+
+            AllocateCache();
+
+            if (needReconfig) {
+                if (null != mCaptureSession) {
+                    Log.d(LOGTAG, "closing existing previewSession");
+                    mCaptureSession.close();
+                    mCaptureSession = null;
+                }
+                createCameraPreviewSession();
+            }
+        } catch (RuntimeException e) {
+            throw new RuntimeException("Interrupted while setCameraPreviewSize.", e);
+        }
+        return true;
+    }
+
+    private class JavaCamera2Frame implements CvCameraViewFrame {
+        @Override
+        public Mat gray() {
+            Image.Plane[] planes = mImage.getPlanes();
+            int w = mImage.getWidth();
+            int h = mImage.getHeight();
+            assert(planes[0].getPixelStride() == 1);
+            ByteBuffer y_plane = planes[0].getBuffer();
+            int y_plane_step = planes[0].getRowStride();
+            mGray = new Mat(h, w, CvType.CV_8UC1, y_plane, y_plane_step);
+            return mGray;
+        }
+
+        @Override
+        public Mat rgba() {
+            Image.Plane[] planes = mImage.getPlanes();
+            int w = mImage.getWidth();
+            int h = mImage.getHeight();
+            int chromaPixelStride = planes[1].getPixelStride();
+
+
+            if (chromaPixelStride == 2) { // Chroma channels are interleaved
+                assert(planes[0].getPixelStride() == 1);
+                assert(planes[2].getPixelStride() == 2);
+                ByteBuffer y_plane = planes[0].getBuffer();
+                int y_plane_step = planes[0].getRowStride();
+                ByteBuffer uv_plane1 = planes[1].getBuffer();
+                int uv_plane1_step = planes[1].getRowStride();
+                ByteBuffer uv_plane2 = planes[2].getBuffer();
+                int uv_plane2_step = planes[2].getRowStride();
+                Mat y_mat = new Mat(h, w, CvType.CV_8UC1, y_plane, y_plane_step);
+                Mat uv_mat1 = new Mat(h / 2, w / 2, CvType.CV_8UC2, uv_plane1, uv_plane1_step);
+                Mat uv_mat2 = new Mat(h / 2, w / 2, CvType.CV_8UC2, uv_plane2, uv_plane2_step);
+                long addr_diff = uv_mat2.dataAddr() - uv_mat1.dataAddr();
+                if (addr_diff > 0) {
+                    assert(addr_diff == 1);
+                    Imgproc.cvtColorTwoPlane(y_mat, uv_mat1, mRgba, Imgproc.COLOR_YUV2RGBA_NV12);
+                } else {
+                    assert(addr_diff == -1);
+                    Imgproc.cvtColorTwoPlane(y_mat, uv_mat2, mRgba, Imgproc.COLOR_YUV2RGBA_NV21);
+                }
+                return mRgba;
+            } else { // Chroma channels are not interleaved
+                byte[] yuv_bytes = new byte[w*(h+h/2)];
+                ByteBuffer y_plane = planes[0].getBuffer();
+                ByteBuffer u_plane = planes[1].getBuffer();
+                ByteBuffer v_plane = planes[2].getBuffer();
+
+                int yuv_bytes_offset = 0;
+
+                int y_plane_step = planes[0].getRowStride();
+                if (y_plane_step == w) {
+                    y_plane.get(yuv_bytes, 0, w*h);
+                    yuv_bytes_offset = w*h;
+                } else {
+                    int padding = y_plane_step - w;
+                    for (int i = 0; i < h; i++){
+                        y_plane.get(yuv_bytes, yuv_bytes_offset, w);
+                        yuv_bytes_offset += w;
+                        if (i < h - 1) {
+                            y_plane.position(y_plane.position() + padding);
+                        }
+                    }
+                    assert(yuv_bytes_offset == w * h);
+                }
+
+                int chromaRowStride = planes[1].getRowStride();
+                int chromaRowPadding = chromaRowStride - w/2;
+
+                if (chromaRowPadding == 0){
+                    // When the row stride of the chroma channels equals their width, we can copy
+                    // the entire channels in one go
+                    u_plane.get(yuv_bytes, yuv_bytes_offset, w*h/4);
+                    yuv_bytes_offset += w*h/4;
+                    v_plane.get(yuv_bytes, yuv_bytes_offset, w*h/4);
+                } else {
+                    // When not equal, we need to copy the channels row by row
+                    for (int i = 0; i < h/2; i++){
+                        u_plane.get(yuv_bytes, yuv_bytes_offset, w/2);
+                        yuv_bytes_offset += w/2;
+                        if (i < h/2-1){
+                            u_plane.position(u_plane.position() + chromaRowPadding);
+                        }
+                    }
+                    for (int i = 0; i < h/2; i++){
+                        v_plane.get(yuv_bytes, yuv_bytes_offset, w/2);
+                        yuv_bytes_offset += w/2;
+                        if (i < h/2-1){
+                            v_plane.position(v_plane.position() + chromaRowPadding);
+                        }
+                    }
+                }
+
+                Mat yuv_mat = new Mat(h+h/2, w, CvType.CV_8UC1);
+                yuv_mat.put(0, 0, yuv_bytes);
+                Imgproc.cvtColor(yuv_mat, mRgba, Imgproc.COLOR_YUV2RGBA_I420, 4);
+                return mRgba;
+            }
+        }
+
+
+        public JavaCamera2Frame(Image image) {
+            super();
+            mImage = image;
+            mRgba = new Mat();
+            mGray = new Mat();
+        }
+
+        public void release() {
+            mRgba.release();
+            mGray.release();
+        }
+
+        private Image mImage;
+        private Mat mRgba;
+        private Mat mGray;
+    };
+}
Index: openCVLibrary3411/src/main/java/org/opencv/android/JavaCameraView.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/android/JavaCameraView.java	(date 1605830247166)
+++ openCVLibrary3411/src/main/java/org/opencv/android/JavaCameraView.java	(date 1605830247166)
@@ -0,0 +1,379 @@
+package org.opencv.android;
+
+import java.util.List;
+
+import android.content.Context;
+import android.graphics.ImageFormat;
+import android.graphics.SurfaceTexture;
+import android.hardware.Camera;
+import android.hardware.Camera.PreviewCallback;
+import android.os.Build;
+import android.util.AttributeSet;
+import android.util.Log;
+import android.view.ViewGroup.LayoutParams;
+
+import org.opencv.BuildConfig;
+import org.opencv.core.CvType;
+import org.opencv.core.Mat;
+import org.opencv.core.Size;
+import org.opencv.imgproc.Imgproc;
+
+/**
+ * This class is an implementation of the Bridge View between OpenCV and Java Camera.
+ * This class relays on the functionality available in base class and only implements
+ * required functions:
+ * connectCamera - opens Java camera and sets the PreviewCallback to be delivered.
+ * disconnectCamera - closes the camera and stops preview.
+ * When frame is delivered via callback from Camera - it processed via OpenCV to be
+ * converted to RGBA32 and then passed to the external callback for modifications if required.
+ */
+public class JavaCameraView extends CameraBridgeViewBase implements PreviewCallback {
+
+    private static final int MAGIC_TEXTURE_ID = 10;
+    private static final String TAG = "JavaCameraView";
+
+    private byte mBuffer[];
+    private Mat[] mFrameChain;
+    private int mChainIdx = 0;
+    private Thread mThread;
+    private boolean mStopThread;
+
+    protected Camera mCamera;
+    protected JavaCameraFrame[] mCameraFrame;
+    private SurfaceTexture mSurfaceTexture;
+    private int mPreviewFormat = ImageFormat.NV21;
+
+    public static class JavaCameraSizeAccessor implements ListItemAccessor {
+
+        @Override
+        public int getWidth(Object obj) {
+            Camera.Size size = (Camera.Size) obj;
+            return size.width;
+        }
+
+        @Override
+        public int getHeight(Object obj) {
+            Camera.Size size = (Camera.Size) obj;
+            return size.height;
+        }
+    }
+
+    public JavaCameraView(Context context, int cameraId) {
+        super(context, cameraId);
+    }
+
+    public JavaCameraView(Context context, AttributeSet attrs) {
+        super(context, attrs);
+    }
+
+    protected boolean initializeCamera(int width, int height) {
+        Log.d(TAG, "Initialize java camera");
+        boolean result = true;
+        synchronized (this) {
+            mCamera = null;
+
+            if (mCameraIndex == CAMERA_ID_ANY) {
+                Log.d(TAG, "Trying to open camera with old open()");
+                try {
+                    mCamera = Camera.open();
+                }
+                catch (Exception e){
+                    Log.e(TAG, "Camera is not available (in use or does not exist): " + e.getLocalizedMessage());
+                }
+
+                if(mCamera == null && Build.VERSION.SDK_INT >= Build.VERSION_CODES.GINGERBREAD) {
+                    boolean connected = false;
+                    for (int camIdx = 0; camIdx < Camera.getNumberOfCameras(); ++camIdx) {
+                        Log.d(TAG, "Trying to open camera with new open(" + Integer.valueOf(camIdx) + ")");
+                        try {
+                            mCamera = Camera.open(camIdx);
+                            connected = true;
+                        } catch (RuntimeException e) {
+                            Log.e(TAG, "Camera #" + camIdx + "failed to open: " + e.getLocalizedMessage());
+                        }
+                        if (connected) break;
+                    }
+                }
+            } else {
+                if (Build.VERSION.SDK_INT >= Build.VERSION_CODES.GINGERBREAD) {
+                    int localCameraIndex = mCameraIndex;
+                    if (mCameraIndex == CAMERA_ID_BACK) {
+                        Log.i(TAG, "Trying to open back camera");
+                        Camera.CameraInfo cameraInfo = new Camera.CameraInfo();
+                        for (int camIdx = 0; camIdx < Camera.getNumberOfCameras(); ++camIdx) {
+                            Camera.getCameraInfo( camIdx, cameraInfo );
+                            if (cameraInfo.facing == Camera.CameraInfo.CAMERA_FACING_BACK) {
+                                localCameraIndex = camIdx;
+                                break;
+                            }
+                        }
+                    } else if (mCameraIndex == CAMERA_ID_FRONT) {
+                        Log.i(TAG, "Trying to open front camera");
+                        Camera.CameraInfo cameraInfo = new Camera.CameraInfo();
+                        for (int camIdx = 0; camIdx < Camera.getNumberOfCameras(); ++camIdx) {
+                            Camera.getCameraInfo( camIdx, cameraInfo );
+                            if (cameraInfo.facing == Camera.CameraInfo.CAMERA_FACING_FRONT) {
+                                localCameraIndex = camIdx;
+                                break;
+                            }
+                        }
+                    }
+                    if (localCameraIndex == CAMERA_ID_BACK) {
+                        Log.e(TAG, "Back camera not found!");
+                    } else if (localCameraIndex == CAMERA_ID_FRONT) {
+                        Log.e(TAG, "Front camera not found!");
+                    } else {
+                        Log.d(TAG, "Trying to open camera with new open(" + Integer.valueOf(localCameraIndex) + ")");
+                        try {
+                            mCamera = Camera.open(localCameraIndex);
+                        } catch (RuntimeException e) {
+                            Log.e(TAG, "Camera #" + localCameraIndex + "failed to open: " + e.getLocalizedMessage());
+                        }
+                    }
+                }
+            }
+
+            if (mCamera == null)
+                return false;
+
+            /* Now set camera parameters */
+            try {
+                Camera.Parameters params = mCamera.getParameters();
+                Log.d(TAG, "getSupportedPreviewSizes()");
+                List<android.hardware.Camera.Size> sizes = params.getSupportedPreviewSizes();
+
+                if (sizes != null) {
+                    /* Select the size that fits surface considering maximum size allowed */
+                    Size frameSize = calculateCameraFrameSize(sizes, new JavaCameraSizeAccessor(), width, height);
+
+                    /* Image format NV21 causes issues in the Android emulators */
+                    if (Build.FINGERPRINT.startsWith("generic")
+                            || Build.FINGERPRINT.startsWith("unknown")
+                            || Build.MODEL.contains("google_sdk")
+                            || Build.MODEL.contains("Emulator")
+                            || Build.MODEL.contains("Android SDK built for x86")
+                            || Build.MANUFACTURER.contains("Genymotion")
+                            || (Build.BRAND.startsWith("generic") && Build.DEVICE.startsWith("generic"))
+                            || "google_sdk".equals(Build.PRODUCT))
+                        params.setPreviewFormat(ImageFormat.YV12);  // "generic" or "android" = android emulator
+                    else
+                        params.setPreviewFormat(ImageFormat.NV21);
+
+                    mPreviewFormat = params.getPreviewFormat();
+
+                    Log.d(TAG, "Set preview size to " + Integer.valueOf((int)frameSize.width) + "x" + Integer.valueOf((int)frameSize.height));
+                    params.setPreviewSize((int)frameSize.width, (int)frameSize.height);
+
+                    if (Build.VERSION.SDK_INT >= Build.VERSION_CODES.ICE_CREAM_SANDWICH && !android.os.Build.MODEL.equals("GT-I9100"))
+                        params.setRecordingHint(true);
+
+                    List<String> FocusModes = params.getSupportedFocusModes();
+                    if (FocusModes != null && FocusModes.contains(Camera.Parameters.FOCUS_MODE_CONTINUOUS_VIDEO))
+                    {
+                        params.setFocusMode(Camera.Parameters.FOCUS_MODE_CONTINUOUS_VIDEO);
+                    }
+
+                    mCamera.setParameters(params);
+                    params = mCamera.getParameters();
+
+                    mFrameWidth = params.getPreviewSize().width;
+                    mFrameHeight = params.getPreviewSize().height;
+
+                    if ((getLayoutParams().width == LayoutParams.MATCH_PARENT) && (getLayoutParams().height == LayoutParams.MATCH_PARENT))
+                        mScale = Math.min(((float)height)/mFrameHeight, ((float)width)/mFrameWidth);
+                    else
+                        mScale = 0;
+
+                    if (mFpsMeter != null) {
+                        mFpsMeter.setResolution(mFrameWidth, mFrameHeight);
+                    }
+
+                    int size = mFrameWidth * mFrameHeight;
+                    size  = size * ImageFormat.getBitsPerPixel(params.getPreviewFormat()) / 8;
+                    mBuffer = new byte[size];
+
+                    mCamera.addCallbackBuffer(mBuffer);
+                    mCamera.setPreviewCallbackWithBuffer(this);
+
+                    mFrameChain = new Mat[2];
+                    mFrameChain[0] = new Mat(mFrameHeight + (mFrameHeight/2), mFrameWidth, CvType.CV_8UC1);
+                    mFrameChain[1] = new Mat(mFrameHeight + (mFrameHeight/2), mFrameWidth, CvType.CV_8UC1);
+
+                    AllocateCache();
+
+                    mCameraFrame = new JavaCameraFrame[2];
+                    mCameraFrame[0] = new JavaCameraFrame(mFrameChain[0], mFrameWidth, mFrameHeight);
+                    mCameraFrame[1] = new JavaCameraFrame(mFrameChain[1], mFrameWidth, mFrameHeight);
+
+                    if (Build.VERSION.SDK_INT >= Build.VERSION_CODES.HONEYCOMB) {
+                        mSurfaceTexture = new SurfaceTexture(MAGIC_TEXTURE_ID);
+                        mCamera.setPreviewTexture(mSurfaceTexture);
+                    } else
+                       mCamera.setPreviewDisplay(null);
+
+                    /* Finally we are ready to start the preview */
+                    Log.d(TAG, "startPreview");
+                    mCamera.startPreview();
+                }
+                else
+                    result = false;
+            } catch (Exception e) {
+                result = false;
+                e.printStackTrace();
+            }
+        }
+
+        return result;
+    }
+
+    protected void releaseCamera() {
+        synchronized (this) {
+            if (mCamera != null) {
+                mCamera.stopPreview();
+                mCamera.setPreviewCallback(null);
+
+                mCamera.release();
+            }
+            mCamera = null;
+            if (mFrameChain != null) {
+                mFrameChain[0].release();
+                mFrameChain[1].release();
+            }
+            if (mCameraFrame != null) {
+                mCameraFrame[0].release();
+                mCameraFrame[1].release();
+            }
+        }
+    }
+
+    private boolean mCameraFrameReady = false;
+
+    @Override
+    protected boolean connectCamera(int width, int height) {
+
+        /* 1. We need to instantiate camera
+         * 2. We need to start thread which will be getting frames
+         */
+        /* First step - initialize camera connection */
+        Log.d(TAG, "Connecting to camera");
+        if (!initializeCamera(width, height))
+            return false;
+
+        mCameraFrameReady = false;
+
+        /* now we can start update thread */
+        Log.d(TAG, "Starting processing thread");
+        mStopThread = false;
+        mThread = new Thread(new CameraWorker());
+        mThread.start();
+
+        return true;
+    }
+
+    @Override
+    protected void disconnectCamera() {
+        /* 1. We need to stop thread which updating the frames
+         * 2. Stop camera and release it
+         */
+        Log.d(TAG, "Disconnecting from camera");
+        try {
+            mStopThread = true;
+            Log.d(TAG, "Notify thread");
+            synchronized (this) {
+                this.notify();
+            }
+            Log.d(TAG, "Waiting for thread");
+            if (mThread != null)
+                mThread.join();
+        } catch (InterruptedException e) {
+            e.printStackTrace();
+        } finally {
+            mThread =  null;
+        }
+
+        /* Now release camera */
+        releaseCamera();
+
+        mCameraFrameReady = false;
+    }
+
+    @Override
+    public void onPreviewFrame(byte[] frame, Camera arg1) {
+        if (BuildConfig.DEBUG)
+            Log.d(TAG, "Preview Frame received. Frame size: " + frame.length);
+        synchronized (this) {
+            mFrameChain[mChainIdx].put(0, 0, frame);
+            mCameraFrameReady = true;
+            this.notify();
+        }
+        if (mCamera != null)
+            mCamera.addCallbackBuffer(mBuffer);
+    }
+
+    private class JavaCameraFrame implements CvCameraViewFrame {
+        @Override
+        public Mat gray() {
+            return mYuvFrameData.submat(0, mHeight, 0, mWidth);
+        }
+
+        @Override
+        public Mat rgba() {
+            if (mPreviewFormat == ImageFormat.NV21)
+                Imgproc.cvtColor(mYuvFrameData, mRgba, Imgproc.COLOR_YUV2RGBA_NV21, 4);
+            else if (mPreviewFormat == ImageFormat.YV12)
+                Imgproc.cvtColor(mYuvFrameData, mRgba, Imgproc.COLOR_YUV2RGB_I420, 4);  // COLOR_YUV2RGBA_YV12 produces inverted colors
+            else
+                throw new IllegalArgumentException("Preview Format can be NV21 or YV12");
+
+            return mRgba;
+        }
+
+        public JavaCameraFrame(Mat Yuv420sp, int width, int height) {
+            super();
+            mWidth = width;
+            mHeight = height;
+            mYuvFrameData = Yuv420sp;
+            mRgba = new Mat();
+        }
+
+        public void release() {
+            mRgba.release();
+        }
+
+        private Mat mYuvFrameData;
+        private Mat mRgba;
+        private int mWidth;
+        private int mHeight;
+    };
+
+    private class CameraWorker implements Runnable {
+
+        @Override
+        public void run() {
+            do {
+                boolean hasFrame = false;
+                synchronized (JavaCameraView.this) {
+                    try {
+                        while (!mCameraFrameReady && !mStopThread) {
+                            JavaCameraView.this.wait();
+                        }
+                    } catch (InterruptedException e) {
+                        e.printStackTrace();
+                    }
+                    if (mCameraFrameReady)
+                    {
+                        mChainIdx = 1 - mChainIdx;
+                        mCameraFrameReady = false;
+                        hasFrame = true;
+                    }
+                }
+
+                if (!mStopThread && hasFrame) {
+                    if (!mFrameChain[1 - mChainIdx].empty())
+                        deliverAndDrawFrame(mCameraFrame[1 - mChainIdx]);
+                }
+            } while (!mStopThread);
+            Log.d(TAG, "Finish processing thread");
+        }
+    }
+}
Index: openCVLibrary3411/src/main/java/org/opencv/android/CameraBridgeViewBase.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/android/CameraBridgeViewBase.java	(date 1605830247084)
+++ openCVLibrary3411/src/main/java/org/opencv/android/CameraBridgeViewBase.java	(date 1605830247084)
@@ -0,0 +1,503 @@
+package org.opencv.android;
+
+import java.util.List;
+
+import org.opencv.BuildConfig;
+import org.opencv.R;
+import org.opencv.core.Mat;
+import org.opencv.core.Size;
+
+import android.app.Activity;
+import android.app.AlertDialog;
+import android.content.Context;
+import android.content.DialogInterface;
+import android.content.res.TypedArray;
+import android.graphics.Bitmap;
+import android.graphics.Canvas;
+import android.graphics.Rect;
+import android.util.AttributeSet;
+import android.util.Log;
+import android.view.SurfaceHolder;
+import android.view.SurfaceView;
+
+/**
+ * This is a basic class, implementing the interaction with Camera and OpenCV library.
+ * The main responsibility of it - is to control when camera can be enabled, process the frame,
+ * call external listener to make any adjustments to the frame and then draw the resulting
+ * frame to the screen.
+ * The clients shall implement CvCameraViewListener.
+ */
+public abstract class CameraBridgeViewBase extends SurfaceView implements SurfaceHolder.Callback {
+
+    private static final String TAG = "CameraBridge";
+    protected static final int MAX_UNSPECIFIED = -1;
+    private static final int STOPPED = 0;
+    private static final int STARTED = 1;
+
+    private int mState = STOPPED;
+    private Bitmap mCacheBitmap;
+    private CvCameraViewListener2 mListener;
+    private boolean mSurfaceExist;
+    private final Object mSyncObject = new Object();
+
+    protected int mFrameWidth;
+    protected int mFrameHeight;
+    protected int mMaxHeight;
+    protected int mMaxWidth;
+    protected float mScale = 0;
+    protected int mPreviewFormat = RGBA;
+    protected int mCameraIndex = CAMERA_ID_ANY;
+    protected boolean mEnabled;
+    protected FpsMeter mFpsMeter = null;
+
+    public static final int CAMERA_ID_ANY   = -1;
+    public static final int CAMERA_ID_BACK  = 99;
+    public static final int CAMERA_ID_FRONT = 98;
+    public static final int RGBA = 1;
+    public static final int GRAY = 2;
+
+    public CameraBridgeViewBase(Context context, int cameraId) {
+        super(context);
+        mCameraIndex = cameraId;
+        getHolder().addCallback(this);
+        mMaxWidth = MAX_UNSPECIFIED;
+        mMaxHeight = MAX_UNSPECIFIED;
+    }
+
+    public CameraBridgeViewBase(Context context, AttributeSet attrs) {
+        super(context, attrs);
+
+        int count = attrs.getAttributeCount();
+        Log.d(TAG, "Attr count: " + Integer.valueOf(count));
+
+        TypedArray styledAttrs = getContext().obtainStyledAttributes(attrs, R.styleable.CameraBridgeViewBase);
+        if (styledAttrs.getBoolean(R.styleable.CameraBridgeViewBase_show_fps, false))
+            enableFpsMeter();
+
+        mCameraIndex = styledAttrs.getInt(R.styleable.CameraBridgeViewBase_camera_id, -1);
+
+        getHolder().addCallback(this);
+        mMaxWidth = MAX_UNSPECIFIED;
+        mMaxHeight = MAX_UNSPECIFIED;
+        styledAttrs.recycle();
+    }
+
+    /**
+     * Sets the camera index
+     * @param cameraIndex new camera index
+     */
+    public void setCameraIndex(int cameraIndex) {
+        this.mCameraIndex = cameraIndex;
+    }
+
+    public interface CvCameraViewListener {
+        /**
+         * This method is invoked when camera preview has started. After this method is invoked
+         * the frames will start to be delivered to client via the onCameraFrame() callback.
+         * @param width -  the width of the frames that will be delivered
+         * @param height - the height of the frames that will be delivered
+         */
+        public void onCameraViewStarted(int width, int height);
+
+        /**
+         * This method is invoked when camera preview has been stopped for some reason.
+         * No frames will be delivered via onCameraFrame() callback after this method is called.
+         */
+        public void onCameraViewStopped();
+
+        /**
+         * This method is invoked when delivery of the frame needs to be done.
+         * The returned values - is a modified frame which needs to be displayed on the screen.
+         * TODO: pass the parameters specifying the format of the frame (BPP, YUV or RGB and etc)
+         */
+        public Mat onCameraFrame(Mat inputFrame);
+    }
+
+    public interface CvCameraViewListener2 {
+        /**
+         * This method is invoked when camera preview has started. After this method is invoked
+         * the frames will start to be delivered to client via the onCameraFrame() callback.
+         * @param width -  the width of the frames that will be delivered
+         * @param height - the height of the frames that will be delivered
+         */
+        public void onCameraViewStarted(int width, int height);
+
+        /**
+         * This method is invoked when camera preview has been stopped for some reason.
+         * No frames will be delivered via onCameraFrame() callback after this method is called.
+         */
+        public void onCameraViewStopped();
+
+        /**
+         * This method is invoked when delivery of the frame needs to be done.
+         * The returned values - is a modified frame which needs to be displayed on the screen.
+         * TODO: pass the parameters specifying the format of the frame (BPP, YUV or RGB and etc)
+         */
+        public Mat onCameraFrame(CvCameraViewFrame inputFrame);
+    };
+
+    protected class CvCameraViewListenerAdapter implements CvCameraViewListener2  {
+        public CvCameraViewListenerAdapter(CvCameraViewListener oldStypeListener) {
+            mOldStyleListener = oldStypeListener;
+        }
+
+        public void onCameraViewStarted(int width, int height) {
+            mOldStyleListener.onCameraViewStarted(width, height);
+        }
+
+        public void onCameraViewStopped() {
+            mOldStyleListener.onCameraViewStopped();
+        }
+
+        public Mat onCameraFrame(CvCameraViewFrame inputFrame) {
+             Mat result = null;
+             switch (mPreviewFormat) {
+                case RGBA:
+                    result = mOldStyleListener.onCameraFrame(inputFrame.rgba());
+                    break;
+                case GRAY:
+                    result = mOldStyleListener.onCameraFrame(inputFrame.gray());
+                    break;
+                default:
+                    Log.e(TAG, "Invalid frame format! Only RGBA and Gray Scale are supported!");
+            };
+
+            return result;
+        }
+
+        public void setFrameFormat(int format) {
+            mPreviewFormat = format;
+        }
+
+        private int mPreviewFormat = RGBA;
+        private CvCameraViewListener mOldStyleListener;
+    };
+
+    /**
+     * This class interface is abstract representation of single frame from camera for onCameraFrame callback
+     * Attention: Do not use objects, that represents this interface out of onCameraFrame callback!
+     */
+    public interface CvCameraViewFrame {
+
+        /**
+         * This method returns RGBA Mat with frame
+         */
+        public Mat rgba();
+
+        /**
+         * This method returns single channel gray scale Mat with frame
+         */
+        public Mat gray();
+    };
+
+    public void surfaceChanged(SurfaceHolder arg0, int arg1, int arg2, int arg3) {
+        Log.d(TAG, "call surfaceChanged event");
+        synchronized(mSyncObject) {
+            if (!mSurfaceExist) {
+                mSurfaceExist = true;
+                checkCurrentState();
+            } else {
+                /** Surface changed. We need to stop camera and restart with new parameters */
+                /* Pretend that old surface has been destroyed */
+                mSurfaceExist = false;
+                checkCurrentState();
+                /* Now use new surface. Say we have it now */
+                mSurfaceExist = true;
+                checkCurrentState();
+            }
+        }
+    }
+
+    public void surfaceCreated(SurfaceHolder holder) {
+        /* Do nothing. Wait until surfaceChanged delivered */
+    }
+
+    public void surfaceDestroyed(SurfaceHolder holder) {
+        synchronized(mSyncObject) {
+            mSurfaceExist = false;
+            checkCurrentState();
+        }
+    }
+
+    /**
+     * This method is provided for clients, so they can enable the camera connection.
+     * The actual onCameraViewStarted callback will be delivered only after both this method is called and surface is available
+     */
+    public void enableView() {
+        synchronized(mSyncObject) {
+            mEnabled = true;
+            checkCurrentState();
+        }
+    }
+
+    /**
+     * This method is provided for clients, so they can disable camera connection and stop
+     * the delivery of frames even though the surface view itself is not destroyed and still stays on the screen
+     */
+    public void disableView() {
+        synchronized(mSyncObject) {
+            mEnabled = false;
+            checkCurrentState();
+        }
+    }
+
+    /**
+     * This method enables label with fps value on the screen
+     */
+    public void enableFpsMeter() {
+        if (mFpsMeter == null) {
+            mFpsMeter = new FpsMeter();
+            mFpsMeter.setResolution(mFrameWidth, mFrameHeight);
+        }
+    }
+
+    public void disableFpsMeter() {
+            mFpsMeter = null;
+    }
+
+    /**
+     *
+     * @param listener
+     */
+
+    public void setCvCameraViewListener(CvCameraViewListener2 listener) {
+        mListener = listener;
+    }
+
+    public void setCvCameraViewListener(CvCameraViewListener listener) {
+        CvCameraViewListenerAdapter adapter = new CvCameraViewListenerAdapter(listener);
+        adapter.setFrameFormat(mPreviewFormat);
+        mListener = adapter;
+    }
+
+    /**
+     * This method sets the maximum size that camera frame is allowed to be. When selecting
+     * size - the biggest size which less or equal the size set will be selected.
+     * As an example - we set setMaxFrameSize(200,200) and we have 176x152 and 320x240 sizes. The
+     * preview frame will be selected with 176x152 size.
+     * This method is useful when need to restrict the size of preview frame for some reason (for example for video recording)
+     * @param maxWidth - the maximum width allowed for camera frame.
+     * @param maxHeight - the maximum height allowed for camera frame
+     */
+    public void setMaxFrameSize(int maxWidth, int maxHeight) {
+        mMaxWidth = maxWidth;
+        mMaxHeight = maxHeight;
+    }
+
+    public void SetCaptureFormat(int format)
+    {
+        mPreviewFormat = format;
+        if (mListener instanceof CvCameraViewListenerAdapter) {
+            CvCameraViewListenerAdapter adapter = (CvCameraViewListenerAdapter) mListener;
+            adapter.setFrameFormat(mPreviewFormat);
+        }
+    }
+
+    /**
+     * Called when mSyncObject lock is held
+     */
+    private void checkCurrentState() {
+        Log.d(TAG, "call checkCurrentState");
+        int targetState;
+
+        if (mEnabled && mSurfaceExist && getVisibility() == VISIBLE) {
+            targetState = STARTED;
+        } else {
+            targetState = STOPPED;
+        }
+
+        if (targetState != mState) {
+            /* The state change detected. Need to exit the current state and enter target state */
+            processExitState(mState);
+            mState = targetState;
+            processEnterState(mState);
+        }
+    }
+
+    private void processEnterState(int state) {
+        Log.d(TAG, "call processEnterState: " + state);
+        switch(state) {
+        case STARTED:
+            onEnterStartedState();
+            if (mListener != null) {
+                mListener.onCameraViewStarted(mFrameWidth, mFrameHeight);
+            }
+            break;
+        case STOPPED:
+            onEnterStoppedState();
+            if (mListener != null) {
+                mListener.onCameraViewStopped();
+            }
+            break;
+        };
+    }
+
+    private void processExitState(int state) {
+        Log.d(TAG, "call processExitState: " + state);
+        switch(state) {
+        case STARTED:
+            onExitStartedState();
+            break;
+        case STOPPED:
+            onExitStoppedState();
+            break;
+        };
+    }
+
+    private void onEnterStoppedState() {
+        /* nothing to do */
+    }
+
+    private void onExitStoppedState() {
+        /* nothing to do */
+    }
+
+    // NOTE: The order of bitmap constructor and camera connection is important for android 4.1.x
+    // Bitmap must be constructed before surface
+    private void onEnterStartedState() {
+        Log.d(TAG, "call onEnterStartedState");
+        /* Connect camera */
+        if (!connectCamera(getWidth(), getHeight())) {
+            AlertDialog ad = new AlertDialog.Builder(getContext()).create();
+            ad.setCancelable(false); // This blocks the 'BACK' button
+            ad.setMessage("It seems that you device does not support camera (or it is locked). Application will be closed.");
+            ad.setButton(DialogInterface.BUTTON_NEUTRAL,  "OK", new DialogInterface.OnClickListener() {
+                public void onClick(DialogInterface dialog, int which) {
+                    dialog.dismiss();
+                    ((Activity) getContext()).finish();
+                }
+            });
+            ad.show();
+
+        }
+    }
+
+    private void onExitStartedState() {
+        disconnectCamera();
+        if (mCacheBitmap != null) {
+            mCacheBitmap.recycle();
+        }
+    }
+
+    /**
+     * This method shall be called by the subclasses when they have valid
+     * object and want it to be delivered to external client (via callback) and
+     * then displayed on the screen.
+     * @param frame - the current frame to be delivered
+     */
+    protected void deliverAndDrawFrame(CvCameraViewFrame frame) {
+        Mat modified;
+
+        if (mListener != null) {
+            modified = mListener.onCameraFrame(frame);
+        } else {
+            modified = frame.rgba();
+        }
+
+        boolean bmpValid = true;
+        if (modified != null) {
+            try {
+                Utils.matToBitmap(modified, mCacheBitmap);
+            } catch(Exception e) {
+                Log.e(TAG, "Mat type: " + modified);
+                Log.e(TAG, "Bitmap type: " + mCacheBitmap.getWidth() + "*" + mCacheBitmap.getHeight());
+                Log.e(TAG, "Utils.matToBitmap() throws an exception: " + e.getMessage());
+                bmpValid = false;
+            }
+        }
+
+        if (bmpValid && mCacheBitmap != null) {
+            Canvas canvas = getHolder().lockCanvas();
+            if (canvas != null) {
+                canvas.drawColor(0, android.graphics.PorterDuff.Mode.CLEAR);
+                if (BuildConfig.DEBUG)
+                    Log.d(TAG, "mStretch value: " + mScale);
+
+                if (mScale != 0) {
+                    canvas.drawBitmap(mCacheBitmap, new Rect(0,0,mCacheBitmap.getWidth(), mCacheBitmap.getHeight()),
+                         new Rect((int)((canvas.getWidth() - mScale*mCacheBitmap.getWidth()) / 2),
+                         (int)((canvas.getHeight() - mScale*mCacheBitmap.getHeight()) / 2),
+                         (int)((canvas.getWidth() - mScale*mCacheBitmap.getWidth()) / 2 + mScale*mCacheBitmap.getWidth()),
+                         (int)((canvas.getHeight() - mScale*mCacheBitmap.getHeight()) / 2 + mScale*mCacheBitmap.getHeight())), null);
+                } else {
+                     canvas.drawBitmap(mCacheBitmap, new Rect(0,0,mCacheBitmap.getWidth(), mCacheBitmap.getHeight()),
+                         new Rect((canvas.getWidth() - mCacheBitmap.getWidth()) / 2,
+                         (canvas.getHeight() - mCacheBitmap.getHeight()) / 2,
+                         (canvas.getWidth() - mCacheBitmap.getWidth()) / 2 + mCacheBitmap.getWidth(),
+                         (canvas.getHeight() - mCacheBitmap.getHeight()) / 2 + mCacheBitmap.getHeight()), null);
+                }
+
+                if (mFpsMeter != null) {
+                    mFpsMeter.measure();
+                    mFpsMeter.draw(canvas, 20, 30);
+                }
+                getHolder().unlockCanvasAndPost(canvas);
+            }
+        }
+    }
+
+    /**
+     * This method is invoked shall perform concrete operation to initialize the camera.
+     * CONTRACT: as a result of this method variables mFrameWidth and mFrameHeight MUST be
+     * initialized with the size of the Camera frames that will be delivered to external processor.
+     * @param width - the width of this SurfaceView
+     * @param height - the height of this SurfaceView
+     */
+    protected abstract boolean connectCamera(int width, int height);
+
+    /**
+     * Disconnects and release the particular camera object being connected to this surface view.
+     * Called when syncObject lock is held
+     */
+    protected abstract void disconnectCamera();
+
+    // NOTE: On Android 4.1.x the function must be called before SurfaceTexture constructor!
+    protected void AllocateCache()
+    {
+        mCacheBitmap = Bitmap.createBitmap(mFrameWidth, mFrameHeight, Bitmap.Config.ARGB_8888);
+    }
+
+    public interface ListItemAccessor {
+        public int getWidth(Object obj);
+        public int getHeight(Object obj);
+    };
+
+    /**
+     * This helper method can be called by subclasses to select camera preview size.
+     * It goes over the list of the supported preview sizes and selects the maximum one which
+     * fits both values set via setMaxFrameSize() and surface frame allocated for this view
+     * @param supportedSizes
+     * @param surfaceWidth
+     * @param surfaceHeight
+     * @return optimal frame size
+     */
+    protected Size calculateCameraFrameSize(List<?> supportedSizes, ListItemAccessor accessor, int surfaceWidth, int surfaceHeight) {
+        int calcWidth = 0;
+        int calcHeight = 0;
+
+        int maxAllowedWidth = (mMaxWidth != MAX_UNSPECIFIED && mMaxWidth < surfaceWidth)? mMaxWidth : surfaceWidth;
+        int maxAllowedHeight = (mMaxHeight != MAX_UNSPECIFIED && mMaxHeight < surfaceHeight)? mMaxHeight : surfaceHeight;
+
+        for (Object size : supportedSizes) {
+            int width = accessor.getWidth(size);
+            int height = accessor.getHeight(size);
+            Log.d(TAG, "trying size: " + width + "x" + height);
+
+            if (width <= maxAllowedWidth && height <= maxAllowedHeight) {
+                if (width >= calcWidth && height >= calcHeight) {
+                    calcWidth = (int) width;
+                    calcHeight = (int) height;
+                }
+            }
+        }
+        if ((calcWidth == 0 || calcHeight == 0) && supportedSizes.size() > 0)
+        {
+            Log.i(TAG, "fallback to the first frame size");
+            Object size = supportedSizes.get(0);
+            calcWidth = accessor.getWidth(size);
+            calcHeight = accessor.getHeight(size);
+        }
+
+        return new Size(calcWidth, calcHeight);
+    }
+}
Index: openCVLibrary3411/src/main/java/org/opencv/android/CameraGLRendererBase.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/android/CameraGLRendererBase.java	(date 1605830247089)
+++ openCVLibrary3411/src/main/java/org/opencv/android/CameraGLRendererBase.java	(date 1605830247089)
@@ -0,0 +1,440 @@
+package org.opencv.android;
+
+import java.nio.ByteBuffer;
+import java.nio.ByteOrder;
+import java.nio.FloatBuffer;
+
+import javax.microedition.khronos.egl.EGLConfig;
+import javax.microedition.khronos.opengles.GL10;
+
+import org.opencv.android.CameraGLSurfaceView.CameraTextureListener;
+
+import android.annotation.TargetApi;
+import android.graphics.SurfaceTexture;
+import android.opengl.GLES11Ext;
+import android.opengl.GLES20;
+import android.opengl.GLSurfaceView;
+import android.util.Log;
+import android.view.View;
+
+@TargetApi(15)
+public abstract class CameraGLRendererBase implements GLSurfaceView.Renderer, SurfaceTexture.OnFrameAvailableListener {
+
+    protected final String LOGTAG = "CameraGLRendererBase";
+
+    // shaders
+    private final String vss = ""
+            + "attribute vec2 vPosition;\n"
+            + "attribute vec2 vTexCoord;\n" + "varying vec2 texCoord;\n"
+            + "void main() {\n" + "  texCoord = vTexCoord;\n"
+            + "  gl_Position = vec4 ( vPosition.x, vPosition.y, 0.0, 1.0 );\n"
+            + "}";
+
+    private final String fssOES = ""
+            + "#extension GL_OES_EGL_image_external : require\n"
+            + "precision mediump float;\n"
+            + "uniform samplerExternalOES sTexture;\n"
+            + "varying vec2 texCoord;\n"
+            + "void main() {\n"
+            + "  gl_FragColor = texture2D(sTexture,texCoord);\n" + "}";
+
+    private final String fss2D = ""
+            + "precision mediump float;\n"
+            + "uniform sampler2D sTexture;\n"
+            + "varying vec2 texCoord;\n"
+            + "void main() {\n"
+            + "  gl_FragColor = texture2D(sTexture,texCoord);\n" + "}";
+
+    // coord-s
+    private final float vertices[] = {
+           -1, -1,
+           -1,  1,
+            1, -1,
+            1,  1 };
+    private final float texCoordOES[] = {
+            0,  1,
+            0,  0,
+            1,  1,
+            1,  0 };
+    private final float texCoord2D[] = {
+            0,  0,
+            0,  1,
+            1,  0,
+            1,  1 };
+
+    private int[] texCamera = {0}, texFBO = {0}, texDraw = {0};
+    private int[] FBO = {0};
+    private int progOES = -1, prog2D = -1;
+    private int vPosOES, vTCOES, vPos2D, vTC2D;
+
+    private FloatBuffer vert, texOES, tex2D;
+
+    protected int mCameraWidth = -1, mCameraHeight = -1;
+    protected int mFBOWidth = -1, mFBOHeight = -1;
+    protected int mMaxCameraWidth = -1, mMaxCameraHeight = -1;
+    protected int mCameraIndex = CameraBridgeViewBase.CAMERA_ID_ANY;
+
+    protected SurfaceTexture mSTexture;
+
+    protected boolean mHaveSurface = false;
+    protected boolean mHaveFBO = false;
+    protected boolean mUpdateST = false;
+    protected boolean mEnabled = true;
+    protected boolean mIsStarted = false;
+
+    protected CameraGLSurfaceView mView;
+
+    protected abstract void openCamera(int id);
+    protected abstract void closeCamera();
+    protected abstract void setCameraPreviewSize(int width, int height); // updates mCameraWidth & mCameraHeight
+
+    public CameraGLRendererBase(CameraGLSurfaceView view) {
+        mView = view;
+        int bytes = vertices.length * Float.SIZE / Byte.SIZE;
+        vert   = ByteBuffer.allocateDirect(bytes).order(ByteOrder.nativeOrder()).asFloatBuffer();
+        texOES = ByteBuffer.allocateDirect(bytes).order(ByteOrder.nativeOrder()).asFloatBuffer();
+        tex2D  = ByteBuffer.allocateDirect(bytes).order(ByteOrder.nativeOrder()).asFloatBuffer();
+        vert.put(vertices).position(0);
+        texOES.put(texCoordOES).position(0);
+        tex2D.put(texCoord2D).position(0);
+    }
+
+    @Override
+    public synchronized void onFrameAvailable(SurfaceTexture surfaceTexture) {
+        //Log.i(LOGTAG, "onFrameAvailable");
+        mUpdateST = true;
+        mView.requestRender();
+    }
+
+    @Override
+    public void onDrawFrame(GL10 gl) {
+        //Log.i(LOGTAG, "onDrawFrame start");
+
+        if (!mHaveFBO)
+            return;
+
+        synchronized(this) {
+            if (mUpdateST) {
+                mSTexture.updateTexImage();
+                mUpdateST = false;
+            }
+
+            GLES20.glClear(GLES20.GL_COLOR_BUFFER_BIT);
+
+            CameraTextureListener texListener = mView.getCameraTextureListener();
+            if(texListener != null) {
+                //Log.d(LOGTAG, "haveUserCallback");
+                // texCamera(OES) -> texFBO
+                drawTex(texCamera[0], true, FBO[0]);
+
+                // call user code (texFBO -> texDraw)
+                boolean modified = texListener.onCameraTexture(texFBO[0], texDraw[0], mCameraWidth, mCameraHeight);
+
+                if(modified) {
+                    // texDraw -> screen
+                    drawTex(texDraw[0], false, 0);
+                } else {
+                    // texFBO -> screen
+                    drawTex(texFBO[0], false, 0);
+                }
+            } else {
+                Log.d(LOGTAG, "texCamera(OES) -> screen");
+                // texCamera(OES) -> screen
+                drawTex(texCamera[0], true, 0);
+            }
+            //Log.i(LOGTAG, "onDrawFrame end");
+        }
+    }
+
+    @Override
+    public void onSurfaceChanged(GL10 gl, int surfaceWidth, int surfaceHeight) {
+        Log.i(LOGTAG, "onSurfaceChanged("+surfaceWidth+"x"+surfaceHeight+")");
+        mHaveSurface = true;
+        updateState();
+        setPreviewSize(surfaceWidth, surfaceHeight);
+    }
+
+    @Override
+    public void onSurfaceCreated(GL10 gl, EGLConfig config) {
+        Log.i(LOGTAG, "onSurfaceCreated");
+        initShaders();
+    }
+
+    private void initShaders() {
+        String strGLVersion = GLES20.glGetString(GLES20.GL_VERSION);
+        if (strGLVersion != null)
+            Log.i(LOGTAG, "OpenGL ES version: " + strGLVersion);
+
+        GLES20.glClearColor(1.0f, 1.0f, 1.0f, 1.0f);
+
+        progOES = loadShader(vss, fssOES);
+        vPosOES = GLES20.glGetAttribLocation(progOES, "vPosition");
+        vTCOES  = GLES20.glGetAttribLocation(progOES, "vTexCoord");
+        GLES20.glEnableVertexAttribArray(vPosOES);
+        GLES20.glEnableVertexAttribArray(vTCOES);
+
+        prog2D  = loadShader(vss, fss2D);
+        vPos2D = GLES20.glGetAttribLocation(prog2D, "vPosition");
+        vTC2D  = GLES20.glGetAttribLocation(prog2D, "vTexCoord");
+        GLES20.glEnableVertexAttribArray(vPos2D);
+        GLES20.glEnableVertexAttribArray(vTC2D);
+    }
+
+    private void initSurfaceTexture() {
+        Log.d(LOGTAG, "initSurfaceTexture");
+        deleteSurfaceTexture();
+        initTexOES(texCamera);
+        mSTexture = new SurfaceTexture(texCamera[0]);
+        mSTexture.setOnFrameAvailableListener(this);
+    }
+
+    private void deleteSurfaceTexture() {
+        Log.d(LOGTAG, "deleteSurfaceTexture");
+        if(mSTexture != null) {
+            mSTexture.release();
+            mSTexture = null;
+            deleteTex(texCamera);
+        }
+    }
+
+    private void initTexOES(int[] tex) {
+        if(tex.length == 1) {
+            GLES20.glGenTextures(1, tex, 0);
+            GLES20.glBindTexture(GLES11Ext.GL_TEXTURE_EXTERNAL_OES, tex[0]);
+            GLES20.glTexParameteri(GLES11Ext.GL_TEXTURE_EXTERNAL_OES, GLES20.GL_TEXTURE_WRAP_S, GLES20.GL_CLAMP_TO_EDGE);
+            GLES20.glTexParameteri(GLES11Ext.GL_TEXTURE_EXTERNAL_OES, GLES20.GL_TEXTURE_WRAP_T, GLES20.GL_CLAMP_TO_EDGE);
+            GLES20.glTexParameteri(GLES11Ext.GL_TEXTURE_EXTERNAL_OES, GLES20.GL_TEXTURE_MIN_FILTER, GLES20.GL_NEAREST);
+            GLES20.glTexParameteri(GLES11Ext.GL_TEXTURE_EXTERNAL_OES, GLES20.GL_TEXTURE_MAG_FILTER, GLES20.GL_NEAREST);
+        }
+    }
+
+    private static void deleteTex(int[] tex) {
+        if(tex.length == 1) {
+            GLES20.glDeleteTextures(1, tex, 0);
+        }
+    }
+
+    private static int loadShader(String vss, String fss) {
+        Log.d("CameraGLRendererBase", "loadShader");
+        int vshader = GLES20.glCreateShader(GLES20.GL_VERTEX_SHADER);
+        GLES20.glShaderSource(vshader, vss);
+        GLES20.glCompileShader(vshader);
+        int[] status = new int[1];
+        GLES20.glGetShaderiv(vshader, GLES20.GL_COMPILE_STATUS, status, 0);
+        if (status[0] == 0) {
+            Log.e("CameraGLRendererBase", "Could not compile vertex shader: "+GLES20.glGetShaderInfoLog(vshader));
+            GLES20.glDeleteShader(vshader);
+            vshader = 0;
+            return 0;
+        }
+
+        int fshader = GLES20.glCreateShader(GLES20.GL_FRAGMENT_SHADER);
+        GLES20.glShaderSource(fshader, fss);
+        GLES20.glCompileShader(fshader);
+        GLES20.glGetShaderiv(fshader, GLES20.GL_COMPILE_STATUS, status, 0);
+        if (status[0] == 0) {
+            Log.e("CameraGLRendererBase", "Could not compile fragment shader:"+GLES20.glGetShaderInfoLog(fshader));
+            GLES20.glDeleteShader(vshader);
+            GLES20.glDeleteShader(fshader);
+            fshader = 0;
+            return 0;
+        }
+
+        int program = GLES20.glCreateProgram();
+        GLES20.glAttachShader(program, vshader);
+        GLES20.glAttachShader(program, fshader);
+        GLES20.glLinkProgram(program);
+        GLES20.glDeleteShader(vshader);
+        GLES20.glDeleteShader(fshader);
+        GLES20.glGetProgramiv(program, GLES20.GL_LINK_STATUS, status, 0);
+        if (status[0] == 0) {
+            Log.e("CameraGLRendererBase", "Could not link shader program: "+GLES20.glGetProgramInfoLog(program));
+            program = 0;
+            return 0;
+        }
+        GLES20.glValidateProgram(program);
+        GLES20.glGetProgramiv(program, GLES20.GL_VALIDATE_STATUS, status, 0);
+        if (status[0] == 0)
+        {
+            Log.e("CameraGLRendererBase", "Shader program validation error: "+GLES20.glGetProgramInfoLog(program));
+            GLES20.glDeleteProgram(program);
+            program = 0;
+            return 0;
+        }
+
+        Log.d("CameraGLRendererBase", "Shader program is built OK");
+
+        return program;
+    }
+
+    private void deleteFBO()
+    {
+        Log.d(LOGTAG, "deleteFBO("+mFBOWidth+"x"+mFBOHeight+")");
+        GLES20.glBindFramebuffer(GLES20.GL_FRAMEBUFFER, 0);
+        GLES20.glDeleteFramebuffers(1, FBO, 0);
+
+        deleteTex(texFBO);
+        deleteTex(texDraw);
+        mFBOWidth = mFBOHeight = 0;
+    }
+
+    private void initFBO(int width, int height)
+    {
+        Log.d(LOGTAG, "initFBO("+width+"x"+height+")");
+
+        deleteFBO();
+
+        GLES20.glGenTextures(1, texDraw, 0);
+        GLES20.glBindTexture(GLES20.GL_TEXTURE_2D, texDraw[0]);
+        GLES20.glTexImage2D(GLES20.GL_TEXTURE_2D, 0, GLES20.GL_RGBA, width, height, 0, GLES20.GL_RGBA, GLES20.GL_UNSIGNED_BYTE, null);
+        GLES20.glTexParameteri(GLES20.GL_TEXTURE_2D, GLES20.GL_TEXTURE_WRAP_S, GLES20.GL_CLAMP_TO_EDGE);
+        GLES20.glTexParameteri(GLES20.GL_TEXTURE_2D, GLES20.GL_TEXTURE_WRAP_T, GLES20.GL_CLAMP_TO_EDGE);
+        GLES20.glTexParameteri(GLES20.GL_TEXTURE_2D, GLES20.GL_TEXTURE_MIN_FILTER, GLES20.GL_NEAREST);
+        GLES20.glTexParameteri(GLES20.GL_TEXTURE_2D, GLES20.GL_TEXTURE_MAG_FILTER, GLES20.GL_NEAREST);
+
+        GLES20.glGenTextures(1, texFBO, 0);
+        GLES20.glBindTexture(GLES20.GL_TEXTURE_2D, texFBO[0]);
+        GLES20.glTexImage2D(GLES20.GL_TEXTURE_2D, 0, GLES20.GL_RGBA, width, height, 0, GLES20.GL_RGBA, GLES20.GL_UNSIGNED_BYTE, null);
+        GLES20.glTexParameteri(GLES20.GL_TEXTURE_2D, GLES20.GL_TEXTURE_WRAP_S, GLES20.GL_CLAMP_TO_EDGE);
+        GLES20.glTexParameteri(GLES20.GL_TEXTURE_2D, GLES20.GL_TEXTURE_WRAP_T, GLES20.GL_CLAMP_TO_EDGE);
+        GLES20.glTexParameteri(GLES20.GL_TEXTURE_2D, GLES20.GL_TEXTURE_MIN_FILTER, GLES20.GL_NEAREST);
+        GLES20.glTexParameteri(GLES20.GL_TEXTURE_2D, GLES20.GL_TEXTURE_MAG_FILTER, GLES20.GL_NEAREST);
+
+        //int hFBO;
+        GLES20.glGenFramebuffers(1, FBO, 0);
+        GLES20.glBindFramebuffer(GLES20.GL_FRAMEBUFFER, FBO[0]);
+        GLES20.glFramebufferTexture2D(GLES20.GL_FRAMEBUFFER, GLES20.GL_COLOR_ATTACHMENT0, GLES20.GL_TEXTURE_2D, texFBO[0], 0);
+        Log.d(LOGTAG, "initFBO error status: " + GLES20.glGetError());
+
+        int FBOstatus = GLES20.glCheckFramebufferStatus(GLES20.GL_FRAMEBUFFER);
+        if (FBOstatus != GLES20.GL_FRAMEBUFFER_COMPLETE)
+            Log.e(LOGTAG, "initFBO failed, status: " + FBOstatus);
+
+        mFBOWidth  = width;
+        mFBOHeight = height;
+    }
+
+    // draw texture to FBO or to screen if fbo == 0
+    private void drawTex(int tex, boolean isOES, int fbo)
+    {
+        GLES20.glBindFramebuffer(GLES20.GL_FRAMEBUFFER, fbo);
+
+        if(fbo == 0)
+            GLES20.glViewport(0, 0, mView.getWidth(), mView.getHeight());
+        else
+            GLES20.glViewport(0, 0, mFBOWidth, mFBOHeight);
+
+        GLES20.glClear(GLES20.GL_COLOR_BUFFER_BIT);
+
+        if(isOES) {
+            GLES20.glUseProgram(progOES);
+            GLES20.glVertexAttribPointer(vPosOES, 2, GLES20.GL_FLOAT, false, 4*2, vert);
+            GLES20.glVertexAttribPointer(vTCOES,  2, GLES20.GL_FLOAT, false, 4*2, texOES);
+        } else {
+            GLES20.glUseProgram(prog2D);
+            GLES20.glVertexAttribPointer(vPos2D, 2, GLES20.GL_FLOAT, false, 4*2, vert);
+            GLES20.glVertexAttribPointer(vTC2D,  2, GLES20.GL_FLOAT, false, 4*2, tex2D);
+        }
+
+        GLES20.glActiveTexture(GLES20.GL_TEXTURE0);
+
+        if(isOES) {
+            GLES20.glBindTexture(GLES11Ext.GL_TEXTURE_EXTERNAL_OES, tex);
+            GLES20.glUniform1i(GLES20.glGetUniformLocation(progOES, "sTexture"), 0);
+        } else {
+            GLES20.glBindTexture(GLES20.GL_TEXTURE_2D, tex);
+            GLES20.glUniform1i(GLES20.glGetUniformLocation(prog2D, "sTexture"), 0);
+        }
+
+        GLES20.glDrawArrays(GLES20.GL_TRIANGLE_STRIP, 0, 4);
+        GLES20.glFlush();
+    }
+
+    public synchronized void enableView() {
+        Log.d(LOGTAG, "enableView");
+        mEnabled = true;
+        updateState();
+    }
+
+    public synchronized void disableView() {
+        Log.d(LOGTAG, "disableView");
+        mEnabled = false;
+        updateState();
+    }
+
+    protected void updateState() {
+        Log.d(LOGTAG, "updateState");
+        Log.d(LOGTAG, "mEnabled="+mEnabled+", mHaveSurface="+mHaveSurface);
+        boolean willStart = mEnabled && mHaveSurface && mView.getVisibility() == View.VISIBLE;
+        if (willStart != mIsStarted) {
+            if(willStart) doStart();
+            else doStop();
+        } else {
+            Log.d(LOGTAG, "keeping State unchanged");
+        }
+        Log.d(LOGTAG, "updateState end");
+    }
+
+    protected synchronized void doStart() {
+        Log.d(LOGTAG, "doStart");
+        initSurfaceTexture();
+        openCamera(mCameraIndex);
+        mIsStarted = true;
+        if(mCameraWidth>0 && mCameraHeight>0)
+            setPreviewSize(mCameraWidth, mCameraHeight); // start preview and call listener.onCameraViewStarted()
+    }
+
+
+    protected void doStop() {
+        Log.d(LOGTAG, "doStop");
+        synchronized(this) {
+            mUpdateST = false;
+            mIsStarted = false;
+            mHaveFBO = false;
+            closeCamera();
+            deleteSurfaceTexture();
+        }
+        CameraTextureListener listener = mView.getCameraTextureListener();
+        if(listener != null) listener.onCameraViewStopped();
+
+    }
+
+    protected void setPreviewSize(int width, int height) {
+        synchronized(this) {
+            mHaveFBO = false;
+            mCameraWidth  = width;
+            mCameraHeight = height;
+            setCameraPreviewSize(width, height); // can change mCameraWidth & mCameraHeight
+            initFBO(mCameraWidth, mCameraHeight);
+            mHaveFBO = true;
+        }
+
+        CameraTextureListener listener = mView.getCameraTextureListener();
+        if(listener != null) listener.onCameraViewStarted(mCameraWidth, mCameraHeight);
+    }
+
+    public void setCameraIndex(int cameraIndex) {
+        disableView();
+        mCameraIndex = cameraIndex;
+        enableView();
+    }
+
+    public void setMaxCameraPreviewSize(int maxWidth, int maxHeight) {
+        disableView();
+        mMaxCameraWidth  = maxWidth;
+        mMaxCameraHeight = maxHeight;
+        enableView();
+    }
+
+    public void onResume() {
+        Log.i(LOGTAG, "onResume");
+    }
+
+    public void onPause() {
+        Log.i(LOGTAG, "onPause");
+        mHaveSurface = false;
+        updateState();
+        mCameraWidth = mCameraHeight = -1;
+    }
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/android/CameraGLSurfaceView.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/android/CameraGLSurfaceView.java	(date 1605830247099)
+++ openCVLibrary3411/src/main/java/org/opencv/android/CameraGLSurfaceView.java	(date 1605830247099)
@@ -0,0 +1,119 @@
+package org.opencv.android;
+
+import org.opencv.R;
+
+import android.content.Context;
+import android.content.res.TypedArray;
+import android.opengl.GLSurfaceView;
+import android.util.AttributeSet;
+import android.util.Log;
+import android.view.SurfaceHolder;
+
+public class CameraGLSurfaceView extends GLSurfaceView {
+
+    private static final String LOGTAG = "CameraGLSurfaceView";
+
+    public interface CameraTextureListener {
+        /**
+         * This method is invoked when camera preview has started. After this method is invoked
+         * the frames will start to be delivered to client via the onCameraFrame() callback.
+         * @param width -  the width of the frames that will be delivered
+         * @param height - the height of the frames that will be delivered
+         */
+        public void onCameraViewStarted(int width, int height);
+
+        /**
+         * This method is invoked when camera preview has been stopped for some reason.
+         * No frames will be delivered via onCameraFrame() callback after this method is called.
+         */
+        public void onCameraViewStopped();
+
+        /**
+         * This method is invoked when a new preview frame from Camera is ready.
+         * @param texIn -  the OpenGL texture ID that contains frame in RGBA format
+         * @param texOut - the OpenGL texture ID that can be used to store modified frame image t display
+         * @param width -  the width of the frame
+         * @param height - the height of the frame
+         * @return `true` if `texOut` should be displayed, `false` - to show `texIn`
+         */
+        public boolean onCameraTexture(int texIn, int texOut, int width, int height);
+    };
+
+    private CameraTextureListener mTexListener;
+    private CameraGLRendererBase mRenderer;
+
+    public CameraGLSurfaceView(Context context, AttributeSet attrs) {
+        super(context, attrs);
+
+        TypedArray styledAttrs = getContext().obtainStyledAttributes(attrs, R.styleable.CameraBridgeViewBase);
+        int cameraIndex = styledAttrs.getInt(R.styleable.CameraBridgeViewBase_camera_id, -1);
+        styledAttrs.recycle();
+
+        if(android.os.Build.VERSION.SDK_INT >= 21)
+            mRenderer = new Camera2Renderer(this);
+        else
+            mRenderer = new CameraRenderer(this);
+
+        setCameraIndex(cameraIndex);
+
+        setEGLContextClientVersion(2);
+        setRenderer(mRenderer);
+        setRenderMode(GLSurfaceView.RENDERMODE_WHEN_DIRTY);
+    }
+
+    public void setCameraTextureListener(CameraTextureListener texListener)
+    {
+        mTexListener = texListener;
+    }
+
+    public CameraTextureListener getCameraTextureListener()
+    {
+        return mTexListener;
+    }
+
+    public void setCameraIndex(int cameraIndex) {
+        mRenderer.setCameraIndex(cameraIndex);
+    }
+
+    public void setMaxCameraPreviewSize(int maxWidth, int maxHeight) {
+        mRenderer.setMaxCameraPreviewSize(maxWidth, maxHeight);
+    }
+
+    @Override
+    public void surfaceCreated(SurfaceHolder holder) {
+        super.surfaceCreated(holder);
+    }
+
+    @Override
+    public void surfaceDestroyed(SurfaceHolder holder) {
+        mRenderer.mHaveSurface = false;
+        super.surfaceDestroyed(holder);
+    }
+
+    @Override
+    public void surfaceChanged(SurfaceHolder holder, int format, int w, int h) {
+        super.surfaceChanged(holder, format, w, h);
+    }
+
+    @Override
+    public void onResume() {
+        Log.i(LOGTAG, "onResume");
+        super.onResume();
+        mRenderer.onResume();
+    }
+
+    @Override
+    public void onPause() {
+        Log.i(LOGTAG, "onPause");
+        mRenderer.onPause();
+        super.onPause();
+    }
+
+    public void enableView() {
+        mRenderer.enableView();
+    }
+
+    public void disableView() {
+        mRenderer.disableView();
+    }
+}
Index: openCVLibrary3411/src/main/java/org/opencv/android/CameraRenderer.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/android/CameraRenderer.java	(date 1605830247112)
+++ openCVLibrary3411/src/main/java/org/opencv/android/CameraRenderer.java	(date 1605830247112)
@@ -0,0 +1,166 @@
+package org.opencv.android;
+
+import java.io.IOException;
+import java.util.List;
+
+import android.annotation.TargetApi;
+import android.hardware.Camera;
+import android.hardware.Camera.Size;
+import android.os.Build;
+import android.util.Log;
+
+@TargetApi(15)
+@SuppressWarnings("deprecation")
+public class CameraRenderer extends CameraGLRendererBase {
+
+    public static final String LOGTAG = "CameraRenderer";
+
+    private Camera mCamera;
+    private boolean mPreviewStarted = false;
+
+    CameraRenderer(CameraGLSurfaceView view) {
+        super(view);
+    }
+
+    @Override
+    protected synchronized void closeCamera() {
+        Log.i(LOGTAG, "closeCamera");
+        if(mCamera != null) {
+            mCamera.stopPreview();
+            mPreviewStarted = false;
+            mCamera.release();
+            mCamera = null;
+        }
+    }
+
+    @Override
+    protected synchronized void openCamera(int id) {
+        Log.i(LOGTAG, "openCamera");
+        closeCamera();
+        if (id == CameraBridgeViewBase.CAMERA_ID_ANY) {
+            Log.d(LOGTAG, "Trying to open camera with old open()");
+            try {
+                mCamera = Camera.open();
+            }
+            catch (Exception e){
+                Log.e(LOGTAG, "Camera is not available (in use or does not exist): " + e.getLocalizedMessage());
+            }
+
+            if(mCamera == null && Build.VERSION.SDK_INT >= Build.VERSION_CODES.GINGERBREAD) {
+                boolean connected = false;
+                for (int camIdx = 0; camIdx < Camera.getNumberOfCameras(); ++camIdx) {
+                    Log.d(LOGTAG, "Trying to open camera with new open(" + camIdx + ")");
+                    try {
+                        mCamera = Camera.open(camIdx);
+                        connected = true;
+                    } catch (RuntimeException e) {
+                        Log.e(LOGTAG, "Camera #" + camIdx + "failed to open: " + e.getLocalizedMessage());
+                    }
+                    if (connected) break;
+                }
+            }
+        } else {
+            if (Build.VERSION.SDK_INT >= Build.VERSION_CODES.GINGERBREAD) {
+                int localCameraIndex = mCameraIndex;
+                if (mCameraIndex == CameraBridgeViewBase.CAMERA_ID_BACK) {
+                    Log.i(LOGTAG, "Trying to open BACK camera");
+                    Camera.CameraInfo cameraInfo = new Camera.CameraInfo();
+                    for (int camIdx = 0; camIdx < Camera.getNumberOfCameras(); ++camIdx) {
+                        Camera.getCameraInfo( camIdx, cameraInfo );
+                        if (cameraInfo.facing == Camera.CameraInfo.CAMERA_FACING_BACK) {
+                            localCameraIndex = camIdx;
+                            break;
+                        }
+                    }
+                } else if (mCameraIndex == CameraBridgeViewBase.CAMERA_ID_FRONT) {
+                    Log.i(LOGTAG, "Trying to open FRONT camera");
+                    Camera.CameraInfo cameraInfo = new Camera.CameraInfo();
+                    for (int camIdx = 0; camIdx < Camera.getNumberOfCameras(); ++camIdx) {
+                        Camera.getCameraInfo( camIdx, cameraInfo );
+                        if (cameraInfo.facing == Camera.CameraInfo.CAMERA_FACING_FRONT) {
+                            localCameraIndex = camIdx;
+                            break;
+                        }
+                    }
+                }
+                if (localCameraIndex == CameraBridgeViewBase.CAMERA_ID_BACK) {
+                    Log.e(LOGTAG, "Back camera not found!");
+                } else if (localCameraIndex == CameraBridgeViewBase.CAMERA_ID_FRONT) {
+                    Log.e(LOGTAG, "Front camera not found!");
+                } else {
+                    Log.d(LOGTAG, "Trying to open camera with new open(" + localCameraIndex + ")");
+                    try {
+                        mCamera = Camera.open(localCameraIndex);
+                    } catch (RuntimeException e) {
+                        Log.e(LOGTAG, "Camera #" + localCameraIndex + "failed to open: " + e.getLocalizedMessage());
+                    }
+                }
+            }
+        }
+        if(mCamera == null) {
+            Log.e(LOGTAG, "Error: can't open camera");
+            return;
+        }
+        Camera.Parameters params = mCamera.getParameters();
+        List<String> FocusModes = params.getSupportedFocusModes();
+        if (FocusModes != null && FocusModes.contains(Camera.Parameters.FOCUS_MODE_CONTINUOUS_VIDEO))
+        {
+            params.setFocusMode(Camera.Parameters.FOCUS_MODE_CONTINUOUS_VIDEO);
+        }
+        mCamera.setParameters(params);
+
+        try {
+            mCamera.setPreviewTexture(mSTexture);
+        } catch (IOException ioe) {
+            Log.e(LOGTAG, "setPreviewTexture() failed: " + ioe.getMessage());
+        }
+    }
+
+    @Override
+    public synchronized void setCameraPreviewSize(int width, int height) {
+        Log.i(LOGTAG, "setCameraPreviewSize: "+width+"x"+height);
+        if(mCamera == null) {
+            Log.e(LOGTAG, "Camera isn't initialized!");
+            return;
+        }
+
+        if(mMaxCameraWidth  > 0 && mMaxCameraWidth  < width)  width  = mMaxCameraWidth;
+        if(mMaxCameraHeight > 0 && mMaxCameraHeight < height) height = mMaxCameraHeight;
+
+        Camera.Parameters param = mCamera.getParameters();
+        List<Size> psize = param.getSupportedPreviewSizes();
+        int bestWidth = 0, bestHeight = 0;
+        if (psize.size() > 0) {
+            float aspect = (float)width / height;
+            for (Size size : psize) {
+                int w = size.width, h = size.height;
+                Log.d(LOGTAG, "checking camera preview size: "+w+"x"+h);
+                if ( w <= width && h <= height &&
+                     w >= bestWidth && h >= bestHeight &&
+                     Math.abs(aspect - (float)w/h) < 0.2 ) {
+                    bestWidth = w;
+                    bestHeight = h;
+                }
+            }
+            if(bestWidth <= 0 || bestHeight <= 0) {
+                bestWidth  = psize.get(0).width;
+                bestHeight = psize.get(0).height;
+                Log.e(LOGTAG, "Error: best size was not selected, using "+bestWidth+" x "+bestHeight);
+            } else {
+                Log.i(LOGTAG, "Selected best size: "+bestWidth+" x "+bestHeight);
+            }
+
+            if(mPreviewStarted) {
+                mCamera.stopPreview();
+                mPreviewStarted = false;
+            }
+            mCameraWidth  = bestWidth;
+            mCameraHeight = bestHeight;
+            param.setPreviewSize(bestWidth, bestHeight);
+        }
+        param.set("orientation", "landscape");
+        mCamera.setParameters(param);
+        mCamera.startPreview();
+        mPreviewStarted = true;
+    }
+}
\ No newline at end of file
Index: openCVLibrary3411/src/main/java/org/opencv/video/Video.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/video/Video.java	(date 1605830248324)
+++ openCVLibrary3411/src/main/java/org/opencv/video/Video.java	(date 1605830248324)
@@ -0,0 +1,1030 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.video;
+
+import java.util.ArrayList;
+import java.util.List;
+import org.opencv.core.Mat;
+import org.opencv.core.MatOfByte;
+import org.opencv.core.MatOfFloat;
+import org.opencv.core.MatOfPoint2f;
+import org.opencv.core.Rect;
+import org.opencv.core.RotatedRect;
+import org.opencv.core.Size;
+import org.opencv.core.TermCriteria;
+import org.opencv.utils.Converters;
+import org.opencv.video.BackgroundSubtractorKNN;
+import org.opencv.video.BackgroundSubtractorMOG2;
+import org.opencv.video.DualTVL1OpticalFlow;
+
+// C++: class Video
+
+public class Video {
+
+    private static final int
+            CV_LKFLOW_INITIAL_GUESSES = 4,
+            CV_LKFLOW_GET_MIN_EIGENVALS = 8;
+
+
+    // C++: enum <unnamed>
+    public static final int
+            OPTFLOW_USE_INITIAL_FLOW = 4,
+            OPTFLOW_LK_GET_MIN_EIGENVALS = 8,
+            OPTFLOW_FARNEBACK_GAUSSIAN = 256,
+            MOTION_TRANSLATION = 0,
+            MOTION_EUCLIDEAN = 1,
+            MOTION_AFFINE = 2,
+            MOTION_HOMOGRAPHY = 3;
+
+
+    //
+    // C++:  Mat cv::estimateRigidTransform(Mat src, Mat dst, bool fullAffine, int ransacMaxIters, double ransacGoodRatio, int ransacSize0)
+    //
+
+    public static Mat estimateRigidTransform(Mat src, Mat dst, boolean fullAffine, int ransacMaxIters, double ransacGoodRatio, int ransacSize0) {
+        return new Mat(estimateRigidTransform_0(src.nativeObj, dst.nativeObj, fullAffine, ransacMaxIters, ransacGoodRatio, ransacSize0));
+    }
+
+
+    //
+    // C++:  Mat cv::estimateRigidTransform(Mat src, Mat dst, bool fullAffine)
+    //
+
+    /**
+     * Computes an optimal affine transformation between two 2D point sets.
+     *
+     * @param src First input 2D point set stored in std::vector or Mat, or an image stored in Mat.
+     * @param dst Second input 2D point set of the same size and the same type as A, or another image.
+     * @param fullAffine If true, the function finds an optimal affine transformation with no additional
+     * restrictions (6 degrees of freedom). Otherwise, the class of transformations to choose from is
+     * limited to combinations of translation, rotation, and uniform scaling (4 degrees of freedom).
+     *
+     * The function finds an optimal affine transform *[A|b]* (a 2 x 3 floating-point matrix) that
+     * approximates best the affine transformation between:
+     *
+     * Two point sets
+     * Two raster images. In this case, the function first finds some features in the src image and
+     *     finds the corresponding features in dst image. After that, the problem is reduced to the first
+     *     case.
+     * In case of point sets, the problem is formulated as follows: you need to find a 2x2 matrix *A* and
+     * 2x1 vector *b* so that:
+     *
+     * \([A^*|b^*] = arg  \min _{[A|b]}  \sum _i  \| \texttt{dst}[i] - A { \texttt{src}[i]}^T - b  \| ^2\)
+     * where src[i] and dst[i] are the i-th points in src and dst, respectively
+     * \([A|b]\) can be either arbitrary (when fullAffine=true ) or have a form of
+     * \(\begin{bmatrix} a_{11} &amp; a_{12} &amp; b_1  \\ -a_{12} &amp; a_{11} &amp; b_2  \end{bmatrix}\)
+     * when fullAffine=false.
+     *
+     * SEE:
+     * estimateAffine2D, estimateAffinePartial2D, getAffineTransform, getPerspectiveTransform, findHomography
+     * @return automatically generated
+     */
+    public static Mat estimateRigidTransform(Mat src, Mat dst, boolean fullAffine) {
+        return new Mat(estimateRigidTransform_1(src.nativeObj, dst.nativeObj, fullAffine));
+    }
+
+
+    //
+    // C++:  Ptr_BackgroundSubtractorKNN cv::createBackgroundSubtractorKNN(int history = 500, double dist2Threshold = 400.0, bool detectShadows = true)
+    //
+
+    /**
+     * Creates KNN Background Subtractor
+     *
+     * @param history Length of the history.
+     * @param dist2Threshold Threshold on the squared distance between the pixel and the sample to decide
+     * whether a pixel is close to that sample. This parameter does not affect the background update.
+     * @param detectShadows If true, the algorithm will detect shadows and mark them. It decreases the
+     * speed a bit, so if you do not need this feature, set the parameter to false.
+     * @return automatically generated
+     */
+    public static BackgroundSubtractorKNN createBackgroundSubtractorKNN(int history, double dist2Threshold, boolean detectShadows) {
+        return BackgroundSubtractorKNN.__fromPtr__(createBackgroundSubtractorKNN_0(history, dist2Threshold, detectShadows));
+    }
+
+    /**
+     * Creates KNN Background Subtractor
+     *
+     * @param history Length of the history.
+     * @param dist2Threshold Threshold on the squared distance between the pixel and the sample to decide
+     * whether a pixel is close to that sample. This parameter does not affect the background update.
+     * speed a bit, so if you do not need this feature, set the parameter to false.
+     * @return automatically generated
+     */
+    public static BackgroundSubtractorKNN createBackgroundSubtractorKNN(int history, double dist2Threshold) {
+        return BackgroundSubtractorKNN.__fromPtr__(createBackgroundSubtractorKNN_1(history, dist2Threshold));
+    }
+
+    /**
+     * Creates KNN Background Subtractor
+     *
+     * @param history Length of the history.
+     * whether a pixel is close to that sample. This parameter does not affect the background update.
+     * speed a bit, so if you do not need this feature, set the parameter to false.
+     * @return automatically generated
+     */
+    public static BackgroundSubtractorKNN createBackgroundSubtractorKNN(int history) {
+        return BackgroundSubtractorKNN.__fromPtr__(createBackgroundSubtractorKNN_2(history));
+    }
+
+    /**
+     * Creates KNN Background Subtractor
+     *
+     * whether a pixel is close to that sample. This parameter does not affect the background update.
+     * speed a bit, so if you do not need this feature, set the parameter to false.
+     * @return automatically generated
+     */
+    public static BackgroundSubtractorKNN createBackgroundSubtractorKNN() {
+        return BackgroundSubtractorKNN.__fromPtr__(createBackgroundSubtractorKNN_3());
+    }
+
+
+    //
+    // C++:  Ptr_BackgroundSubtractorMOG2 cv::createBackgroundSubtractorMOG2(int history = 500, double varThreshold = 16, bool detectShadows = true)
+    //
+
+    /**
+     * Creates MOG2 Background Subtractor
+     *
+     * @param history Length of the history.
+     * @param varThreshold Threshold on the squared Mahalanobis distance between the pixel and the model
+     * to decide whether a pixel is well described by the background model. This parameter does not
+     * affect the background update.
+     * @param detectShadows If true, the algorithm will detect shadows and mark them. It decreases the
+     * speed a bit, so if you do not need this feature, set the parameter to false.
+     * @return automatically generated
+     */
+    public static BackgroundSubtractorMOG2 createBackgroundSubtractorMOG2(int history, double varThreshold, boolean detectShadows) {
+        return BackgroundSubtractorMOG2.__fromPtr__(createBackgroundSubtractorMOG2_0(history, varThreshold, detectShadows));
+    }
+
+    /**
+     * Creates MOG2 Background Subtractor
+     *
+     * @param history Length of the history.
+     * @param varThreshold Threshold on the squared Mahalanobis distance between the pixel and the model
+     * to decide whether a pixel is well described by the background model. This parameter does not
+     * affect the background update.
+     * speed a bit, so if you do not need this feature, set the parameter to false.
+     * @return automatically generated
+     */
+    public static BackgroundSubtractorMOG2 createBackgroundSubtractorMOG2(int history, double varThreshold) {
+        return BackgroundSubtractorMOG2.__fromPtr__(createBackgroundSubtractorMOG2_1(history, varThreshold));
+    }
+
+    /**
+     * Creates MOG2 Background Subtractor
+     *
+     * @param history Length of the history.
+     * to decide whether a pixel is well described by the background model. This parameter does not
+     * affect the background update.
+     * speed a bit, so if you do not need this feature, set the parameter to false.
+     * @return automatically generated
+     */
+    public static BackgroundSubtractorMOG2 createBackgroundSubtractorMOG2(int history) {
+        return BackgroundSubtractorMOG2.__fromPtr__(createBackgroundSubtractorMOG2_2(history));
+    }
+
+    /**
+     * Creates MOG2 Background Subtractor
+     *
+     * to decide whether a pixel is well described by the background model. This parameter does not
+     * affect the background update.
+     * speed a bit, so if you do not need this feature, set the parameter to false.
+     * @return automatically generated
+     */
+    public static BackgroundSubtractorMOG2 createBackgroundSubtractorMOG2() {
+        return BackgroundSubtractorMOG2.__fromPtr__(createBackgroundSubtractorMOG2_3());
+    }
+
+
+    //
+    // C++:  Ptr_DualTVL1OpticalFlow cv::createOptFlow_DualTVL1()
+    //
+
+    /**
+     * Creates instance of cv::DenseOpticalFlow
+     * @return automatically generated
+     */
+    public static DualTVL1OpticalFlow createOptFlow_DualTVL1() {
+        return DualTVL1OpticalFlow.__fromPtr__(createOptFlow_DualTVL1_0());
+    }
+
+
+    //
+    // C++:  RotatedRect cv::CamShift(Mat probImage, Rect& window, TermCriteria criteria)
+    //
+
+    /**
+     * Finds an object center, size, and orientation.
+     *
+     * @param probImage Back projection of the object histogram. See calcBackProject.
+     * @param window Initial search window.
+     * @param criteria Stop criteria for the underlying meanShift.
+     * returns
+     * (in old interfaces) Number of iterations CAMSHIFT took to converge
+     * The function implements the CAMSHIFT object tracking algorithm CITE: Bradski98 . First, it finds an
+     * object center using meanShift and then adjusts the window size and finds the optimal rotation. The
+     * function returns the rotated rectangle structure that includes the object position, size, and
+     * orientation. The next position of the search window can be obtained with RotatedRect::boundingRect()
+     *
+     * See the OpenCV sample camshiftdemo.c that tracks colored objects.
+     *
+     * <b>Note:</b>
+     * <ul>
+     *   <li>
+     *    (Python) A sample explaining the camshift tracking algorithm can be found at
+     *     opencv_source_code/samples/python/camshift.py
+     *   </li>
+     * </ul>
+     * @return automatically generated
+     */
+    public static RotatedRect CamShift(Mat probImage, Rect window, TermCriteria criteria) {
+        double[] window_out = new double[4];
+        RotatedRect retVal = new RotatedRect(CamShift_0(probImage.nativeObj, window.x, window.y, window.width, window.height, window_out, criteria.type, criteria.maxCount, criteria.epsilon));
+        if(window!=null){ window.x = (int)window_out[0]; window.y = (int)window_out[1]; window.width = (int)window_out[2]; window.height = (int)window_out[3]; } 
+        return retVal;
+    }
+
+
+    //
+    // C++:  double cv::computeECC(Mat templateImage, Mat inputImage, Mat inputMask = Mat())
+    //
+
+    /**
+     * Computes the Enhanced Correlation Coefficient value between two images CITE: EP08 .
+     *
+     * @param templateImage single-channel template image; CV_8U or CV_32F array.
+     * @param inputImage single-channel input image to be warped to provide an image similar to
+     *  templateImage, same type as templateImage.
+     * @param inputMask An optional mask to indicate valid values of inputImage.
+     *
+     * SEE:
+     * findTransformECC
+     * @return automatically generated
+     */
+    public static double computeECC(Mat templateImage, Mat inputImage, Mat inputMask) {
+        return computeECC_0(templateImage.nativeObj, inputImage.nativeObj, inputMask.nativeObj);
+    }
+
+    /**
+     * Computes the Enhanced Correlation Coefficient value between two images CITE: EP08 .
+     *
+     * @param templateImage single-channel template image; CV_8U or CV_32F array.
+     * @param inputImage single-channel input image to be warped to provide an image similar to
+     *  templateImage, same type as templateImage.
+     *
+     * SEE:
+     * findTransformECC
+     * @return automatically generated
+     */
+    public static double computeECC(Mat templateImage, Mat inputImage) {
+        return computeECC_1(templateImage.nativeObj, inputImage.nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::findTransformECC(Mat templateImage, Mat inputImage, Mat& warpMatrix, int motionType, TermCriteria criteria, Mat inputMask, int gaussFiltSize)
+    //
+
+    /**
+     * Finds the geometric transform (warp) between two images in terms of the ECC criterion CITE: EP08 .
+     *
+     * @param templateImage single-channel template image; CV_8U or CV_32F array.
+     * @param inputImage single-channel input image which should be warped with the final warpMatrix in
+     * order to provide an image similar to templateImage, same type as templateImage.
+     * @param warpMatrix floating-point \(2\times 3\) or \(3\times 3\) mapping matrix (warp).
+     * @param motionType parameter, specifying the type of motion:
+     * <ul>
+     *   <li>
+     *     <b>MOTION_TRANSLATION</b> sets a translational motion model; warpMatrix is \(2\times 3\) with
+     *      the first \(2\times 2\) part being the unity matrix and the rest two parameters being
+     *      estimated.
+     *   </li>
+     *   <li>
+     *     <b>MOTION_EUCLIDEAN</b> sets a Euclidean (rigid) transformation as motion model; three
+     *      parameters are estimated; warpMatrix is \(2\times 3\).
+     *   </li>
+     *   <li>
+     *     <b>MOTION_AFFINE</b> sets an affine motion model (DEFAULT); six parameters are estimated;
+     *      warpMatrix is \(2\times 3\).
+     *   </li>
+     *   <li>
+     *     <b>MOTION_HOMOGRAPHY</b> sets a homography as a motion model; eight parameters are
+     *      estimated;\{@code warpMatrix\} is \(3\times 3\).
+     * @param criteria parameter, specifying the termination criteria of the ECC algorithm;
+     * criteria.epsilon defines the threshold of the increment in the correlation coefficient between two
+     * iterations (a negative criteria.epsilon makes criteria.maxcount the only termination criterion).
+     * Default values are shown in the declaration above.
+     * @param inputMask An optional mask to indicate valid values of inputImage.
+     * @param gaussFiltSize An optional value indicating size of gaussian blur filter; (DEFAULT: 5)
+     *   </li>
+     * </ul>
+     *
+     * The function estimates the optimum transformation (warpMatrix) with respect to ECC criterion
+     * (CITE: EP08), that is
+     *
+     * \(\texttt{warpMatrix} = \arg\max_{W} \texttt{ECC}(\texttt{templateImage}(x,y),\texttt{inputImage}(x',y'))\)
+     *
+     * where
+     *
+     * \(\begin{bmatrix} x' \\ y' \end{bmatrix} = W \cdot \begin{bmatrix} x \\ y \\ 1 \end{bmatrix}\)
+     *
+     * (the equation holds with homogeneous coordinates for homography). It returns the final enhanced
+     * correlation coefficient, that is the correlation coefficient between the template image and the
+     * final warped input image. When a \(3\times 3\) matrix is given with motionType =0, 1 or 2, the third
+     * row is ignored.
+     *
+     * Unlike findHomography and estimateRigidTransform, the function findTransformECC implements an
+     * area-based alignment that builds on intensity similarities. In essence, the function updates the
+     * initial transformation that roughly aligns the images. If this information is missing, the identity
+     * warp (unity matrix) is used as an initialization. Note that if images undergo strong
+     * displacements/rotations, an initial transformation that roughly aligns the images is necessary
+     * (e.g., a simple euclidean/similarity transform that allows for the images showing the same image
+     * content approximately). Use inverse warping in the second image to take an image close to the first
+     * one, i.e. use the flag WARP_INVERSE_MAP with warpAffine or warpPerspective. See also the OpenCV
+     * sample image_alignment.cpp that demonstrates the use of the function. Note that the function throws
+     * an exception if algorithm does not converges.
+     *
+     * SEE:
+     * computeECC, estimateAffine2D, estimateAffinePartial2D, findHomography
+     * @return automatically generated
+     */
+    public static double findTransformECC(Mat templateImage, Mat inputImage, Mat warpMatrix, int motionType, TermCriteria criteria, Mat inputMask, int gaussFiltSize) {
+        return findTransformECC_0(templateImage.nativeObj, inputImage.nativeObj, warpMatrix.nativeObj, motionType, criteria.type, criteria.maxCount, criteria.epsilon, inputMask.nativeObj, gaussFiltSize);
+    }
+
+
+    //
+    // C++:  int cv::buildOpticalFlowPyramid(Mat img, vector_Mat& pyramid, Size winSize, int maxLevel, bool withDerivatives = true, int pyrBorder = BORDER_REFLECT_101, int derivBorder = BORDER_CONSTANT, bool tryReuseInputImage = true)
+    //
+
+    /**
+     * Constructs the image pyramid which can be passed to calcOpticalFlowPyrLK.
+     *
+     * @param img 8-bit input image.
+     * @param pyramid output pyramid.
+     * @param winSize window size of optical flow algorithm. Must be not less than winSize argument of
+     * calcOpticalFlowPyrLK. It is needed to calculate required padding for pyramid levels.
+     * @param maxLevel 0-based maximal pyramid level number.
+     * @param withDerivatives set to precompute gradients for the every pyramid level. If pyramid is
+     * constructed without the gradients then calcOpticalFlowPyrLK will calculate them internally.
+     * @param pyrBorder the border mode for pyramid layers.
+     * @param derivBorder the border mode for gradients.
+     * @param tryReuseInputImage put ROI of input image into the pyramid if possible. You can pass false
+     * to force data copying.
+     * @return number of levels in constructed pyramid. Can be less than maxLevel.
+     */
+    public static int buildOpticalFlowPyramid(Mat img, List<Mat> pyramid, Size winSize, int maxLevel, boolean withDerivatives, int pyrBorder, int derivBorder, boolean tryReuseInputImage) {
+        Mat pyramid_mat = new Mat();
+        int retVal = buildOpticalFlowPyramid_0(img.nativeObj, pyramid_mat.nativeObj, winSize.width, winSize.height, maxLevel, withDerivatives, pyrBorder, derivBorder, tryReuseInputImage);
+        Converters.Mat_to_vector_Mat(pyramid_mat, pyramid);
+        pyramid_mat.release();
+        return retVal;
+    }
+
+    /**
+     * Constructs the image pyramid which can be passed to calcOpticalFlowPyrLK.
+     *
+     * @param img 8-bit input image.
+     * @param pyramid output pyramid.
+     * @param winSize window size of optical flow algorithm. Must be not less than winSize argument of
+     * calcOpticalFlowPyrLK. It is needed to calculate required padding for pyramid levels.
+     * @param maxLevel 0-based maximal pyramid level number.
+     * @param withDerivatives set to precompute gradients for the every pyramid level. If pyramid is
+     * constructed without the gradients then calcOpticalFlowPyrLK will calculate them internally.
+     * @param pyrBorder the border mode for pyramid layers.
+     * @param derivBorder the border mode for gradients.
+     * to force data copying.
+     * @return number of levels in constructed pyramid. Can be less than maxLevel.
+     */
+    public static int buildOpticalFlowPyramid(Mat img, List<Mat> pyramid, Size winSize, int maxLevel, boolean withDerivatives, int pyrBorder, int derivBorder) {
+        Mat pyramid_mat = new Mat();
+        int retVal = buildOpticalFlowPyramid_1(img.nativeObj, pyramid_mat.nativeObj, winSize.width, winSize.height, maxLevel, withDerivatives, pyrBorder, derivBorder);
+        Converters.Mat_to_vector_Mat(pyramid_mat, pyramid);
+        pyramid_mat.release();
+        return retVal;
+    }
+
+    /**
+     * Constructs the image pyramid which can be passed to calcOpticalFlowPyrLK.
+     *
+     * @param img 8-bit input image.
+     * @param pyramid output pyramid.
+     * @param winSize window size of optical flow algorithm. Must be not less than winSize argument of
+     * calcOpticalFlowPyrLK. It is needed to calculate required padding for pyramid levels.
+     * @param maxLevel 0-based maximal pyramid level number.
+     * @param withDerivatives set to precompute gradients for the every pyramid level. If pyramid is
+     * constructed without the gradients then calcOpticalFlowPyrLK will calculate them internally.
+     * @param pyrBorder the border mode for pyramid layers.
+     * to force data copying.
+     * @return number of levels in constructed pyramid. Can be less than maxLevel.
+     */
+    public static int buildOpticalFlowPyramid(Mat img, List<Mat> pyramid, Size winSize, int maxLevel, boolean withDerivatives, int pyrBorder) {
+        Mat pyramid_mat = new Mat();
+        int retVal = buildOpticalFlowPyramid_2(img.nativeObj, pyramid_mat.nativeObj, winSize.width, winSize.height, maxLevel, withDerivatives, pyrBorder);
+        Converters.Mat_to_vector_Mat(pyramid_mat, pyramid);
+        pyramid_mat.release();
+        return retVal;
+    }
+
+    /**
+     * Constructs the image pyramid which can be passed to calcOpticalFlowPyrLK.
+     *
+     * @param img 8-bit input image.
+     * @param pyramid output pyramid.
+     * @param winSize window size of optical flow algorithm. Must be not less than winSize argument of
+     * calcOpticalFlowPyrLK. It is needed to calculate required padding for pyramid levels.
+     * @param maxLevel 0-based maximal pyramid level number.
+     * @param withDerivatives set to precompute gradients for the every pyramid level. If pyramid is
+     * constructed without the gradients then calcOpticalFlowPyrLK will calculate them internally.
+     * to force data copying.
+     * @return number of levels in constructed pyramid. Can be less than maxLevel.
+     */
+    public static int buildOpticalFlowPyramid(Mat img, List<Mat> pyramid, Size winSize, int maxLevel, boolean withDerivatives) {
+        Mat pyramid_mat = new Mat();
+        int retVal = buildOpticalFlowPyramid_3(img.nativeObj, pyramid_mat.nativeObj, winSize.width, winSize.height, maxLevel, withDerivatives);
+        Converters.Mat_to_vector_Mat(pyramid_mat, pyramid);
+        pyramid_mat.release();
+        return retVal;
+    }
+
+    /**
+     * Constructs the image pyramid which can be passed to calcOpticalFlowPyrLK.
+     *
+     * @param img 8-bit input image.
+     * @param pyramid output pyramid.
+     * @param winSize window size of optical flow algorithm. Must be not less than winSize argument of
+     * calcOpticalFlowPyrLK. It is needed to calculate required padding for pyramid levels.
+     * @param maxLevel 0-based maximal pyramid level number.
+     * constructed without the gradients then calcOpticalFlowPyrLK will calculate them internally.
+     * to force data copying.
+     * @return number of levels in constructed pyramid. Can be less than maxLevel.
+     */
+    public static int buildOpticalFlowPyramid(Mat img, List<Mat> pyramid, Size winSize, int maxLevel) {
+        Mat pyramid_mat = new Mat();
+        int retVal = buildOpticalFlowPyramid_4(img.nativeObj, pyramid_mat.nativeObj, winSize.width, winSize.height, maxLevel);
+        Converters.Mat_to_vector_Mat(pyramid_mat, pyramid);
+        pyramid_mat.release();
+        return retVal;
+    }
+
+
+    //
+    // C++:  int cv::meanShift(Mat probImage, Rect& window, TermCriteria criteria)
+    //
+
+    /**
+     * Finds an object on a back projection image.
+     *
+     * @param probImage Back projection of the object histogram. See calcBackProject for details.
+     * @param window Initial search window.
+     * @param criteria Stop criteria for the iterative search algorithm.
+     * returns
+     * :   Number of iterations CAMSHIFT took to converge.
+     * The function implements the iterative object search algorithm. It takes the input back projection of
+     * an object and the initial position. The mass center in window of the back projection image is
+     * computed and the search window center shifts to the mass center. The procedure is repeated until the
+     * specified number of iterations criteria.maxCount is done or until the window center shifts by less
+     * than criteria.epsilon. The algorithm is used inside CamShift and, unlike CamShift , the search
+     * window size or orientation do not change during the search. You can simply pass the output of
+     * calcBackProject to this function. But better results can be obtained if you pre-filter the back
+     * projection and remove the noise. For example, you can do this by retrieving connected components
+     * with findContours , throwing away contours with small area ( contourArea ), and rendering the
+     * remaining contours with drawContours.
+     * @return automatically generated
+     */
+    public static int meanShift(Mat probImage, Rect window, TermCriteria criteria) {
+        double[] window_out = new double[4];
+        int retVal = meanShift_0(probImage.nativeObj, window.x, window.y, window.width, window.height, window_out, criteria.type, criteria.maxCount, criteria.epsilon);
+        if(window!=null){ window.x = (int)window_out[0]; window.y = (int)window_out[1]; window.width = (int)window_out[2]; window.height = (int)window_out[3]; } 
+        return retVal;
+    }
+
+
+    //
+    // C++:  void cv::calcOpticalFlowFarneback(Mat prev, Mat next, Mat& flow, double pyr_scale, int levels, int winsize, int iterations, int poly_n, double poly_sigma, int flags)
+    //
+
+    /**
+     * Computes a dense optical flow using the Gunnar Farneback's algorithm.
+     *
+     * @param prev first 8-bit single-channel input image.
+     * @param next second input image of the same size and the same type as prev.
+     * @param flow computed flow image that has the same size as prev and type CV_32FC2.
+     * @param pyr_scale parameter, specifying the image scale (&lt;1) to build pyramids for each image;
+     * pyr_scale=0.5 means a classical pyramid, where each next layer is twice smaller than the previous
+     * one.
+     * @param levels number of pyramid layers including the initial image; levels=1 means that no extra
+     * layers are created and only the original images are used.
+     * @param winsize averaging window size; larger values increase the algorithm robustness to image
+     * noise and give more chances for fast motion detection, but yield more blurred motion field.
+     * @param iterations number of iterations the algorithm does at each pyramid level.
+     * @param poly_n size of the pixel neighborhood used to find polynomial expansion in each pixel;
+     * larger values mean that the image will be approximated with smoother surfaces, yielding more
+     * robust algorithm and more blurred motion field, typically poly_n =5 or 7.
+     * @param poly_sigma standard deviation of the Gaussian that is used to smooth derivatives used as a
+     * basis for the polynomial expansion; for poly_n=5, you can set poly_sigma=1.1, for poly_n=7, a
+     * good value would be poly_sigma=1.5.
+     * @param flags operation flags that can be a combination of the following:
+     * <ul>
+     *   <li>
+     *     <b>OPTFLOW_USE_INITIAL_FLOW</b> uses the input flow as an initial flow approximation.
+     *   </li>
+     *   <li>
+     *     <b>OPTFLOW_FARNEBACK_GAUSSIAN</b> uses the Gaussian \(\texttt{winsize}\times\texttt{winsize}\)
+     *      filter instead of a box filter of the same size for optical flow estimation; usually, this
+     *      option gives z more accurate flow than with a box filter, at the cost of lower speed;
+     *      normally, winsize for a Gaussian window should be set to a larger value to achieve the same
+     *      level of robustness.
+     *   </li>
+     * </ul>
+     *
+     * The function finds an optical flow for each prev pixel using the CITE: Farneback2003 algorithm so that
+     *
+     * \(\texttt{prev} (y,x)  \sim \texttt{next} ( y + \texttt{flow} (y,x)[1],  x + \texttt{flow} (y,x)[0])\)
+     *
+     * <b>Note:</b>
+     *
+     * <ul>
+     *   <li>
+     *    An example using the optical flow algorithm described by Gunnar Farneback can be found at
+     *     opencv_source_code/samples/cpp/fback.cpp
+     *   </li>
+     *   <li>
+     *    (Python) An example using the optical flow algorithm described by Gunnar Farneback can be
+     *     found at opencv_source_code/samples/python/opt_flow.py
+     *   </li>
+     * </ul>
+     */
+    public static void calcOpticalFlowFarneback(Mat prev, Mat next, Mat flow, double pyr_scale, int levels, int winsize, int iterations, int poly_n, double poly_sigma, int flags) {
+        calcOpticalFlowFarneback_0(prev.nativeObj, next.nativeObj, flow.nativeObj, pyr_scale, levels, winsize, iterations, poly_n, poly_sigma, flags);
+    }
+
+
+    //
+    // C++:  void cv::calcOpticalFlowPyrLK(Mat prevImg, Mat nextImg, vector_Point2f prevPts, vector_Point2f& nextPts, vector_uchar& status, vector_float& err, Size winSize = Size(21,21), int maxLevel = 3, TermCriteria criteria = TermCriteria(TermCriteria::COUNT+TermCriteria::EPS, 30, 0.01), int flags = 0, double minEigThreshold = 1e-4)
+    //
+
+    /**
+     * Calculates an optical flow for a sparse feature set using the iterative Lucas-Kanade method with
+     * pyramids.
+     *
+     * @param prevImg first 8-bit input image or pyramid constructed by buildOpticalFlowPyramid.
+     * @param nextImg second input image or pyramid of the same size and the same type as prevImg.
+     * @param prevPts vector of 2D points for which the flow needs to be found; point coordinates must be
+     * single-precision floating-point numbers.
+     * @param nextPts output vector of 2D points (with single-precision floating-point coordinates)
+     * containing the calculated new positions of input features in the second image; when
+     * OPTFLOW_USE_INITIAL_FLOW flag is passed, the vector must have the same size as in the input.
+     * @param status output status vector (of unsigned chars); each element of the vector is set to 1 if
+     * the flow for the corresponding features has been found, otherwise, it is set to 0.
+     * @param err output vector of errors; each element of the vector is set to an error for the
+     * corresponding feature, type of the error measure can be set in flags parameter; if the flow wasn't
+     * found then the error is not defined (use the status parameter to find such cases).
+     * @param winSize size of the search window at each pyramid level.
+     * @param maxLevel 0-based maximal pyramid level number; if set to 0, pyramids are not used (single
+     * level), if set to 1, two levels are used, and so on; if pyramids are passed to input then
+     * algorithm will use as many levels as pyramids have but no more than maxLevel.
+     * @param criteria parameter, specifying the termination criteria of the iterative search algorithm
+     * (after the specified maximum number of iterations criteria.maxCount or when the search window
+     * moves by less than criteria.epsilon.
+     * @param flags operation flags:
+     * <ul>
+     *   <li>
+     *     <b>OPTFLOW_USE_INITIAL_FLOW</b> uses initial estimations, stored in nextPts; if the flag is
+     *      not set, then prevPts is copied to nextPts and is considered the initial estimate.
+     *   </li>
+     *   <li>
+     *     <b>OPTFLOW_LK_GET_MIN_EIGENVALS</b> use minimum eigen values as an error measure (see
+     *      minEigThreshold description); if the flag is not set, then L1 distance between patches
+     *      around the original and a moved point, divided by number of pixels in a window, is used as a
+     *      error measure.
+     * @param minEigThreshold the algorithm calculates the minimum eigen value of a 2x2 normal matrix of
+     * optical flow equations (this matrix is called a spatial gradient matrix in CITE: Bouguet00), divided
+     * by number of pixels in a window; if this value is less than minEigThreshold, then a corresponding
+     * feature is filtered out and its flow is not processed, so it allows to remove bad points and get a
+     * performance boost.
+     *   </li>
+     * </ul>
+     *
+     * The function implements a sparse iterative version of the Lucas-Kanade optical flow in pyramids. See
+     * CITE: Bouguet00 . The function is parallelized with the TBB library.
+     *
+     * <b>Note:</b>
+     *
+     * <ul>
+     *   <li>
+     *    An example using the Lucas-Kanade optical flow algorithm can be found at
+     *     opencv_source_code/samples/cpp/lkdemo.cpp
+     *   </li>
+     *   <li>
+     *    (Python) An example using the Lucas-Kanade optical flow algorithm can be found at
+     *     opencv_source_code/samples/python/lk_track.py
+     *   </li>
+     *   <li>
+     *    (Python) An example using the Lucas-Kanade tracker for homography matching can be found at
+     *     opencv_source_code/samples/python/lk_homography.py
+     *   </li>
+     * </ul>
+     */
+    public static void calcOpticalFlowPyrLK(Mat prevImg, Mat nextImg, MatOfPoint2f prevPts, MatOfPoint2f nextPts, MatOfByte status, MatOfFloat err, Size winSize, int maxLevel, TermCriteria criteria, int flags, double minEigThreshold) {
+        Mat prevPts_mat = prevPts;
+        Mat nextPts_mat = nextPts;
+        Mat status_mat = status;
+        Mat err_mat = err;
+        calcOpticalFlowPyrLK_0(prevImg.nativeObj, nextImg.nativeObj, prevPts_mat.nativeObj, nextPts_mat.nativeObj, status_mat.nativeObj, err_mat.nativeObj, winSize.width, winSize.height, maxLevel, criteria.type, criteria.maxCount, criteria.epsilon, flags, minEigThreshold);
+    }
+
+    /**
+     * Calculates an optical flow for a sparse feature set using the iterative Lucas-Kanade method with
+     * pyramids.
+     *
+     * @param prevImg first 8-bit input image or pyramid constructed by buildOpticalFlowPyramid.
+     * @param nextImg second input image or pyramid of the same size and the same type as prevImg.
+     * @param prevPts vector of 2D points for which the flow needs to be found; point coordinates must be
+     * single-precision floating-point numbers.
+     * @param nextPts output vector of 2D points (with single-precision floating-point coordinates)
+     * containing the calculated new positions of input features in the second image; when
+     * OPTFLOW_USE_INITIAL_FLOW flag is passed, the vector must have the same size as in the input.
+     * @param status output status vector (of unsigned chars); each element of the vector is set to 1 if
+     * the flow for the corresponding features has been found, otherwise, it is set to 0.
+     * @param err output vector of errors; each element of the vector is set to an error for the
+     * corresponding feature, type of the error measure can be set in flags parameter; if the flow wasn't
+     * found then the error is not defined (use the status parameter to find such cases).
+     * @param winSize size of the search window at each pyramid level.
+     * @param maxLevel 0-based maximal pyramid level number; if set to 0, pyramids are not used (single
+     * level), if set to 1, two levels are used, and so on; if pyramids are passed to input then
+     * algorithm will use as many levels as pyramids have but no more than maxLevel.
+     * @param criteria parameter, specifying the termination criteria of the iterative search algorithm
+     * (after the specified maximum number of iterations criteria.maxCount or when the search window
+     * moves by less than criteria.epsilon.
+     * @param flags operation flags:
+     * <ul>
+     *   <li>
+     *     <b>OPTFLOW_USE_INITIAL_FLOW</b> uses initial estimations, stored in nextPts; if the flag is
+     *      not set, then prevPts is copied to nextPts and is considered the initial estimate.
+     *   </li>
+     *   <li>
+     *     <b>OPTFLOW_LK_GET_MIN_EIGENVALS</b> use minimum eigen values as an error measure (see
+     *      minEigThreshold description); if the flag is not set, then L1 distance between patches
+     *      around the original and a moved point, divided by number of pixels in a window, is used as a
+     *      error measure.
+     * optical flow equations (this matrix is called a spatial gradient matrix in CITE: Bouguet00), divided
+     * by number of pixels in a window; if this value is less than minEigThreshold, then a corresponding
+     * feature is filtered out and its flow is not processed, so it allows to remove bad points and get a
+     * performance boost.
+     *   </li>
+     * </ul>
+     *
+     * The function implements a sparse iterative version of the Lucas-Kanade optical flow in pyramids. See
+     * CITE: Bouguet00 . The function is parallelized with the TBB library.
+     *
+     * <b>Note:</b>
+     *
+     * <ul>
+     *   <li>
+     *    An example using the Lucas-Kanade optical flow algorithm can be found at
+     *     opencv_source_code/samples/cpp/lkdemo.cpp
+     *   </li>
+     *   <li>
+     *    (Python) An example using the Lucas-Kanade optical flow algorithm can be found at
+     *     opencv_source_code/samples/python/lk_track.py
+     *   </li>
+     *   <li>
+     *    (Python) An example using the Lucas-Kanade tracker for homography matching can be found at
+     *     opencv_source_code/samples/python/lk_homography.py
+     *   </li>
+     * </ul>
+     */
+    public static void calcOpticalFlowPyrLK(Mat prevImg, Mat nextImg, MatOfPoint2f prevPts, MatOfPoint2f nextPts, MatOfByte status, MatOfFloat err, Size winSize, int maxLevel, TermCriteria criteria, int flags) {
+        Mat prevPts_mat = prevPts;
+        Mat nextPts_mat = nextPts;
+        Mat status_mat = status;
+        Mat err_mat = err;
+        calcOpticalFlowPyrLK_1(prevImg.nativeObj, nextImg.nativeObj, prevPts_mat.nativeObj, nextPts_mat.nativeObj, status_mat.nativeObj, err_mat.nativeObj, winSize.width, winSize.height, maxLevel, criteria.type, criteria.maxCount, criteria.epsilon, flags);
+    }
+
+    /**
+     * Calculates an optical flow for a sparse feature set using the iterative Lucas-Kanade method with
+     * pyramids.
+     *
+     * @param prevImg first 8-bit input image or pyramid constructed by buildOpticalFlowPyramid.
+     * @param nextImg second input image or pyramid of the same size and the same type as prevImg.
+     * @param prevPts vector of 2D points for which the flow needs to be found; point coordinates must be
+     * single-precision floating-point numbers.
+     * @param nextPts output vector of 2D points (with single-precision floating-point coordinates)
+     * containing the calculated new positions of input features in the second image; when
+     * OPTFLOW_USE_INITIAL_FLOW flag is passed, the vector must have the same size as in the input.
+     * @param status output status vector (of unsigned chars); each element of the vector is set to 1 if
+     * the flow for the corresponding features has been found, otherwise, it is set to 0.
+     * @param err output vector of errors; each element of the vector is set to an error for the
+     * corresponding feature, type of the error measure can be set in flags parameter; if the flow wasn't
+     * found then the error is not defined (use the status parameter to find such cases).
+     * @param winSize size of the search window at each pyramid level.
+     * @param maxLevel 0-based maximal pyramid level number; if set to 0, pyramids are not used (single
+     * level), if set to 1, two levels are used, and so on; if pyramids are passed to input then
+     * algorithm will use as many levels as pyramids have but no more than maxLevel.
+     * @param criteria parameter, specifying the termination criteria of the iterative search algorithm
+     * (after the specified maximum number of iterations criteria.maxCount or when the search window
+     * moves by less than criteria.epsilon.
+     * <ul>
+     *   <li>
+     *     <b>OPTFLOW_USE_INITIAL_FLOW</b> uses initial estimations, stored in nextPts; if the flag is
+     *      not set, then prevPts is copied to nextPts and is considered the initial estimate.
+     *   </li>
+     *   <li>
+     *     <b>OPTFLOW_LK_GET_MIN_EIGENVALS</b> use minimum eigen values as an error measure (see
+     *      minEigThreshold description); if the flag is not set, then L1 distance between patches
+     *      around the original and a moved point, divided by number of pixels in a window, is used as a
+     *      error measure.
+     * optical flow equations (this matrix is called a spatial gradient matrix in CITE: Bouguet00), divided
+     * by number of pixels in a window; if this value is less than minEigThreshold, then a corresponding
+     * feature is filtered out and its flow is not processed, so it allows to remove bad points and get a
+     * performance boost.
+     *   </li>
+     * </ul>
+     *
+     * The function implements a sparse iterative version of the Lucas-Kanade optical flow in pyramids. See
+     * CITE: Bouguet00 . The function is parallelized with the TBB library.
+     *
+     * <b>Note:</b>
+     *
+     * <ul>
+     *   <li>
+     *    An example using the Lucas-Kanade optical flow algorithm can be found at
+     *     opencv_source_code/samples/cpp/lkdemo.cpp
+     *   </li>
+     *   <li>
+     *    (Python) An example using the Lucas-Kanade optical flow algorithm can be found at
+     *     opencv_source_code/samples/python/lk_track.py
+     *   </li>
+     *   <li>
+     *    (Python) An example using the Lucas-Kanade tracker for homography matching can be found at
+     *     opencv_source_code/samples/python/lk_homography.py
+     *   </li>
+     * </ul>
+     */
+    public static void calcOpticalFlowPyrLK(Mat prevImg, Mat nextImg, MatOfPoint2f prevPts, MatOfPoint2f nextPts, MatOfByte status, MatOfFloat err, Size winSize, int maxLevel, TermCriteria criteria) {
+        Mat prevPts_mat = prevPts;
+        Mat nextPts_mat = nextPts;
+        Mat status_mat = status;
+        Mat err_mat = err;
+        calcOpticalFlowPyrLK_2(prevImg.nativeObj, nextImg.nativeObj, prevPts_mat.nativeObj, nextPts_mat.nativeObj, status_mat.nativeObj, err_mat.nativeObj, winSize.width, winSize.height, maxLevel, criteria.type, criteria.maxCount, criteria.epsilon);
+    }
+
+    /**
+     * Calculates an optical flow for a sparse feature set using the iterative Lucas-Kanade method with
+     * pyramids.
+     *
+     * @param prevImg first 8-bit input image or pyramid constructed by buildOpticalFlowPyramid.
+     * @param nextImg second input image or pyramid of the same size and the same type as prevImg.
+     * @param prevPts vector of 2D points for which the flow needs to be found; point coordinates must be
+     * single-precision floating-point numbers.
+     * @param nextPts output vector of 2D points (with single-precision floating-point coordinates)
+     * containing the calculated new positions of input features in the second image; when
+     * OPTFLOW_USE_INITIAL_FLOW flag is passed, the vector must have the same size as in the input.
+     * @param status output status vector (of unsigned chars); each element of the vector is set to 1 if
+     * the flow for the corresponding features has been found, otherwise, it is set to 0.
+     * @param err output vector of errors; each element of the vector is set to an error for the
+     * corresponding feature, type of the error measure can be set in flags parameter; if the flow wasn't
+     * found then the error is not defined (use the status parameter to find such cases).
+     * @param winSize size of the search window at each pyramid level.
+     * @param maxLevel 0-based maximal pyramid level number; if set to 0, pyramids are not used (single
+     * level), if set to 1, two levels are used, and so on; if pyramids are passed to input then
+     * algorithm will use as many levels as pyramids have but no more than maxLevel.
+     * (after the specified maximum number of iterations criteria.maxCount or when the search window
+     * moves by less than criteria.epsilon.
+     * <ul>
+     *   <li>
+     *     <b>OPTFLOW_USE_INITIAL_FLOW</b> uses initial estimations, stored in nextPts; if the flag is
+     *      not set, then prevPts is copied to nextPts and is considered the initial estimate.
+     *   </li>
+     *   <li>
+     *     <b>OPTFLOW_LK_GET_MIN_EIGENVALS</b> use minimum eigen values as an error measure (see
+     *      minEigThreshold description); if the flag is not set, then L1 distance between patches
+     *      around the original and a moved point, divided by number of pixels in a window, is used as a
+     *      error measure.
+     * optical flow equations (this matrix is called a spatial gradient matrix in CITE: Bouguet00), divided
+     * by number of pixels in a window; if this value is less than minEigThreshold, then a corresponding
+     * feature is filtered out and its flow is not processed, so it allows to remove bad points and get a
+     * performance boost.
+     *   </li>
+     * </ul>
+     *
+     * The function implements a sparse iterative version of the Lucas-Kanade optical flow in pyramids. See
+     * CITE: Bouguet00 . The function is parallelized with the TBB library.
+     *
+     * <b>Note:</b>
+     *
+     * <ul>
+     *   <li>
+     *    An example using the Lucas-Kanade optical flow algorithm can be found at
+     *     opencv_source_code/samples/cpp/lkdemo.cpp
+     *   </li>
+     *   <li>
+     *    (Python) An example using the Lucas-Kanade optical flow algorithm can be found at
+     *     opencv_source_code/samples/python/lk_track.py
+     *   </li>
+     *   <li>
+     *    (Python) An example using the Lucas-Kanade tracker for homography matching can be found at
+     *     opencv_source_code/samples/python/lk_homography.py
+     *   </li>
+     * </ul>
+     */
+    public static void calcOpticalFlowPyrLK(Mat prevImg, Mat nextImg, MatOfPoint2f prevPts, MatOfPoint2f nextPts, MatOfByte status, MatOfFloat err, Size winSize, int maxLevel) {
+        Mat prevPts_mat = prevPts;
+        Mat nextPts_mat = nextPts;
+        Mat status_mat = status;
+        Mat err_mat = err;
+        calcOpticalFlowPyrLK_3(prevImg.nativeObj, nextImg.nativeObj, prevPts_mat.nativeObj, nextPts_mat.nativeObj, status_mat.nativeObj, err_mat.nativeObj, winSize.width, winSize.height, maxLevel);
+    }
+
+    /**
+     * Calculates an optical flow for a sparse feature set using the iterative Lucas-Kanade method with
+     * pyramids.
+     *
+     * @param prevImg first 8-bit input image or pyramid constructed by buildOpticalFlowPyramid.
+     * @param nextImg second input image or pyramid of the same size and the same type as prevImg.
+     * @param prevPts vector of 2D points for which the flow needs to be found; point coordinates must be
+     * single-precision floating-point numbers.
+     * @param nextPts output vector of 2D points (with single-precision floating-point coordinates)
+     * containing the calculated new positions of input features in the second image; when
+     * OPTFLOW_USE_INITIAL_FLOW flag is passed, the vector must have the same size as in the input.
+     * @param status output status vector (of unsigned chars); each element of the vector is set to 1 if
+     * the flow for the corresponding features has been found, otherwise, it is set to 0.
+     * @param err output vector of errors; each element of the vector is set to an error for the
+     * corresponding feature, type of the error measure can be set in flags parameter; if the flow wasn't
+     * found then the error is not defined (use the status parameter to find such cases).
+     * @param winSize size of the search window at each pyramid level.
+     * level), if set to 1, two levels are used, and so on; if pyramids are passed to input then
+     * algorithm will use as many levels as pyramids have but no more than maxLevel.
+     * (after the specified maximum number of iterations criteria.maxCount or when the search window
+     * moves by less than criteria.epsilon.
+     * <ul>
+     *   <li>
+     *     <b>OPTFLOW_USE_INITIAL_FLOW</b> uses initial estimations, stored in nextPts; if the flag is
+     *      not set, then prevPts is copied to nextPts and is considered the initial estimate.
+     *   </li>
+     *   <li>
+     *     <b>OPTFLOW_LK_GET_MIN_EIGENVALS</b> use minimum eigen values as an error measure (see
+     *      minEigThreshold description); if the flag is not set, then L1 distance between patches
+     *      around the original and a moved point, divided by number of pixels in a window, is used as a
+     *      error measure.
+     * optical flow equations (this matrix is called a spatial gradient matrix in CITE: Bouguet00), divided
+     * by number of pixels in a window; if this value is less than minEigThreshold, then a corresponding
+     * feature is filtered out and its flow is not processed, so it allows to remove bad points and get a
+     * performance boost.
+     *   </li>
+     * </ul>
+     *
+     * The function implements a sparse iterative version of the Lucas-Kanade optical flow in pyramids. See
+     * CITE: Bouguet00 . The function is parallelized with the TBB library.
+     *
+     * <b>Note:</b>
+     *
+     * <ul>
+     *   <li>
+     *    An example using the Lucas-Kanade optical flow algorithm can be found at
+     *     opencv_source_code/samples/cpp/lkdemo.cpp
+     *   </li>
+     *   <li>
+     *    (Python) An example using the Lucas-Kanade optical flow algorithm can be found at
+     *     opencv_source_code/samples/python/lk_track.py
+     *   </li>
+     *   <li>
+     *    (Python) An example using the Lucas-Kanade tracker for homography matching can be found at
+     *     opencv_source_code/samples/python/lk_homography.py
+     *   </li>
+     * </ul>
+     */
+    public static void calcOpticalFlowPyrLK(Mat prevImg, Mat nextImg, MatOfPoint2f prevPts, MatOfPoint2f nextPts, MatOfByte status, MatOfFloat err, Size winSize) {
+        Mat prevPts_mat = prevPts;
+        Mat nextPts_mat = nextPts;
+        Mat status_mat = status;
+        Mat err_mat = err;
+        calcOpticalFlowPyrLK_4(prevImg.nativeObj, nextImg.nativeObj, prevPts_mat.nativeObj, nextPts_mat.nativeObj, status_mat.nativeObj, err_mat.nativeObj, winSize.width, winSize.height);
+    }
+
+    /**
+     * Calculates an optical flow for a sparse feature set using the iterative Lucas-Kanade method with
+     * pyramids.
+     *
+     * @param prevImg first 8-bit input image or pyramid constructed by buildOpticalFlowPyramid.
+     * @param nextImg second input image or pyramid of the same size and the same type as prevImg.
+     * @param prevPts vector of 2D points for which the flow needs to be found; point coordinates must be
+     * single-precision floating-point numbers.
+     * @param nextPts output vector of 2D points (with single-precision floating-point coordinates)
+     * containing the calculated new positions of input features in the second image; when
+     * OPTFLOW_USE_INITIAL_FLOW flag is passed, the vector must have the same size as in the input.
+     * @param status output status vector (of unsigned chars); each element of the vector is set to 1 if
+     * the flow for the corresponding features has been found, otherwise, it is set to 0.
+     * @param err output vector of errors; each element of the vector is set to an error for the
+     * corresponding feature, type of the error measure can be set in flags parameter; if the flow wasn't
+     * found then the error is not defined (use the status parameter to find such cases).
+     * level), if set to 1, two levels are used, and so on; if pyramids are passed to input then
+     * algorithm will use as many levels as pyramids have but no more than maxLevel.
+     * (after the specified maximum number of iterations criteria.maxCount or when the search window
+     * moves by less than criteria.epsilon.
+     * <ul>
+     *   <li>
+     *     <b>OPTFLOW_USE_INITIAL_FLOW</b> uses initial estimations, stored in nextPts; if the flag is
+     *      not set, then prevPts is copied to nextPts and is considered the initial estimate.
+     *   </li>
+     *   <li>
+     *     <b>OPTFLOW_LK_GET_MIN_EIGENVALS</b> use minimum eigen values as an error measure (see
+     *      minEigThreshold description); if the flag is not set, then L1 distance between patches
+     *      around the original and a moved point, divided by number of pixels in a window, is used as a
+     *      error measure.
+     * optical flow equations (this matrix is called a spatial gradient matrix in CITE: Bouguet00), divided
+     * by number of pixels in a window; if this value is less than minEigThreshold, then a corresponding
+     * feature is filtered out and its flow is not processed, so it allows to remove bad points and get a
+     * performance boost.
+     *   </li>
+     * </ul>
+     *
+     * The function implements a sparse iterative version of the Lucas-Kanade optical flow in pyramids. See
+     * CITE: Bouguet00 . The function is parallelized with the TBB library.
+     *
+     * <b>Note:</b>
+     *
+     * <ul>
+     *   <li>
+     *    An example using the Lucas-Kanade optical flow algorithm can be found at
+     *     opencv_source_code/samples/cpp/lkdemo.cpp
+     *   </li>
+     *   <li>
+     *    (Python) An example using the Lucas-Kanade optical flow algorithm can be found at
+     *     opencv_source_code/samples/python/lk_track.py
+     *   </li>
+     *   <li>
+     *    (Python) An example using the Lucas-Kanade tracker for homography matching can be found at
+     *     opencv_source_code/samples/python/lk_homography.py
+     *   </li>
+     * </ul>
+     */
+    public static void calcOpticalFlowPyrLK(Mat prevImg, Mat nextImg, MatOfPoint2f prevPts, MatOfPoint2f nextPts, MatOfByte status, MatOfFloat err) {
+        Mat prevPts_mat = prevPts;
+        Mat nextPts_mat = nextPts;
+        Mat status_mat = status;
+        Mat err_mat = err;
+        calcOpticalFlowPyrLK_5(prevImg.nativeObj, nextImg.nativeObj, prevPts_mat.nativeObj, nextPts_mat.nativeObj, status_mat.nativeObj, err_mat.nativeObj);
+    }
+
+
+
+
+    // C++:  Mat cv::estimateRigidTransform(Mat src, Mat dst, bool fullAffine, int ransacMaxIters, double ransacGoodRatio, int ransacSize0)
+    private static native long estimateRigidTransform_0(long src_nativeObj, long dst_nativeObj, boolean fullAffine, int ransacMaxIters, double ransacGoodRatio, int ransacSize0);
+
+    // C++:  Mat cv::estimateRigidTransform(Mat src, Mat dst, bool fullAffine)
+    private static native long estimateRigidTransform_1(long src_nativeObj, long dst_nativeObj, boolean fullAffine);
+
+    // C++:  Ptr_BackgroundSubtractorKNN cv::createBackgroundSubtractorKNN(int history = 500, double dist2Threshold = 400.0, bool detectShadows = true)
+    private static native long createBackgroundSubtractorKNN_0(int history, double dist2Threshold, boolean detectShadows);
+    private static native long createBackgroundSubtractorKNN_1(int history, double dist2Threshold);
+    private static native long createBackgroundSubtractorKNN_2(int history);
+    private static native long createBackgroundSubtractorKNN_3();
+
+    // C++:  Ptr_BackgroundSubtractorMOG2 cv::createBackgroundSubtractorMOG2(int history = 500, double varThreshold = 16, bool detectShadows = true)
+    private static native long createBackgroundSubtractorMOG2_0(int history, double varThreshold, boolean detectShadows);
+    private static native long createBackgroundSubtractorMOG2_1(int history, double varThreshold);
+    private static native long createBackgroundSubtractorMOG2_2(int history);
+    private static native long createBackgroundSubtractorMOG2_3();
+
+    // C++:  Ptr_DualTVL1OpticalFlow cv::createOptFlow_DualTVL1()
+    private static native long createOptFlow_DualTVL1_0();
+
+    // C++:  RotatedRect cv::CamShift(Mat probImage, Rect& window, TermCriteria criteria)
+    private static native double[] CamShift_0(long probImage_nativeObj, int window_x, int window_y, int window_width, int window_height, double[] window_out, int criteria_type, int criteria_maxCount, double criteria_epsilon);
+
+    // C++:  double cv::computeECC(Mat templateImage, Mat inputImage, Mat inputMask = Mat())
+    private static native double computeECC_0(long templateImage_nativeObj, long inputImage_nativeObj, long inputMask_nativeObj);
+    private static native double computeECC_1(long templateImage_nativeObj, long inputImage_nativeObj);
+
+    // C++:  double cv::findTransformECC(Mat templateImage, Mat inputImage, Mat& warpMatrix, int motionType, TermCriteria criteria, Mat inputMask, int gaussFiltSize)
+    private static native double findTransformECC_0(long templateImage_nativeObj, long inputImage_nativeObj, long warpMatrix_nativeObj, int motionType, int criteria_type, int criteria_maxCount, double criteria_epsilon, long inputMask_nativeObj, int gaussFiltSize);
+
+    // C++:  int cv::buildOpticalFlowPyramid(Mat img, vector_Mat& pyramid, Size winSize, int maxLevel, bool withDerivatives = true, int pyrBorder = BORDER_REFLECT_101, int derivBorder = BORDER_CONSTANT, bool tryReuseInputImage = true)
+    private static native int buildOpticalFlowPyramid_0(long img_nativeObj, long pyramid_mat_nativeObj, double winSize_width, double winSize_height, int maxLevel, boolean withDerivatives, int pyrBorder, int derivBorder, boolean tryReuseInputImage);
+    private static native int buildOpticalFlowPyramid_1(long img_nativeObj, long pyramid_mat_nativeObj, double winSize_width, double winSize_height, int maxLevel, boolean withDerivatives, int pyrBorder, int derivBorder);
+    private static native int buildOpticalFlowPyramid_2(long img_nativeObj, long pyramid_mat_nativeObj, double winSize_width, double winSize_height, int maxLevel, boolean withDerivatives, int pyrBorder);
+    private static native int buildOpticalFlowPyramid_3(long img_nativeObj, long pyramid_mat_nativeObj, double winSize_width, double winSize_height, int maxLevel, boolean withDerivatives);
+    private static native int buildOpticalFlowPyramid_4(long img_nativeObj, long pyramid_mat_nativeObj, double winSize_width, double winSize_height, int maxLevel);
+
+    // C++:  int cv::meanShift(Mat probImage, Rect& window, TermCriteria criteria)
+    private static native int meanShift_0(long probImage_nativeObj, int window_x, int window_y, int window_width, int window_height, double[] window_out, int criteria_type, int criteria_maxCount, double criteria_epsilon);
+
+    // C++:  void cv::calcOpticalFlowFarneback(Mat prev, Mat next, Mat& flow, double pyr_scale, int levels, int winsize, int iterations, int poly_n, double poly_sigma, int flags)
+    private static native void calcOpticalFlowFarneback_0(long prev_nativeObj, long next_nativeObj, long flow_nativeObj, double pyr_scale, int levels, int winsize, int iterations, int poly_n, double poly_sigma, int flags);
+
+    // C++:  void cv::calcOpticalFlowPyrLK(Mat prevImg, Mat nextImg, vector_Point2f prevPts, vector_Point2f& nextPts, vector_uchar& status, vector_float& err, Size winSize = Size(21,21), int maxLevel = 3, TermCriteria criteria = TermCriteria(TermCriteria::COUNT+TermCriteria::EPS, 30, 0.01), int flags = 0, double minEigThreshold = 1e-4)
+    private static native void calcOpticalFlowPyrLK_0(long prevImg_nativeObj, long nextImg_nativeObj, long prevPts_mat_nativeObj, long nextPts_mat_nativeObj, long status_mat_nativeObj, long err_mat_nativeObj, double winSize_width, double winSize_height, int maxLevel, int criteria_type, int criteria_maxCount, double criteria_epsilon, int flags, double minEigThreshold);
+    private static native void calcOpticalFlowPyrLK_1(long prevImg_nativeObj, long nextImg_nativeObj, long prevPts_mat_nativeObj, long nextPts_mat_nativeObj, long status_mat_nativeObj, long err_mat_nativeObj, double winSize_width, double winSize_height, int maxLevel, int criteria_type, int criteria_maxCount, double criteria_epsilon, int flags);
+    private static native void calcOpticalFlowPyrLK_2(long prevImg_nativeObj, long nextImg_nativeObj, long prevPts_mat_nativeObj, long nextPts_mat_nativeObj, long status_mat_nativeObj, long err_mat_nativeObj, double winSize_width, double winSize_height, int maxLevel, int criteria_type, int criteria_maxCount, double criteria_epsilon);
+    private static native void calcOpticalFlowPyrLK_3(long prevImg_nativeObj, long nextImg_nativeObj, long prevPts_mat_nativeObj, long nextPts_mat_nativeObj, long status_mat_nativeObj, long err_mat_nativeObj, double winSize_width, double winSize_height, int maxLevel);
+    private static native void calcOpticalFlowPyrLK_4(long prevImg_nativeObj, long nextImg_nativeObj, long prevPts_mat_nativeObj, long nextPts_mat_nativeObj, long status_mat_nativeObj, long err_mat_nativeObj, double winSize_width, double winSize_height);
+    private static native void calcOpticalFlowPyrLK_5(long prevImg_nativeObj, long nextImg_nativeObj, long prevPts_mat_nativeObj, long nextPts_mat_nativeObj, long status_mat_nativeObj, long err_mat_nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/android/AsyncServiceHelper.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/android/AsyncServiceHelper.java	(date 1605830247069)
+++ openCVLibrary3411/src/main/java/org/opencv/android/AsyncServiceHelper.java	(date 1605830247069)
@@ -0,0 +1,391 @@
+package org.opencv.android;
+
+import java.io.File;
+import java.util.StringTokenizer;
+
+import org.opencv.core.Core;
+import org.opencv.engine.OpenCVEngineInterface;
+
+import android.content.ComponentName;
+import android.content.Context;
+import android.content.Intent;
+import android.content.ServiceConnection;
+import android.net.Uri;
+import android.os.IBinder;
+import android.os.RemoteException;
+import android.util.Log;
+
+class AsyncServiceHelper
+{
+    public static boolean initOpenCV(String Version, final Context AppContext,
+            final LoaderCallbackInterface Callback)
+    {
+        AsyncServiceHelper helper = new AsyncServiceHelper(Version, AppContext, Callback);
+        Intent intent = new Intent("org.opencv.engine.BIND");
+        intent.setPackage("org.opencv.engine");
+        if (AppContext.bindService(intent, helper.mServiceConnection, Context.BIND_AUTO_CREATE))
+        {
+            return true;
+        }
+        else
+        {
+            AppContext.unbindService(helper.mServiceConnection);
+            InstallService(AppContext, Callback);
+            return false;
+        }
+    }
+
+    protected AsyncServiceHelper(String Version, Context AppContext, LoaderCallbackInterface Callback)
+    {
+        mOpenCVersion = Version;
+        mUserAppCallback = Callback;
+        mAppContext = AppContext;
+    }
+
+    protected static final String TAG = "OpenCVManager/Helper";
+    protected static final int MINIMUM_ENGINE_VERSION = 2;
+    protected OpenCVEngineInterface mEngineService;
+    protected LoaderCallbackInterface mUserAppCallback;
+    protected String mOpenCVersion;
+    protected Context mAppContext;
+    protected static boolean mServiceInstallationProgress = false;
+    protected static boolean mLibraryInstallationProgress = false;
+
+    protected static boolean InstallServiceQuiet(Context context)
+    {
+        boolean result = true;
+        try
+        {
+            Intent intent = new Intent(Intent.ACTION_VIEW, Uri.parse(OPEN_CV_SERVICE_URL));
+            intent.addFlags(Intent.FLAG_ACTIVITY_NEW_TASK);
+            context.startActivity(intent);
+        }
+        catch(Exception e)
+        {
+            result = false;
+        }
+
+        return result;
+    }
+
+    protected static void InstallService(final Context AppContext, final LoaderCallbackInterface Callback)
+    {
+        if (!mServiceInstallationProgress)
+        {
+                Log.d(TAG, "Request new service installation");
+                InstallCallbackInterface InstallQuery = new InstallCallbackInterface() {
+                private LoaderCallbackInterface mUserAppCallback = Callback;
+                public String getPackageName()
+                {
+                    return "OpenCV Manager";
+                }
+                public void install() {
+                    Log.d(TAG, "Trying to install OpenCV Manager via Google Play");
+
+                    boolean result = InstallServiceQuiet(AppContext);
+                    if (result)
+                    {
+                        mServiceInstallationProgress = true;
+                        Log.d(TAG, "Package installation started");
+                    }
+                    else
+                    {
+                        Log.d(TAG, "OpenCV package was not installed!");
+                        int Status = LoaderCallbackInterface.MARKET_ERROR;
+                        Log.d(TAG, "Init finished with status " + Status);
+                        Log.d(TAG, "Unbind from service");
+                        Log.d(TAG, "Calling using callback");
+                        mUserAppCallback.onManagerConnected(Status);
+                    }
+                }
+
+                public void cancel()
+                {
+                    Log.d(TAG, "OpenCV library installation was canceled");
+                    int Status = LoaderCallbackInterface.INSTALL_CANCELED;
+                    Log.d(TAG, "Init finished with status " + Status);
+                    Log.d(TAG, "Calling using callback");
+                    mUserAppCallback.onManagerConnected(Status);
+                }
+
+                public void wait_install()
+                {
+                    Log.e(TAG, "Installation was not started! Nothing to wait!");
+                }
+            };
+
+            Callback.onPackageInstall(InstallCallbackInterface.NEW_INSTALLATION, InstallQuery);
+        }
+        else
+        {
+            Log.d(TAG, "Waiting current installation process");
+            InstallCallbackInterface WaitQuery = new InstallCallbackInterface() {
+                private LoaderCallbackInterface mUserAppCallback = Callback;
+                public String getPackageName()
+                {
+                    return "OpenCV Manager";
+                }
+                public void install()
+                {
+                    Log.e(TAG, "Nothing to install we just wait current installation");
+                }
+                public void cancel()
+                {
+                    Log.d(TAG, "Waiting for OpenCV canceled by user");
+                    mServiceInstallationProgress = false;
+                    int Status = LoaderCallbackInterface.INSTALL_CANCELED;
+                    Log.d(TAG, "Init finished with status " + Status);
+                    Log.d(TAG, "Calling using callback");
+                    mUserAppCallback.onManagerConnected(Status);
+                }
+                public void wait_install()
+                {
+                     InstallServiceQuiet(AppContext);
+                }
+            };
+
+            Callback.onPackageInstall(InstallCallbackInterface.INSTALLATION_PROGRESS, WaitQuery);
+        }
+    }
+
+    /**
+     *  URL of OpenCV Manager page on Google Play Market.
+     */
+    protected static final String OPEN_CV_SERVICE_URL = "market://details?id=org.opencv.engine";
+
+    protected ServiceConnection mServiceConnection = new ServiceConnection()
+    {
+        public void onServiceConnected(ComponentName className, IBinder service)
+        {
+            Log.d(TAG, "Service connection created");
+            mEngineService = OpenCVEngineInterface.Stub.asInterface(service);
+            if (null == mEngineService)
+            {
+                Log.d(TAG, "OpenCV Manager Service connection fails. May be service was not installed?");
+                InstallService(mAppContext, mUserAppCallback);
+            }
+            else
+            {
+                mServiceInstallationProgress = false;
+                try
+                {
+                    if (mEngineService.getEngineVersion() < MINIMUM_ENGINE_VERSION)
+                    {
+                        Log.d(TAG, "Init finished with status " + LoaderCallbackInterface.INCOMPATIBLE_MANAGER_VERSION);
+                        Log.d(TAG, "Unbind from service");
+                        mAppContext.unbindService(mServiceConnection);
+                        Log.d(TAG, "Calling using callback");
+                        mUserAppCallback.onManagerConnected(LoaderCallbackInterface.INCOMPATIBLE_MANAGER_VERSION);
+                        return;
+                    }
+
+                    Log.d(TAG, "Trying to get library path");
+                    String path = mEngineService.getLibPathByVersion(mOpenCVersion);
+                    if ((null == path) || (path.length() == 0))
+                    {
+                        if (!mLibraryInstallationProgress)
+                        {
+                            InstallCallbackInterface InstallQuery = new InstallCallbackInterface() {
+                                public String getPackageName()
+                                {
+                                    return "OpenCV library";
+                                }
+                                public void install() {
+                                    Log.d(TAG, "Trying to install OpenCV lib via Google Play");
+                                    try
+                                    {
+                                        if (mEngineService.installVersion(mOpenCVersion))
+                                        {
+                                            mLibraryInstallationProgress = true;
+                                            Log.d(TAG, "Package installation started");
+                                            Log.d(TAG, "Unbind from service");
+                                            mAppContext.unbindService(mServiceConnection);
+                                        }
+                                        else
+                                        {
+                                            Log.d(TAG, "OpenCV package was not installed!");
+                                            Log.d(TAG, "Init finished with status " + LoaderCallbackInterface.MARKET_ERROR);
+                                            Log.d(TAG, "Unbind from service");
+                                            mAppContext.unbindService(mServiceConnection);
+                                            Log.d(TAG, "Calling using callback");
+                                            mUserAppCallback.onManagerConnected(LoaderCallbackInterface.MARKET_ERROR);
+                                        }
+                                    } catch (RemoteException e) {
+                                        e.printStackTrace();;
+                                        Log.d(TAG, "Init finished with status " + LoaderCallbackInterface.INIT_FAILED);
+                                        Log.d(TAG, "Unbind from service");
+                                        mAppContext.unbindService(mServiceConnection);
+                                        Log.d(TAG, "Calling using callback");
+                                        mUserAppCallback.onManagerConnected(LoaderCallbackInterface.INIT_FAILED);
+                                    }
+                                }
+                                public void cancel() {
+                                    Log.d(TAG, "OpenCV library installation was canceled");
+                                    Log.d(TAG, "Init finished with status " + LoaderCallbackInterface.INSTALL_CANCELED);
+                                    Log.d(TAG, "Unbind from service");
+                                    mAppContext.unbindService(mServiceConnection);
+                                    Log.d(TAG, "Calling using callback");
+                                    mUserAppCallback.onManagerConnected(LoaderCallbackInterface.INSTALL_CANCELED);
+                                }
+                                public void wait_install() {
+                                    Log.e(TAG, "Installation was not started! Nothing to wait!");
+                                }
+                            };
+
+                            mUserAppCallback.onPackageInstall(InstallCallbackInterface.NEW_INSTALLATION, InstallQuery);
+                        }
+                        else
+                        {
+                            InstallCallbackInterface WaitQuery = new InstallCallbackInterface() {
+                                public String getPackageName()
+                                {
+                                    return "OpenCV library";
+                                }
+
+                                public void install() {
+                                    Log.e(TAG, "Nothing to install we just wait current installation");
+                                }
+                                public void cancel()
+                                {
+                                    Log.d(TAG, "OpenCV library installation was canceled");
+                                    mLibraryInstallationProgress = false;
+                                    Log.d(TAG, "Init finished with status " + LoaderCallbackInterface.INSTALL_CANCELED);
+                                    Log.d(TAG, "Unbind from service");
+                                    mAppContext.unbindService(mServiceConnection);
+                                    Log.d(TAG, "Calling using callback");
+                                        mUserAppCallback.onManagerConnected(LoaderCallbackInterface.INSTALL_CANCELED);
+                                }
+                                public void wait_install() {
+                                    Log.d(TAG, "Waiting for current installation");
+                                    try
+                                    {
+                                        if (!mEngineService.installVersion(mOpenCVersion))
+                                        {
+                                            Log.d(TAG, "OpenCV package was not installed!");
+                                            Log.d(TAG, "Init finished with status " + LoaderCallbackInterface.MARKET_ERROR);
+                                            Log.d(TAG, "Calling using callback");
+                                            mUserAppCallback.onManagerConnected(LoaderCallbackInterface.MARKET_ERROR);
+                                        }
+                                        else
+                                        {
+                                            Log.d(TAG, "Waiting for package installation");
+                                        }
+
+                                        Log.d(TAG, "Unbind from service");
+                                        mAppContext.unbindService(mServiceConnection);
+
+                                    } catch (RemoteException e) {
+                                        e.printStackTrace();
+                                        Log.d(TAG, "Init finished with status " + LoaderCallbackInterface.INIT_FAILED);
+                                        Log.d(TAG, "Unbind from service");
+                                        mAppContext.unbindService(mServiceConnection);
+                                        Log.d(TAG, "Calling using callback");
+                                        mUserAppCallback.onManagerConnected(LoaderCallbackInterface.INIT_FAILED);
+                                    }
+                               }
+                            };
+
+                            mUserAppCallback.onPackageInstall(InstallCallbackInterface.INSTALLATION_PROGRESS, WaitQuery);
+                        }
+                        return;
+                    }
+                    else
+                    {
+                        Log.d(TAG, "Trying to get library list");
+                        mLibraryInstallationProgress = false;
+                        String libs = mEngineService.getLibraryList(mOpenCVersion);
+                        Log.d(TAG, "Library list: \"" + libs + "\"");
+                        Log.d(TAG, "First attempt to load libs");
+                        int status;
+                        if (initOpenCVLibs(path, libs))
+                        {
+                            Log.d(TAG, "First attempt to load libs is OK");
+                            String eol = System.getProperty("line.separator");
+                            for (String str : Core.getBuildInformation().split(eol))
+                                Log.i(TAG, str);
+
+                            status = LoaderCallbackInterface.SUCCESS;
+                        }
+                        else
+                        {
+                            Log.d(TAG, "First attempt to load libs fails");
+                            status = LoaderCallbackInterface.INIT_FAILED;
+                        }
+
+                        Log.d(TAG, "Init finished with status " + status);
+                        Log.d(TAG, "Unbind from service");
+                        mAppContext.unbindService(mServiceConnection);
+                        Log.d(TAG, "Calling using callback");
+                        mUserAppCallback.onManagerConnected(status);
+                    }
+                }
+                catch (RemoteException e)
+                {
+                    e.printStackTrace();
+                    Log.d(TAG, "Init finished with status " + LoaderCallbackInterface.INIT_FAILED);
+                    Log.d(TAG, "Unbind from service");
+                    mAppContext.unbindService(mServiceConnection);
+                    Log.d(TAG, "Calling using callback");
+                    mUserAppCallback.onManagerConnected(LoaderCallbackInterface.INIT_FAILED);
+                }
+            }
+        }
+
+        public void onServiceDisconnected(ComponentName className)
+        {
+            mEngineService = null;
+        }
+    };
+
+    private boolean loadLibrary(String AbsPath)
+    {
+        boolean result = true;
+
+        Log.d(TAG, "Trying to load library " + AbsPath);
+        try
+        {
+            System.load(AbsPath);
+            Log.d(TAG, "OpenCV libs init was ok!");
+        }
+        catch(UnsatisfiedLinkError e)
+        {
+            Log.d(TAG, "Cannot load library \"" + AbsPath + "\"");
+            e.printStackTrace();
+            result = false;
+        }
+
+        return result;
+    }
+
+    private boolean initOpenCVLibs(String Path, String Libs)
+    {
+        Log.d(TAG, "Trying to init OpenCV libs");
+        if ((null != Path) && (Path.length() != 0))
+        {
+            boolean result = true;
+            if ((null != Libs) && (Libs.length() != 0))
+            {
+                Log.d(TAG, "Trying to load libs by dependency list");
+                StringTokenizer splitter = new StringTokenizer(Libs, ";");
+                while(splitter.hasMoreTokens())
+                {
+                    String AbsLibraryPath = Path + File.separator + splitter.nextToken();
+                    result &= loadLibrary(AbsLibraryPath);
+                }
+            }
+            else
+            {
+                // If the dependencies list is not defined or empty.
+                String AbsLibraryPath = Path + File.separator + "libopencv_java3.so";
+                result = loadLibrary(AbsLibraryPath);
+            }
+
+            return result;
+        }
+        else
+        {
+            Log.d(TAG, "Library path \"" + Path + "\" is empty");
+            return false;
+        }
+    }
+}
Index: app/src/main/java/com/maksym/findthis/Utils/Constants.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- app/src/main/java/com/maksym/findthis/Utils/Constants.java	(revision cc086539aa8e7367bf4575d32b97aa049d6faad9)
+++ app/src/main/java/com/maksym/findthis/Utils/Constants.java	(date 1607972156878)
@@ -20,7 +20,7 @@
     public static final int SIFT_DETECTOR_ID  = 8;   // scale rot affine (partially) invariant
 
     public static final String [] BIG_DETECTORS = {"AKAZE","BRISK","MSER","ORB","SIFT"};
-    public static final String [] SMALL_DETECTORS = {"FAST","GFTT"};
+    public static final String [] SMALL_DETECTORS = {"ORB","GFTT"};
 
 
     public static final String DATA_DIR_NAME = "Objects/";
@@ -32,7 +32,7 @@
 
 
     public static final float RATIO_THRESHOLD = 0.75f;
-    public static final double MATCHED_FEATURES_THRESHOLD = 0.5;
+    public static final double MATCHED_FEATURES_THRESHOLD = 0.2;
 
 
 }
Index: openCVLibrary3411/src/main/java/org/opencv/android/BaseLoaderCallback.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/android/BaseLoaderCallback.java	(date 1605830247079)
+++ openCVLibrary3411/src/main/java/org/opencv/android/BaseLoaderCallback.java	(date 1605830247079)
@@ -0,0 +1,141 @@
+package org.opencv.android;
+
+import android.app.Activity;
+import android.app.AlertDialog;
+import android.content.Context;
+import android.content.DialogInterface;
+import android.content.DialogInterface.OnClickListener;
+import android.util.Log;
+
+/**
+ * Basic implementation of LoaderCallbackInterface.
+ */
+public abstract class BaseLoaderCallback implements LoaderCallbackInterface {
+
+    public BaseLoaderCallback(Context AppContext) {
+        mAppContext = AppContext;
+    }
+
+    public void onManagerConnected(int status)
+    {
+        switch (status)
+        {
+            /** OpenCV initialization was successful. **/
+            case LoaderCallbackInterface.SUCCESS:
+            {
+                /** Application must override this method to handle successful library initialization. **/
+            } break;
+            /** OpenCV loader can not start Google Play Market. **/
+            case LoaderCallbackInterface.MARKET_ERROR:
+            {
+                Log.e(TAG, "Package installation failed!");
+                AlertDialog MarketErrorMessage = new AlertDialog.Builder(mAppContext).create();
+                MarketErrorMessage.setTitle("OpenCV Manager");
+                MarketErrorMessage.setMessage("Package installation failed!");
+                MarketErrorMessage.setCancelable(false); // This blocks the 'BACK' button
+                MarketErrorMessage.setButton(AlertDialog.BUTTON_POSITIVE, "OK", new OnClickListener() {
+                    public void onClick(DialogInterface dialog, int which) {
+                        finish();
+                    }
+                });
+                MarketErrorMessage.show();
+            } break;
+            /** Package installation has been canceled. **/
+            case LoaderCallbackInterface.INSTALL_CANCELED:
+            {
+                Log.d(TAG, "OpenCV library installation was canceled by user");
+                finish();
+            } break;
+            /** Application is incompatible with this version of OpenCV Manager. Possibly, a service update is required. **/
+            case LoaderCallbackInterface.INCOMPATIBLE_MANAGER_VERSION:
+            {
+                Log.d(TAG, "OpenCV Manager Service is uncompatible with this app!");
+                AlertDialog IncomatibilityMessage = new AlertDialog.Builder(mAppContext).create();
+                IncomatibilityMessage.setTitle("OpenCV Manager");
+                IncomatibilityMessage.setMessage("OpenCV Manager service is incompatible with this app. Try to update it via Google Play.");
+                IncomatibilityMessage.setCancelable(false); // This blocks the 'BACK' button
+                IncomatibilityMessage.setButton(AlertDialog.BUTTON_POSITIVE, "OK", new OnClickListener() {
+                    public void onClick(DialogInterface dialog, int which) {
+                        finish();
+                    }
+                });
+                IncomatibilityMessage.show();
+            } break;
+            /** Other status, i.e. INIT_FAILED. **/
+            default:
+            {
+                Log.e(TAG, "OpenCV loading failed!");
+                AlertDialog InitFailedDialog = new AlertDialog.Builder(mAppContext).create();
+                InitFailedDialog.setTitle("OpenCV error");
+                InitFailedDialog.setMessage("OpenCV was not initialised correctly. Application will be shut down");
+                InitFailedDialog.setCancelable(false); // This blocks the 'BACK' button
+                InitFailedDialog.setButton(AlertDialog.BUTTON_POSITIVE, "OK", new OnClickListener() {
+
+                    public void onClick(DialogInterface dialog, int which) {
+                        finish();
+                    }
+                });
+
+                InitFailedDialog.show();
+            } break;
+        }
+    }
+
+    public void onPackageInstall(final int operation, final InstallCallbackInterface callback)
+    {
+        switch (operation)
+        {
+            case InstallCallbackInterface.NEW_INSTALLATION:
+            {
+                AlertDialog InstallMessage = new AlertDialog.Builder(mAppContext).create();
+                InstallMessage.setTitle("Package not found");
+                InstallMessage.setMessage(callback.getPackageName() + " package was not found! Try to install it?");
+                InstallMessage.setCancelable(false); // This blocks the 'BACK' button
+                InstallMessage.setButton(AlertDialog.BUTTON_POSITIVE, "Yes", new OnClickListener()
+                {
+                    public void onClick(DialogInterface dialog, int which)
+                    {
+                        callback.install();
+                    }
+                });
+
+                InstallMessage.setButton(AlertDialog.BUTTON_NEGATIVE, "No", new OnClickListener() {
+
+                    public void onClick(DialogInterface dialog, int which)
+                    {
+                        callback.cancel();
+                    }
+                });
+
+                InstallMessage.show();
+            } break;
+            case InstallCallbackInterface.INSTALLATION_PROGRESS:
+            {
+                AlertDialog WaitMessage = new AlertDialog.Builder(mAppContext).create();
+                WaitMessage.setTitle("OpenCV is not ready");
+                WaitMessage.setMessage("Installation is in progress. Wait or exit?");
+                WaitMessage.setCancelable(false); // This blocks the 'BACK' button
+                WaitMessage.setButton(AlertDialog.BUTTON_POSITIVE, "Wait", new OnClickListener() {
+                    public void onClick(DialogInterface dialog, int which) {
+                        callback.wait_install();
+                    }
+                });
+                WaitMessage.setButton(AlertDialog.BUTTON_NEGATIVE, "Exit", new OnClickListener() {
+                    public void onClick(DialogInterface dialog, int which) {
+                        callback.cancel();
+                    }
+                });
+
+                WaitMessage.show();
+            } break;
+        }
+    }
+
+    void finish()
+    {
+        ((Activity) mAppContext).finish();
+    }
+
+    protected Context mAppContext;
+    private final static String TAG = "OCV/BaseLoaderCallback";
+}
Index: openCVLibrary3411/src/main/java/org/opencv/android/Camera2Renderer.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/android/Camera2Renderer.java	(date 1605830247081)
+++ openCVLibrary3411/src/main/java/org/opencv/android/Camera2Renderer.java	(date 1605830247081)
@@ -0,0 +1,302 @@
+package org.opencv.android;
+
+import java.util.Arrays;
+import java.util.concurrent.Semaphore;
+import java.util.concurrent.TimeUnit;
+import android.annotation.TargetApi;
+import android.content.Context;
+import android.graphics.SurfaceTexture;
+import android.hardware.camera2.CameraAccessException;
+import android.hardware.camera2.CameraCaptureSession;
+import android.hardware.camera2.CameraCharacteristics;
+import android.hardware.camera2.CameraDevice;
+import android.hardware.camera2.CameraManager;
+import android.hardware.camera2.CaptureRequest;
+import android.hardware.camera2.params.StreamConfigurationMap;
+import android.os.Handler;
+import android.os.HandlerThread;
+import android.util.Log;
+import android.util.Size;
+import android.view.Surface;
+
+@TargetApi(21)
+public class Camera2Renderer extends CameraGLRendererBase {
+
+    protected final String LOGTAG = "Camera2Renderer";
+    private CameraDevice mCameraDevice;
+    private CameraCaptureSession mCaptureSession;
+    private CaptureRequest.Builder mPreviewRequestBuilder;
+    private String mCameraID;
+    private Size mPreviewSize = new Size(-1, -1);
+
+    private HandlerThread mBackgroundThread;
+    private Handler mBackgroundHandler;
+    private Semaphore mCameraOpenCloseLock = new Semaphore(1);
+
+    Camera2Renderer(CameraGLSurfaceView view) {
+        super(view);
+    }
+
+    @Override
+    protected void doStart() {
+        Log.d(LOGTAG, "doStart");
+        startBackgroundThread();
+        super.doStart();
+    }
+
+
+    @Override
+    protected void doStop() {
+        Log.d(LOGTAG, "doStop");
+        super.doStop();
+        stopBackgroundThread();
+    }
+
+    boolean cacPreviewSize(final int width, final int height) {
+        Log.i(LOGTAG, "cacPreviewSize: "+width+"x"+height);
+        if(mCameraID == null) {
+            Log.e(LOGTAG, "Camera isn't initialized!");
+            return false;
+        }
+        CameraManager manager = (CameraManager) mView.getContext()
+                .getSystemService(Context.CAMERA_SERVICE);
+        try {
+            CameraCharacteristics characteristics = manager
+                    .getCameraCharacteristics(mCameraID);
+            StreamConfigurationMap map = characteristics
+                    .get(CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);
+            int bestWidth = 0, bestHeight = 0;
+            float aspect = (float)width / height;
+            for (Size psize : map.getOutputSizes(SurfaceTexture.class)) {
+                int w = psize.getWidth(), h = psize.getHeight();
+                Log.d(LOGTAG, "trying size: "+w+"x"+h);
+                if ( width >= w && height >= h &&
+                     bestWidth <= w && bestHeight <= h &&
+                     Math.abs(aspect - (float)w/h) < 0.2 ) {
+                    bestWidth = w;
+                    bestHeight = h;
+                }
+            }
+            Log.i(LOGTAG, "best size: "+bestWidth+"x"+bestHeight);
+            if( bestWidth == 0 || bestHeight == 0 ||
+                mPreviewSize.getWidth() == bestWidth &&
+                mPreviewSize.getHeight() == bestHeight )
+                return false;
+            else {
+                mPreviewSize = new Size(bestWidth, bestHeight);
+                return true;
+            }
+        } catch (CameraAccessException e) {
+            Log.e(LOGTAG, "cacPreviewSize - Camera Access Exception");
+        } catch (IllegalArgumentException e) {
+            Log.e(LOGTAG, "cacPreviewSize - Illegal Argument Exception");
+        } catch (SecurityException e) {
+            Log.e(LOGTAG, "cacPreviewSize - Security Exception");
+        }
+        return false;
+    }
+
+    @Override
+    protected void openCamera(int id) {
+        Log.i(LOGTAG, "openCamera");
+        CameraManager manager = (CameraManager) mView.getContext().getSystemService(Context.CAMERA_SERVICE);
+        try {
+            String camList[] = manager.getCameraIdList();
+            if(camList.length == 0) {
+                Log.e(LOGTAG, "Error: camera isn't detected.");
+                return;
+            }
+            if(id == CameraBridgeViewBase.CAMERA_ID_ANY) {
+                mCameraID = camList[0];
+            } else {
+                for (String cameraID : camList) {
+                    CameraCharacteristics characteristics = manager.getCameraCharacteristics(cameraID);
+                    if( id == CameraBridgeViewBase.CAMERA_ID_BACK &&
+                        characteristics.get(CameraCharacteristics.LENS_FACING) == CameraCharacteristics.LENS_FACING_BACK ||
+                        id == CameraBridgeViewBase.CAMERA_ID_FRONT &&
+                        characteristics.get(CameraCharacteristics.LENS_FACING) == CameraCharacteristics.LENS_FACING_FRONT) {
+                        mCameraID = cameraID;
+                        break;
+                    }
+                }
+            }
+            if(mCameraID != null) {
+                if (!mCameraOpenCloseLock.tryAcquire(2500, TimeUnit.MILLISECONDS)) {
+                    throw new RuntimeException(
+                            "Time out waiting to lock camera opening.");
+                }
+                Log.i(LOGTAG, "Opening camera: " + mCameraID);
+                manager.openCamera(mCameraID, mStateCallback, mBackgroundHandler);
+            }
+        } catch (CameraAccessException e) {
+            Log.e(LOGTAG, "OpenCamera - Camera Access Exception");
+        } catch (IllegalArgumentException e) {
+            Log.e(LOGTAG, "OpenCamera - Illegal Argument Exception");
+        } catch (SecurityException e) {
+            Log.e(LOGTAG, "OpenCamera - Security Exception");
+        } catch (InterruptedException e) {
+            Log.e(LOGTAG, "OpenCamera - Interrupted Exception");
+        }
+    }
+
+    @Override
+    protected void closeCamera() {
+        Log.i(LOGTAG, "closeCamera");
+        try {
+            mCameraOpenCloseLock.acquire();
+            if (null != mCaptureSession) {
+                mCaptureSession.close();
+                mCaptureSession = null;
+            }
+            if (null != mCameraDevice) {
+                mCameraDevice.close();
+                mCameraDevice = null;
+            }
+        } catch (InterruptedException e) {
+            throw new RuntimeException("Interrupted while trying to lock camera closing.", e);
+        } finally {
+            mCameraOpenCloseLock.release();
+        }
+    }
+
+    private final CameraDevice.StateCallback mStateCallback = new CameraDevice.StateCallback() {
+
+        @Override
+        public void onOpened(CameraDevice cameraDevice) {
+            mCameraDevice = cameraDevice;
+            mCameraOpenCloseLock.release();
+            createCameraPreviewSession();
+        }
+
+        @Override
+        public void onDisconnected(CameraDevice cameraDevice) {
+            cameraDevice.close();
+            mCameraDevice = null;
+            mCameraOpenCloseLock.release();
+        }
+
+        @Override
+        public void onError(CameraDevice cameraDevice, int error) {
+            cameraDevice.close();
+            mCameraDevice = null;
+            mCameraOpenCloseLock.release();
+        }
+
+    };
+
+    private void createCameraPreviewSession() {
+        int w=mPreviewSize.getWidth(), h=mPreviewSize.getHeight();
+        Log.i(LOGTAG, "createCameraPreviewSession("+w+"x"+h+")");
+        if(w<0 || h<0)
+            return;
+        try {
+            mCameraOpenCloseLock.acquire();
+            if (null == mCameraDevice) {
+                mCameraOpenCloseLock.release();
+                Log.e(LOGTAG, "createCameraPreviewSession: camera isn't opened");
+                return;
+            }
+            if (null != mCaptureSession) {
+                mCameraOpenCloseLock.release();
+                Log.e(LOGTAG, "createCameraPreviewSession: mCaptureSession is already started");
+                return;
+            }
+            if(null == mSTexture) {
+                mCameraOpenCloseLock.release();
+                Log.e(LOGTAG, "createCameraPreviewSession: preview SurfaceTexture is null");
+                return;
+            }
+            mSTexture.setDefaultBufferSize(w, h);
+
+            Surface surface = new Surface(mSTexture);
+
+            mPreviewRequestBuilder = mCameraDevice
+                    .createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
+            mPreviewRequestBuilder.addTarget(surface);
+
+            mCameraDevice.createCaptureSession(Arrays.asList(surface),
+                    new CameraCaptureSession.StateCallback() {
+                        @Override
+                        public void onConfigured( CameraCaptureSession cameraCaptureSession) {
+                            mCaptureSession = cameraCaptureSession;
+                            try {
+                                mPreviewRequestBuilder.set(CaptureRequest.CONTROL_AF_MODE, CaptureRequest.CONTROL_AF_MODE_CONTINUOUS_PICTURE);
+                                mPreviewRequestBuilder.set(CaptureRequest.CONTROL_AE_MODE, CaptureRequest.CONTROL_AE_MODE_ON_AUTO_FLASH);
+
+                                mCaptureSession.setRepeatingRequest(mPreviewRequestBuilder.build(), null, mBackgroundHandler);
+                                Log.i(LOGTAG, "CameraPreviewSession has been started");
+                            } catch (CameraAccessException e) {
+                                Log.e(LOGTAG, "createCaptureSession failed");
+                            }
+                            mCameraOpenCloseLock.release();
+                        }
+
+                        @Override
+                        public void onConfigureFailed(
+                                CameraCaptureSession cameraCaptureSession) {
+                            Log.e(LOGTAG, "createCameraPreviewSession failed");
+                            mCameraOpenCloseLock.release();
+                        }
+                    }, mBackgroundHandler);
+        } catch (CameraAccessException e) {
+            Log.e(LOGTAG, "createCameraPreviewSession");
+        } catch (InterruptedException e) {
+            throw new RuntimeException(
+                    "Interrupted while createCameraPreviewSession", e);
+        }
+        finally {
+            //mCameraOpenCloseLock.release();
+        }
+    }
+
+    private void startBackgroundThread() {
+        Log.i(LOGTAG, "startBackgroundThread");
+        stopBackgroundThread();
+        mBackgroundThread = new HandlerThread("CameraBackground");
+        mBackgroundThread.start();
+        mBackgroundHandler = new Handler(mBackgroundThread.getLooper());
+    }
+
+    private void stopBackgroundThread() {
+        Log.i(LOGTAG, "stopBackgroundThread");
+        if(mBackgroundThread == null)
+            return;
+        mBackgroundThread.quitSafely();
+        try {
+            mBackgroundThread.join();
+            mBackgroundThread = null;
+            mBackgroundHandler = null;
+        } catch (InterruptedException e) {
+            Log.e(LOGTAG, "stopBackgroundThread");
+        }
+    }
+
+    @Override
+    protected void setCameraPreviewSize(int width, int height) {
+        Log.i(LOGTAG, "setCameraPreviewSize("+width+"x"+height+")");
+        if(mMaxCameraWidth  > 0 && mMaxCameraWidth  < width)  width  = mMaxCameraWidth;
+        if(mMaxCameraHeight > 0 && mMaxCameraHeight < height) height = mMaxCameraHeight;
+        try {
+            mCameraOpenCloseLock.acquire();
+
+            boolean needReconfig = cacPreviewSize(width, height);
+            mCameraWidth  = mPreviewSize.getWidth();
+            mCameraHeight = mPreviewSize.getHeight();
+
+            if( !needReconfig ) {
+                mCameraOpenCloseLock.release();
+                return;
+            }
+            if (null != mCaptureSession) {
+                Log.d(LOGTAG, "closing existing previewSession");
+                mCaptureSession.close();
+                mCaptureSession = null;
+            }
+            mCameraOpenCloseLock.release();
+            createCameraPreviewSession();
+        } catch (InterruptedException e) {
+            mCameraOpenCloseLock.release();
+            throw new RuntimeException("Interrupted while setCameraPreviewSize.", e);
+        }
+    }
+}
Index: openCVLibrary3411/src/main/java/org/opencv/video/FarnebackOpticalFlow.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/video/FarnebackOpticalFlow.java	(date 1605830248280)
+++ openCVLibrary3411/src/main/java/org/opencv/video/FarnebackOpticalFlow.java	(date 1605830248280)
@@ -0,0 +1,274 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.video;
+
+import org.opencv.video.DenseOpticalFlow;
+import org.opencv.video.FarnebackOpticalFlow;
+
+// C++: class FarnebackOpticalFlow
+/**
+ * Class computing a dense optical flow using the Gunnar Farneback's algorithm.
+ */
+public class FarnebackOpticalFlow extends DenseOpticalFlow {
+
+    protected FarnebackOpticalFlow(long addr) { super(addr); }
+
+    // internal usage only
+    public static FarnebackOpticalFlow __fromPtr__(long addr) { return new FarnebackOpticalFlow(addr); }
+
+    //
+    // C++: static Ptr_FarnebackOpticalFlow cv::FarnebackOpticalFlow::create(int numLevels = 5, double pyrScale = 0.5, bool fastPyramids = false, int winSize = 13, int numIters = 10, int polyN = 5, double polySigma = 1.1, int flags = 0)
+    //
+
+    public static FarnebackOpticalFlow create(int numLevels, double pyrScale, boolean fastPyramids, int winSize, int numIters, int polyN, double polySigma, int flags) {
+        return FarnebackOpticalFlow.__fromPtr__(create_0(numLevels, pyrScale, fastPyramids, winSize, numIters, polyN, polySigma, flags));
+    }
+
+    public static FarnebackOpticalFlow create(int numLevels, double pyrScale, boolean fastPyramids, int winSize, int numIters, int polyN, double polySigma) {
+        return FarnebackOpticalFlow.__fromPtr__(create_1(numLevels, pyrScale, fastPyramids, winSize, numIters, polyN, polySigma));
+    }
+
+    public static FarnebackOpticalFlow create(int numLevels, double pyrScale, boolean fastPyramids, int winSize, int numIters, int polyN) {
+        return FarnebackOpticalFlow.__fromPtr__(create_2(numLevels, pyrScale, fastPyramids, winSize, numIters, polyN));
+    }
+
+    public static FarnebackOpticalFlow create(int numLevels, double pyrScale, boolean fastPyramids, int winSize, int numIters) {
+        return FarnebackOpticalFlow.__fromPtr__(create_3(numLevels, pyrScale, fastPyramids, winSize, numIters));
+    }
+
+    public static FarnebackOpticalFlow create(int numLevels, double pyrScale, boolean fastPyramids, int winSize) {
+        return FarnebackOpticalFlow.__fromPtr__(create_4(numLevels, pyrScale, fastPyramids, winSize));
+    }
+
+    public static FarnebackOpticalFlow create(int numLevels, double pyrScale, boolean fastPyramids) {
+        return FarnebackOpticalFlow.__fromPtr__(create_5(numLevels, pyrScale, fastPyramids));
+    }
+
+    public static FarnebackOpticalFlow create(int numLevels, double pyrScale) {
+        return FarnebackOpticalFlow.__fromPtr__(create_6(numLevels, pyrScale));
+    }
+
+    public static FarnebackOpticalFlow create(int numLevels) {
+        return FarnebackOpticalFlow.__fromPtr__(create_7(numLevels));
+    }
+
+    public static FarnebackOpticalFlow create() {
+        return FarnebackOpticalFlow.__fromPtr__(create_8());
+    }
+
+
+    //
+    // C++:  bool cv::FarnebackOpticalFlow::getFastPyramids()
+    //
+
+    public boolean getFastPyramids() {
+        return getFastPyramids_0(nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::FarnebackOpticalFlow::getPolySigma()
+    //
+
+    public double getPolySigma() {
+        return getPolySigma_0(nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::FarnebackOpticalFlow::getPyrScale()
+    //
+
+    public double getPyrScale() {
+        return getPyrScale_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::FarnebackOpticalFlow::getFlags()
+    //
+
+    public int getFlags() {
+        return getFlags_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::FarnebackOpticalFlow::getNumIters()
+    //
+
+    public int getNumIters() {
+        return getNumIters_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::FarnebackOpticalFlow::getNumLevels()
+    //
+
+    public int getNumLevels() {
+        return getNumLevels_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::FarnebackOpticalFlow::getPolyN()
+    //
+
+    public int getPolyN() {
+        return getPolyN_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::FarnebackOpticalFlow::getWinSize()
+    //
+
+    public int getWinSize() {
+        return getWinSize_0(nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::FarnebackOpticalFlow::setFastPyramids(bool fastPyramids)
+    //
+
+    public void setFastPyramids(boolean fastPyramids) {
+        setFastPyramids_0(nativeObj, fastPyramids);
+    }
+
+
+    //
+    // C++:  void cv::FarnebackOpticalFlow::setFlags(int flags)
+    //
+
+    public void setFlags(int flags) {
+        setFlags_0(nativeObj, flags);
+    }
+
+
+    //
+    // C++:  void cv::FarnebackOpticalFlow::setNumIters(int numIters)
+    //
+
+    public void setNumIters(int numIters) {
+        setNumIters_0(nativeObj, numIters);
+    }
+
+
+    //
+    // C++:  void cv::FarnebackOpticalFlow::setNumLevels(int numLevels)
+    //
+
+    public void setNumLevels(int numLevels) {
+        setNumLevels_0(nativeObj, numLevels);
+    }
+
+
+    //
+    // C++:  void cv::FarnebackOpticalFlow::setPolyN(int polyN)
+    //
+
+    public void setPolyN(int polyN) {
+        setPolyN_0(nativeObj, polyN);
+    }
+
+
+    //
+    // C++:  void cv::FarnebackOpticalFlow::setPolySigma(double polySigma)
+    //
+
+    public void setPolySigma(double polySigma) {
+        setPolySigma_0(nativeObj, polySigma);
+    }
+
+
+    //
+    // C++:  void cv::FarnebackOpticalFlow::setPyrScale(double pyrScale)
+    //
+
+    public void setPyrScale(double pyrScale) {
+        setPyrScale_0(nativeObj, pyrScale);
+    }
+
+
+    //
+    // C++:  void cv::FarnebackOpticalFlow::setWinSize(int winSize)
+    //
+
+    public void setWinSize(int winSize) {
+        setWinSize_0(nativeObj, winSize);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++: static Ptr_FarnebackOpticalFlow cv::FarnebackOpticalFlow::create(int numLevels = 5, double pyrScale = 0.5, bool fastPyramids = false, int winSize = 13, int numIters = 10, int polyN = 5, double polySigma = 1.1, int flags = 0)
+    private static native long create_0(int numLevels, double pyrScale, boolean fastPyramids, int winSize, int numIters, int polyN, double polySigma, int flags);
+    private static native long create_1(int numLevels, double pyrScale, boolean fastPyramids, int winSize, int numIters, int polyN, double polySigma);
+    private static native long create_2(int numLevels, double pyrScale, boolean fastPyramids, int winSize, int numIters, int polyN);
+    private static native long create_3(int numLevels, double pyrScale, boolean fastPyramids, int winSize, int numIters);
+    private static native long create_4(int numLevels, double pyrScale, boolean fastPyramids, int winSize);
+    private static native long create_5(int numLevels, double pyrScale, boolean fastPyramids);
+    private static native long create_6(int numLevels, double pyrScale);
+    private static native long create_7(int numLevels);
+    private static native long create_8();
+
+    // C++:  bool cv::FarnebackOpticalFlow::getFastPyramids()
+    private static native boolean getFastPyramids_0(long nativeObj);
+
+    // C++:  double cv::FarnebackOpticalFlow::getPolySigma()
+    private static native double getPolySigma_0(long nativeObj);
+
+    // C++:  double cv::FarnebackOpticalFlow::getPyrScale()
+    private static native double getPyrScale_0(long nativeObj);
+
+    // C++:  int cv::FarnebackOpticalFlow::getFlags()
+    private static native int getFlags_0(long nativeObj);
+
+    // C++:  int cv::FarnebackOpticalFlow::getNumIters()
+    private static native int getNumIters_0(long nativeObj);
+
+    // C++:  int cv::FarnebackOpticalFlow::getNumLevels()
+    private static native int getNumLevels_0(long nativeObj);
+
+    // C++:  int cv::FarnebackOpticalFlow::getPolyN()
+    private static native int getPolyN_0(long nativeObj);
+
+    // C++:  int cv::FarnebackOpticalFlow::getWinSize()
+    private static native int getWinSize_0(long nativeObj);
+
+    // C++:  void cv::FarnebackOpticalFlow::setFastPyramids(bool fastPyramids)
+    private static native void setFastPyramids_0(long nativeObj, boolean fastPyramids);
+
+    // C++:  void cv::FarnebackOpticalFlow::setFlags(int flags)
+    private static native void setFlags_0(long nativeObj, int flags);
+
+    // C++:  void cv::FarnebackOpticalFlow::setNumIters(int numIters)
+    private static native void setNumIters_0(long nativeObj, int numIters);
+
+    // C++:  void cv::FarnebackOpticalFlow::setNumLevels(int numLevels)
+    private static native void setNumLevels_0(long nativeObj, int numLevels);
+
+    // C++:  void cv::FarnebackOpticalFlow::setPolyN(int polyN)
+    private static native void setPolyN_0(long nativeObj, int polyN);
+
+    // C++:  void cv::FarnebackOpticalFlow::setPolySigma(double polySigma)
+    private static native void setPolySigma_0(long nativeObj, double polySigma);
+
+    // C++:  void cv::FarnebackOpticalFlow::setPyrScale(double pyrScale)
+    private static native void setPyrScale_0(long nativeObj, double pyrScale);
+
+    // C++:  void cv::FarnebackOpticalFlow::setWinSize(int winSize)
+    private static native void setWinSize_0(long nativeObj, int winSize);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/video/KalmanFilter.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/video/KalmanFilter.java	(date 1605830248285)
+++ openCVLibrary3411/src/main/java/org/opencv/video/KalmanFilter.java	(date 1605830248285)
@@ -0,0 +1,376 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.video;
+
+import org.opencv.core.Mat;
+
+// C++: class KalmanFilter
+/**
+ * Kalman filter class.
+ *
+ * The class implements a standard Kalman filter &lt;http://en.wikipedia.org/wiki/Kalman_filter&gt;,
+ * CITE: Welch95 . However, you can modify transitionMatrix, controlMatrix, and measurementMatrix to get
+ * an extended Kalman filter functionality.
+ * <b>Note:</b> In C API when CvKalman\* kalmanFilter structure is not needed anymore, it should be released
+ * with cvReleaseKalman(&amp;kalmanFilter)
+ */
+public class KalmanFilter {
+
+    protected final long nativeObj;
+    protected KalmanFilter(long addr) { nativeObj = addr; }
+
+    public long getNativeObjAddr() { return nativeObj; }
+
+    // internal usage only
+    public static KalmanFilter __fromPtr__(long addr) { return new KalmanFilter(addr); }
+
+    //
+    // C++:   cv::KalmanFilter::KalmanFilter(int dynamParams, int measureParams, int controlParams = 0, int type = CV_32F)
+    //
+
+    /**
+     *
+     *     @param dynamParams Dimensionality of the state.
+     *     @param measureParams Dimensionality of the measurement.
+     *     @param controlParams Dimensionality of the control vector.
+     *     @param type Type of the created matrices that should be CV_32F or CV_64F.
+     */
+    public KalmanFilter(int dynamParams, int measureParams, int controlParams, int type) {
+        nativeObj = KalmanFilter_0(dynamParams, measureParams, controlParams, type);
+    }
+
+    /**
+     *
+     *     @param dynamParams Dimensionality of the state.
+     *     @param measureParams Dimensionality of the measurement.
+     *     @param controlParams Dimensionality of the control vector.
+     */
+    public KalmanFilter(int dynamParams, int measureParams, int controlParams) {
+        nativeObj = KalmanFilter_1(dynamParams, measureParams, controlParams);
+    }
+
+    /**
+     *
+     *     @param dynamParams Dimensionality of the state.
+     *     @param measureParams Dimensionality of the measurement.
+     */
+    public KalmanFilter(int dynamParams, int measureParams) {
+        nativeObj = KalmanFilter_2(dynamParams, measureParams);
+    }
+
+
+    //
+    // C++:   cv::KalmanFilter::KalmanFilter()
+    //
+
+    public KalmanFilter() {
+        nativeObj = KalmanFilter_3();
+    }
+
+
+    //
+    // C++:  Mat cv::KalmanFilter::correct(Mat measurement)
+    //
+
+    /**
+     * Updates the predicted state from the measurement.
+     *
+     *     @param measurement The measured system parameters
+     * @return automatically generated
+     */
+    public Mat correct(Mat measurement) {
+        return new Mat(correct_0(nativeObj, measurement.nativeObj));
+    }
+
+
+    //
+    // C++:  Mat cv::KalmanFilter::predict(Mat control = Mat())
+    //
+
+    /**
+     * Computes a predicted state.
+     *
+     *     @param control The optional input control
+     * @return automatically generated
+     */
+    public Mat predict(Mat control) {
+        return new Mat(predict_0(nativeObj, control.nativeObj));
+    }
+
+    /**
+     * Computes a predicted state.
+     *
+     * @return automatically generated
+     */
+    public Mat predict() {
+        return new Mat(predict_1(nativeObj));
+    }
+
+
+    //
+    // C++: Mat KalmanFilter::statePre
+    //
+
+    public Mat get_statePre() {
+        return new Mat(get_statePre_0(nativeObj));
+    }
+
+
+    //
+    // C++: void KalmanFilter::statePre
+    //
+
+    public void set_statePre(Mat statePre) {
+        set_statePre_0(nativeObj, statePre.nativeObj);
+    }
+
+
+    //
+    // C++: Mat KalmanFilter::statePost
+    //
+
+    public Mat get_statePost() {
+        return new Mat(get_statePost_0(nativeObj));
+    }
+
+
+    //
+    // C++: void KalmanFilter::statePost
+    //
+
+    public void set_statePost(Mat statePost) {
+        set_statePost_0(nativeObj, statePost.nativeObj);
+    }
+
+
+    //
+    // C++: Mat KalmanFilter::transitionMatrix
+    //
+
+    public Mat get_transitionMatrix() {
+        return new Mat(get_transitionMatrix_0(nativeObj));
+    }
+
+
+    //
+    // C++: void KalmanFilter::transitionMatrix
+    //
+
+    public void set_transitionMatrix(Mat transitionMatrix) {
+        set_transitionMatrix_0(nativeObj, transitionMatrix.nativeObj);
+    }
+
+
+    //
+    // C++: Mat KalmanFilter::controlMatrix
+    //
+
+    public Mat get_controlMatrix() {
+        return new Mat(get_controlMatrix_0(nativeObj));
+    }
+
+
+    //
+    // C++: void KalmanFilter::controlMatrix
+    //
+
+    public void set_controlMatrix(Mat controlMatrix) {
+        set_controlMatrix_0(nativeObj, controlMatrix.nativeObj);
+    }
+
+
+    //
+    // C++: Mat KalmanFilter::measurementMatrix
+    //
+
+    public Mat get_measurementMatrix() {
+        return new Mat(get_measurementMatrix_0(nativeObj));
+    }
+
+
+    //
+    // C++: void KalmanFilter::measurementMatrix
+    //
+
+    public void set_measurementMatrix(Mat measurementMatrix) {
+        set_measurementMatrix_0(nativeObj, measurementMatrix.nativeObj);
+    }
+
+
+    //
+    // C++: Mat KalmanFilter::processNoiseCov
+    //
+
+    public Mat get_processNoiseCov() {
+        return new Mat(get_processNoiseCov_0(nativeObj));
+    }
+
+
+    //
+    // C++: void KalmanFilter::processNoiseCov
+    //
+
+    public void set_processNoiseCov(Mat processNoiseCov) {
+        set_processNoiseCov_0(nativeObj, processNoiseCov.nativeObj);
+    }
+
+
+    //
+    // C++: Mat KalmanFilter::measurementNoiseCov
+    //
+
+    public Mat get_measurementNoiseCov() {
+        return new Mat(get_measurementNoiseCov_0(nativeObj));
+    }
+
+
+    //
+    // C++: void KalmanFilter::measurementNoiseCov
+    //
+
+    public void set_measurementNoiseCov(Mat measurementNoiseCov) {
+        set_measurementNoiseCov_0(nativeObj, measurementNoiseCov.nativeObj);
+    }
+
+
+    //
+    // C++: Mat KalmanFilter::errorCovPre
+    //
+
+    public Mat get_errorCovPre() {
+        return new Mat(get_errorCovPre_0(nativeObj));
+    }
+
+
+    //
+    // C++: void KalmanFilter::errorCovPre
+    //
+
+    public void set_errorCovPre(Mat errorCovPre) {
+        set_errorCovPre_0(nativeObj, errorCovPre.nativeObj);
+    }
+
+
+    //
+    // C++: Mat KalmanFilter::gain
+    //
+
+    public Mat get_gain() {
+        return new Mat(get_gain_0(nativeObj));
+    }
+
+
+    //
+    // C++: void KalmanFilter::gain
+    //
+
+    public void set_gain(Mat gain) {
+        set_gain_0(nativeObj, gain.nativeObj);
+    }
+
+
+    //
+    // C++: Mat KalmanFilter::errorCovPost
+    //
+
+    public Mat get_errorCovPost() {
+        return new Mat(get_errorCovPost_0(nativeObj));
+    }
+
+
+    //
+    // C++: void KalmanFilter::errorCovPost
+    //
+
+    public void set_errorCovPost(Mat errorCovPost) {
+        set_errorCovPost_0(nativeObj, errorCovPost.nativeObj);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++:   cv::KalmanFilter::KalmanFilter(int dynamParams, int measureParams, int controlParams = 0, int type = CV_32F)
+    private static native long KalmanFilter_0(int dynamParams, int measureParams, int controlParams, int type);
+    private static native long KalmanFilter_1(int dynamParams, int measureParams, int controlParams);
+    private static native long KalmanFilter_2(int dynamParams, int measureParams);
+
+    // C++:   cv::KalmanFilter::KalmanFilter()
+    private static native long KalmanFilter_3();
+
+    // C++:  Mat cv::KalmanFilter::correct(Mat measurement)
+    private static native long correct_0(long nativeObj, long measurement_nativeObj);
+
+    // C++:  Mat cv::KalmanFilter::predict(Mat control = Mat())
+    private static native long predict_0(long nativeObj, long control_nativeObj);
+    private static native long predict_1(long nativeObj);
+
+    // C++: Mat KalmanFilter::statePre
+    private static native long get_statePre_0(long nativeObj);
+
+    // C++: void KalmanFilter::statePre
+    private static native void set_statePre_0(long nativeObj, long statePre_nativeObj);
+
+    // C++: Mat KalmanFilter::statePost
+    private static native long get_statePost_0(long nativeObj);
+
+    // C++: void KalmanFilter::statePost
+    private static native void set_statePost_0(long nativeObj, long statePost_nativeObj);
+
+    // C++: Mat KalmanFilter::transitionMatrix
+    private static native long get_transitionMatrix_0(long nativeObj);
+
+    // C++: void KalmanFilter::transitionMatrix
+    private static native void set_transitionMatrix_0(long nativeObj, long transitionMatrix_nativeObj);
+
+    // C++: Mat KalmanFilter::controlMatrix
+    private static native long get_controlMatrix_0(long nativeObj);
+
+    // C++: void KalmanFilter::controlMatrix
+    private static native void set_controlMatrix_0(long nativeObj, long controlMatrix_nativeObj);
+
+    // C++: Mat KalmanFilter::measurementMatrix
+    private static native long get_measurementMatrix_0(long nativeObj);
+
+    // C++: void KalmanFilter::measurementMatrix
+    private static native void set_measurementMatrix_0(long nativeObj, long measurementMatrix_nativeObj);
+
+    // C++: Mat KalmanFilter::processNoiseCov
+    private static native long get_processNoiseCov_0(long nativeObj);
+
+    // C++: void KalmanFilter::processNoiseCov
+    private static native void set_processNoiseCov_0(long nativeObj, long processNoiseCov_nativeObj);
+
+    // C++: Mat KalmanFilter::measurementNoiseCov
+    private static native long get_measurementNoiseCov_0(long nativeObj);
+
+    // C++: void KalmanFilter::measurementNoiseCov
+    private static native void set_measurementNoiseCov_0(long nativeObj, long measurementNoiseCov_nativeObj);
+
+    // C++: Mat KalmanFilter::errorCovPre
+    private static native long get_errorCovPre_0(long nativeObj);
+
+    // C++: void KalmanFilter::errorCovPre
+    private static native void set_errorCovPre_0(long nativeObj, long errorCovPre_nativeObj);
+
+    // C++: Mat KalmanFilter::gain
+    private static native long get_gain_0(long nativeObj);
+
+    // C++: void KalmanFilter::gain
+    private static native void set_gain_0(long nativeObj, long gain_nativeObj);
+
+    // C++: Mat KalmanFilter::errorCovPost
+    private static native long get_errorCovPost_0(long nativeObj);
+
+    // C++: void KalmanFilter::errorCovPost
+    private static native void set_errorCovPost_0(long nativeObj, long errorCovPost_nativeObj);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/video/SparseOpticalFlow.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/video/SparseOpticalFlow.java	(date 1605830248287)
+++ openCVLibrary3411/src/main/java/org/opencv/video/SparseOpticalFlow.java	(date 1605830248287)
@@ -0,0 +1,68 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.video;
+
+import org.opencv.core.Algorithm;
+import org.opencv.core.Mat;
+
+// C++: class SparseOpticalFlow
+/**
+ * Base interface for sparse optical flow algorithms.
+ */
+public class SparseOpticalFlow extends Algorithm {
+
+    protected SparseOpticalFlow(long addr) { super(addr); }
+
+    // internal usage only
+    public static SparseOpticalFlow __fromPtr__(long addr) { return new SparseOpticalFlow(addr); }
+
+    //
+    // C++:  void cv::SparseOpticalFlow::calc(Mat prevImg, Mat nextImg, Mat prevPts, Mat& nextPts, Mat& status, Mat& err = cv::Mat())
+    //
+
+    /**
+     * Calculates a sparse optical flow.
+     *
+     *     @param prevImg First input image.
+     *     @param nextImg Second input image of the same size and the same type as prevImg.
+     *     @param prevPts Vector of 2D points for which the flow needs to be found.
+     *     @param nextPts Output vector of 2D points containing the calculated new positions of input features in the second image.
+     *     @param status Output status vector. Each element of the vector is set to 1 if the
+     *                   flow for the corresponding features has been found. Otherwise, it is set to 0.
+     *     @param err Optional output vector that contains error response for each point (inverse confidence).
+     */
+    public void calc(Mat prevImg, Mat nextImg, Mat prevPts, Mat nextPts, Mat status, Mat err) {
+        calc_0(nativeObj, prevImg.nativeObj, nextImg.nativeObj, prevPts.nativeObj, nextPts.nativeObj, status.nativeObj, err.nativeObj);
+    }
+
+    /**
+     * Calculates a sparse optical flow.
+     *
+     *     @param prevImg First input image.
+     *     @param nextImg Second input image of the same size and the same type as prevImg.
+     *     @param prevPts Vector of 2D points for which the flow needs to be found.
+     *     @param nextPts Output vector of 2D points containing the calculated new positions of input features in the second image.
+     *     @param status Output status vector. Each element of the vector is set to 1 if the
+     *                   flow for the corresponding features has been found. Otherwise, it is set to 0.
+     */
+    public void calc(Mat prevImg, Mat nextImg, Mat prevPts, Mat nextPts, Mat status) {
+        calc_1(nativeObj, prevImg.nativeObj, nextImg.nativeObj, prevPts.nativeObj, nextPts.nativeObj, status.nativeObj);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++:  void cv::SparseOpticalFlow::calc(Mat prevImg, Mat nextImg, Mat prevPts, Mat& nextPts, Mat& status, Mat& err = cv::Mat())
+    private static native void calc_0(long nativeObj, long prevImg_nativeObj, long nextImg_nativeObj, long prevPts_nativeObj, long nextPts_nativeObj, long status_nativeObj, long err_nativeObj);
+    private static native void calc_1(long nativeObj, long prevImg_nativeObj, long nextImg_nativeObj, long prevPts_nativeObj, long nextPts_nativeObj, long status_nativeObj);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/video/SparsePyrLKOpticalFlow.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/video/SparsePyrLKOpticalFlow.java	(date 1605830248301)
+++ openCVLibrary3411/src/main/java/org/opencv/video/SparsePyrLKOpticalFlow.java	(date 1605830248301)
@@ -0,0 +1,194 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.video;
+
+import org.opencv.core.Size;
+import org.opencv.core.TermCriteria;
+import org.opencv.video.SparseOpticalFlow;
+import org.opencv.video.SparsePyrLKOpticalFlow;
+
+// C++: class SparsePyrLKOpticalFlow
+/**
+ * Class used for calculating a sparse optical flow.
+ *
+ * The class can calculate an optical flow for a sparse feature set using the
+ * iterative Lucas-Kanade method with pyramids.
+ *
+ * SEE: calcOpticalFlowPyrLK
+ */
+public class SparsePyrLKOpticalFlow extends SparseOpticalFlow {
+
+    protected SparsePyrLKOpticalFlow(long addr) { super(addr); }
+
+    // internal usage only
+    public static SparsePyrLKOpticalFlow __fromPtr__(long addr) { return new SparsePyrLKOpticalFlow(addr); }
+
+    //
+    // C++: static Ptr_SparsePyrLKOpticalFlow cv::SparsePyrLKOpticalFlow::create(Size winSize = Size(21, 21), int maxLevel = 3, TermCriteria crit = TermCriteria(TermCriteria::COUNT+TermCriteria::EPS, 30, 0.01), int flags = 0, double minEigThreshold = 1e-4)
+    //
+
+    public static SparsePyrLKOpticalFlow create(Size winSize, int maxLevel, TermCriteria crit, int flags, double minEigThreshold) {
+        return SparsePyrLKOpticalFlow.__fromPtr__(create_0(winSize.width, winSize.height, maxLevel, crit.type, crit.maxCount, crit.epsilon, flags, minEigThreshold));
+    }
+
+    public static SparsePyrLKOpticalFlow create(Size winSize, int maxLevel, TermCriteria crit, int flags) {
+        return SparsePyrLKOpticalFlow.__fromPtr__(create_1(winSize.width, winSize.height, maxLevel, crit.type, crit.maxCount, crit.epsilon, flags));
+    }
+
+    public static SparsePyrLKOpticalFlow create(Size winSize, int maxLevel, TermCriteria crit) {
+        return SparsePyrLKOpticalFlow.__fromPtr__(create_2(winSize.width, winSize.height, maxLevel, crit.type, crit.maxCount, crit.epsilon));
+    }
+
+    public static SparsePyrLKOpticalFlow create(Size winSize, int maxLevel) {
+        return SparsePyrLKOpticalFlow.__fromPtr__(create_3(winSize.width, winSize.height, maxLevel));
+    }
+
+    public static SparsePyrLKOpticalFlow create(Size winSize) {
+        return SparsePyrLKOpticalFlow.__fromPtr__(create_4(winSize.width, winSize.height));
+    }
+
+    public static SparsePyrLKOpticalFlow create() {
+        return SparsePyrLKOpticalFlow.__fromPtr__(create_5());
+    }
+
+
+    //
+    // C++:  Size cv::SparsePyrLKOpticalFlow::getWinSize()
+    //
+
+    public Size getWinSize() {
+        return new Size(getWinSize_0(nativeObj));
+    }
+
+
+    //
+    // C++:  TermCriteria cv::SparsePyrLKOpticalFlow::getTermCriteria()
+    //
+
+    public TermCriteria getTermCriteria() {
+        return new TermCriteria(getTermCriteria_0(nativeObj));
+    }
+
+
+    //
+    // C++:  double cv::SparsePyrLKOpticalFlow::getMinEigThreshold()
+    //
+
+    public double getMinEigThreshold() {
+        return getMinEigThreshold_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::SparsePyrLKOpticalFlow::getFlags()
+    //
+
+    public int getFlags() {
+        return getFlags_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::SparsePyrLKOpticalFlow::getMaxLevel()
+    //
+
+    public int getMaxLevel() {
+        return getMaxLevel_0(nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::SparsePyrLKOpticalFlow::setFlags(int flags)
+    //
+
+    public void setFlags(int flags) {
+        setFlags_0(nativeObj, flags);
+    }
+
+
+    //
+    // C++:  void cv::SparsePyrLKOpticalFlow::setMaxLevel(int maxLevel)
+    //
+
+    public void setMaxLevel(int maxLevel) {
+        setMaxLevel_0(nativeObj, maxLevel);
+    }
+
+
+    //
+    // C++:  void cv::SparsePyrLKOpticalFlow::setMinEigThreshold(double minEigThreshold)
+    //
+
+    public void setMinEigThreshold(double minEigThreshold) {
+        setMinEigThreshold_0(nativeObj, minEigThreshold);
+    }
+
+
+    //
+    // C++:  void cv::SparsePyrLKOpticalFlow::setTermCriteria(TermCriteria crit)
+    //
+
+    public void setTermCriteria(TermCriteria crit) {
+        setTermCriteria_0(nativeObj, crit.type, crit.maxCount, crit.epsilon);
+    }
+
+
+    //
+    // C++:  void cv::SparsePyrLKOpticalFlow::setWinSize(Size winSize)
+    //
+
+    public void setWinSize(Size winSize) {
+        setWinSize_0(nativeObj, winSize.width, winSize.height);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++: static Ptr_SparsePyrLKOpticalFlow cv::SparsePyrLKOpticalFlow::create(Size winSize = Size(21, 21), int maxLevel = 3, TermCriteria crit = TermCriteria(TermCriteria::COUNT+TermCriteria::EPS, 30, 0.01), int flags = 0, double minEigThreshold = 1e-4)
+    private static native long create_0(double winSize_width, double winSize_height, int maxLevel, int crit_type, int crit_maxCount, double crit_epsilon, int flags, double minEigThreshold);
+    private static native long create_1(double winSize_width, double winSize_height, int maxLevel, int crit_type, int crit_maxCount, double crit_epsilon, int flags);
+    private static native long create_2(double winSize_width, double winSize_height, int maxLevel, int crit_type, int crit_maxCount, double crit_epsilon);
+    private static native long create_3(double winSize_width, double winSize_height, int maxLevel);
+    private static native long create_4(double winSize_width, double winSize_height);
+    private static native long create_5();
+
+    // C++:  Size cv::SparsePyrLKOpticalFlow::getWinSize()
+    private static native double[] getWinSize_0(long nativeObj);
+
+    // C++:  TermCriteria cv::SparsePyrLKOpticalFlow::getTermCriteria()
+    private static native double[] getTermCriteria_0(long nativeObj);
+
+    // C++:  double cv::SparsePyrLKOpticalFlow::getMinEigThreshold()
+    private static native double getMinEigThreshold_0(long nativeObj);
+
+    // C++:  int cv::SparsePyrLKOpticalFlow::getFlags()
+    private static native int getFlags_0(long nativeObj);
+
+    // C++:  int cv::SparsePyrLKOpticalFlow::getMaxLevel()
+    private static native int getMaxLevel_0(long nativeObj);
+
+    // C++:  void cv::SparsePyrLKOpticalFlow::setFlags(int flags)
+    private static native void setFlags_0(long nativeObj, int flags);
+
+    // C++:  void cv::SparsePyrLKOpticalFlow::setMaxLevel(int maxLevel)
+    private static native void setMaxLevel_0(long nativeObj, int maxLevel);
+
+    // C++:  void cv::SparsePyrLKOpticalFlow::setMinEigThreshold(double minEigThreshold)
+    private static native void setMinEigThreshold_0(long nativeObj, double minEigThreshold);
+
+    // C++:  void cv::SparsePyrLKOpticalFlow::setTermCriteria(TermCriteria crit)
+    private static native void setTermCriteria_0(long nativeObj, int crit_type, int crit_maxCount, double crit_epsilon);
+
+    // C++:  void cv::SparsePyrLKOpticalFlow::setWinSize(Size winSize)
+    private static native void setWinSize_0(long nativeObj, double winSize_width, double winSize_height);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/video/BackgroundSubtractorKNN.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/video/BackgroundSubtractorKNN.java	(date 1605830248197)
+++ openCVLibrary3411/src/main/java/org/opencv/video/BackgroundSubtractorKNN.java	(date 1605830248197)
@@ -0,0 +1,275 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.video;
+
+import org.opencv.video.BackgroundSubtractor;
+
+// C++: class BackgroundSubtractorKNN
+/**
+ * K-nearest neighbours - based Background/Foreground Segmentation Algorithm.
+ *
+ * The class implements the K-nearest neighbours background subtraction described in CITE: Zivkovic2006 .
+ * Very efficient if number of foreground pixels is low.
+ */
+public class BackgroundSubtractorKNN extends BackgroundSubtractor {
+
+    protected BackgroundSubtractorKNN(long addr) { super(addr); }
+
+    // internal usage only
+    public static BackgroundSubtractorKNN __fromPtr__(long addr) { return new BackgroundSubtractorKNN(addr); }
+
+    //
+    // C++:  bool cv::BackgroundSubtractorKNN::getDetectShadows()
+    //
+
+    /**
+     * Returns the shadow detection flag
+     *
+     *     If true, the algorithm detects shadows and marks them. See createBackgroundSubtractorKNN for
+     *     details.
+     * @return automatically generated
+     */
+    public boolean getDetectShadows() {
+        return getDetectShadows_0(nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::BackgroundSubtractorKNN::getDist2Threshold()
+    //
+
+    /**
+     * Returns the threshold on the squared distance between the pixel and the sample
+     *
+     *     The threshold on the squared distance between the pixel and the sample to decide whether a pixel is
+     *     close to a data sample.
+     * @return automatically generated
+     */
+    public double getDist2Threshold() {
+        return getDist2Threshold_0(nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::BackgroundSubtractorKNN::getShadowThreshold()
+    //
+
+    /**
+     * Returns the shadow threshold
+     *
+     *     A shadow is detected if pixel is a darker version of the background. The shadow threshold (Tau in
+     *     the paper) is a threshold defining how much darker the shadow can be. Tau= 0.5 means that if a pixel
+     *     is more than twice darker then it is not shadow. See Prati, Mikic, Trivedi and Cucchiara,
+     * Detecting Moving Shadows...*, IEEE PAMI,2003.
+     * @return automatically generated
+     */
+    public double getShadowThreshold() {
+        return getShadowThreshold_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::BackgroundSubtractorKNN::getHistory()
+    //
+
+    /**
+     * Returns the number of last frames that affect the background model
+     * @return automatically generated
+     */
+    public int getHistory() {
+        return getHistory_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::BackgroundSubtractorKNN::getNSamples()
+    //
+
+    /**
+     * Returns the number of data samples in the background model
+     * @return automatically generated
+     */
+    public int getNSamples() {
+        return getNSamples_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::BackgroundSubtractorKNN::getShadowValue()
+    //
+
+    /**
+     * Returns the shadow value
+     *
+     *     Shadow value is the value used to mark shadows in the foreground mask. Default value is 127. Value 0
+     *     in the mask always means background, 255 means foreground.
+     * @return automatically generated
+     */
+    public int getShadowValue() {
+        return getShadowValue_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::BackgroundSubtractorKNN::getkNNSamples()
+    //
+
+    /**
+     * Returns the number of neighbours, the k in the kNN.
+     *
+     *     K is the number of samples that need to be within dist2Threshold in order to decide that that
+     *     pixel is matching the kNN background model.
+     * @return automatically generated
+     */
+    public int getkNNSamples() {
+        return getkNNSamples_0(nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::BackgroundSubtractorKNN::setDetectShadows(bool detectShadows)
+    //
+
+    /**
+     * Enables or disables shadow detection
+     * @param detectShadows automatically generated
+     */
+    public void setDetectShadows(boolean detectShadows) {
+        setDetectShadows_0(nativeObj, detectShadows);
+    }
+
+
+    //
+    // C++:  void cv::BackgroundSubtractorKNN::setDist2Threshold(double _dist2Threshold)
+    //
+
+    /**
+     * Sets the threshold on the squared distance
+     * @param _dist2Threshold automatically generated
+     */
+    public void setDist2Threshold(double _dist2Threshold) {
+        setDist2Threshold_0(nativeObj, _dist2Threshold);
+    }
+
+
+    //
+    // C++:  void cv::BackgroundSubtractorKNN::setHistory(int history)
+    //
+
+    /**
+     * Sets the number of last frames that affect the background model
+     * @param history automatically generated
+     */
+    public void setHistory(int history) {
+        setHistory_0(nativeObj, history);
+    }
+
+
+    //
+    // C++:  void cv::BackgroundSubtractorKNN::setNSamples(int _nN)
+    //
+
+    /**
+     * Sets the number of data samples in the background model.
+     *
+     *     The model needs to be reinitalized to reserve memory.
+     * @param _nN automatically generated
+     */
+    public void setNSamples(int _nN) {
+        setNSamples_0(nativeObj, _nN);
+    }
+
+
+    //
+    // C++:  void cv::BackgroundSubtractorKNN::setShadowThreshold(double threshold)
+    //
+
+    /**
+     * Sets the shadow threshold
+     * @param threshold automatically generated
+     */
+    public void setShadowThreshold(double threshold) {
+        setShadowThreshold_0(nativeObj, threshold);
+    }
+
+
+    //
+    // C++:  void cv::BackgroundSubtractorKNN::setShadowValue(int value)
+    //
+
+    /**
+     * Sets the shadow value
+     * @param value automatically generated
+     */
+    public void setShadowValue(int value) {
+        setShadowValue_0(nativeObj, value);
+    }
+
+
+    //
+    // C++:  void cv::BackgroundSubtractorKNN::setkNNSamples(int _nkNN)
+    //
+
+    /**
+     * Sets the k in the kNN. How many nearest neighbours need to match.
+     * @param _nkNN automatically generated
+     */
+    public void setkNNSamples(int _nkNN) {
+        setkNNSamples_0(nativeObj, _nkNN);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++:  bool cv::BackgroundSubtractorKNN::getDetectShadows()
+    private static native boolean getDetectShadows_0(long nativeObj);
+
+    // C++:  double cv::BackgroundSubtractorKNN::getDist2Threshold()
+    private static native double getDist2Threshold_0(long nativeObj);
+
+    // C++:  double cv::BackgroundSubtractorKNN::getShadowThreshold()
+    private static native double getShadowThreshold_0(long nativeObj);
+
+    // C++:  int cv::BackgroundSubtractorKNN::getHistory()
+    private static native int getHistory_0(long nativeObj);
+
+    // C++:  int cv::BackgroundSubtractorKNN::getNSamples()
+    private static native int getNSamples_0(long nativeObj);
+
+    // C++:  int cv::BackgroundSubtractorKNN::getShadowValue()
+    private static native int getShadowValue_0(long nativeObj);
+
+    // C++:  int cv::BackgroundSubtractorKNN::getkNNSamples()
+    private static native int getkNNSamples_0(long nativeObj);
+
+    // C++:  void cv::BackgroundSubtractorKNN::setDetectShadows(bool detectShadows)
+    private static native void setDetectShadows_0(long nativeObj, boolean detectShadows);
+
+    // C++:  void cv::BackgroundSubtractorKNN::setDist2Threshold(double _dist2Threshold)
+    private static native void setDist2Threshold_0(long nativeObj, double _dist2Threshold);
+
+    // C++:  void cv::BackgroundSubtractorKNN::setHistory(int history)
+    private static native void setHistory_0(long nativeObj, int history);
+
+    // C++:  void cv::BackgroundSubtractorKNN::setNSamples(int _nN)
+    private static native void setNSamples_0(long nativeObj, int _nN);
+
+    // C++:  void cv::BackgroundSubtractorKNN::setShadowThreshold(double threshold)
+    private static native void setShadowThreshold_0(long nativeObj, double threshold);
+
+    // C++:  void cv::BackgroundSubtractorKNN::setShadowValue(int value)
+    private static native void setShadowValue_0(long nativeObj, int value);
+
+    // C++:  void cv::BackgroundSubtractorKNN::setkNNSamples(int _nkNN)
+    private static native void setkNNSamples_0(long nativeObj, int _nkNN);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/video/BackgroundSubtractorMOG2.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/video/BackgroundSubtractorMOG2.java	(date 1605830248215)
+++ openCVLibrary3411/src/main/java/org/opencv/video/BackgroundSubtractorMOG2.java	(date 1605830248215)
@@ -0,0 +1,467 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.video;
+
+import org.opencv.core.Mat;
+import org.opencv.video.BackgroundSubtractor;
+
+// C++: class BackgroundSubtractorMOG2
+/**
+ * Gaussian Mixture-based Background/Foreground Segmentation Algorithm.
+ *
+ * The class implements the Gaussian mixture model background subtraction described in CITE: Zivkovic2004
+ * and CITE: Zivkovic2006 .
+ */
+public class BackgroundSubtractorMOG2 extends BackgroundSubtractor {
+
+    protected BackgroundSubtractorMOG2(long addr) { super(addr); }
+
+    // internal usage only
+    public static BackgroundSubtractorMOG2 __fromPtr__(long addr) { return new BackgroundSubtractorMOG2(addr); }
+
+    //
+    // C++:  bool cv::BackgroundSubtractorMOG2::getDetectShadows()
+    //
+
+    /**
+     * Returns the shadow detection flag
+     *
+     *     If true, the algorithm detects shadows and marks them. See createBackgroundSubtractorMOG2 for
+     *     details.
+     * @return automatically generated
+     */
+    public boolean getDetectShadows() {
+        return getDetectShadows_0(nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::BackgroundSubtractorMOG2::getBackgroundRatio()
+    //
+
+    /**
+     * Returns the "background ratio" parameter of the algorithm
+     *
+     *     If a foreground pixel keeps semi-constant value for about backgroundRatio\*history frames, it's
+     *     considered background and added to the model as a center of a new component. It corresponds to TB
+     *     parameter in the paper.
+     * @return automatically generated
+     */
+    public double getBackgroundRatio() {
+        return getBackgroundRatio_0(nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::BackgroundSubtractorMOG2::getComplexityReductionThreshold()
+    //
+
+    /**
+     * Returns the complexity reduction threshold
+     *
+     *     This parameter defines the number of samples needed to accept to prove the component exists. CT=0.05
+     *     is a default value for all the samples. By setting CT=0 you get an algorithm very similar to the
+     *     standard Stauffer&amp;Grimson algorithm.
+     * @return automatically generated
+     */
+    public double getComplexityReductionThreshold() {
+        return getComplexityReductionThreshold_0(nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::BackgroundSubtractorMOG2::getShadowThreshold()
+    //
+
+    /**
+     * Returns the shadow threshold
+     *
+     *     A shadow is detected if pixel is a darker version of the background. The shadow threshold (Tau in
+     *     the paper) is a threshold defining how much darker the shadow can be. Tau= 0.5 means that if a pixel
+     *     is more than twice darker then it is not shadow. See Prati, Mikic, Trivedi and Cucchiara,
+     * Detecting Moving Shadows...*, IEEE PAMI,2003.
+     * @return automatically generated
+     */
+    public double getShadowThreshold() {
+        return getShadowThreshold_0(nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::BackgroundSubtractorMOG2::getVarInit()
+    //
+
+    /**
+     * Returns the initial variance of each gaussian component
+     * @return automatically generated
+     */
+    public double getVarInit() {
+        return getVarInit_0(nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::BackgroundSubtractorMOG2::getVarMax()
+    //
+
+    public double getVarMax() {
+        return getVarMax_0(nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::BackgroundSubtractorMOG2::getVarMin()
+    //
+
+    public double getVarMin() {
+        return getVarMin_0(nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::BackgroundSubtractorMOG2::getVarThreshold()
+    //
+
+    /**
+     * Returns the variance threshold for the pixel-model match
+     *
+     *     The main threshold on the squared Mahalanobis distance to decide if the sample is well described by
+     *     the background model or not. Related to Cthr from the paper.
+     * @return automatically generated
+     */
+    public double getVarThreshold() {
+        return getVarThreshold_0(nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::BackgroundSubtractorMOG2::getVarThresholdGen()
+    //
+
+    /**
+     * Returns the variance threshold for the pixel-model match used for new mixture component generation
+     *
+     *     Threshold for the squared Mahalanobis distance that helps decide when a sample is close to the
+     *     existing components (corresponds to Tg in the paper). If a pixel is not close to any component, it
+     *     is considered foreground or added as a new component. 3 sigma =&gt; Tg=3\*3=9 is default. A smaller Tg
+     *     value generates more components. A higher Tg value may result in a small number of components but
+     *     they can grow too large.
+     * @return automatically generated
+     */
+    public double getVarThresholdGen() {
+        return getVarThresholdGen_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::BackgroundSubtractorMOG2::getHistory()
+    //
+
+    /**
+     * Returns the number of last frames that affect the background model
+     * @return automatically generated
+     */
+    public int getHistory() {
+        return getHistory_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::BackgroundSubtractorMOG2::getNMixtures()
+    //
+
+    /**
+     * Returns the number of gaussian components in the background model
+     * @return automatically generated
+     */
+    public int getNMixtures() {
+        return getNMixtures_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::BackgroundSubtractorMOG2::getShadowValue()
+    //
+
+    /**
+     * Returns the shadow value
+     *
+     *     Shadow value is the value used to mark shadows in the foreground mask. Default value is 127. Value 0
+     *     in the mask always means background, 255 means foreground.
+     * @return automatically generated
+     */
+    public int getShadowValue() {
+        return getShadowValue_0(nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::BackgroundSubtractorMOG2::apply(Mat image, Mat& fgmask, double learningRate = -1)
+    //
+
+    /**
+     * Computes a foreground mask.
+     *
+     *     @param image Next video frame. Floating point frame will be used without scaling and should be in range \([0,255]\).
+     *     @param fgmask The output foreground mask as an 8-bit binary image.
+     *     @param learningRate The value between 0 and 1 that indicates how fast the background model is
+     *     learnt. Negative parameter value makes the algorithm to use some automatically chosen learning
+     *     rate. 0 means that the background model is not updated at all, 1 means that the background model
+     *     is completely reinitialized from the last frame.
+     */
+    public void apply(Mat image, Mat fgmask, double learningRate) {
+        apply_0(nativeObj, image.nativeObj, fgmask.nativeObj, learningRate);
+    }
+
+    /**
+     * Computes a foreground mask.
+     *
+     *     @param image Next video frame. Floating point frame will be used without scaling and should be in range \([0,255]\).
+     *     @param fgmask The output foreground mask as an 8-bit binary image.
+     *     learnt. Negative parameter value makes the algorithm to use some automatically chosen learning
+     *     rate. 0 means that the background model is not updated at all, 1 means that the background model
+     *     is completely reinitialized from the last frame.
+     */
+    public void apply(Mat image, Mat fgmask) {
+        apply_1(nativeObj, image.nativeObj, fgmask.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::BackgroundSubtractorMOG2::setBackgroundRatio(double ratio)
+    //
+
+    /**
+     * Sets the "background ratio" parameter of the algorithm
+     * @param ratio automatically generated
+     */
+    public void setBackgroundRatio(double ratio) {
+        setBackgroundRatio_0(nativeObj, ratio);
+    }
+
+
+    //
+    // C++:  void cv::BackgroundSubtractorMOG2::setComplexityReductionThreshold(double ct)
+    //
+
+    /**
+     * Sets the complexity reduction threshold
+     * @param ct automatically generated
+     */
+    public void setComplexityReductionThreshold(double ct) {
+        setComplexityReductionThreshold_0(nativeObj, ct);
+    }
+
+
+    //
+    // C++:  void cv::BackgroundSubtractorMOG2::setDetectShadows(bool detectShadows)
+    //
+
+    /**
+     * Enables or disables shadow detection
+     * @param detectShadows automatically generated
+     */
+    public void setDetectShadows(boolean detectShadows) {
+        setDetectShadows_0(nativeObj, detectShadows);
+    }
+
+
+    //
+    // C++:  void cv::BackgroundSubtractorMOG2::setHistory(int history)
+    //
+
+    /**
+     * Sets the number of last frames that affect the background model
+     * @param history automatically generated
+     */
+    public void setHistory(int history) {
+        setHistory_0(nativeObj, history);
+    }
+
+
+    //
+    // C++:  void cv::BackgroundSubtractorMOG2::setNMixtures(int nmixtures)
+    //
+
+    /**
+     * Sets the number of gaussian components in the background model.
+     *
+     *     The model needs to be reinitalized to reserve memory.
+     * @param nmixtures automatically generated
+     */
+    public void setNMixtures(int nmixtures) {
+        setNMixtures_0(nativeObj, nmixtures);
+    }
+
+
+    //
+    // C++:  void cv::BackgroundSubtractorMOG2::setShadowThreshold(double threshold)
+    //
+
+    /**
+     * Sets the shadow threshold
+     * @param threshold automatically generated
+     */
+    public void setShadowThreshold(double threshold) {
+        setShadowThreshold_0(nativeObj, threshold);
+    }
+
+
+    //
+    // C++:  void cv::BackgroundSubtractorMOG2::setShadowValue(int value)
+    //
+
+    /**
+     * Sets the shadow value
+     * @param value automatically generated
+     */
+    public void setShadowValue(int value) {
+        setShadowValue_0(nativeObj, value);
+    }
+
+
+    //
+    // C++:  void cv::BackgroundSubtractorMOG2::setVarInit(double varInit)
+    //
+
+    /**
+     * Sets the initial variance of each gaussian component
+     * @param varInit automatically generated
+     */
+    public void setVarInit(double varInit) {
+        setVarInit_0(nativeObj, varInit);
+    }
+
+
+    //
+    // C++:  void cv::BackgroundSubtractorMOG2::setVarMax(double varMax)
+    //
+
+    public void setVarMax(double varMax) {
+        setVarMax_0(nativeObj, varMax);
+    }
+
+
+    //
+    // C++:  void cv::BackgroundSubtractorMOG2::setVarMin(double varMin)
+    //
+
+    public void setVarMin(double varMin) {
+        setVarMin_0(nativeObj, varMin);
+    }
+
+
+    //
+    // C++:  void cv::BackgroundSubtractorMOG2::setVarThreshold(double varThreshold)
+    //
+
+    /**
+     * Sets the variance threshold for the pixel-model match
+     * @param varThreshold automatically generated
+     */
+    public void setVarThreshold(double varThreshold) {
+        setVarThreshold_0(nativeObj, varThreshold);
+    }
+
+
+    //
+    // C++:  void cv::BackgroundSubtractorMOG2::setVarThresholdGen(double varThresholdGen)
+    //
+
+    /**
+     * Sets the variance threshold for the pixel-model match used for new mixture component generation
+     * @param varThresholdGen automatically generated
+     */
+    public void setVarThresholdGen(double varThresholdGen) {
+        setVarThresholdGen_0(nativeObj, varThresholdGen);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++:  bool cv::BackgroundSubtractorMOG2::getDetectShadows()
+    private static native boolean getDetectShadows_0(long nativeObj);
+
+    // C++:  double cv::BackgroundSubtractorMOG2::getBackgroundRatio()
+    private static native double getBackgroundRatio_0(long nativeObj);
+
+    // C++:  double cv::BackgroundSubtractorMOG2::getComplexityReductionThreshold()
+    private static native double getComplexityReductionThreshold_0(long nativeObj);
+
+    // C++:  double cv::BackgroundSubtractorMOG2::getShadowThreshold()
+    private static native double getShadowThreshold_0(long nativeObj);
+
+    // C++:  double cv::BackgroundSubtractorMOG2::getVarInit()
+    private static native double getVarInit_0(long nativeObj);
+
+    // C++:  double cv::BackgroundSubtractorMOG2::getVarMax()
+    private static native double getVarMax_0(long nativeObj);
+
+    // C++:  double cv::BackgroundSubtractorMOG2::getVarMin()
+    private static native double getVarMin_0(long nativeObj);
+
+    // C++:  double cv::BackgroundSubtractorMOG2::getVarThreshold()
+    private static native double getVarThreshold_0(long nativeObj);
+
+    // C++:  double cv::BackgroundSubtractorMOG2::getVarThresholdGen()
+    private static native double getVarThresholdGen_0(long nativeObj);
+
+    // C++:  int cv::BackgroundSubtractorMOG2::getHistory()
+    private static native int getHistory_0(long nativeObj);
+
+    // C++:  int cv::BackgroundSubtractorMOG2::getNMixtures()
+    private static native int getNMixtures_0(long nativeObj);
+
+    // C++:  int cv::BackgroundSubtractorMOG2::getShadowValue()
+    private static native int getShadowValue_0(long nativeObj);
+
+    // C++:  void cv::BackgroundSubtractorMOG2::apply(Mat image, Mat& fgmask, double learningRate = -1)
+    private static native void apply_0(long nativeObj, long image_nativeObj, long fgmask_nativeObj, double learningRate);
+    private static native void apply_1(long nativeObj, long image_nativeObj, long fgmask_nativeObj);
+
+    // C++:  void cv::BackgroundSubtractorMOG2::setBackgroundRatio(double ratio)
+    private static native void setBackgroundRatio_0(long nativeObj, double ratio);
+
+    // C++:  void cv::BackgroundSubtractorMOG2::setComplexityReductionThreshold(double ct)
+    private static native void setComplexityReductionThreshold_0(long nativeObj, double ct);
+
+    // C++:  void cv::BackgroundSubtractorMOG2::setDetectShadows(bool detectShadows)
+    private static native void setDetectShadows_0(long nativeObj, boolean detectShadows);
+
+    // C++:  void cv::BackgroundSubtractorMOG2::setHistory(int history)
+    private static native void setHistory_0(long nativeObj, int history);
+
+    // C++:  void cv::BackgroundSubtractorMOG2::setNMixtures(int nmixtures)
+    private static native void setNMixtures_0(long nativeObj, int nmixtures);
+
+    // C++:  void cv::BackgroundSubtractorMOG2::setShadowThreshold(double threshold)
+    private static native void setShadowThreshold_0(long nativeObj, double threshold);
+
+    // C++:  void cv::BackgroundSubtractorMOG2::setShadowValue(int value)
+    private static native void setShadowValue_0(long nativeObj, int value);
+
+    // C++:  void cv::BackgroundSubtractorMOG2::setVarInit(double varInit)
+    private static native void setVarInit_0(long nativeObj, double varInit);
+
+    // C++:  void cv::BackgroundSubtractorMOG2::setVarMax(double varMax)
+    private static native void setVarMax_0(long nativeObj, double varMax);
+
+    // C++:  void cv::BackgroundSubtractorMOG2::setVarMin(double varMin)
+    private static native void setVarMin_0(long nativeObj, double varMin);
+
+    // C++:  void cv::BackgroundSubtractorMOG2::setVarThreshold(double varThreshold)
+    private static native void setVarThreshold_0(long nativeObj, double varThreshold);
+
+    // C++:  void cv::BackgroundSubtractorMOG2::setVarThresholdGen(double varThresholdGen)
+    private static native void setVarThresholdGen_0(long nativeObj, double varThresholdGen);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/video/DenseOpticalFlow.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/video/DenseOpticalFlow.java	(date 1605830248248)
+++ openCVLibrary3411/src/main/java/org/opencv/video/DenseOpticalFlow.java	(date 1605830248248)
@@ -0,0 +1,62 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.video;
+
+import org.opencv.core.Algorithm;
+import org.opencv.core.Mat;
+
+// C++: class DenseOpticalFlow
+
+public class DenseOpticalFlow extends Algorithm {
+
+    protected DenseOpticalFlow(long addr) { super(addr); }
+
+    // internal usage only
+    public static DenseOpticalFlow __fromPtr__(long addr) { return new DenseOpticalFlow(addr); }
+
+    //
+    // C++:  void cv::DenseOpticalFlow::calc(Mat I0, Mat I1, Mat& flow)
+    //
+
+    /**
+     * Calculates an optical flow.
+     *
+     *     @param I0 first 8-bit single-channel input image.
+     *     @param I1 second input image of the same size and the same type as prev.
+     *     @param flow computed flow image that has the same size as prev and type CV_32FC2.
+     */
+    public void calc(Mat I0, Mat I1, Mat flow) {
+        calc_0(nativeObj, I0.nativeObj, I1.nativeObj, flow.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::DenseOpticalFlow::collectGarbage()
+    //
+
+    /**
+     * Releases all inner buffers.
+     */
+    public void collectGarbage() {
+        collectGarbage_0(nativeObj);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++:  void cv::DenseOpticalFlow::calc(Mat I0, Mat I1, Mat& flow)
+    private static native void calc_0(long nativeObj, long I0_nativeObj, long I1_nativeObj, long flow_nativeObj);
+
+    // C++:  void cv::DenseOpticalFlow::collectGarbage()
+    private static native void collectGarbage_0(long nativeObj);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/video/DualTVL1OpticalFlow.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/video/DualTVL1OpticalFlow.java	(date 1605830248266)
+++ openCVLibrary3411/src/main/java/org/opencv/video/DualTVL1OpticalFlow.java	(date 1605830248266)
@@ -0,0 +1,684 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.video;
+
+import org.opencv.video.DenseOpticalFlow;
+import org.opencv.video.DualTVL1OpticalFlow;
+
+// C++: class DualTVL1OpticalFlow
+/**
+ * "Dual TV L1" Optical Flow Algorithm.
+ *
+ * The class implements the "Dual TV L1" optical flow algorithm described in CITE: Zach2007 and
+ * CITE: Javier2012 .
+ * Here are important members of the class that control the algorithm, which you can set after
+ * constructing the class instance:
+ *
+ * <ul>
+ *   <li>
+ *    member double tau
+ *     Time step of the numerical scheme.
+ *   </li>
+ * </ul>
+ *
+ * <ul>
+ *   <li>
+ *    member double lambda
+ *     Weight parameter for the data term, attachment parameter. This is the most relevant
+ *     parameter, which determines the smoothness of the output. The smaller this parameter is,
+ *     the smoother the solutions we obtain. It depends on the range of motions of the images, so
+ *     its value should be adapted to each image sequence.
+ *   </li>
+ * </ul>
+ *
+ * <ul>
+ *   <li>
+ *    member double theta
+ *     Weight parameter for (u - v)\^2, tightness parameter. It serves as a link between the
+ *     attachment and the regularization terms. In theory, it should have a small value in order
+ *     to maintain both parts in correspondence. The method is stable for a large range of values
+ *     of this parameter.
+ *   </li>
+ * </ul>
+ *
+ * <ul>
+ *   <li>
+ *    member int nscales
+ *     Number of scales used to create the pyramid of images.
+ *   </li>
+ * </ul>
+ *
+ * <ul>
+ *   <li>
+ *    member int warps
+ *     Number of warpings per scale. Represents the number of times that I1(x+u0) and grad(
+ *     I1(x+u0) ) are computed per scale. This is a parameter that assures the stability of the
+ *     method. It also affects the running time, so it is a compromise between speed and
+ *     accuracy.
+ *   </li>
+ * </ul>
+ *
+ * <ul>
+ *   <li>
+ *    member double epsilon
+ *     Stopping criterion threshold used in the numerical scheme, which is a trade-off between
+ *     precision and running time. A small value will yield more accurate solutions at the
+ *     expense of a slower convergence.
+ *   </li>
+ * </ul>
+ *
+ * <ul>
+ *   <li>
+ *    member int iterations
+ *     Stopping criterion iterations number used in the numerical scheme.
+ *   </li>
+ * </ul>
+ *
+ * C. Zach, T. Pock and H. Bischof, "A Duality Based Approach for Realtime TV-L1 Optical Flow".
+ * Javier Sanchez, Enric Meinhardt-Llopis and Gabriele Facciolo. "TV-L1 Optical Flow Estimation".
+ */
+public class DualTVL1OpticalFlow extends DenseOpticalFlow {
+
+    protected DualTVL1OpticalFlow(long addr) { super(addr); }
+
+    // internal usage only
+    public static DualTVL1OpticalFlow __fromPtr__(long addr) { return new DualTVL1OpticalFlow(addr); }
+
+    //
+    // C++: static Ptr_DualTVL1OpticalFlow cv::DualTVL1OpticalFlow::create(double tau = 0.25, double lambda = 0.15, double theta = 0.3, int nscales = 5, int warps = 5, double epsilon = 0.01, int innnerIterations = 30, int outerIterations = 10, double scaleStep = 0.8, double gamma = 0.0, int medianFiltering = 5, bool useInitialFlow = false)
+    //
+
+    /**
+     * Creates instance of cv::DualTVL1OpticalFlow
+     * @param tau automatically generated
+     * @param lambda automatically generated
+     * @param theta automatically generated
+     * @param nscales automatically generated
+     * @param warps automatically generated
+     * @param epsilon automatically generated
+     * @param innnerIterations automatically generated
+     * @param outerIterations automatically generated
+     * @param scaleStep automatically generated
+     * @param gamma automatically generated
+     * @param medianFiltering automatically generated
+     * @param useInitialFlow automatically generated
+     * @return automatically generated
+     */
+    public static DualTVL1OpticalFlow create(double tau, double lambda, double theta, int nscales, int warps, double epsilon, int innnerIterations, int outerIterations, double scaleStep, double gamma, int medianFiltering, boolean useInitialFlow) {
+        return DualTVL1OpticalFlow.__fromPtr__(create_0(tau, lambda, theta, nscales, warps, epsilon, innnerIterations, outerIterations, scaleStep, gamma, medianFiltering, useInitialFlow));
+    }
+
+    /**
+     * Creates instance of cv::DualTVL1OpticalFlow
+     * @param tau automatically generated
+     * @param lambda automatically generated
+     * @param theta automatically generated
+     * @param nscales automatically generated
+     * @param warps automatically generated
+     * @param epsilon automatically generated
+     * @param innnerIterations automatically generated
+     * @param outerIterations automatically generated
+     * @param scaleStep automatically generated
+     * @param gamma automatically generated
+     * @param medianFiltering automatically generated
+     * @return automatically generated
+     */
+    public static DualTVL1OpticalFlow create(double tau, double lambda, double theta, int nscales, int warps, double epsilon, int innnerIterations, int outerIterations, double scaleStep, double gamma, int medianFiltering) {
+        return DualTVL1OpticalFlow.__fromPtr__(create_1(tau, lambda, theta, nscales, warps, epsilon, innnerIterations, outerIterations, scaleStep, gamma, medianFiltering));
+    }
+
+    /**
+     * Creates instance of cv::DualTVL1OpticalFlow
+     * @param tau automatically generated
+     * @param lambda automatically generated
+     * @param theta automatically generated
+     * @param nscales automatically generated
+     * @param warps automatically generated
+     * @param epsilon automatically generated
+     * @param innnerIterations automatically generated
+     * @param outerIterations automatically generated
+     * @param scaleStep automatically generated
+     * @param gamma automatically generated
+     * @return automatically generated
+     */
+    public static DualTVL1OpticalFlow create(double tau, double lambda, double theta, int nscales, int warps, double epsilon, int innnerIterations, int outerIterations, double scaleStep, double gamma) {
+        return DualTVL1OpticalFlow.__fromPtr__(create_2(tau, lambda, theta, nscales, warps, epsilon, innnerIterations, outerIterations, scaleStep, gamma));
+    }
+
+    /**
+     * Creates instance of cv::DualTVL1OpticalFlow
+     * @param tau automatically generated
+     * @param lambda automatically generated
+     * @param theta automatically generated
+     * @param nscales automatically generated
+     * @param warps automatically generated
+     * @param epsilon automatically generated
+     * @param innnerIterations automatically generated
+     * @param outerIterations automatically generated
+     * @param scaleStep automatically generated
+     * @return automatically generated
+     */
+    public static DualTVL1OpticalFlow create(double tau, double lambda, double theta, int nscales, int warps, double epsilon, int innnerIterations, int outerIterations, double scaleStep) {
+        return DualTVL1OpticalFlow.__fromPtr__(create_3(tau, lambda, theta, nscales, warps, epsilon, innnerIterations, outerIterations, scaleStep));
+    }
+
+    /**
+     * Creates instance of cv::DualTVL1OpticalFlow
+     * @param tau automatically generated
+     * @param lambda automatically generated
+     * @param theta automatically generated
+     * @param nscales automatically generated
+     * @param warps automatically generated
+     * @param epsilon automatically generated
+     * @param innnerIterations automatically generated
+     * @param outerIterations automatically generated
+     * @return automatically generated
+     */
+    public static DualTVL1OpticalFlow create(double tau, double lambda, double theta, int nscales, int warps, double epsilon, int innnerIterations, int outerIterations) {
+        return DualTVL1OpticalFlow.__fromPtr__(create_4(tau, lambda, theta, nscales, warps, epsilon, innnerIterations, outerIterations));
+    }
+
+    /**
+     * Creates instance of cv::DualTVL1OpticalFlow
+     * @param tau automatically generated
+     * @param lambda automatically generated
+     * @param theta automatically generated
+     * @param nscales automatically generated
+     * @param warps automatically generated
+     * @param epsilon automatically generated
+     * @param innnerIterations automatically generated
+     * @return automatically generated
+     */
+    public static DualTVL1OpticalFlow create(double tau, double lambda, double theta, int nscales, int warps, double epsilon, int innnerIterations) {
+        return DualTVL1OpticalFlow.__fromPtr__(create_5(tau, lambda, theta, nscales, warps, epsilon, innnerIterations));
+    }
+
+    /**
+     * Creates instance of cv::DualTVL1OpticalFlow
+     * @param tau automatically generated
+     * @param lambda automatically generated
+     * @param theta automatically generated
+     * @param nscales automatically generated
+     * @param warps automatically generated
+     * @param epsilon automatically generated
+     * @return automatically generated
+     */
+    public static DualTVL1OpticalFlow create(double tau, double lambda, double theta, int nscales, int warps, double epsilon) {
+        return DualTVL1OpticalFlow.__fromPtr__(create_6(tau, lambda, theta, nscales, warps, epsilon));
+    }
+
+    /**
+     * Creates instance of cv::DualTVL1OpticalFlow
+     * @param tau automatically generated
+     * @param lambda automatically generated
+     * @param theta automatically generated
+     * @param nscales automatically generated
+     * @param warps automatically generated
+     * @return automatically generated
+     */
+    public static DualTVL1OpticalFlow create(double tau, double lambda, double theta, int nscales, int warps) {
+        return DualTVL1OpticalFlow.__fromPtr__(create_7(tau, lambda, theta, nscales, warps));
+    }
+
+    /**
+     * Creates instance of cv::DualTVL1OpticalFlow
+     * @param tau automatically generated
+     * @param lambda automatically generated
+     * @param theta automatically generated
+     * @param nscales automatically generated
+     * @return automatically generated
+     */
+    public static DualTVL1OpticalFlow create(double tau, double lambda, double theta, int nscales) {
+        return DualTVL1OpticalFlow.__fromPtr__(create_8(tau, lambda, theta, nscales));
+    }
+
+    /**
+     * Creates instance of cv::DualTVL1OpticalFlow
+     * @param tau automatically generated
+     * @param lambda automatically generated
+     * @param theta automatically generated
+     * @return automatically generated
+     */
+    public static DualTVL1OpticalFlow create(double tau, double lambda, double theta) {
+        return DualTVL1OpticalFlow.__fromPtr__(create_9(tau, lambda, theta));
+    }
+
+    /**
+     * Creates instance of cv::DualTVL1OpticalFlow
+     * @param tau automatically generated
+     * @param lambda automatically generated
+     * @return automatically generated
+     */
+    public static DualTVL1OpticalFlow create(double tau, double lambda) {
+        return DualTVL1OpticalFlow.__fromPtr__(create_10(tau, lambda));
+    }
+
+    /**
+     * Creates instance of cv::DualTVL1OpticalFlow
+     * @param tau automatically generated
+     * @return automatically generated
+     */
+    public static DualTVL1OpticalFlow create(double tau) {
+        return DualTVL1OpticalFlow.__fromPtr__(create_11(tau));
+    }
+
+    /**
+     * Creates instance of cv::DualTVL1OpticalFlow
+     * @return automatically generated
+     */
+    public static DualTVL1OpticalFlow create() {
+        return DualTVL1OpticalFlow.__fromPtr__(create_12());
+    }
+
+
+    //
+    // C++:  bool cv::DualTVL1OpticalFlow::getUseInitialFlow()
+    //
+
+    /**
+     * SEE: setUseInitialFlow
+     * @return automatically generated
+     */
+    public boolean getUseInitialFlow() {
+        return getUseInitialFlow_0(nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::DualTVL1OpticalFlow::getEpsilon()
+    //
+
+    /**
+     * SEE: setEpsilon
+     * @return automatically generated
+     */
+    public double getEpsilon() {
+        return getEpsilon_0(nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::DualTVL1OpticalFlow::getGamma()
+    //
+
+    /**
+     * SEE: setGamma
+     * @return automatically generated
+     */
+    public double getGamma() {
+        return getGamma_0(nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::DualTVL1OpticalFlow::getLambda()
+    //
+
+    /**
+     * SEE: setLambda
+     * @return automatically generated
+     */
+    public double getLambda() {
+        return getLambda_0(nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::DualTVL1OpticalFlow::getScaleStep()
+    //
+
+    /**
+     * SEE: setScaleStep
+     * @return automatically generated
+     */
+    public double getScaleStep() {
+        return getScaleStep_0(nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::DualTVL1OpticalFlow::getTau()
+    //
+
+    /**
+     * SEE: setTau
+     * @return automatically generated
+     */
+    public double getTau() {
+        return getTau_0(nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::DualTVL1OpticalFlow::getTheta()
+    //
+
+    /**
+     * SEE: setTheta
+     * @return automatically generated
+     */
+    public double getTheta() {
+        return getTheta_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::DualTVL1OpticalFlow::getInnerIterations()
+    //
+
+    /**
+     * SEE: setInnerIterations
+     * @return automatically generated
+     */
+    public int getInnerIterations() {
+        return getInnerIterations_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::DualTVL1OpticalFlow::getMedianFiltering()
+    //
+
+    /**
+     * SEE: setMedianFiltering
+     * @return automatically generated
+     */
+    public int getMedianFiltering() {
+        return getMedianFiltering_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::DualTVL1OpticalFlow::getOuterIterations()
+    //
+
+    /**
+     * SEE: setOuterIterations
+     * @return automatically generated
+     */
+    public int getOuterIterations() {
+        return getOuterIterations_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::DualTVL1OpticalFlow::getScalesNumber()
+    //
+
+    /**
+     * SEE: setScalesNumber
+     * @return automatically generated
+     */
+    public int getScalesNumber() {
+        return getScalesNumber_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::DualTVL1OpticalFlow::getWarpingsNumber()
+    //
+
+    /**
+     * SEE: setWarpingsNumber
+     * @return automatically generated
+     */
+    public int getWarpingsNumber() {
+        return getWarpingsNumber_0(nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::DualTVL1OpticalFlow::setEpsilon(double val)
+    //
+
+    /**
+     *  getEpsilon SEE: getEpsilon
+     * @param val automatically generated
+     */
+    public void setEpsilon(double val) {
+        setEpsilon_0(nativeObj, val);
+    }
+
+
+    //
+    // C++:  void cv::DualTVL1OpticalFlow::setGamma(double val)
+    //
+
+    /**
+     *  getGamma SEE: getGamma
+     * @param val automatically generated
+     */
+    public void setGamma(double val) {
+        setGamma_0(nativeObj, val);
+    }
+
+
+    //
+    // C++:  void cv::DualTVL1OpticalFlow::setInnerIterations(int val)
+    //
+
+    /**
+     *  getInnerIterations SEE: getInnerIterations
+     * @param val automatically generated
+     */
+    public void setInnerIterations(int val) {
+        setInnerIterations_0(nativeObj, val);
+    }
+
+
+    //
+    // C++:  void cv::DualTVL1OpticalFlow::setLambda(double val)
+    //
+
+    /**
+     *  getLambda SEE: getLambda
+     * @param val automatically generated
+     */
+    public void setLambda(double val) {
+        setLambda_0(nativeObj, val);
+    }
+
+
+    //
+    // C++:  void cv::DualTVL1OpticalFlow::setMedianFiltering(int val)
+    //
+
+    /**
+     *  getMedianFiltering SEE: getMedianFiltering
+     * @param val automatically generated
+     */
+    public void setMedianFiltering(int val) {
+        setMedianFiltering_0(nativeObj, val);
+    }
+
+
+    //
+    // C++:  void cv::DualTVL1OpticalFlow::setOuterIterations(int val)
+    //
+
+    /**
+     *  getOuterIterations SEE: getOuterIterations
+     * @param val automatically generated
+     */
+    public void setOuterIterations(int val) {
+        setOuterIterations_0(nativeObj, val);
+    }
+
+
+    //
+    // C++:  void cv::DualTVL1OpticalFlow::setScaleStep(double val)
+    //
+
+    /**
+     *  getScaleStep SEE: getScaleStep
+     * @param val automatically generated
+     */
+    public void setScaleStep(double val) {
+        setScaleStep_0(nativeObj, val);
+    }
+
+
+    //
+    // C++:  void cv::DualTVL1OpticalFlow::setScalesNumber(int val)
+    //
+
+    /**
+     *  getScalesNumber SEE: getScalesNumber
+     * @param val automatically generated
+     */
+    public void setScalesNumber(int val) {
+        setScalesNumber_0(nativeObj, val);
+    }
+
+
+    //
+    // C++:  void cv::DualTVL1OpticalFlow::setTau(double val)
+    //
+
+    /**
+     *  getTau SEE: getTau
+     * @param val automatically generated
+     */
+    public void setTau(double val) {
+        setTau_0(nativeObj, val);
+    }
+
+
+    //
+    // C++:  void cv::DualTVL1OpticalFlow::setTheta(double val)
+    //
+
+    /**
+     *  getTheta SEE: getTheta
+     * @param val automatically generated
+     */
+    public void setTheta(double val) {
+        setTheta_0(nativeObj, val);
+    }
+
+
+    //
+    // C++:  void cv::DualTVL1OpticalFlow::setUseInitialFlow(bool val)
+    //
+
+    /**
+     *  getUseInitialFlow SEE: getUseInitialFlow
+     * @param val automatically generated
+     */
+    public void setUseInitialFlow(boolean val) {
+        setUseInitialFlow_0(nativeObj, val);
+    }
+
+
+    //
+    // C++:  void cv::DualTVL1OpticalFlow::setWarpingsNumber(int val)
+    //
+
+    /**
+     *  getWarpingsNumber SEE: getWarpingsNumber
+     * @param val automatically generated
+     */
+    public void setWarpingsNumber(int val) {
+        setWarpingsNumber_0(nativeObj, val);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++: static Ptr_DualTVL1OpticalFlow cv::DualTVL1OpticalFlow::create(double tau = 0.25, double lambda = 0.15, double theta = 0.3, int nscales = 5, int warps = 5, double epsilon = 0.01, int innnerIterations = 30, int outerIterations = 10, double scaleStep = 0.8, double gamma = 0.0, int medianFiltering = 5, bool useInitialFlow = false)
+    private static native long create_0(double tau, double lambda, double theta, int nscales, int warps, double epsilon, int innnerIterations, int outerIterations, double scaleStep, double gamma, int medianFiltering, boolean useInitialFlow);
+    private static native long create_1(double tau, double lambda, double theta, int nscales, int warps, double epsilon, int innnerIterations, int outerIterations, double scaleStep, double gamma, int medianFiltering);
+    private static native long create_2(double tau, double lambda, double theta, int nscales, int warps, double epsilon, int innnerIterations, int outerIterations, double scaleStep, double gamma);
+    private static native long create_3(double tau, double lambda, double theta, int nscales, int warps, double epsilon, int innnerIterations, int outerIterations, double scaleStep);
+    private static native long create_4(double tau, double lambda, double theta, int nscales, int warps, double epsilon, int innnerIterations, int outerIterations);
+    private static native long create_5(double tau, double lambda, double theta, int nscales, int warps, double epsilon, int innnerIterations);
+    private static native long create_6(double tau, double lambda, double theta, int nscales, int warps, double epsilon);
+    private static native long create_7(double tau, double lambda, double theta, int nscales, int warps);
+    private static native long create_8(double tau, double lambda, double theta, int nscales);
+    private static native long create_9(double tau, double lambda, double theta);
+    private static native long create_10(double tau, double lambda);
+    private static native long create_11(double tau);
+    private static native long create_12();
+
+    // C++:  bool cv::DualTVL1OpticalFlow::getUseInitialFlow()
+    private static native boolean getUseInitialFlow_0(long nativeObj);
+
+    // C++:  double cv::DualTVL1OpticalFlow::getEpsilon()
+    private static native double getEpsilon_0(long nativeObj);
+
+    // C++:  double cv::DualTVL1OpticalFlow::getGamma()
+    private static native double getGamma_0(long nativeObj);
+
+    // C++:  double cv::DualTVL1OpticalFlow::getLambda()
+    private static native double getLambda_0(long nativeObj);
+
+    // C++:  double cv::DualTVL1OpticalFlow::getScaleStep()
+    private static native double getScaleStep_0(long nativeObj);
+
+    // C++:  double cv::DualTVL1OpticalFlow::getTau()
+    private static native double getTau_0(long nativeObj);
+
+    // C++:  double cv::DualTVL1OpticalFlow::getTheta()
+    private static native double getTheta_0(long nativeObj);
+
+    // C++:  int cv::DualTVL1OpticalFlow::getInnerIterations()
+    private static native int getInnerIterations_0(long nativeObj);
+
+    // C++:  int cv::DualTVL1OpticalFlow::getMedianFiltering()
+    private static native int getMedianFiltering_0(long nativeObj);
+
+    // C++:  int cv::DualTVL1OpticalFlow::getOuterIterations()
+    private static native int getOuterIterations_0(long nativeObj);
+
+    // C++:  int cv::DualTVL1OpticalFlow::getScalesNumber()
+    private static native int getScalesNumber_0(long nativeObj);
+
+    // C++:  int cv::DualTVL1OpticalFlow::getWarpingsNumber()
+    private static native int getWarpingsNumber_0(long nativeObj);
+
+    // C++:  void cv::DualTVL1OpticalFlow::setEpsilon(double val)
+    private static native void setEpsilon_0(long nativeObj, double val);
+
+    // C++:  void cv::DualTVL1OpticalFlow::setGamma(double val)
+    private static native void setGamma_0(long nativeObj, double val);
+
+    // C++:  void cv::DualTVL1OpticalFlow::setInnerIterations(int val)
+    private static native void setInnerIterations_0(long nativeObj, int val);
+
+    // C++:  void cv::DualTVL1OpticalFlow::setLambda(double val)
+    private static native void setLambda_0(long nativeObj, double val);
+
+    // C++:  void cv::DualTVL1OpticalFlow::setMedianFiltering(int val)
+    private static native void setMedianFiltering_0(long nativeObj, int val);
+
+    // C++:  void cv::DualTVL1OpticalFlow::setOuterIterations(int val)
+    private static native void setOuterIterations_0(long nativeObj, int val);
+
+    // C++:  void cv::DualTVL1OpticalFlow::setScaleStep(double val)
+    private static native void setScaleStep_0(long nativeObj, double val);
+
+    // C++:  void cv::DualTVL1OpticalFlow::setScalesNumber(int val)
+    private static native void setScalesNumber_0(long nativeObj, int val);
+
+    // C++:  void cv::DualTVL1OpticalFlow::setTau(double val)
+    private static native void setTau_0(long nativeObj, double val);
+
+    // C++:  void cv::DualTVL1OpticalFlow::setTheta(double val)
+    private static native void setTheta_0(long nativeObj, double val);
+
+    // C++:  void cv::DualTVL1OpticalFlow::setUseInitialFlow(bool val)
+    private static native void setUseInitialFlow_0(long nativeObj, boolean val);
+
+    // C++:  void cv::DualTVL1OpticalFlow::setWarpingsNumber(int val)
+    private static native void setWarpingsNumber_0(long nativeObj, int val);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/photo/TonemapMantiuk.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/photo/TonemapMantiuk.java	(date 1605830248162)
+++ openCVLibrary3411/src/main/java/org/opencv/photo/TonemapMantiuk.java	(date 1605830248162)
@@ -0,0 +1,81 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.photo;
+
+import org.opencv.photo.Tonemap;
+
+// C++: class TonemapMantiuk
+/**
+ * This algorithm transforms image to contrast using gradients on all levels of gaussian pyramid,
+ * transforms contrast values to HVS response and scales the response. After this the image is
+ * reconstructed from new contrast values.
+ *
+ * For more information see CITE: MM06 .
+ */
+public class TonemapMantiuk extends Tonemap {
+
+    protected TonemapMantiuk(long addr) { super(addr); }
+
+    // internal usage only
+    public static TonemapMantiuk __fromPtr__(long addr) { return new TonemapMantiuk(addr); }
+
+    //
+    // C++:  float cv::TonemapMantiuk::getSaturation()
+    //
+
+    public float getSaturation() {
+        return getSaturation_0(nativeObj);
+    }
+
+
+    //
+    // C++:  float cv::TonemapMantiuk::getScale()
+    //
+
+    public float getScale() {
+        return getScale_0(nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::TonemapMantiuk::setSaturation(float saturation)
+    //
+
+    public void setSaturation(float saturation) {
+        setSaturation_0(nativeObj, saturation);
+    }
+
+
+    //
+    // C++:  void cv::TonemapMantiuk::setScale(float scale)
+    //
+
+    public void setScale(float scale) {
+        setScale_0(nativeObj, scale);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++:  float cv::TonemapMantiuk::getSaturation()
+    private static native float getSaturation_0(long nativeObj);
+
+    // C++:  float cv::TonemapMantiuk::getScale()
+    private static native float getScale_0(long nativeObj);
+
+    // C++:  void cv::TonemapMantiuk::setSaturation(float saturation)
+    private static native void setSaturation_0(long nativeObj, float saturation);
+
+    // C++:  void cv::TonemapMantiuk::setScale(float scale)
+    private static native void setScale_0(long nativeObj, float scale);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/photo/TonemapReinhard.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/photo/TonemapReinhard.java	(date 1605830248167)
+++ openCVLibrary3411/src/main/java/org/opencv/photo/TonemapReinhard.java	(date 1605830248167)
@@ -0,0 +1,106 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.photo;
+
+import org.opencv.photo.Tonemap;
+
+// C++: class TonemapReinhard
+/**
+ * This is a global tonemapping operator that models human visual system.
+ *
+ * Mapping function is controlled by adaptation parameter, that is computed using light adaptation and
+ * color adaptation.
+ *
+ * For more information see CITE: RD05 .
+ */
+public class TonemapReinhard extends Tonemap {
+
+    protected TonemapReinhard(long addr) { super(addr); }
+
+    // internal usage only
+    public static TonemapReinhard __fromPtr__(long addr) { return new TonemapReinhard(addr); }
+
+    //
+    // C++:  float cv::TonemapReinhard::getColorAdaptation()
+    //
+
+    public float getColorAdaptation() {
+        return getColorAdaptation_0(nativeObj);
+    }
+
+
+    //
+    // C++:  float cv::TonemapReinhard::getIntensity()
+    //
+
+    public float getIntensity() {
+        return getIntensity_0(nativeObj);
+    }
+
+
+    //
+    // C++:  float cv::TonemapReinhard::getLightAdaptation()
+    //
+
+    public float getLightAdaptation() {
+        return getLightAdaptation_0(nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::TonemapReinhard::setColorAdaptation(float color_adapt)
+    //
+
+    public void setColorAdaptation(float color_adapt) {
+        setColorAdaptation_0(nativeObj, color_adapt);
+    }
+
+
+    //
+    // C++:  void cv::TonemapReinhard::setIntensity(float intensity)
+    //
+
+    public void setIntensity(float intensity) {
+        setIntensity_0(nativeObj, intensity);
+    }
+
+
+    //
+    // C++:  void cv::TonemapReinhard::setLightAdaptation(float light_adapt)
+    //
+
+    public void setLightAdaptation(float light_adapt) {
+        setLightAdaptation_0(nativeObj, light_adapt);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++:  float cv::TonemapReinhard::getColorAdaptation()
+    private static native float getColorAdaptation_0(long nativeObj);
+
+    // C++:  float cv::TonemapReinhard::getIntensity()
+    private static native float getIntensity_0(long nativeObj);
+
+    // C++:  float cv::TonemapReinhard::getLightAdaptation()
+    private static native float getLightAdaptation_0(long nativeObj);
+
+    // C++:  void cv::TonemapReinhard::setColorAdaptation(float color_adapt)
+    private static native void setColorAdaptation_0(long nativeObj, float color_adapt);
+
+    // C++:  void cv::TonemapReinhard::setIntensity(float intensity)
+    private static native void setIntensity_0(long nativeObj, float intensity);
+
+    // C++:  void cv::TonemapReinhard::setLightAdaptation(float light_adapt)
+    private static native void setLightAdaptation_0(long nativeObj, float light_adapt);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/utils/Converters.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/utils/Converters.java	(date 1605830248180)
+++ openCVLibrary3411/src/main/java/org/opencv/utils/Converters.java	(date 1605830248180)
@@ -0,0 +1,806 @@
+package org.opencv.utils;
+
+import java.util.ArrayList;
+import java.util.List;
+
+import org.opencv.core.CvType;
+import org.opencv.core.Mat;
+import org.opencv.core.MatOfByte;
+import org.opencv.core.MatOfDMatch;
+import org.opencv.core.MatOfKeyPoint;
+import org.opencv.core.MatOfPoint;
+import org.opencv.core.MatOfPoint2f;
+import org.opencv.core.MatOfPoint3f;
+import org.opencv.core.Point;
+import org.opencv.core.Point3;
+import org.opencv.core.Size;
+import org.opencv.core.Rect;
+import org.opencv.core.RotatedRect;
+import org.opencv.core.Rect2d;
+import org.opencv.core.DMatch;
+import org.opencv.core.KeyPoint;
+
+public class Converters {
+
+    public static Mat vector_Point_to_Mat(List<Point> pts) {
+        return vector_Point_to_Mat(pts, CvType.CV_32S);
+    }
+
+    public static Mat vector_Point2f_to_Mat(List<Point> pts) {
+        return vector_Point_to_Mat(pts, CvType.CV_32F);
+    }
+
+    public static Mat vector_Point2d_to_Mat(List<Point> pts) {
+        return vector_Point_to_Mat(pts, CvType.CV_64F);
+    }
+
+    public static Mat vector_Point_to_Mat(List<Point> pts, int typeDepth) {
+        Mat res;
+        int count = (pts != null) ? pts.size() : 0;
+        if (count > 0) {
+            switch (typeDepth) {
+            case CvType.CV_32S: {
+                res = new Mat(count, 1, CvType.CV_32SC2);
+                int[] buff = new int[count * 2];
+                for (int i = 0; i < count; i++) {
+                    Point p = pts.get(i);
+                    buff[i * 2] = (int) p.x;
+                    buff[i * 2 + 1] = (int) p.y;
+                }
+                res.put(0, 0, buff);
+            }
+                break;
+
+            case CvType.CV_32F: {
+                res = new Mat(count, 1, CvType.CV_32FC2);
+                float[] buff = new float[count * 2];
+                for (int i = 0; i < count; i++) {
+                    Point p = pts.get(i);
+                    buff[i * 2] = (float) p.x;
+                    buff[i * 2 + 1] = (float) p.y;
+                }
+                res.put(0, 0, buff);
+            }
+                break;
+
+            case CvType.CV_64F: {
+                res = new Mat(count, 1, CvType.CV_64FC2);
+                double[] buff = new double[count * 2];
+                for (int i = 0; i < count; i++) {
+                    Point p = pts.get(i);
+                    buff[i * 2] = p.x;
+                    buff[i * 2 + 1] = p.y;
+                }
+                res.put(0, 0, buff);
+            }
+                break;
+
+            default:
+                throw new IllegalArgumentException("'typeDepth' can be CV_32S, CV_32F or CV_64F");
+            }
+        } else {
+            res = new Mat();
+        }
+        return res;
+    }
+
+    public static Mat vector_Point3i_to_Mat(List<Point3> pts) {
+        return vector_Point3_to_Mat(pts, CvType.CV_32S);
+    }
+
+    public static Mat vector_Point3f_to_Mat(List<Point3> pts) {
+        return vector_Point3_to_Mat(pts, CvType.CV_32F);
+    }
+
+    public static Mat vector_Point3d_to_Mat(List<Point3> pts) {
+        return vector_Point3_to_Mat(pts, CvType.CV_64F);
+    }
+
+    public static Mat vector_Point3_to_Mat(List<Point3> pts, int typeDepth) {
+        Mat res;
+        int count = (pts != null) ? pts.size() : 0;
+        if (count > 0) {
+            switch (typeDepth) {
+            case CvType.CV_32S: {
+                res = new Mat(count, 1, CvType.CV_32SC3);
+                int[] buff = new int[count * 3];
+                for (int i = 0; i < count; i++) {
+                    Point3 p = pts.get(i);
+                    buff[i * 3] = (int) p.x;
+                    buff[i * 3 + 1] = (int) p.y;
+                    buff[i * 3 + 2] = (int) p.z;
+                }
+                res.put(0, 0, buff);
+            }
+                break;
+
+            case CvType.CV_32F: {
+                res = new Mat(count, 1, CvType.CV_32FC3);
+                float[] buff = new float[count * 3];
+                for (int i = 0; i < count; i++) {
+                    Point3 p = pts.get(i);
+                    buff[i * 3] = (float) p.x;
+                    buff[i * 3 + 1] = (float) p.y;
+                    buff[i * 3 + 2] = (float) p.z;
+                }
+                res.put(0, 0, buff);
+            }
+                break;
+
+            case CvType.CV_64F: {
+                res = new Mat(count, 1, CvType.CV_64FC3);
+                double[] buff = new double[count * 3];
+                for (int i = 0; i < count; i++) {
+                    Point3 p = pts.get(i);
+                    buff[i * 3] = p.x;
+                    buff[i * 3 + 1] = p.y;
+                    buff[i * 3 + 2] = p.z;
+                }
+                res.put(0, 0, buff);
+            }
+                break;
+
+            default:
+                throw new IllegalArgumentException("'typeDepth' can be CV_32S, CV_32F or CV_64F");
+            }
+        } else {
+            res = new Mat();
+        }
+        return res;
+    }
+
+    public static void Mat_to_vector_Point2f(Mat m, List<Point> pts) {
+        Mat_to_vector_Point(m, pts);
+    }
+
+    public static void Mat_to_vector_Point2d(Mat m, List<Point> pts) {
+        Mat_to_vector_Point(m, pts);
+    }
+
+    public static void Mat_to_vector_Point(Mat m, List<Point> pts) {
+        if (pts == null)
+            throw new IllegalArgumentException("Output List can't be null");
+        int count = m.rows();
+        int type = m.type();
+        if (m.cols() != 1)
+            throw new IllegalArgumentException("Input Mat should have one column\n" + m);
+
+        pts.clear();
+        if (type == CvType.CV_32SC2) {
+            int[] buff = new int[2 * count];
+            m.get(0, 0, buff);
+            for (int i = 0; i < count; i++) {
+                pts.add(new Point(buff[i * 2], buff[i * 2 + 1]));
+            }
+        } else if (type == CvType.CV_32FC2) {
+            float[] buff = new float[2 * count];
+            m.get(0, 0, buff);
+            for (int i = 0; i < count; i++) {
+                pts.add(new Point(buff[i * 2], buff[i * 2 + 1]));
+            }
+        } else if (type == CvType.CV_64FC2) {
+            double[] buff = new double[2 * count];
+            m.get(0, 0, buff);
+            for (int i = 0; i < count; i++) {
+                pts.add(new Point(buff[i * 2], buff[i * 2 + 1]));
+            }
+        } else {
+            throw new IllegalArgumentException(
+                    "Input Mat should be of CV_32SC2, CV_32FC2 or CV_64FC2 type\n" + m);
+        }
+    }
+
+    public static void Mat_to_vector_Point3i(Mat m, List<Point3> pts) {
+        Mat_to_vector_Point3(m, pts);
+    }
+
+    public static void Mat_to_vector_Point3f(Mat m, List<Point3> pts) {
+        Mat_to_vector_Point3(m, pts);
+    }
+
+    public static void Mat_to_vector_Point3d(Mat m, List<Point3> pts) {
+        Mat_to_vector_Point3(m, pts);
+    }
+
+    public static void Mat_to_vector_Point3(Mat m, List<Point3> pts) {
+        if (pts == null)
+            throw new IllegalArgumentException("Output List can't be null");
+        int count = m.rows();
+        int type = m.type();
+        if (m.cols() != 1)
+            throw new IllegalArgumentException("Input Mat should have one column\n" + m);
+
+        pts.clear();
+        if (type == CvType.CV_32SC3) {
+            int[] buff = new int[3 * count];
+            m.get(0, 0, buff);
+            for (int i = 0; i < count; i++) {
+                pts.add(new Point3(buff[i * 3], buff[i * 3 + 1], buff[i * 3 + 2]));
+            }
+        } else if (type == CvType.CV_32FC3) {
+            float[] buff = new float[3 * count];
+            m.get(0, 0, buff);
+            for (int i = 0; i < count; i++) {
+                pts.add(new Point3(buff[i * 3], buff[i * 3 + 1], buff[i * 3 + 2]));
+            }
+        } else if (type == CvType.CV_64FC3) {
+            double[] buff = new double[3 * count];
+            m.get(0, 0, buff);
+            for (int i = 0; i < count; i++) {
+                pts.add(new Point3(buff[i * 3], buff[i * 3 + 1], buff[i * 3 + 2]));
+            }
+        } else {
+            throw new IllegalArgumentException(
+                    "Input Mat should be of CV_32SC3, CV_32FC3 or CV_64FC3 type\n" + m);
+        }
+    }
+
+    public static Mat vector_Mat_to_Mat(List<Mat> mats) {
+        Mat res;
+        int count = (mats != null) ? mats.size() : 0;
+        if (count > 0) {
+            res = new Mat(count, 1, CvType.CV_32SC2);
+            int[] buff = new int[count * 2];
+            for (int i = 0; i < count; i++) {
+                long addr = mats.get(i).nativeObj;
+                buff[i * 2] = (int) (addr >> 32);
+                buff[i * 2 + 1] = (int) (addr & 0xffffffff);
+            }
+            res.put(0, 0, buff);
+        } else {
+            res = new Mat();
+        }
+        return res;
+    }
+
+    public static void Mat_to_vector_Mat(Mat m, List<Mat> mats) {
+        if (mats == null)
+            throw new IllegalArgumentException("mats == null");
+        int count = m.rows();
+        if (CvType.CV_32SC2 != m.type() || m.cols() != 1)
+            throw new IllegalArgumentException(
+                    "CvType.CV_32SC2 != m.type() ||  m.cols()!=1\n" + m);
+
+        mats.clear();
+        int[] buff = new int[count * 2];
+        m.get(0, 0, buff);
+        for (int i = 0; i < count; i++) {
+            long addr = (((long) buff[i * 2]) << 32) | (((long) buff[i * 2 + 1]) & 0xffffffffL);
+            mats.add(new Mat(addr));
+        }
+    }
+
+    public static Mat vector_float_to_Mat(List<Float> fs) {
+        Mat res;
+        int count = (fs != null) ? fs.size() : 0;
+        if (count > 0) {
+            res = new Mat(count, 1, CvType.CV_32FC1);
+            float[] buff = new float[count];
+            for (int i = 0; i < count; i++) {
+                float f = fs.get(i);
+                buff[i] = f;
+            }
+            res.put(0, 0, buff);
+        } else {
+            res = new Mat();
+        }
+        return res;
+    }
+
+    public static void Mat_to_vector_float(Mat m, List<Float> fs) {
+        if (fs == null)
+            throw new IllegalArgumentException("fs == null");
+        int count = m.rows();
+        if (CvType.CV_32FC1 != m.type() || m.cols() != 1)
+            throw new IllegalArgumentException(
+                    "CvType.CV_32FC1 != m.type() ||  m.cols()!=1\n" + m);
+
+        fs.clear();
+        float[] buff = new float[count];
+        m.get(0, 0, buff);
+        for (int i = 0; i < count; i++) {
+            fs.add(buff[i]);
+        }
+    }
+
+    public static Mat vector_uchar_to_Mat(List<Byte> bs) {
+        Mat res;
+        int count = (bs != null) ? bs.size() : 0;
+        if (count > 0) {
+            res = new Mat(count, 1, CvType.CV_8UC1);
+            byte[] buff = new byte[count];
+            for (int i = 0; i < count; i++) {
+                byte b = bs.get(i);
+                buff[i] = b;
+            }
+            res.put(0, 0, buff);
+        } else {
+            res = new Mat();
+        }
+        return res;
+    }
+
+    public static void Mat_to_vector_uchar(Mat m, List<Byte> us) {
+        if (us == null)
+            throw new IllegalArgumentException("Output List can't be null");
+        int count = m.rows();
+        if (CvType.CV_8UC1 != m.type() || m.cols() != 1)
+            throw new IllegalArgumentException(
+                    "CvType.CV_8UC1 != m.type() ||  m.cols()!=1\n" + m);
+
+        us.clear();
+        byte[] buff = new byte[count];
+        m.get(0, 0, buff);
+        for (int i = 0; i < count; i++) {
+            us.add(buff[i]);
+        }
+    }
+
+    public static Mat vector_char_to_Mat(List<Byte> bs) {
+        Mat res;
+        int count = (bs != null) ? bs.size() : 0;
+        if (count > 0) {
+            res = new Mat(count, 1, CvType.CV_8SC1);
+            byte[] buff = new byte[count];
+            for (int i = 0; i < count; i++) {
+                byte b = bs.get(i);
+                buff[i] = b;
+            }
+            res.put(0, 0, buff);
+        } else {
+            res = new Mat();
+        }
+        return res;
+    }
+
+    public static Mat vector_int_to_Mat(List<Integer> is) {
+        Mat res;
+        int count = (is != null) ? is.size() : 0;
+        if (count > 0) {
+            res = new Mat(count, 1, CvType.CV_32SC1);
+            int[] buff = new int[count];
+            for (int i = 0; i < count; i++) {
+                int v = is.get(i);
+                buff[i] = v;
+            }
+            res.put(0, 0, buff);
+        } else {
+            res = new Mat();
+        }
+        return res;
+    }
+
+    public static void Mat_to_vector_int(Mat m, List<Integer> is) {
+        if (is == null)
+            throw new IllegalArgumentException("is == null");
+        int count = m.rows();
+        if (CvType.CV_32SC1 != m.type() || m.cols() != 1)
+            throw new IllegalArgumentException(
+                    "CvType.CV_32SC1 != m.type() ||  m.cols()!=1\n" + m);
+
+        is.clear();
+        int[] buff = new int[count];
+        m.get(0, 0, buff);
+        for (int i = 0; i < count; i++) {
+            is.add(buff[i]);
+        }
+    }
+
+    public static void Mat_to_vector_char(Mat m, List<Byte> bs) {
+        if (bs == null)
+            throw new IllegalArgumentException("Output List can't be null");
+        int count = m.rows();
+        if (CvType.CV_8SC1 != m.type() || m.cols() != 1)
+            throw new IllegalArgumentException(
+                    "CvType.CV_8SC1 != m.type() ||  m.cols()!=1\n" + m);
+
+        bs.clear();
+        byte[] buff = new byte[count];
+        m.get(0, 0, buff);
+        for (int i = 0; i < count; i++) {
+            bs.add(buff[i]);
+        }
+    }
+
+    public static Mat vector_Rect_to_Mat(List<Rect> rs) {
+        Mat res;
+        int count = (rs != null) ? rs.size() : 0;
+        if (count > 0) {
+            res = new Mat(count, 1, CvType.CV_32SC4);
+            int[] buff = new int[4 * count];
+            for (int i = 0; i < count; i++) {
+                Rect r = rs.get(i);
+                buff[4 * i] = r.x;
+                buff[4 * i + 1] = r.y;
+                buff[4 * i + 2] = r.width;
+                buff[4 * i + 3] = r.height;
+            }
+            res.put(0, 0, buff);
+        } else {
+            res = new Mat();
+        }
+        return res;
+    }
+
+    public static void Mat_to_vector_Rect(Mat m, List<Rect> rs) {
+        if (rs == null)
+            throw new IllegalArgumentException("rs == null");
+        int count = m.rows();
+        if (CvType.CV_32SC4 != m.type() || m.cols() != 1)
+            throw new IllegalArgumentException(
+                    "CvType.CV_32SC4 != m.type() ||  m.rows()!=1\n" + m);
+
+        rs.clear();
+        int[] buff = new int[4 * count];
+        m.get(0, 0, buff);
+        for (int i = 0; i < count; i++) {
+            rs.add(new Rect(buff[4 * i], buff[4 * i + 1], buff[4 * i + 2], buff[4 * i + 3]));
+        }
+    }
+
+    public static Mat vector_Rect2d_to_Mat(List<Rect2d> rs) {
+        Mat res;
+        int count = (rs != null) ? rs.size() : 0;
+        if (count > 0) {
+            res = new Mat(count, 1, CvType.CV_64FC4);
+            double[] buff = new double[4 * count];
+            for (int i = 0; i < count; i++) {
+                Rect2d r = rs.get(i);
+                buff[4 * i] = r.x;
+                buff[4 * i + 1] = r.y;
+                buff[4 * i + 2] = r.width;
+                buff[4 * i + 3] = r.height;
+            }
+            res.put(0, 0, buff);
+        } else {
+            res = new Mat();
+        }
+        return res;
+    }
+
+    public static void Mat_to_vector_Rect2d(Mat m, List<Rect2d> rs) {
+        if (rs == null)
+            throw new IllegalArgumentException("rs == null");
+        int count = m.rows();
+        if (CvType.CV_64FC4 != m.type() || m.cols() != 1)
+            throw new IllegalArgumentException(
+                                                         "CvType.CV_64FC4 != m.type() ||  m.rows()!=1\n" + m);
+
+        rs.clear();
+        double[] buff = new double[4 * count];
+        m.get(0, 0, buff);
+        for (int i = 0; i < count; i++) {
+            rs.add(new Rect2d(buff[4 * i], buff[4 * i + 1], buff[4 * i + 2], buff[4 * i + 3]));
+        }
+    }
+
+    public static Mat vector_KeyPoint_to_Mat(List<KeyPoint> kps) {
+        Mat res;
+        int count = (kps != null) ? kps.size() : 0;
+        if (count > 0) {
+            res = new Mat(count, 1, CvType.CV_64FC(7));
+            double[] buff = new double[count * 7];
+            for (int i = 0; i < count; i++) {
+                KeyPoint kp = kps.get(i);
+                buff[7 * i] = kp.pt.x;
+                buff[7 * i + 1] = kp.pt.y;
+                buff[7 * i + 2] = kp.size;
+                buff[7 * i + 3] = kp.angle;
+                buff[7 * i + 4] = kp.response;
+                buff[7 * i + 5] = kp.octave;
+                buff[7 * i + 6] = kp.class_id;
+            }
+            res.put(0, 0, buff);
+        } else {
+            res = new Mat();
+        }
+        return res;
+    }
+
+    public static void Mat_to_vector_KeyPoint(Mat m, List<KeyPoint> kps) {
+        if (kps == null)
+            throw new IllegalArgumentException("Output List can't be null");
+        int count = m.rows();
+        if (CvType.CV_64FC(7) != m.type() || m.cols() != 1)
+            throw new IllegalArgumentException(
+                    "CvType.CV_64FC(7) != m.type() ||  m.cols()!=1\n" + m);
+
+        kps.clear();
+        double[] buff = new double[7 * count];
+        m.get(0, 0, buff);
+        for (int i = 0; i < count; i++) {
+            kps.add(new KeyPoint((float) buff[7 * i], (float) buff[7 * i + 1], (float) buff[7 * i + 2], (float) buff[7 * i + 3],
+                    (float) buff[7 * i + 4], (int) buff[7 * i + 5], (int) buff[7 * i + 6]));
+        }
+    }
+
+    // vector_vector_Point
+    public static Mat vector_vector_Point_to_Mat(List<MatOfPoint> pts, List<Mat> mats) {
+        Mat res;
+        int lCount = (pts != null) ? pts.size() : 0;
+        if (lCount > 0) {
+            mats.addAll(pts);
+            res = vector_Mat_to_Mat(mats);
+        } else {
+            res = new Mat();
+        }
+        return res;
+    }
+
+    public static void Mat_to_vector_vector_Point(Mat m, List<MatOfPoint> pts) {
+        if (pts == null)
+            throw new IllegalArgumentException("Output List can't be null");
+
+        if (m == null)
+            throw new IllegalArgumentException("Input Mat can't be null");
+
+        List<Mat> mats = new ArrayList<Mat>(m.rows());
+        Mat_to_vector_Mat(m, mats);
+        for (Mat mi : mats) {
+            MatOfPoint pt = new MatOfPoint(mi);
+            pts.add(pt);
+            mi.release();
+        }
+        mats.clear();
+    }
+
+    // vector_vector_Point2f
+    public static void Mat_to_vector_vector_Point2f(Mat m, List<MatOfPoint2f> pts) {
+        if (pts == null)
+            throw new IllegalArgumentException("Output List can't be null");
+
+        if (m == null)
+            throw new IllegalArgumentException("Input Mat can't be null");
+
+        List<Mat> mats = new ArrayList<Mat>(m.rows());
+        Mat_to_vector_Mat(m, mats);
+        for (Mat mi : mats) {
+            MatOfPoint2f pt = new MatOfPoint2f(mi);
+            pts.add(pt);
+            mi.release();
+        }
+        mats.clear();
+    }
+
+    // vector_vector_Point2f
+    public static Mat vector_vector_Point2f_to_Mat(List<MatOfPoint2f> pts, List<Mat> mats) {
+        Mat res;
+        int lCount = (pts != null) ? pts.size() : 0;
+        if (lCount > 0) {
+            mats.addAll(pts);
+            res = vector_Mat_to_Mat(mats);
+        } else {
+            res = new Mat();
+        }
+        return res;
+    }
+
+    // vector_vector_Point3f
+    public static void Mat_to_vector_vector_Point3f(Mat m, List<MatOfPoint3f> pts) {
+        if (pts == null)
+            throw new IllegalArgumentException("Output List can't be null");
+
+        if (m == null)
+            throw new IllegalArgumentException("Input Mat can't be null");
+
+        List<Mat> mats = new ArrayList<Mat>(m.rows());
+        Mat_to_vector_Mat(m, mats);
+        for (Mat mi : mats) {
+            MatOfPoint3f pt = new MatOfPoint3f(mi);
+            pts.add(pt);
+            mi.release();
+        }
+        mats.clear();
+    }
+
+    // vector_vector_Point3f
+    public static Mat vector_vector_Point3f_to_Mat(List<MatOfPoint3f> pts, List<Mat> mats) {
+        Mat res;
+        int lCount = (pts != null) ? pts.size() : 0;
+        if (lCount > 0) {
+            mats.addAll(pts);
+            res = vector_Mat_to_Mat(mats);
+        } else {
+            res = new Mat();
+        }
+        return res;
+    }
+
+    // vector_vector_KeyPoint
+    public static Mat vector_vector_KeyPoint_to_Mat(List<MatOfKeyPoint> kps, List<Mat> mats) {
+        Mat res;
+        int lCount = (kps != null) ? kps.size() : 0;
+        if (lCount > 0) {
+            mats.addAll(kps);
+            res = vector_Mat_to_Mat(mats);
+        } else {
+            res = new Mat();
+        }
+        return res;
+    }
+
+    public static void Mat_to_vector_vector_KeyPoint(Mat m, List<MatOfKeyPoint> kps) {
+        if (kps == null)
+            throw new IllegalArgumentException("Output List can't be null");
+
+        if (m == null)
+            throw new IllegalArgumentException("Input Mat can't be null");
+
+        List<Mat> mats = new ArrayList<Mat>(m.rows());
+        Mat_to_vector_Mat(m, mats);
+        for (Mat mi : mats) {
+            MatOfKeyPoint vkp = new MatOfKeyPoint(mi);
+            kps.add(vkp);
+            mi.release();
+        }
+        mats.clear();
+    }
+
+    public static Mat vector_double_to_Mat(List<Double> ds) {
+        Mat res;
+        int count = (ds != null) ? ds.size() : 0;
+        if (count > 0) {
+            res = new Mat(count, 1, CvType.CV_64FC1);
+            double[] buff = new double[count];
+            for (int i = 0; i < count; i++) {
+                double v = ds.get(i);
+                buff[i] = v;
+            }
+            res.put(0, 0, buff);
+        } else {
+            res = new Mat();
+        }
+        return res;
+    }
+
+    public static void Mat_to_vector_double(Mat m, List<Double> ds) {
+        if (ds == null)
+            throw new IllegalArgumentException("ds == null");
+        int count = m.rows();
+        if (CvType.CV_64FC1 != m.type() || m.cols() != 1)
+            throw new IllegalArgumentException(
+                    "CvType.CV_64FC1 != m.type() ||  m.cols()!=1\n" + m);
+
+        ds.clear();
+        double[] buff = new double[count];
+        m.get(0, 0, buff);
+        for (int i = 0; i < count; i++) {
+            ds.add(buff[i]);
+        }
+    }
+
+    public static Mat vector_DMatch_to_Mat(List<DMatch> matches) {
+        Mat res;
+        int count = (matches != null) ? matches.size() : 0;
+        if (count > 0) {
+            res = new Mat(count, 1, CvType.CV_64FC4);
+            double[] buff = new double[count * 4];
+            for (int i = 0; i < count; i++) {
+                DMatch m = matches.get(i);
+                buff[4 * i] = m.queryIdx;
+                buff[4 * i + 1] = m.trainIdx;
+                buff[4 * i + 2] = m.imgIdx;
+                buff[4 * i + 3] = m.distance;
+            }
+            res.put(0, 0, buff);
+        } else {
+            res = new Mat();
+        }
+        return res;
+    }
+
+    public static void Mat_to_vector_DMatch(Mat m, List<DMatch> matches) {
+        if (matches == null)
+            throw new IllegalArgumentException("Output List can't be null");
+        int count = m.rows();
+        if (CvType.CV_64FC4 != m.type() || m.cols() != 1)
+            throw new IllegalArgumentException(
+                    "CvType.CV_64FC4 != m.type() ||  m.cols()!=1\n" + m);
+
+        matches.clear();
+        double[] buff = new double[4 * count];
+        m.get(0, 0, buff);
+        for (int i = 0; i < count; i++) {
+            matches.add(new DMatch((int) buff[4 * i], (int) buff[4 * i + 1], (int) buff[4 * i + 2], (float) buff[4 * i + 3]));
+        }
+    }
+
+    // vector_vector_DMatch
+    public static Mat vector_vector_DMatch_to_Mat(List<MatOfDMatch> lvdm, List<Mat> mats) {
+        Mat res;
+        int lCount = (lvdm != null) ? lvdm.size() : 0;
+        if (lCount > 0) {
+            mats.addAll(lvdm);
+            res = vector_Mat_to_Mat(mats);
+        } else {
+            res = new Mat();
+        }
+        return res;
+    }
+
+    public static void Mat_to_vector_vector_DMatch(Mat m, List<MatOfDMatch> lvdm) {
+        if (lvdm == null)
+            throw new IllegalArgumentException("Output List can't be null");
+
+        if (m == null)
+            throw new IllegalArgumentException("Input Mat can't be null");
+
+        List<Mat> mats = new ArrayList<Mat>(m.rows());
+        Mat_to_vector_Mat(m, mats);
+        lvdm.clear();
+        for (Mat mi : mats) {
+            MatOfDMatch vdm = new MatOfDMatch(mi);
+            lvdm.add(vdm);
+            mi.release();
+        }
+        mats.clear();
+    }
+
+    // vector_vector_char
+    public static Mat vector_vector_char_to_Mat(List<MatOfByte> lvb, List<Mat> mats) {
+        Mat res;
+        int lCount = (lvb != null) ? lvb.size() : 0;
+        if (lCount > 0) {
+            mats.addAll(lvb);
+            res = vector_Mat_to_Mat(mats);
+        } else {
+            res = new Mat();
+        }
+        return res;
+    }
+
+    public static void Mat_to_vector_vector_char(Mat m, List<List<Byte>> llb) {
+        if (llb == null)
+            throw new IllegalArgumentException("Output List can't be null");
+
+        if (m == null)
+            throw new IllegalArgumentException("Input Mat can't be null");
+
+        List<Mat> mats = new ArrayList<Mat>(m.rows());
+        Mat_to_vector_Mat(m, mats);
+        for (Mat mi : mats) {
+            List<Byte> lb = new ArrayList<Byte>();
+            Mat_to_vector_char(mi, lb);
+            llb.add(lb);
+            mi.release();
+        }
+        mats.clear();
+    }
+
+    public static Mat vector_RotatedRect_to_Mat(List<RotatedRect> rs) {
+        Mat res;
+        int count = (rs != null) ? rs.size() : 0;
+        if (count > 0) {
+            res = new Mat(count, 1, CvType.CV_32FC(5));
+            float[] buff = new float[5 * count];
+            for (int i = 0; i < count; i++) {
+                RotatedRect r = rs.get(i);
+                buff[5 * i] = (float)r.center.x;
+                buff[5 * i + 1] = (float)r.center.y;
+                buff[5 * i + 2] = (float)r.size.width;
+                buff[5 * i + 3] = (float)r.size.height;
+                buff[5 * i + 4] = (float)r.angle;
+            }
+            res.put(0, 0, buff);
+        } else {
+            res = new Mat();
+        }
+        return res;
+    }
+
+    public static void Mat_to_vector_RotatedRect(Mat m, List<RotatedRect> rs) {
+        if (rs == null)
+            throw new IllegalArgumentException("rs == null");
+        int count = m.rows();
+        if (CvType.CV_32FC(5) != m.type() || m.cols() != 1)
+            throw new IllegalArgumentException(
+                    "CvType.CV_32FC5 != m.type() ||  m.rows()!=1\n" + m);
+
+        rs.clear();
+        float[] buff = new float[5 * count];
+        m.get(0, 0, buff);
+        for (int i = 0; i < count; i++) {
+            rs.add(new RotatedRect(new Point(buff[5 * i], buff[5 * i + 1]), new Size(buff[5 * i + 2], buff[5 * i + 3]), buff[5 * i + 4]));
+        }
+    }
+}
Index: openCVLibrary3411/src/main/java/org/opencv/video/BackgroundSubtractor.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/video/BackgroundSubtractor.java	(date 1605830248183)
+++ openCVLibrary3411/src/main/java/org/opencv/video/BackgroundSubtractor.java	(date 1605830248183)
@@ -0,0 +1,89 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.video;
+
+import org.opencv.core.Algorithm;
+import org.opencv.core.Mat;
+
+// C++: class BackgroundSubtractor
+/**
+ * Base class for background/foreground segmentation. :
+ *
+ * The class is only used to define the common interface for the whole family of background/foreground
+ * segmentation algorithms.
+ */
+public class BackgroundSubtractor extends Algorithm {
+
+    protected BackgroundSubtractor(long addr) { super(addr); }
+
+    // internal usage only
+    public static BackgroundSubtractor __fromPtr__(long addr) { return new BackgroundSubtractor(addr); }
+
+    //
+    // C++:  void cv::BackgroundSubtractor::apply(Mat image, Mat& fgmask, double learningRate = -1)
+    //
+
+    /**
+     * Computes a foreground mask.
+     *
+     *     @param image Next video frame.
+     *     @param fgmask The output foreground mask as an 8-bit binary image.
+     *     @param learningRate The value between 0 and 1 that indicates how fast the background model is
+     *     learnt. Negative parameter value makes the algorithm to use some automatically chosen learning
+     *     rate. 0 means that the background model is not updated at all, 1 means that the background model
+     *     is completely reinitialized from the last frame.
+     */
+    public void apply(Mat image, Mat fgmask, double learningRate) {
+        apply_0(nativeObj, image.nativeObj, fgmask.nativeObj, learningRate);
+    }
+
+    /**
+     * Computes a foreground mask.
+     *
+     *     @param image Next video frame.
+     *     @param fgmask The output foreground mask as an 8-bit binary image.
+     *     learnt. Negative parameter value makes the algorithm to use some automatically chosen learning
+     *     rate. 0 means that the background model is not updated at all, 1 means that the background model
+     *     is completely reinitialized from the last frame.
+     */
+    public void apply(Mat image, Mat fgmask) {
+        apply_1(nativeObj, image.nativeObj, fgmask.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::BackgroundSubtractor::getBackgroundImage(Mat& backgroundImage)
+    //
+
+    /**
+     * Computes a background image.
+     *
+     *     @param backgroundImage The output background image.
+     *
+     *     <b>Note:</b> Sometimes the background image can be very blurry, as it contain the average background
+     *     statistics.
+     */
+    public void getBackgroundImage(Mat backgroundImage) {
+        getBackgroundImage_0(nativeObj, backgroundImage.nativeObj);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++:  void cv::BackgroundSubtractor::apply(Mat image, Mat& fgmask, double learningRate = -1)
+    private static native void apply_0(long nativeObj, long image_nativeObj, long fgmask_nativeObj, double learningRate);
+    private static native void apply_1(long nativeObj, long image_nativeObj, long fgmask_nativeObj);
+
+    // C++:  void cv::BackgroundSubtractor::getBackgroundImage(Mat& backgroundImage)
+    private static native void getBackgroundImage_0(long nativeObj, long backgroundImage_nativeObj);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/photo/MergeRobertson.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/photo/MergeRobertson.java	(date 1605830248107)
+++ openCVLibrary3411/src/main/java/org/opencv/photo/MergeRobertson.java	(date 1605830248107)
@@ -0,0 +1,62 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.photo;
+
+import java.util.ArrayList;
+import java.util.List;
+import org.opencv.core.Mat;
+import org.opencv.photo.MergeExposures;
+import org.opencv.utils.Converters;
+
+// C++: class MergeRobertson
+/**
+ * The resulting HDR image is calculated as weighted average of the exposures considering exposure
+ * values and camera response.
+ *
+ * For more information see CITE: RB99 .
+ */
+public class MergeRobertson extends MergeExposures {
+
+    protected MergeRobertson(long addr) { super(addr); }
+
+    // internal usage only
+    public static MergeRobertson __fromPtr__(long addr) { return new MergeRobertson(addr); }
+
+    //
+    // C++:  void cv::MergeRobertson::process(vector_Mat src, Mat& dst, Mat times, Mat response)
+    //
+
+    public void process(List<Mat> src, Mat dst, Mat times, Mat response) {
+        Mat src_mat = Converters.vector_Mat_to_Mat(src);
+        process_0(nativeObj, src_mat.nativeObj, dst.nativeObj, times.nativeObj, response.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::MergeRobertson::process(vector_Mat src, Mat& dst, Mat times)
+    //
+
+    public void process(List<Mat> src, Mat dst, Mat times) {
+        Mat src_mat = Converters.vector_Mat_to_Mat(src);
+        process_1(nativeObj, src_mat.nativeObj, dst.nativeObj, times.nativeObj);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++:  void cv::MergeRobertson::process(vector_Mat src, Mat& dst, Mat times, Mat response)
+    private static native void process_0(long nativeObj, long src_mat_nativeObj, long dst_nativeObj, long times_nativeObj, long response_nativeObj);
+
+    // C++:  void cv::MergeRobertson::process(vector_Mat src, Mat& dst, Mat times)
+    private static native void process_1(long nativeObj, long src_mat_nativeObj, long dst_nativeObj, long times_nativeObj);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/photo/Photo.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/photo/Photo.java	(date 1605830248143)
+++ openCVLibrary3411/src/main/java/org/opencv/photo/Photo.java	(date 1605830248143)
@@ -0,0 +1,1919 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.photo;
+
+import java.util.ArrayList;
+import java.util.List;
+import org.opencv.core.Mat;
+import org.opencv.core.MatOfFloat;
+import org.opencv.core.Point;
+import org.opencv.photo.AlignMTB;
+import org.opencv.photo.CalibrateDebevec;
+import org.opencv.photo.CalibrateRobertson;
+import org.opencv.photo.MergeDebevec;
+import org.opencv.photo.MergeMertens;
+import org.opencv.photo.MergeRobertson;
+import org.opencv.photo.Tonemap;
+import org.opencv.photo.TonemapDrago;
+import org.opencv.photo.TonemapMantiuk;
+import org.opencv.photo.TonemapReinhard;
+import org.opencv.utils.Converters;
+
+// C++: class Photo
+
+public class Photo {
+
+    private static final int
+            CV_INPAINT_NS = 0,
+            CV_INPAINT_TELEA = 1;
+
+
+    // C++: enum <unnamed>
+    public static final int
+            INPAINT_NS = 0,
+            INPAINT_TELEA = 1,
+            LDR_SIZE = 256,
+            NORMAL_CLONE = 1,
+            MIXED_CLONE = 2,
+            MONOCHROME_TRANSFER = 3,
+            RECURS_FILTER = 1,
+            NORMCONV_FILTER = 2;
+
+
+    //
+    // C++:  Ptr_AlignMTB cv::createAlignMTB(int max_bits = 6, int exclude_range = 4, bool cut = true)
+    //
+
+    /**
+     * Creates AlignMTB object
+     *
+     * @param max_bits logarithm to the base 2 of maximal shift in each dimension. Values of 5 and 6 are
+     * usually good enough (31 and 63 pixels shift respectively).
+     * @param exclude_range range for exclusion bitmap that is constructed to suppress noise around the
+     * median value.
+     * @param cut if true cuts images, otherwise fills the new regions with zeros.
+     * @return automatically generated
+     */
+    public static AlignMTB createAlignMTB(int max_bits, int exclude_range, boolean cut) {
+        return AlignMTB.__fromPtr__(createAlignMTB_0(max_bits, exclude_range, cut));
+    }
+
+    /**
+     * Creates AlignMTB object
+     *
+     * @param max_bits logarithm to the base 2 of maximal shift in each dimension. Values of 5 and 6 are
+     * usually good enough (31 and 63 pixels shift respectively).
+     * @param exclude_range range for exclusion bitmap that is constructed to suppress noise around the
+     * median value.
+     * @return automatically generated
+     */
+    public static AlignMTB createAlignMTB(int max_bits, int exclude_range) {
+        return AlignMTB.__fromPtr__(createAlignMTB_1(max_bits, exclude_range));
+    }
+
+    /**
+     * Creates AlignMTB object
+     *
+     * @param max_bits logarithm to the base 2 of maximal shift in each dimension. Values of 5 and 6 are
+     * usually good enough (31 and 63 pixels shift respectively).
+     * median value.
+     * @return automatically generated
+     */
+    public static AlignMTB createAlignMTB(int max_bits) {
+        return AlignMTB.__fromPtr__(createAlignMTB_2(max_bits));
+    }
+
+    /**
+     * Creates AlignMTB object
+     *
+     * usually good enough (31 and 63 pixels shift respectively).
+     * median value.
+     * @return automatically generated
+     */
+    public static AlignMTB createAlignMTB() {
+        return AlignMTB.__fromPtr__(createAlignMTB_3());
+    }
+
+
+    //
+    // C++:  Ptr_CalibrateDebevec cv::createCalibrateDebevec(int samples = 70, float lambda = 10.0f, bool random = false)
+    //
+
+    /**
+     * Creates CalibrateDebevec object
+     *
+     * @param samples number of pixel locations to use
+     * @param lambda smoothness term weight. Greater values produce smoother results, but can alter the
+     * response.
+     * @param random if true sample pixel locations are chosen at random, otherwise they form a
+     * rectangular grid.
+     * @return automatically generated
+     */
+    public static CalibrateDebevec createCalibrateDebevec(int samples, float lambda, boolean random) {
+        return CalibrateDebevec.__fromPtr__(createCalibrateDebevec_0(samples, lambda, random));
+    }
+
+    /**
+     * Creates CalibrateDebevec object
+     *
+     * @param samples number of pixel locations to use
+     * @param lambda smoothness term weight. Greater values produce smoother results, but can alter the
+     * response.
+     * rectangular grid.
+     * @return automatically generated
+     */
+    public static CalibrateDebevec createCalibrateDebevec(int samples, float lambda) {
+        return CalibrateDebevec.__fromPtr__(createCalibrateDebevec_1(samples, lambda));
+    }
+
+    /**
+     * Creates CalibrateDebevec object
+     *
+     * @param samples number of pixel locations to use
+     * response.
+     * rectangular grid.
+     * @return automatically generated
+     */
+    public static CalibrateDebevec createCalibrateDebevec(int samples) {
+        return CalibrateDebevec.__fromPtr__(createCalibrateDebevec_2(samples));
+    }
+
+    /**
+     * Creates CalibrateDebevec object
+     *
+     * response.
+     * rectangular grid.
+     * @return automatically generated
+     */
+    public static CalibrateDebevec createCalibrateDebevec() {
+        return CalibrateDebevec.__fromPtr__(createCalibrateDebevec_3());
+    }
+
+
+    //
+    // C++:  Ptr_CalibrateRobertson cv::createCalibrateRobertson(int max_iter = 30, float threshold = 0.01f)
+    //
+
+    /**
+     * Creates CalibrateRobertson object
+     *
+     * @param max_iter maximal number of Gauss-Seidel solver iterations.
+     * @param threshold target difference between results of two successive steps of the minimization.
+     * @return automatically generated
+     */
+    public static CalibrateRobertson createCalibrateRobertson(int max_iter, float threshold) {
+        return CalibrateRobertson.__fromPtr__(createCalibrateRobertson_0(max_iter, threshold));
+    }
+
+    /**
+     * Creates CalibrateRobertson object
+     *
+     * @param max_iter maximal number of Gauss-Seidel solver iterations.
+     * @return automatically generated
+     */
+    public static CalibrateRobertson createCalibrateRobertson(int max_iter) {
+        return CalibrateRobertson.__fromPtr__(createCalibrateRobertson_1(max_iter));
+    }
+
+    /**
+     * Creates CalibrateRobertson object
+     *
+     * @return automatically generated
+     */
+    public static CalibrateRobertson createCalibrateRobertson() {
+        return CalibrateRobertson.__fromPtr__(createCalibrateRobertson_2());
+    }
+
+
+    //
+    // C++:  Ptr_MergeDebevec cv::createMergeDebevec()
+    //
+
+    /**
+     * Creates MergeDebevec object
+     * @return automatically generated
+     */
+    public static MergeDebevec createMergeDebevec() {
+        return MergeDebevec.__fromPtr__(createMergeDebevec_0());
+    }
+
+
+    //
+    // C++:  Ptr_MergeMertens cv::createMergeMertens(float contrast_weight = 1.0f, float saturation_weight = 1.0f, float exposure_weight = 0.0f)
+    //
+
+    /**
+     * Creates MergeMertens object
+     *
+     * @param contrast_weight contrast measure weight. See MergeMertens.
+     * @param saturation_weight saturation measure weight
+     * @param exposure_weight well-exposedness measure weight
+     * @return automatically generated
+     */
+    public static MergeMertens createMergeMertens(float contrast_weight, float saturation_weight, float exposure_weight) {
+        return MergeMertens.__fromPtr__(createMergeMertens_0(contrast_weight, saturation_weight, exposure_weight));
+    }
+
+    /**
+     * Creates MergeMertens object
+     *
+     * @param contrast_weight contrast measure weight. See MergeMertens.
+     * @param saturation_weight saturation measure weight
+     * @return automatically generated
+     */
+    public static MergeMertens createMergeMertens(float contrast_weight, float saturation_weight) {
+        return MergeMertens.__fromPtr__(createMergeMertens_1(contrast_weight, saturation_weight));
+    }
+
+    /**
+     * Creates MergeMertens object
+     *
+     * @param contrast_weight contrast measure weight. See MergeMertens.
+     * @return automatically generated
+     */
+    public static MergeMertens createMergeMertens(float contrast_weight) {
+        return MergeMertens.__fromPtr__(createMergeMertens_2(contrast_weight));
+    }
+
+    /**
+     * Creates MergeMertens object
+     *
+     * @return automatically generated
+     */
+    public static MergeMertens createMergeMertens() {
+        return MergeMertens.__fromPtr__(createMergeMertens_3());
+    }
+
+
+    //
+    // C++:  Ptr_MergeRobertson cv::createMergeRobertson()
+    //
+
+    /**
+     * Creates MergeRobertson object
+     * @return automatically generated
+     */
+    public static MergeRobertson createMergeRobertson() {
+        return MergeRobertson.__fromPtr__(createMergeRobertson_0());
+    }
+
+
+    //
+    // C++:  Ptr_Tonemap cv::createTonemap(float gamma = 1.0f)
+    //
+
+    /**
+     * Creates simple linear mapper with gamma correction
+     *
+     * @param gamma positive value for gamma correction. Gamma value of 1.0 implies no correction, gamma
+     * equal to 2.2f is suitable for most displays.
+     * Generally gamma &gt; 1 brightens the image and gamma &lt; 1 darkens it.
+     * @return automatically generated
+     */
+    public static Tonemap createTonemap(float gamma) {
+        return Tonemap.__fromPtr__(createTonemap_0(gamma));
+    }
+
+    /**
+     * Creates simple linear mapper with gamma correction
+     *
+     * equal to 2.2f is suitable for most displays.
+     * Generally gamma &gt; 1 brightens the image and gamma &lt; 1 darkens it.
+     * @return automatically generated
+     */
+    public static Tonemap createTonemap() {
+        return Tonemap.__fromPtr__(createTonemap_1());
+    }
+
+
+    //
+    // C++:  Ptr_TonemapDrago cv::createTonemapDrago(float gamma = 1.0f, float saturation = 1.0f, float bias = 0.85f)
+    //
+
+    /**
+     * Creates TonemapDrago object
+     *
+     * @param gamma gamma value for gamma correction. See createTonemap
+     * @param saturation positive saturation enhancement value. 1.0 preserves saturation, values greater
+     * than 1 increase saturation and values less than 1 decrease it.
+     * @param bias value for bias function in [0, 1] range. Values from 0.7 to 0.9 usually give best
+     * results, default value is 0.85.
+     * @return automatically generated
+     */
+    public static TonemapDrago createTonemapDrago(float gamma, float saturation, float bias) {
+        return TonemapDrago.__fromPtr__(createTonemapDrago_0(gamma, saturation, bias));
+    }
+
+    /**
+     * Creates TonemapDrago object
+     *
+     * @param gamma gamma value for gamma correction. See createTonemap
+     * @param saturation positive saturation enhancement value. 1.0 preserves saturation, values greater
+     * than 1 increase saturation and values less than 1 decrease it.
+     * results, default value is 0.85.
+     * @return automatically generated
+     */
+    public static TonemapDrago createTonemapDrago(float gamma, float saturation) {
+        return TonemapDrago.__fromPtr__(createTonemapDrago_1(gamma, saturation));
+    }
+
+    /**
+     * Creates TonemapDrago object
+     *
+     * @param gamma gamma value for gamma correction. See createTonemap
+     * than 1 increase saturation and values less than 1 decrease it.
+     * results, default value is 0.85.
+     * @return automatically generated
+     */
+    public static TonemapDrago createTonemapDrago(float gamma) {
+        return TonemapDrago.__fromPtr__(createTonemapDrago_2(gamma));
+    }
+
+    /**
+     * Creates TonemapDrago object
+     *
+     * than 1 increase saturation and values less than 1 decrease it.
+     * results, default value is 0.85.
+     * @return automatically generated
+     */
+    public static TonemapDrago createTonemapDrago() {
+        return TonemapDrago.__fromPtr__(createTonemapDrago_3());
+    }
+
+
+    //
+    // C++:  Ptr_TonemapMantiuk cv::createTonemapMantiuk(float gamma = 1.0f, float scale = 0.7f, float saturation = 1.0f)
+    //
+
+    /**
+     * Creates TonemapMantiuk object
+     *
+     * @param gamma gamma value for gamma correction. See createTonemap
+     * @param scale contrast scale factor. HVS response is multiplied by this parameter, thus compressing
+     * dynamic range. Values from 0.6 to 0.9 produce best results.
+     * @param saturation saturation enhancement value. See createTonemapDrago
+     * @return automatically generated
+     */
+    public static TonemapMantiuk createTonemapMantiuk(float gamma, float scale, float saturation) {
+        return TonemapMantiuk.__fromPtr__(createTonemapMantiuk_0(gamma, scale, saturation));
+    }
+
+    /**
+     * Creates TonemapMantiuk object
+     *
+     * @param gamma gamma value for gamma correction. See createTonemap
+     * @param scale contrast scale factor. HVS response is multiplied by this parameter, thus compressing
+     * dynamic range. Values from 0.6 to 0.9 produce best results.
+     * @return automatically generated
+     */
+    public static TonemapMantiuk createTonemapMantiuk(float gamma, float scale) {
+        return TonemapMantiuk.__fromPtr__(createTonemapMantiuk_1(gamma, scale));
+    }
+
+    /**
+     * Creates TonemapMantiuk object
+     *
+     * @param gamma gamma value for gamma correction. See createTonemap
+     * dynamic range. Values from 0.6 to 0.9 produce best results.
+     * @return automatically generated
+     */
+    public static TonemapMantiuk createTonemapMantiuk(float gamma) {
+        return TonemapMantiuk.__fromPtr__(createTonemapMantiuk_2(gamma));
+    }
+
+    /**
+     * Creates TonemapMantiuk object
+     *
+     * dynamic range. Values from 0.6 to 0.9 produce best results.
+     * @return automatically generated
+     */
+    public static TonemapMantiuk createTonemapMantiuk() {
+        return TonemapMantiuk.__fromPtr__(createTonemapMantiuk_3());
+    }
+
+
+    //
+    // C++:  Ptr_TonemapReinhard cv::createTonemapReinhard(float gamma = 1.0f, float intensity = 0.0f, float light_adapt = 1.0f, float color_adapt = 0.0f)
+    //
+
+    /**
+     * Creates TonemapReinhard object
+     *
+     * @param gamma gamma value for gamma correction. See createTonemap
+     * @param intensity result intensity in [-8, 8] range. Greater intensity produces brighter results.
+     * @param light_adapt light adaptation in [0, 1] range. If 1 adaptation is based only on pixel
+     * value, if 0 it's global, otherwise it's a weighted mean of this two cases.
+     * @param color_adapt chromatic adaptation in [0, 1] range. If 1 channels are treated independently,
+     * if 0 adaptation level is the same for each channel.
+     * @return automatically generated
+     */
+    public static TonemapReinhard createTonemapReinhard(float gamma, float intensity, float light_adapt, float color_adapt) {
+        return TonemapReinhard.__fromPtr__(createTonemapReinhard_0(gamma, intensity, light_adapt, color_adapt));
+    }
+
+    /**
+     * Creates TonemapReinhard object
+     *
+     * @param gamma gamma value for gamma correction. See createTonemap
+     * @param intensity result intensity in [-8, 8] range. Greater intensity produces brighter results.
+     * @param light_adapt light adaptation in [0, 1] range. If 1 adaptation is based only on pixel
+     * value, if 0 it's global, otherwise it's a weighted mean of this two cases.
+     * if 0 adaptation level is the same for each channel.
+     * @return automatically generated
+     */
+    public static TonemapReinhard createTonemapReinhard(float gamma, float intensity, float light_adapt) {
+        return TonemapReinhard.__fromPtr__(createTonemapReinhard_1(gamma, intensity, light_adapt));
+    }
+
+    /**
+     * Creates TonemapReinhard object
+     *
+     * @param gamma gamma value for gamma correction. See createTonemap
+     * @param intensity result intensity in [-8, 8] range. Greater intensity produces brighter results.
+     * value, if 0 it's global, otherwise it's a weighted mean of this two cases.
+     * if 0 adaptation level is the same for each channel.
+     * @return automatically generated
+     */
+    public static TonemapReinhard createTonemapReinhard(float gamma, float intensity) {
+        return TonemapReinhard.__fromPtr__(createTonemapReinhard_2(gamma, intensity));
+    }
+
+    /**
+     * Creates TonemapReinhard object
+     *
+     * @param gamma gamma value for gamma correction. See createTonemap
+     * value, if 0 it's global, otherwise it's a weighted mean of this two cases.
+     * if 0 adaptation level is the same for each channel.
+     * @return automatically generated
+     */
+    public static TonemapReinhard createTonemapReinhard(float gamma) {
+        return TonemapReinhard.__fromPtr__(createTonemapReinhard_3(gamma));
+    }
+
+    /**
+     * Creates TonemapReinhard object
+     *
+     * value, if 0 it's global, otherwise it's a weighted mean of this two cases.
+     * if 0 adaptation level is the same for each channel.
+     * @return automatically generated
+     */
+    public static TonemapReinhard createTonemapReinhard() {
+        return TonemapReinhard.__fromPtr__(createTonemapReinhard_4());
+    }
+
+
+    //
+    // C++:  void cv::colorChange(Mat src, Mat mask, Mat& dst, float red_mul = 1.0f, float green_mul = 1.0f, float blue_mul = 1.0f)
+    //
+
+    /**
+     * Given an original color image, two differently colored versions of this image can be mixed
+     * seamlessly.
+     *
+     * @param src Input 8-bit 3-channel image.
+     * @param mask Input 8-bit 1 or 3-channel image.
+     * @param dst Output image with the same size and type as src .
+     * @param red_mul R-channel multiply factor.
+     * @param green_mul G-channel multiply factor.
+     * @param blue_mul B-channel multiply factor.
+     *
+     * Multiplication factor is between .5 to 2.5.
+     */
+    public static void colorChange(Mat src, Mat mask, Mat dst, float red_mul, float green_mul, float blue_mul) {
+        colorChange_0(src.nativeObj, mask.nativeObj, dst.nativeObj, red_mul, green_mul, blue_mul);
+    }
+
+    /**
+     * Given an original color image, two differently colored versions of this image can be mixed
+     * seamlessly.
+     *
+     * @param src Input 8-bit 3-channel image.
+     * @param mask Input 8-bit 1 or 3-channel image.
+     * @param dst Output image with the same size and type as src .
+     * @param red_mul R-channel multiply factor.
+     * @param green_mul G-channel multiply factor.
+     *
+     * Multiplication factor is between .5 to 2.5.
+     */
+    public static void colorChange(Mat src, Mat mask, Mat dst, float red_mul, float green_mul) {
+        colorChange_1(src.nativeObj, mask.nativeObj, dst.nativeObj, red_mul, green_mul);
+    }
+
+    /**
+     * Given an original color image, two differently colored versions of this image can be mixed
+     * seamlessly.
+     *
+     * @param src Input 8-bit 3-channel image.
+     * @param mask Input 8-bit 1 or 3-channel image.
+     * @param dst Output image with the same size and type as src .
+     * @param red_mul R-channel multiply factor.
+     *
+     * Multiplication factor is between .5 to 2.5.
+     */
+    public static void colorChange(Mat src, Mat mask, Mat dst, float red_mul) {
+        colorChange_2(src.nativeObj, mask.nativeObj, dst.nativeObj, red_mul);
+    }
+
+    /**
+     * Given an original color image, two differently colored versions of this image can be mixed
+     * seamlessly.
+     *
+     * @param src Input 8-bit 3-channel image.
+     * @param mask Input 8-bit 1 or 3-channel image.
+     * @param dst Output image with the same size and type as src .
+     *
+     * Multiplication factor is between .5 to 2.5.
+     */
+    public static void colorChange(Mat src, Mat mask, Mat dst) {
+        colorChange_3(src.nativeObj, mask.nativeObj, dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::decolor(Mat src, Mat& grayscale, Mat& color_boost)
+    //
+
+    /**
+     * Transforms a color image to a grayscale image. It is a basic tool in digital printing, stylized
+     * black-and-white photograph rendering, and in many single channel image processing applications
+     * CITE: CL12 .
+     *
+     * @param src Input 8-bit 3-channel image.
+     * @param grayscale Output 8-bit 1-channel image.
+     * @param color_boost Output 8-bit 3-channel image.
+     *
+     * This function is to be applied on color images.
+     */
+    public static void decolor(Mat src, Mat grayscale, Mat color_boost) {
+        decolor_0(src.nativeObj, grayscale.nativeObj, color_boost.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::denoise_TVL1(vector_Mat observations, Mat result, double lambda = 1.0, int niters = 30)
+    //
+
+    /**
+     * Primal-dual algorithm is an algorithm for solving special types of variational problems (that is,
+     * finding a function to minimize some functional). As the image denoising, in particular, may be seen
+     * as the variational problem, primal-dual algorithm then can be used to perform denoising and this is
+     * exactly what is implemented.
+     *
+     * It should be noted, that this implementation was taken from the July 2013 blog entry
+     * CITE: MA13 , which also contained (slightly more general) ready-to-use source code on Python.
+     * Subsequently, that code was rewritten on C++ with the usage of openCV by Vadim Pisarevsky at the end
+     * of July 2013 and finally it was slightly adapted by later authors.
+     *
+     * Although the thorough discussion and justification of the algorithm involved may be found in
+     * CITE: ChambolleEtAl, it might make sense to skim over it here, following CITE: MA13 . To begin
+     * with, we consider the 1-byte gray-level images as the functions from the rectangular domain of
+     * pixels (it may be seen as set
+     * \(\left\{(x,y)\in\mathbb{N}\times\mathbb{N}\mid 1\leq x\leq n,\;1\leq y\leq m\right\}\) for some
+     * \(m,\;n\in\mathbb{N}\)) into \(\{0,1,\dots,255\}\). We shall denote the noised images as \(f_i\) and with
+     * this view, given some image \(x\) of the same size, we may measure how bad it is by the formula
+     *
+     * \(\left\|\left\|\nabla x\right\|\right\| + \lambda\sum_i\left\|\left\|x-f_i\right\|\right\|\)
+     *
+     * \(\|\|\cdot\|\|\) here denotes \(L_2\)-norm and as you see, the first addend states that we want our
+     * image to be smooth (ideally, having zero gradient, thus being constant) and the second states that
+     * we want our result to be close to the observations we've got. If we treat \(x\) as a function, this is
+     * exactly the functional what we seek to minimize and here the Primal-Dual algorithm comes into play.
+     *
+     * @param observations This array should contain one or more noised versions of the image that is to
+     * be restored.
+     * @param result Here the denoised image will be stored. There is no need to do pre-allocation of
+     * storage space, as it will be automatically allocated, if necessary.
+     * @param lambda Corresponds to \(\lambda\) in the formulas above. As it is enlarged, the smooth
+     * (blurred) images are treated more favorably than detailed (but maybe more noised) ones. Roughly
+     * speaking, as it becomes smaller, the result will be more blur but more sever outliers will be
+     * removed.
+     * @param niters Number of iterations that the algorithm will run. Of course, as more iterations as
+     * better, but it is hard to quantitatively refine this statement, so just use the default and
+     * increase it if the results are poor.
+     */
+    public static void denoise_TVL1(List<Mat> observations, Mat result, double lambda, int niters) {
+        Mat observations_mat = Converters.vector_Mat_to_Mat(observations);
+        denoise_TVL1_0(observations_mat.nativeObj, result.nativeObj, lambda, niters);
+    }
+
+    /**
+     * Primal-dual algorithm is an algorithm for solving special types of variational problems (that is,
+     * finding a function to minimize some functional). As the image denoising, in particular, may be seen
+     * as the variational problem, primal-dual algorithm then can be used to perform denoising and this is
+     * exactly what is implemented.
+     *
+     * It should be noted, that this implementation was taken from the July 2013 blog entry
+     * CITE: MA13 , which also contained (slightly more general) ready-to-use source code on Python.
+     * Subsequently, that code was rewritten on C++ with the usage of openCV by Vadim Pisarevsky at the end
+     * of July 2013 and finally it was slightly adapted by later authors.
+     *
+     * Although the thorough discussion and justification of the algorithm involved may be found in
+     * CITE: ChambolleEtAl, it might make sense to skim over it here, following CITE: MA13 . To begin
+     * with, we consider the 1-byte gray-level images as the functions from the rectangular domain of
+     * pixels (it may be seen as set
+     * \(\left\{(x,y)\in\mathbb{N}\times\mathbb{N}\mid 1\leq x\leq n,\;1\leq y\leq m\right\}\) for some
+     * \(m,\;n\in\mathbb{N}\)) into \(\{0,1,\dots,255\}\). We shall denote the noised images as \(f_i\) and with
+     * this view, given some image \(x\) of the same size, we may measure how bad it is by the formula
+     *
+     * \(\left\|\left\|\nabla x\right\|\right\| + \lambda\sum_i\left\|\left\|x-f_i\right\|\right\|\)
+     *
+     * \(\|\|\cdot\|\|\) here denotes \(L_2\)-norm and as you see, the first addend states that we want our
+     * image to be smooth (ideally, having zero gradient, thus being constant) and the second states that
+     * we want our result to be close to the observations we've got. If we treat \(x\) as a function, this is
+     * exactly the functional what we seek to minimize and here the Primal-Dual algorithm comes into play.
+     *
+     * @param observations This array should contain one or more noised versions of the image that is to
+     * be restored.
+     * @param result Here the denoised image will be stored. There is no need to do pre-allocation of
+     * storage space, as it will be automatically allocated, if necessary.
+     * @param lambda Corresponds to \(\lambda\) in the formulas above. As it is enlarged, the smooth
+     * (blurred) images are treated more favorably than detailed (but maybe more noised) ones. Roughly
+     * speaking, as it becomes smaller, the result will be more blur but more sever outliers will be
+     * removed.
+     * better, but it is hard to quantitatively refine this statement, so just use the default and
+     * increase it if the results are poor.
+     */
+    public static void denoise_TVL1(List<Mat> observations, Mat result, double lambda) {
+        Mat observations_mat = Converters.vector_Mat_to_Mat(observations);
+        denoise_TVL1_1(observations_mat.nativeObj, result.nativeObj, lambda);
+    }
+
+    /**
+     * Primal-dual algorithm is an algorithm for solving special types of variational problems (that is,
+     * finding a function to minimize some functional). As the image denoising, in particular, may be seen
+     * as the variational problem, primal-dual algorithm then can be used to perform denoising and this is
+     * exactly what is implemented.
+     *
+     * It should be noted, that this implementation was taken from the July 2013 blog entry
+     * CITE: MA13 , which also contained (slightly more general) ready-to-use source code on Python.
+     * Subsequently, that code was rewritten on C++ with the usage of openCV by Vadim Pisarevsky at the end
+     * of July 2013 and finally it was slightly adapted by later authors.
+     *
+     * Although the thorough discussion and justification of the algorithm involved may be found in
+     * CITE: ChambolleEtAl, it might make sense to skim over it here, following CITE: MA13 . To begin
+     * with, we consider the 1-byte gray-level images as the functions from the rectangular domain of
+     * pixels (it may be seen as set
+     * \(\left\{(x,y)\in\mathbb{N}\times\mathbb{N}\mid 1\leq x\leq n,\;1\leq y\leq m\right\}\) for some
+     * \(m,\;n\in\mathbb{N}\)) into \(\{0,1,\dots,255\}\). We shall denote the noised images as \(f_i\) and with
+     * this view, given some image \(x\) of the same size, we may measure how bad it is by the formula
+     *
+     * \(\left\|\left\|\nabla x\right\|\right\| + \lambda\sum_i\left\|\left\|x-f_i\right\|\right\|\)
+     *
+     * \(\|\|\cdot\|\|\) here denotes \(L_2\)-norm and as you see, the first addend states that we want our
+     * image to be smooth (ideally, having zero gradient, thus being constant) and the second states that
+     * we want our result to be close to the observations we've got. If we treat \(x\) as a function, this is
+     * exactly the functional what we seek to minimize and here the Primal-Dual algorithm comes into play.
+     *
+     * @param observations This array should contain one or more noised versions of the image that is to
+     * be restored.
+     * @param result Here the denoised image will be stored. There is no need to do pre-allocation of
+     * storage space, as it will be automatically allocated, if necessary.
+     * (blurred) images are treated more favorably than detailed (but maybe more noised) ones. Roughly
+     * speaking, as it becomes smaller, the result will be more blur but more sever outliers will be
+     * removed.
+     * better, but it is hard to quantitatively refine this statement, so just use the default and
+     * increase it if the results are poor.
+     */
+    public static void denoise_TVL1(List<Mat> observations, Mat result) {
+        Mat observations_mat = Converters.vector_Mat_to_Mat(observations);
+        denoise_TVL1_2(observations_mat.nativeObj, result.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::detailEnhance(Mat src, Mat& dst, float sigma_s = 10, float sigma_r = 0.15f)
+    //
+
+    /**
+     * This filter enhances the details of a particular image.
+     *
+     * @param src Input 8-bit 3-channel image.
+     * @param dst Output image with the same size and type as src.
+     * @param sigma_s %Range between 0 to 200.
+     * @param sigma_r %Range between 0 to 1.
+     */
+    public static void detailEnhance(Mat src, Mat dst, float sigma_s, float sigma_r) {
+        detailEnhance_0(src.nativeObj, dst.nativeObj, sigma_s, sigma_r);
+    }
+
+    /**
+     * This filter enhances the details of a particular image.
+     *
+     * @param src Input 8-bit 3-channel image.
+     * @param dst Output image with the same size and type as src.
+     * @param sigma_s %Range between 0 to 200.
+     */
+    public static void detailEnhance(Mat src, Mat dst, float sigma_s) {
+        detailEnhance_1(src.nativeObj, dst.nativeObj, sigma_s);
+    }
+
+    /**
+     * This filter enhances the details of a particular image.
+     *
+     * @param src Input 8-bit 3-channel image.
+     * @param dst Output image with the same size and type as src.
+     */
+    public static void detailEnhance(Mat src, Mat dst) {
+        detailEnhance_2(src.nativeObj, dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::edgePreservingFilter(Mat src, Mat& dst, int flags = 1, float sigma_s = 60, float sigma_r = 0.4f)
+    //
+
+    /**
+     * Filtering is the fundamental operation in image and video processing. Edge-preserving smoothing
+     * filters are used in many different applications CITE: EM11 .
+     *
+     * @param src Input 8-bit 3-channel image.
+     * @param dst Output 8-bit 3-channel image.
+     * @param flags Edge preserving filters: cv::RECURS_FILTER or cv::NORMCONV_FILTER
+     * @param sigma_s %Range between 0 to 200.
+     * @param sigma_r %Range between 0 to 1.
+     */
+    public static void edgePreservingFilter(Mat src, Mat dst, int flags, float sigma_s, float sigma_r) {
+        edgePreservingFilter_0(src.nativeObj, dst.nativeObj, flags, sigma_s, sigma_r);
+    }
+
+    /**
+     * Filtering is the fundamental operation in image and video processing. Edge-preserving smoothing
+     * filters are used in many different applications CITE: EM11 .
+     *
+     * @param src Input 8-bit 3-channel image.
+     * @param dst Output 8-bit 3-channel image.
+     * @param flags Edge preserving filters: cv::RECURS_FILTER or cv::NORMCONV_FILTER
+     * @param sigma_s %Range between 0 to 200.
+     */
+    public static void edgePreservingFilter(Mat src, Mat dst, int flags, float sigma_s) {
+        edgePreservingFilter_1(src.nativeObj, dst.nativeObj, flags, sigma_s);
+    }
+
+    /**
+     * Filtering is the fundamental operation in image and video processing. Edge-preserving smoothing
+     * filters are used in many different applications CITE: EM11 .
+     *
+     * @param src Input 8-bit 3-channel image.
+     * @param dst Output 8-bit 3-channel image.
+     * @param flags Edge preserving filters: cv::RECURS_FILTER or cv::NORMCONV_FILTER
+     */
+    public static void edgePreservingFilter(Mat src, Mat dst, int flags) {
+        edgePreservingFilter_2(src.nativeObj, dst.nativeObj, flags);
+    }
+
+    /**
+     * Filtering is the fundamental operation in image and video processing. Edge-preserving smoothing
+     * filters are used in many different applications CITE: EM11 .
+     *
+     * @param src Input 8-bit 3-channel image.
+     * @param dst Output 8-bit 3-channel image.
+     */
+    public static void edgePreservingFilter(Mat src, Mat dst) {
+        edgePreservingFilter_3(src.nativeObj, dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::fastNlMeansDenoising(Mat src, Mat& dst, float h = 3, int templateWindowSize = 7, int searchWindowSize = 21)
+    //
+
+    /**
+     * Perform image denoising using Non-local Means Denoising algorithm
+     * &lt;http://www.ipol.im/pub/algo/bcm_non_local_means_denoising/&gt; with several computational
+     * optimizations. Noise expected to be a gaussian white noise
+     *
+     * @param src Input 8-bit 1-channel, 2-channel, 3-channel or 4-channel image.
+     * @param dst Output image with the same size and type as src .
+     * @param templateWindowSize Size in pixels of the template patch that is used to compute weights.
+     * Should be odd. Recommended value 7 pixels
+     * @param searchWindowSize Size in pixels of the window that is used to compute weighted average for
+     * given pixel. Should be odd. Affect performance linearly: greater searchWindowsSize - greater
+     * denoising time. Recommended value 21 pixels
+     * @param h Parameter regulating filter strength. Big h value perfectly removes noise but also
+     * removes image details, smaller h value preserves details but also preserves some noise
+     *
+     * This function expected to be applied to grayscale images. For colored images look at
+     * fastNlMeansDenoisingColored. Advanced usage of this functions can be manual denoising of colored
+     * image in different colorspaces. Such approach is used in fastNlMeansDenoisingColored by converting
+     * image to CIELAB colorspace and then separately denoise L and AB components with different h
+     * parameter.
+     */
+    public static void fastNlMeansDenoising(Mat src, Mat dst, float h, int templateWindowSize, int searchWindowSize) {
+        fastNlMeansDenoising_0(src.nativeObj, dst.nativeObj, h, templateWindowSize, searchWindowSize);
+    }
+
+    /**
+     * Perform image denoising using Non-local Means Denoising algorithm
+     * &lt;http://www.ipol.im/pub/algo/bcm_non_local_means_denoising/&gt; with several computational
+     * optimizations. Noise expected to be a gaussian white noise
+     *
+     * @param src Input 8-bit 1-channel, 2-channel, 3-channel or 4-channel image.
+     * @param dst Output image with the same size and type as src .
+     * @param templateWindowSize Size in pixels of the template patch that is used to compute weights.
+     * Should be odd. Recommended value 7 pixels
+     * given pixel. Should be odd. Affect performance linearly: greater searchWindowsSize - greater
+     * denoising time. Recommended value 21 pixels
+     * @param h Parameter regulating filter strength. Big h value perfectly removes noise but also
+     * removes image details, smaller h value preserves details but also preserves some noise
+     *
+     * This function expected to be applied to grayscale images. For colored images look at
+     * fastNlMeansDenoisingColored. Advanced usage of this functions can be manual denoising of colored
+     * image in different colorspaces. Such approach is used in fastNlMeansDenoisingColored by converting
+     * image to CIELAB colorspace and then separately denoise L and AB components with different h
+     * parameter.
+     */
+    public static void fastNlMeansDenoising(Mat src, Mat dst, float h, int templateWindowSize) {
+        fastNlMeansDenoising_1(src.nativeObj, dst.nativeObj, h, templateWindowSize);
+    }
+
+    /**
+     * Perform image denoising using Non-local Means Denoising algorithm
+     * &lt;http://www.ipol.im/pub/algo/bcm_non_local_means_denoising/&gt; with several computational
+     * optimizations. Noise expected to be a gaussian white noise
+     *
+     * @param src Input 8-bit 1-channel, 2-channel, 3-channel or 4-channel image.
+     * @param dst Output image with the same size and type as src .
+     * Should be odd. Recommended value 7 pixels
+     * given pixel. Should be odd. Affect performance linearly: greater searchWindowsSize - greater
+     * denoising time. Recommended value 21 pixels
+     * @param h Parameter regulating filter strength. Big h value perfectly removes noise but also
+     * removes image details, smaller h value preserves details but also preserves some noise
+     *
+     * This function expected to be applied to grayscale images. For colored images look at
+     * fastNlMeansDenoisingColored. Advanced usage of this functions can be manual denoising of colored
+     * image in different colorspaces. Such approach is used in fastNlMeansDenoisingColored by converting
+     * image to CIELAB colorspace and then separately denoise L and AB components with different h
+     * parameter.
+     */
+    public static void fastNlMeansDenoising(Mat src, Mat dst, float h) {
+        fastNlMeansDenoising_2(src.nativeObj, dst.nativeObj, h);
+    }
+
+    /**
+     * Perform image denoising using Non-local Means Denoising algorithm
+     * &lt;http://www.ipol.im/pub/algo/bcm_non_local_means_denoising/&gt; with several computational
+     * optimizations. Noise expected to be a gaussian white noise
+     *
+     * @param src Input 8-bit 1-channel, 2-channel, 3-channel or 4-channel image.
+     * @param dst Output image with the same size and type as src .
+     * Should be odd. Recommended value 7 pixels
+     * given pixel. Should be odd. Affect performance linearly: greater searchWindowsSize - greater
+     * denoising time. Recommended value 21 pixels
+     * removes image details, smaller h value preserves details but also preserves some noise
+     *
+     * This function expected to be applied to grayscale images. For colored images look at
+     * fastNlMeansDenoisingColored. Advanced usage of this functions can be manual denoising of colored
+     * image in different colorspaces. Such approach is used in fastNlMeansDenoisingColored by converting
+     * image to CIELAB colorspace and then separately denoise L and AB components with different h
+     * parameter.
+     */
+    public static void fastNlMeansDenoising(Mat src, Mat dst) {
+        fastNlMeansDenoising_3(src.nativeObj, dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::fastNlMeansDenoising(Mat src, Mat& dst, vector_float h, int templateWindowSize = 7, int searchWindowSize = 21, int normType = NORM_L2)
+    //
+
+    /**
+     * Perform image denoising using Non-local Means Denoising algorithm
+     * &lt;http://www.ipol.im/pub/algo/bcm_non_local_means_denoising/&gt; with several computational
+     * optimizations. Noise expected to be a gaussian white noise
+     *
+     * @param src Input 8-bit or 16-bit (only with NORM_L1) 1-channel,
+     * 2-channel, 3-channel or 4-channel image.
+     * @param dst Output image with the same size and type as src .
+     * @param templateWindowSize Size in pixels of the template patch that is used to compute weights.
+     * Should be odd. Recommended value 7 pixels
+     * @param searchWindowSize Size in pixels of the window that is used to compute weighted average for
+     * given pixel. Should be odd. Affect performance linearly: greater searchWindowsSize - greater
+     * denoising time. Recommended value 21 pixels
+     * @param h Array of parameters regulating filter strength, either one
+     * parameter applied to all channels or one per channel in dst. Big h value
+     * perfectly removes noise but also removes image details, smaller h
+     * value preserves details but also preserves some noise
+     * @param normType Type of norm used for weight calculation. Can be either NORM_L2 or NORM_L1
+     *
+     * This function expected to be applied to grayscale images. For colored images look at
+     * fastNlMeansDenoisingColored. Advanced usage of this functions can be manual denoising of colored
+     * image in different colorspaces. Such approach is used in fastNlMeansDenoisingColored by converting
+     * image to CIELAB colorspace and then separately denoise L and AB components with different h
+     * parameter.
+     */
+    public static void fastNlMeansDenoising(Mat src, Mat dst, MatOfFloat h, int templateWindowSize, int searchWindowSize, int normType) {
+        Mat h_mat = h;
+        fastNlMeansDenoising_4(src.nativeObj, dst.nativeObj, h_mat.nativeObj, templateWindowSize, searchWindowSize, normType);
+    }
+
+    /**
+     * Perform image denoising using Non-local Means Denoising algorithm
+     * &lt;http://www.ipol.im/pub/algo/bcm_non_local_means_denoising/&gt; with several computational
+     * optimizations. Noise expected to be a gaussian white noise
+     *
+     * @param src Input 8-bit or 16-bit (only with NORM_L1) 1-channel,
+     * 2-channel, 3-channel or 4-channel image.
+     * @param dst Output image with the same size and type as src .
+     * @param templateWindowSize Size in pixels of the template patch that is used to compute weights.
+     * Should be odd. Recommended value 7 pixels
+     * @param searchWindowSize Size in pixels of the window that is used to compute weighted average for
+     * given pixel. Should be odd. Affect performance linearly: greater searchWindowsSize - greater
+     * denoising time. Recommended value 21 pixels
+     * @param h Array of parameters regulating filter strength, either one
+     * parameter applied to all channels or one per channel in dst. Big h value
+     * perfectly removes noise but also removes image details, smaller h
+     * value preserves details but also preserves some noise
+     *
+     * This function expected to be applied to grayscale images. For colored images look at
+     * fastNlMeansDenoisingColored. Advanced usage of this functions can be manual denoising of colored
+     * image in different colorspaces. Such approach is used in fastNlMeansDenoisingColored by converting
+     * image to CIELAB colorspace and then separately denoise L and AB components with different h
+     * parameter.
+     */
+    public static void fastNlMeansDenoising(Mat src, Mat dst, MatOfFloat h, int templateWindowSize, int searchWindowSize) {
+        Mat h_mat = h;
+        fastNlMeansDenoising_5(src.nativeObj, dst.nativeObj, h_mat.nativeObj, templateWindowSize, searchWindowSize);
+    }
+
+    /**
+     * Perform image denoising using Non-local Means Denoising algorithm
+     * &lt;http://www.ipol.im/pub/algo/bcm_non_local_means_denoising/&gt; with several computational
+     * optimizations. Noise expected to be a gaussian white noise
+     *
+     * @param src Input 8-bit or 16-bit (only with NORM_L1) 1-channel,
+     * 2-channel, 3-channel or 4-channel image.
+     * @param dst Output image with the same size and type as src .
+     * @param templateWindowSize Size in pixels of the template patch that is used to compute weights.
+     * Should be odd. Recommended value 7 pixels
+     * given pixel. Should be odd. Affect performance linearly: greater searchWindowsSize - greater
+     * denoising time. Recommended value 21 pixels
+     * @param h Array of parameters regulating filter strength, either one
+     * parameter applied to all channels or one per channel in dst. Big h value
+     * perfectly removes noise but also removes image details, smaller h
+     * value preserves details but also preserves some noise
+     *
+     * This function expected to be applied to grayscale images. For colored images look at
+     * fastNlMeansDenoisingColored. Advanced usage of this functions can be manual denoising of colored
+     * image in different colorspaces. Such approach is used in fastNlMeansDenoisingColored by converting
+     * image to CIELAB colorspace and then separately denoise L and AB components with different h
+     * parameter.
+     */
+    public static void fastNlMeansDenoising(Mat src, Mat dst, MatOfFloat h, int templateWindowSize) {
+        Mat h_mat = h;
+        fastNlMeansDenoising_6(src.nativeObj, dst.nativeObj, h_mat.nativeObj, templateWindowSize);
+    }
+
+    /**
+     * Perform image denoising using Non-local Means Denoising algorithm
+     * &lt;http://www.ipol.im/pub/algo/bcm_non_local_means_denoising/&gt; with several computational
+     * optimizations. Noise expected to be a gaussian white noise
+     *
+     * @param src Input 8-bit or 16-bit (only with NORM_L1) 1-channel,
+     * 2-channel, 3-channel or 4-channel image.
+     * @param dst Output image with the same size and type as src .
+     * Should be odd. Recommended value 7 pixels
+     * given pixel. Should be odd. Affect performance linearly: greater searchWindowsSize - greater
+     * denoising time. Recommended value 21 pixels
+     * @param h Array of parameters regulating filter strength, either one
+     * parameter applied to all channels or one per channel in dst. Big h value
+     * perfectly removes noise but also removes image details, smaller h
+     * value preserves details but also preserves some noise
+     *
+     * This function expected to be applied to grayscale images. For colored images look at
+     * fastNlMeansDenoisingColored. Advanced usage of this functions can be manual denoising of colored
+     * image in different colorspaces. Such approach is used in fastNlMeansDenoisingColored by converting
+     * image to CIELAB colorspace and then separately denoise L and AB components with different h
+     * parameter.
+     */
+    public static void fastNlMeansDenoising(Mat src, Mat dst, MatOfFloat h) {
+        Mat h_mat = h;
+        fastNlMeansDenoising_7(src.nativeObj, dst.nativeObj, h_mat.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::fastNlMeansDenoisingColored(Mat src, Mat& dst, float h = 3, float hColor = 3, int templateWindowSize = 7, int searchWindowSize = 21)
+    //
+
+    /**
+     * Modification of fastNlMeansDenoising function for colored images
+     *
+     * @param src Input 8-bit 3-channel image.
+     * @param dst Output image with the same size and type as src .
+     * @param templateWindowSize Size in pixels of the template patch that is used to compute weights.
+     * Should be odd. Recommended value 7 pixels
+     * @param searchWindowSize Size in pixels of the window that is used to compute weighted average for
+     * given pixel. Should be odd. Affect performance linearly: greater searchWindowsSize - greater
+     * denoising time. Recommended value 21 pixels
+     * @param h Parameter regulating filter strength for luminance component. Bigger h value perfectly
+     * removes noise but also removes image details, smaller h value preserves details but also preserves
+     * some noise
+     * @param hColor The same as h but for color components. For most images value equals 10
+     * will be enough to remove colored noise and do not distort colors
+     *
+     * The function converts image to CIELAB colorspace and then separately denoise L and AB components
+     * with given h parameters using fastNlMeansDenoising function.
+     */
+    public static void fastNlMeansDenoisingColored(Mat src, Mat dst, float h, float hColor, int templateWindowSize, int searchWindowSize) {
+        fastNlMeansDenoisingColored_0(src.nativeObj, dst.nativeObj, h, hColor, templateWindowSize, searchWindowSize);
+    }
+
+    /**
+     * Modification of fastNlMeansDenoising function for colored images
+     *
+     * @param src Input 8-bit 3-channel image.
+     * @param dst Output image with the same size and type as src .
+     * @param templateWindowSize Size in pixels of the template patch that is used to compute weights.
+     * Should be odd. Recommended value 7 pixels
+     * given pixel. Should be odd. Affect performance linearly: greater searchWindowsSize - greater
+     * denoising time. Recommended value 21 pixels
+     * @param h Parameter regulating filter strength for luminance component. Bigger h value perfectly
+     * removes noise but also removes image details, smaller h value preserves details but also preserves
+     * some noise
+     * @param hColor The same as h but for color components. For most images value equals 10
+     * will be enough to remove colored noise and do not distort colors
+     *
+     * The function converts image to CIELAB colorspace and then separately denoise L and AB components
+     * with given h parameters using fastNlMeansDenoising function.
+     */
+    public static void fastNlMeansDenoisingColored(Mat src, Mat dst, float h, float hColor, int templateWindowSize) {
+        fastNlMeansDenoisingColored_1(src.nativeObj, dst.nativeObj, h, hColor, templateWindowSize);
+    }
+
+    /**
+     * Modification of fastNlMeansDenoising function for colored images
+     *
+     * @param src Input 8-bit 3-channel image.
+     * @param dst Output image with the same size and type as src .
+     * Should be odd. Recommended value 7 pixels
+     * given pixel. Should be odd. Affect performance linearly: greater searchWindowsSize - greater
+     * denoising time. Recommended value 21 pixels
+     * @param h Parameter regulating filter strength for luminance component. Bigger h value perfectly
+     * removes noise but also removes image details, smaller h value preserves details but also preserves
+     * some noise
+     * @param hColor The same as h but for color components. For most images value equals 10
+     * will be enough to remove colored noise and do not distort colors
+     *
+     * The function converts image to CIELAB colorspace and then separately denoise L and AB components
+     * with given h parameters using fastNlMeansDenoising function.
+     */
+    public static void fastNlMeansDenoisingColored(Mat src, Mat dst, float h, float hColor) {
+        fastNlMeansDenoisingColored_2(src.nativeObj, dst.nativeObj, h, hColor);
+    }
+
+    /**
+     * Modification of fastNlMeansDenoising function for colored images
+     *
+     * @param src Input 8-bit 3-channel image.
+     * @param dst Output image with the same size and type as src .
+     * Should be odd. Recommended value 7 pixels
+     * given pixel. Should be odd. Affect performance linearly: greater searchWindowsSize - greater
+     * denoising time. Recommended value 21 pixels
+     * @param h Parameter regulating filter strength for luminance component. Bigger h value perfectly
+     * removes noise but also removes image details, smaller h value preserves details but also preserves
+     * some noise
+     * will be enough to remove colored noise and do not distort colors
+     *
+     * The function converts image to CIELAB colorspace and then separately denoise L and AB components
+     * with given h parameters using fastNlMeansDenoising function.
+     */
+    public static void fastNlMeansDenoisingColored(Mat src, Mat dst, float h) {
+        fastNlMeansDenoisingColored_3(src.nativeObj, dst.nativeObj, h);
+    }
+
+    /**
+     * Modification of fastNlMeansDenoising function for colored images
+     *
+     * @param src Input 8-bit 3-channel image.
+     * @param dst Output image with the same size and type as src .
+     * Should be odd. Recommended value 7 pixels
+     * given pixel. Should be odd. Affect performance linearly: greater searchWindowsSize - greater
+     * denoising time. Recommended value 21 pixels
+     * removes noise but also removes image details, smaller h value preserves details but also preserves
+     * some noise
+     * will be enough to remove colored noise and do not distort colors
+     *
+     * The function converts image to CIELAB colorspace and then separately denoise L and AB components
+     * with given h parameters using fastNlMeansDenoising function.
+     */
+    public static void fastNlMeansDenoisingColored(Mat src, Mat dst) {
+        fastNlMeansDenoisingColored_4(src.nativeObj, dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::fastNlMeansDenoisingColoredMulti(vector_Mat srcImgs, Mat& dst, int imgToDenoiseIndex, int temporalWindowSize, float h = 3, float hColor = 3, int templateWindowSize = 7, int searchWindowSize = 21)
+    //
+
+    /**
+     * Modification of fastNlMeansDenoisingMulti function for colored images sequences
+     *
+     * @param srcImgs Input 8-bit 3-channel images sequence. All images should have the same type and
+     * size.
+     * @param imgToDenoiseIndex Target image to denoise index in srcImgs sequence
+     * @param temporalWindowSize Number of surrounding images to use for target image denoising. Should
+     * be odd. Images from imgToDenoiseIndex - temporalWindowSize / 2 to
+     * imgToDenoiseIndex - temporalWindowSize / 2 from srcImgs will be used to denoise
+     * srcImgs[imgToDenoiseIndex] image.
+     * @param dst Output image with the same size and type as srcImgs images.
+     * @param templateWindowSize Size in pixels of the template patch that is used to compute weights.
+     * Should be odd. Recommended value 7 pixels
+     * @param searchWindowSize Size in pixels of the window that is used to compute weighted average for
+     * given pixel. Should be odd. Affect performance linearly: greater searchWindowsSize - greater
+     * denoising time. Recommended value 21 pixels
+     * @param h Parameter regulating filter strength for luminance component. Bigger h value perfectly
+     * removes noise but also removes image details, smaller h value preserves details but also preserves
+     * some noise.
+     * @param hColor The same as h but for color components.
+     *
+     * The function converts images to CIELAB colorspace and then separately denoise L and AB components
+     * with given h parameters using fastNlMeansDenoisingMulti function.
+     */
+    public static void fastNlMeansDenoisingColoredMulti(List<Mat> srcImgs, Mat dst, int imgToDenoiseIndex, int temporalWindowSize, float h, float hColor, int templateWindowSize, int searchWindowSize) {
+        Mat srcImgs_mat = Converters.vector_Mat_to_Mat(srcImgs);
+        fastNlMeansDenoisingColoredMulti_0(srcImgs_mat.nativeObj, dst.nativeObj, imgToDenoiseIndex, temporalWindowSize, h, hColor, templateWindowSize, searchWindowSize);
+    }
+
+    /**
+     * Modification of fastNlMeansDenoisingMulti function for colored images sequences
+     *
+     * @param srcImgs Input 8-bit 3-channel images sequence. All images should have the same type and
+     * size.
+     * @param imgToDenoiseIndex Target image to denoise index in srcImgs sequence
+     * @param temporalWindowSize Number of surrounding images to use for target image denoising. Should
+     * be odd. Images from imgToDenoiseIndex - temporalWindowSize / 2 to
+     * imgToDenoiseIndex - temporalWindowSize / 2 from srcImgs will be used to denoise
+     * srcImgs[imgToDenoiseIndex] image.
+     * @param dst Output image with the same size and type as srcImgs images.
+     * @param templateWindowSize Size in pixels of the template patch that is used to compute weights.
+     * Should be odd. Recommended value 7 pixels
+     * given pixel. Should be odd. Affect performance linearly: greater searchWindowsSize - greater
+     * denoising time. Recommended value 21 pixels
+     * @param h Parameter regulating filter strength for luminance component. Bigger h value perfectly
+     * removes noise but also removes image details, smaller h value preserves details but also preserves
+     * some noise.
+     * @param hColor The same as h but for color components.
+     *
+     * The function converts images to CIELAB colorspace and then separately denoise L and AB components
+     * with given h parameters using fastNlMeansDenoisingMulti function.
+     */
+    public static void fastNlMeansDenoisingColoredMulti(List<Mat> srcImgs, Mat dst, int imgToDenoiseIndex, int temporalWindowSize, float h, float hColor, int templateWindowSize) {
+        Mat srcImgs_mat = Converters.vector_Mat_to_Mat(srcImgs);
+        fastNlMeansDenoisingColoredMulti_1(srcImgs_mat.nativeObj, dst.nativeObj, imgToDenoiseIndex, temporalWindowSize, h, hColor, templateWindowSize);
+    }
+
+    /**
+     * Modification of fastNlMeansDenoisingMulti function for colored images sequences
+     *
+     * @param srcImgs Input 8-bit 3-channel images sequence. All images should have the same type and
+     * size.
+     * @param imgToDenoiseIndex Target image to denoise index in srcImgs sequence
+     * @param temporalWindowSize Number of surrounding images to use for target image denoising. Should
+     * be odd. Images from imgToDenoiseIndex - temporalWindowSize / 2 to
+     * imgToDenoiseIndex - temporalWindowSize / 2 from srcImgs will be used to denoise
+     * srcImgs[imgToDenoiseIndex] image.
+     * @param dst Output image with the same size and type as srcImgs images.
+     * Should be odd. Recommended value 7 pixels
+     * given pixel. Should be odd. Affect performance linearly: greater searchWindowsSize - greater
+     * denoising time. Recommended value 21 pixels
+     * @param h Parameter regulating filter strength for luminance component. Bigger h value perfectly
+     * removes noise but also removes image details, smaller h value preserves details but also preserves
+     * some noise.
+     * @param hColor The same as h but for color components.
+     *
+     * The function converts images to CIELAB colorspace and then separately denoise L and AB components
+     * with given h parameters using fastNlMeansDenoisingMulti function.
+     */
+    public static void fastNlMeansDenoisingColoredMulti(List<Mat> srcImgs, Mat dst, int imgToDenoiseIndex, int temporalWindowSize, float h, float hColor) {
+        Mat srcImgs_mat = Converters.vector_Mat_to_Mat(srcImgs);
+        fastNlMeansDenoisingColoredMulti_2(srcImgs_mat.nativeObj, dst.nativeObj, imgToDenoiseIndex, temporalWindowSize, h, hColor);
+    }
+
+    /**
+     * Modification of fastNlMeansDenoisingMulti function for colored images sequences
+     *
+     * @param srcImgs Input 8-bit 3-channel images sequence. All images should have the same type and
+     * size.
+     * @param imgToDenoiseIndex Target image to denoise index in srcImgs sequence
+     * @param temporalWindowSize Number of surrounding images to use for target image denoising. Should
+     * be odd. Images from imgToDenoiseIndex - temporalWindowSize / 2 to
+     * imgToDenoiseIndex - temporalWindowSize / 2 from srcImgs will be used to denoise
+     * srcImgs[imgToDenoiseIndex] image.
+     * @param dst Output image with the same size and type as srcImgs images.
+     * Should be odd. Recommended value 7 pixels
+     * given pixel. Should be odd. Affect performance linearly: greater searchWindowsSize - greater
+     * denoising time. Recommended value 21 pixels
+     * @param h Parameter regulating filter strength for luminance component. Bigger h value perfectly
+     * removes noise but also removes image details, smaller h value preserves details but also preserves
+     * some noise.
+     *
+     * The function converts images to CIELAB colorspace and then separately denoise L and AB components
+     * with given h parameters using fastNlMeansDenoisingMulti function.
+     */
+    public static void fastNlMeansDenoisingColoredMulti(List<Mat> srcImgs, Mat dst, int imgToDenoiseIndex, int temporalWindowSize, float h) {
+        Mat srcImgs_mat = Converters.vector_Mat_to_Mat(srcImgs);
+        fastNlMeansDenoisingColoredMulti_3(srcImgs_mat.nativeObj, dst.nativeObj, imgToDenoiseIndex, temporalWindowSize, h);
+    }
+
+    /**
+     * Modification of fastNlMeansDenoisingMulti function for colored images sequences
+     *
+     * @param srcImgs Input 8-bit 3-channel images sequence. All images should have the same type and
+     * size.
+     * @param imgToDenoiseIndex Target image to denoise index in srcImgs sequence
+     * @param temporalWindowSize Number of surrounding images to use for target image denoising. Should
+     * be odd. Images from imgToDenoiseIndex - temporalWindowSize / 2 to
+     * imgToDenoiseIndex - temporalWindowSize / 2 from srcImgs will be used to denoise
+     * srcImgs[imgToDenoiseIndex] image.
+     * @param dst Output image with the same size and type as srcImgs images.
+     * Should be odd. Recommended value 7 pixels
+     * given pixel. Should be odd. Affect performance linearly: greater searchWindowsSize - greater
+     * denoising time. Recommended value 21 pixels
+     * removes noise but also removes image details, smaller h value preserves details but also preserves
+     * some noise.
+     *
+     * The function converts images to CIELAB colorspace and then separately denoise L and AB components
+     * with given h parameters using fastNlMeansDenoisingMulti function.
+     */
+    public static void fastNlMeansDenoisingColoredMulti(List<Mat> srcImgs, Mat dst, int imgToDenoiseIndex, int temporalWindowSize) {
+        Mat srcImgs_mat = Converters.vector_Mat_to_Mat(srcImgs);
+        fastNlMeansDenoisingColoredMulti_4(srcImgs_mat.nativeObj, dst.nativeObj, imgToDenoiseIndex, temporalWindowSize);
+    }
+
+
+    //
+    // C++:  void cv::fastNlMeansDenoisingMulti(vector_Mat srcImgs, Mat& dst, int imgToDenoiseIndex, int temporalWindowSize, float h = 3, int templateWindowSize = 7, int searchWindowSize = 21)
+    //
+
+    /**
+     * Modification of fastNlMeansDenoising function for images sequence where consecutive images have been
+     * captured in small period of time. For example video. This version of the function is for grayscale
+     * images or for manual manipulation with colorspaces. For more details see
+     * &lt;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.131.6394&gt;
+     *
+     * @param srcImgs Input 8-bit 1-channel, 2-channel, 3-channel or
+     * 4-channel images sequence. All images should have the same type and
+     * size.
+     * @param imgToDenoiseIndex Target image to denoise index in srcImgs sequence
+     * @param temporalWindowSize Number of surrounding images to use for target image denoising. Should
+     * be odd. Images from imgToDenoiseIndex - temporalWindowSize / 2 to
+     * imgToDenoiseIndex - temporalWindowSize / 2 from srcImgs will be used to denoise
+     * srcImgs[imgToDenoiseIndex] image.
+     * @param dst Output image with the same size and type as srcImgs images.
+     * @param templateWindowSize Size in pixels of the template patch that is used to compute weights.
+     * Should be odd. Recommended value 7 pixels
+     * @param searchWindowSize Size in pixels of the window that is used to compute weighted average for
+     * given pixel. Should be odd. Affect performance linearly: greater searchWindowsSize - greater
+     * denoising time. Recommended value 21 pixels
+     * @param h Parameter regulating filter strength. Bigger h value
+     * perfectly removes noise but also removes image details, smaller h
+     * value preserves details but also preserves some noise
+     */
+    public static void fastNlMeansDenoisingMulti(List<Mat> srcImgs, Mat dst, int imgToDenoiseIndex, int temporalWindowSize, float h, int templateWindowSize, int searchWindowSize) {
+        Mat srcImgs_mat = Converters.vector_Mat_to_Mat(srcImgs);
+        fastNlMeansDenoisingMulti_0(srcImgs_mat.nativeObj, dst.nativeObj, imgToDenoiseIndex, temporalWindowSize, h, templateWindowSize, searchWindowSize);
+    }
+
+    /**
+     * Modification of fastNlMeansDenoising function for images sequence where consecutive images have been
+     * captured in small period of time. For example video. This version of the function is for grayscale
+     * images or for manual manipulation with colorspaces. For more details see
+     * &lt;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.131.6394&gt;
+     *
+     * @param srcImgs Input 8-bit 1-channel, 2-channel, 3-channel or
+     * 4-channel images sequence. All images should have the same type and
+     * size.
+     * @param imgToDenoiseIndex Target image to denoise index in srcImgs sequence
+     * @param temporalWindowSize Number of surrounding images to use for target image denoising. Should
+     * be odd. Images from imgToDenoiseIndex - temporalWindowSize / 2 to
+     * imgToDenoiseIndex - temporalWindowSize / 2 from srcImgs will be used to denoise
+     * srcImgs[imgToDenoiseIndex] image.
+     * @param dst Output image with the same size and type as srcImgs images.
+     * @param templateWindowSize Size in pixels of the template patch that is used to compute weights.
+     * Should be odd. Recommended value 7 pixels
+     * given pixel. Should be odd. Affect performance linearly: greater searchWindowsSize - greater
+     * denoising time. Recommended value 21 pixels
+     * @param h Parameter regulating filter strength. Bigger h value
+     * perfectly removes noise but also removes image details, smaller h
+     * value preserves details but also preserves some noise
+     */
+    public static void fastNlMeansDenoisingMulti(List<Mat> srcImgs, Mat dst, int imgToDenoiseIndex, int temporalWindowSize, float h, int templateWindowSize) {
+        Mat srcImgs_mat = Converters.vector_Mat_to_Mat(srcImgs);
+        fastNlMeansDenoisingMulti_1(srcImgs_mat.nativeObj, dst.nativeObj, imgToDenoiseIndex, temporalWindowSize, h, templateWindowSize);
+    }
+
+    /**
+     * Modification of fastNlMeansDenoising function for images sequence where consecutive images have been
+     * captured in small period of time. For example video. This version of the function is for grayscale
+     * images or for manual manipulation with colorspaces. For more details see
+     * &lt;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.131.6394&gt;
+     *
+     * @param srcImgs Input 8-bit 1-channel, 2-channel, 3-channel or
+     * 4-channel images sequence. All images should have the same type and
+     * size.
+     * @param imgToDenoiseIndex Target image to denoise index in srcImgs sequence
+     * @param temporalWindowSize Number of surrounding images to use for target image denoising. Should
+     * be odd. Images from imgToDenoiseIndex - temporalWindowSize / 2 to
+     * imgToDenoiseIndex - temporalWindowSize / 2 from srcImgs will be used to denoise
+     * srcImgs[imgToDenoiseIndex] image.
+     * @param dst Output image with the same size and type as srcImgs images.
+     * Should be odd. Recommended value 7 pixels
+     * given pixel. Should be odd. Affect performance linearly: greater searchWindowsSize - greater
+     * denoising time. Recommended value 21 pixels
+     * @param h Parameter regulating filter strength. Bigger h value
+     * perfectly removes noise but also removes image details, smaller h
+     * value preserves details but also preserves some noise
+     */
+    public static void fastNlMeansDenoisingMulti(List<Mat> srcImgs, Mat dst, int imgToDenoiseIndex, int temporalWindowSize, float h) {
+        Mat srcImgs_mat = Converters.vector_Mat_to_Mat(srcImgs);
+        fastNlMeansDenoisingMulti_2(srcImgs_mat.nativeObj, dst.nativeObj, imgToDenoiseIndex, temporalWindowSize, h);
+    }
+
+    /**
+     * Modification of fastNlMeansDenoising function for images sequence where consecutive images have been
+     * captured in small period of time. For example video. This version of the function is for grayscale
+     * images or for manual manipulation with colorspaces. For more details see
+     * &lt;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.131.6394&gt;
+     *
+     * @param srcImgs Input 8-bit 1-channel, 2-channel, 3-channel or
+     * 4-channel images sequence. All images should have the same type and
+     * size.
+     * @param imgToDenoiseIndex Target image to denoise index in srcImgs sequence
+     * @param temporalWindowSize Number of surrounding images to use for target image denoising. Should
+     * be odd. Images from imgToDenoiseIndex - temporalWindowSize / 2 to
+     * imgToDenoiseIndex - temporalWindowSize / 2 from srcImgs will be used to denoise
+     * srcImgs[imgToDenoiseIndex] image.
+     * @param dst Output image with the same size and type as srcImgs images.
+     * Should be odd. Recommended value 7 pixels
+     * given pixel. Should be odd. Affect performance linearly: greater searchWindowsSize - greater
+     * denoising time. Recommended value 21 pixels
+     * perfectly removes noise but also removes image details, smaller h
+     * value preserves details but also preserves some noise
+     */
+    public static void fastNlMeansDenoisingMulti(List<Mat> srcImgs, Mat dst, int imgToDenoiseIndex, int temporalWindowSize) {
+        Mat srcImgs_mat = Converters.vector_Mat_to_Mat(srcImgs);
+        fastNlMeansDenoisingMulti_3(srcImgs_mat.nativeObj, dst.nativeObj, imgToDenoiseIndex, temporalWindowSize);
+    }
+
+
+    //
+    // C++:  void cv::fastNlMeansDenoisingMulti(vector_Mat srcImgs, Mat& dst, int imgToDenoiseIndex, int temporalWindowSize, vector_float h, int templateWindowSize = 7, int searchWindowSize = 21, int normType = NORM_L2)
+    //
+
+    /**
+     * Modification of fastNlMeansDenoising function for images sequence where consecutive images have been
+     * captured in small period of time. For example video. This version of the function is for grayscale
+     * images or for manual manipulation with colorspaces. For more details see
+     * &lt;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.131.6394&gt;
+     *
+     * @param srcImgs Input 8-bit or 16-bit (only with NORM_L1) 1-channel,
+     * 2-channel, 3-channel or 4-channel images sequence. All images should
+     * have the same type and size.
+     * @param imgToDenoiseIndex Target image to denoise index in srcImgs sequence
+     * @param temporalWindowSize Number of surrounding images to use for target image denoising. Should
+     * be odd. Images from imgToDenoiseIndex - temporalWindowSize / 2 to
+     * imgToDenoiseIndex - temporalWindowSize / 2 from srcImgs will be used to denoise
+     * srcImgs[imgToDenoiseIndex] image.
+     * @param dst Output image with the same size and type as srcImgs images.
+     * @param templateWindowSize Size in pixels of the template patch that is used to compute weights.
+     * Should be odd. Recommended value 7 pixels
+     * @param searchWindowSize Size in pixels of the window that is used to compute weighted average for
+     * given pixel. Should be odd. Affect performance linearly: greater searchWindowsSize - greater
+     * denoising time. Recommended value 21 pixels
+     * @param h Array of parameters regulating filter strength, either one
+     * parameter applied to all channels or one per channel in dst. Big h value
+     * perfectly removes noise but also removes image details, smaller h
+     * value preserves details but also preserves some noise
+     * @param normType Type of norm used for weight calculation. Can be either NORM_L2 or NORM_L1
+     */
+    public static void fastNlMeansDenoisingMulti(List<Mat> srcImgs, Mat dst, int imgToDenoiseIndex, int temporalWindowSize, MatOfFloat h, int templateWindowSize, int searchWindowSize, int normType) {
+        Mat srcImgs_mat = Converters.vector_Mat_to_Mat(srcImgs);
+        Mat h_mat = h;
+        fastNlMeansDenoisingMulti_4(srcImgs_mat.nativeObj, dst.nativeObj, imgToDenoiseIndex, temporalWindowSize, h_mat.nativeObj, templateWindowSize, searchWindowSize, normType);
+    }
+
+    /**
+     * Modification of fastNlMeansDenoising function for images sequence where consecutive images have been
+     * captured in small period of time. For example video. This version of the function is for grayscale
+     * images or for manual manipulation with colorspaces. For more details see
+     * &lt;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.131.6394&gt;
+     *
+     * @param srcImgs Input 8-bit or 16-bit (only with NORM_L1) 1-channel,
+     * 2-channel, 3-channel or 4-channel images sequence. All images should
+     * have the same type and size.
+     * @param imgToDenoiseIndex Target image to denoise index in srcImgs sequence
+     * @param temporalWindowSize Number of surrounding images to use for target image denoising. Should
+     * be odd. Images from imgToDenoiseIndex - temporalWindowSize / 2 to
+     * imgToDenoiseIndex - temporalWindowSize / 2 from srcImgs will be used to denoise
+     * srcImgs[imgToDenoiseIndex] image.
+     * @param dst Output image with the same size and type as srcImgs images.
+     * @param templateWindowSize Size in pixels of the template patch that is used to compute weights.
+     * Should be odd. Recommended value 7 pixels
+     * @param searchWindowSize Size in pixels of the window that is used to compute weighted average for
+     * given pixel. Should be odd. Affect performance linearly: greater searchWindowsSize - greater
+     * denoising time. Recommended value 21 pixels
+     * @param h Array of parameters regulating filter strength, either one
+     * parameter applied to all channels or one per channel in dst. Big h value
+     * perfectly removes noise but also removes image details, smaller h
+     * value preserves details but also preserves some noise
+     */
+    public static void fastNlMeansDenoisingMulti(List<Mat> srcImgs, Mat dst, int imgToDenoiseIndex, int temporalWindowSize, MatOfFloat h, int templateWindowSize, int searchWindowSize) {
+        Mat srcImgs_mat = Converters.vector_Mat_to_Mat(srcImgs);
+        Mat h_mat = h;
+        fastNlMeansDenoisingMulti_5(srcImgs_mat.nativeObj, dst.nativeObj, imgToDenoiseIndex, temporalWindowSize, h_mat.nativeObj, templateWindowSize, searchWindowSize);
+    }
+
+    /**
+     * Modification of fastNlMeansDenoising function for images sequence where consecutive images have been
+     * captured in small period of time. For example video. This version of the function is for grayscale
+     * images or for manual manipulation with colorspaces. For more details see
+     * &lt;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.131.6394&gt;
+     *
+     * @param srcImgs Input 8-bit or 16-bit (only with NORM_L1) 1-channel,
+     * 2-channel, 3-channel or 4-channel images sequence. All images should
+     * have the same type and size.
+     * @param imgToDenoiseIndex Target image to denoise index in srcImgs sequence
+     * @param temporalWindowSize Number of surrounding images to use for target image denoising. Should
+     * be odd. Images from imgToDenoiseIndex - temporalWindowSize / 2 to
+     * imgToDenoiseIndex - temporalWindowSize / 2 from srcImgs will be used to denoise
+     * srcImgs[imgToDenoiseIndex] image.
+     * @param dst Output image with the same size and type as srcImgs images.
+     * @param templateWindowSize Size in pixels of the template patch that is used to compute weights.
+     * Should be odd. Recommended value 7 pixels
+     * given pixel. Should be odd. Affect performance linearly: greater searchWindowsSize - greater
+     * denoising time. Recommended value 21 pixels
+     * @param h Array of parameters regulating filter strength, either one
+     * parameter applied to all channels or one per channel in dst. Big h value
+     * perfectly removes noise but also removes image details, smaller h
+     * value preserves details but also preserves some noise
+     */
+    public static void fastNlMeansDenoisingMulti(List<Mat> srcImgs, Mat dst, int imgToDenoiseIndex, int temporalWindowSize, MatOfFloat h, int templateWindowSize) {
+        Mat srcImgs_mat = Converters.vector_Mat_to_Mat(srcImgs);
+        Mat h_mat = h;
+        fastNlMeansDenoisingMulti_6(srcImgs_mat.nativeObj, dst.nativeObj, imgToDenoiseIndex, temporalWindowSize, h_mat.nativeObj, templateWindowSize);
+    }
+
+    /**
+     * Modification of fastNlMeansDenoising function for images sequence where consecutive images have been
+     * captured in small period of time. For example video. This version of the function is for grayscale
+     * images or for manual manipulation with colorspaces. For more details see
+     * &lt;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.131.6394&gt;
+     *
+     * @param srcImgs Input 8-bit or 16-bit (only with NORM_L1) 1-channel,
+     * 2-channel, 3-channel or 4-channel images sequence. All images should
+     * have the same type and size.
+     * @param imgToDenoiseIndex Target image to denoise index in srcImgs sequence
+     * @param temporalWindowSize Number of surrounding images to use for target image denoising. Should
+     * be odd. Images from imgToDenoiseIndex - temporalWindowSize / 2 to
+     * imgToDenoiseIndex - temporalWindowSize / 2 from srcImgs will be used to denoise
+     * srcImgs[imgToDenoiseIndex] image.
+     * @param dst Output image with the same size and type as srcImgs images.
+     * Should be odd. Recommended value 7 pixels
+     * given pixel. Should be odd. Affect performance linearly: greater searchWindowsSize - greater
+     * denoising time. Recommended value 21 pixels
+     * @param h Array of parameters regulating filter strength, either one
+     * parameter applied to all channels or one per channel in dst. Big h value
+     * perfectly removes noise but also removes image details, smaller h
+     * value preserves details but also preserves some noise
+     */
+    public static void fastNlMeansDenoisingMulti(List<Mat> srcImgs, Mat dst, int imgToDenoiseIndex, int temporalWindowSize, MatOfFloat h) {
+        Mat srcImgs_mat = Converters.vector_Mat_to_Mat(srcImgs);
+        Mat h_mat = h;
+        fastNlMeansDenoisingMulti_7(srcImgs_mat.nativeObj, dst.nativeObj, imgToDenoiseIndex, temporalWindowSize, h_mat.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::illuminationChange(Mat src, Mat mask, Mat& dst, float alpha = 0.2f, float beta = 0.4f)
+    //
+
+    /**
+     * Applying an appropriate non-linear transformation to the gradient field inside the selection and
+     * then integrating back with a Poisson solver, modifies locally the apparent illumination of an image.
+     *
+     * @param src Input 8-bit 3-channel image.
+     * @param mask Input 8-bit 1 or 3-channel image.
+     * @param dst Output image with the same size and type as src.
+     * @param alpha Value ranges between 0-2.
+     * @param beta Value ranges between 0-2.
+     *
+     * This is useful to highlight under-exposed foreground objects or to reduce specular reflections.
+     */
+    public static void illuminationChange(Mat src, Mat mask, Mat dst, float alpha, float beta) {
+        illuminationChange_0(src.nativeObj, mask.nativeObj, dst.nativeObj, alpha, beta);
+    }
+
+    /**
+     * Applying an appropriate non-linear transformation to the gradient field inside the selection and
+     * then integrating back with a Poisson solver, modifies locally the apparent illumination of an image.
+     *
+     * @param src Input 8-bit 3-channel image.
+     * @param mask Input 8-bit 1 or 3-channel image.
+     * @param dst Output image with the same size and type as src.
+     * @param alpha Value ranges between 0-2.
+     *
+     * This is useful to highlight under-exposed foreground objects or to reduce specular reflections.
+     */
+    public static void illuminationChange(Mat src, Mat mask, Mat dst, float alpha) {
+        illuminationChange_1(src.nativeObj, mask.nativeObj, dst.nativeObj, alpha);
+    }
+
+    /**
+     * Applying an appropriate non-linear transformation to the gradient field inside the selection and
+     * then integrating back with a Poisson solver, modifies locally the apparent illumination of an image.
+     *
+     * @param src Input 8-bit 3-channel image.
+     * @param mask Input 8-bit 1 or 3-channel image.
+     * @param dst Output image with the same size and type as src.
+     *
+     * This is useful to highlight under-exposed foreground objects or to reduce specular reflections.
+     */
+    public static void illuminationChange(Mat src, Mat mask, Mat dst) {
+        illuminationChange_2(src.nativeObj, mask.nativeObj, dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::inpaint(Mat src, Mat inpaintMask, Mat& dst, double inpaintRadius, int flags)
+    //
+
+    /**
+     * Restores the selected region in an image using the region neighborhood.
+     *
+     * @param src Input 8-bit, 16-bit unsigned or 32-bit float 1-channel or 8-bit 3-channel image.
+     * @param inpaintMask Inpainting mask, 8-bit 1-channel image. Non-zero pixels indicate the area that
+     * needs to be inpainted.
+     * @param dst Output image with the same size and type as src .
+     * @param inpaintRadius Radius of a circular neighborhood of each point inpainted that is considered
+     * by the algorithm.
+     * @param flags Inpainting method that could be cv::INPAINT_NS or cv::INPAINT_TELEA
+     *
+     * The function reconstructs the selected image area from the pixel near the area boundary. The
+     * function may be used to remove dust and scratches from a scanned photo, or to remove undesirable
+     * objects from still images or video. See &lt;http://en.wikipedia.org/wiki/Inpainting&gt; for more details.
+     *
+     * <b>Note:</b>
+     * <ul>
+     *   <li>
+     *       An example using the inpainting technique can be found at
+     *         opencv_source_code/samples/cpp/inpaint.cpp
+     *   </li>
+     *   <li>
+     *       (Python) An example using the inpainting technique can be found at
+     *         opencv_source_code/samples/python/inpaint.py
+     *   </li>
+     * </ul>
+     */
+    public static void inpaint(Mat src, Mat inpaintMask, Mat dst, double inpaintRadius, int flags) {
+        inpaint_0(src.nativeObj, inpaintMask.nativeObj, dst.nativeObj, inpaintRadius, flags);
+    }
+
+
+    //
+    // C++:  void cv::pencilSketch(Mat src, Mat& dst1, Mat& dst2, float sigma_s = 60, float sigma_r = 0.07f, float shade_factor = 0.02f)
+    //
+
+    /**
+     * Pencil-like non-photorealistic line drawing
+     *
+     * @param src Input 8-bit 3-channel image.
+     * @param dst1 Output 8-bit 1-channel image.
+     * @param dst2 Output image with the same size and type as src.
+     * @param sigma_s %Range between 0 to 200.
+     * @param sigma_r %Range between 0 to 1.
+     * @param shade_factor %Range between 0 to 0.1.
+     */
+    public static void pencilSketch(Mat src, Mat dst1, Mat dst2, float sigma_s, float sigma_r, float shade_factor) {
+        pencilSketch_0(src.nativeObj, dst1.nativeObj, dst2.nativeObj, sigma_s, sigma_r, shade_factor);
+    }
+
+    /**
+     * Pencil-like non-photorealistic line drawing
+     *
+     * @param src Input 8-bit 3-channel image.
+     * @param dst1 Output 8-bit 1-channel image.
+     * @param dst2 Output image with the same size and type as src.
+     * @param sigma_s %Range between 0 to 200.
+     * @param sigma_r %Range between 0 to 1.
+     */
+    public static void pencilSketch(Mat src, Mat dst1, Mat dst2, float sigma_s, float sigma_r) {
+        pencilSketch_1(src.nativeObj, dst1.nativeObj, dst2.nativeObj, sigma_s, sigma_r);
+    }
+
+    /**
+     * Pencil-like non-photorealistic line drawing
+     *
+     * @param src Input 8-bit 3-channel image.
+     * @param dst1 Output 8-bit 1-channel image.
+     * @param dst2 Output image with the same size and type as src.
+     * @param sigma_s %Range between 0 to 200.
+     */
+    public static void pencilSketch(Mat src, Mat dst1, Mat dst2, float sigma_s) {
+        pencilSketch_2(src.nativeObj, dst1.nativeObj, dst2.nativeObj, sigma_s);
+    }
+
+    /**
+     * Pencil-like non-photorealistic line drawing
+     *
+     * @param src Input 8-bit 3-channel image.
+     * @param dst1 Output 8-bit 1-channel image.
+     * @param dst2 Output image with the same size and type as src.
+     */
+    public static void pencilSketch(Mat src, Mat dst1, Mat dst2) {
+        pencilSketch_3(src.nativeObj, dst1.nativeObj, dst2.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::seamlessClone(Mat src, Mat dst, Mat mask, Point p, Mat& blend, int flags)
+    //
+
+    /**
+     * Image editing tasks concern either global changes (color/intensity corrections, filters,
+     * deformations) or local changes concerned to a selection. Here we are interested in achieving local
+     * changes, ones that are restricted to a region manually selected (ROI), in a seamless and effortless
+     * manner. The extent of the changes ranges from slight distortions to complete replacement by novel
+     * content CITE: PM03 .
+     *
+     * @param src Input 8-bit 3-channel image.
+     * @param dst Input 8-bit 3-channel image.
+     * @param mask Input 8-bit 1 or 3-channel image.
+     * @param p Point in dst image where object is placed.
+     * @param blend Output image with the same size and type as dst.
+     * @param flags Cloning method that could be cv::NORMAL_CLONE, cv::MIXED_CLONE or cv::MONOCHROME_TRANSFER
+     */
+    public static void seamlessClone(Mat src, Mat dst, Mat mask, Point p, Mat blend, int flags) {
+        seamlessClone_0(src.nativeObj, dst.nativeObj, mask.nativeObj, p.x, p.y, blend.nativeObj, flags);
+    }
+
+
+    //
+    // C++:  void cv::stylization(Mat src, Mat& dst, float sigma_s = 60, float sigma_r = 0.45f)
+    //
+
+    /**
+     * Stylization aims to produce digital imagery with a wide variety of effects not focused on
+     * photorealism. Edge-aware filters are ideal for stylization, as they can abstract regions of low
+     * contrast while preserving, or enhancing, high-contrast features.
+     *
+     * @param src Input 8-bit 3-channel image.
+     * @param dst Output image with the same size and type as src.
+     * @param sigma_s %Range between 0 to 200.
+     * @param sigma_r %Range between 0 to 1.
+     */
+    public static void stylization(Mat src, Mat dst, float sigma_s, float sigma_r) {
+        stylization_0(src.nativeObj, dst.nativeObj, sigma_s, sigma_r);
+    }
+
+    /**
+     * Stylization aims to produce digital imagery with a wide variety of effects not focused on
+     * photorealism. Edge-aware filters are ideal for stylization, as they can abstract regions of low
+     * contrast while preserving, or enhancing, high-contrast features.
+     *
+     * @param src Input 8-bit 3-channel image.
+     * @param dst Output image with the same size and type as src.
+     * @param sigma_s %Range between 0 to 200.
+     */
+    public static void stylization(Mat src, Mat dst, float sigma_s) {
+        stylization_1(src.nativeObj, dst.nativeObj, sigma_s);
+    }
+
+    /**
+     * Stylization aims to produce digital imagery with a wide variety of effects not focused on
+     * photorealism. Edge-aware filters are ideal for stylization, as they can abstract regions of low
+     * contrast while preserving, or enhancing, high-contrast features.
+     *
+     * @param src Input 8-bit 3-channel image.
+     * @param dst Output image with the same size and type as src.
+     */
+    public static void stylization(Mat src, Mat dst) {
+        stylization_2(src.nativeObj, dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::textureFlattening(Mat src, Mat mask, Mat& dst, float low_threshold = 30, float high_threshold = 45, int kernel_size = 3)
+    //
+
+    /**
+     * By retaining only the gradients at edge locations, before integrating with the Poisson solver, one
+     * washes out the texture of the selected region, giving its contents a flat aspect. Here Canny Edge %Detector is used.
+     *
+     * @param src Input 8-bit 3-channel image.
+     * @param mask Input 8-bit 1 or 3-channel image.
+     * @param dst Output image with the same size and type as src.
+     * @param low_threshold %Range from 0 to 100.
+     * @param high_threshold Value &gt; 100.
+     * @param kernel_size The size of the Sobel kernel to be used.
+     *
+     * <b>Note:</b>
+     * The algorithm assumes that the color of the source image is close to that of the destination. This
+     * assumption means that when the colors don't match, the source image color gets tinted toward the
+     * color of the destination image.
+     */
+    public static void textureFlattening(Mat src, Mat mask, Mat dst, float low_threshold, float high_threshold, int kernel_size) {
+        textureFlattening_0(src.nativeObj, mask.nativeObj, dst.nativeObj, low_threshold, high_threshold, kernel_size);
+    }
+
+    /**
+     * By retaining only the gradients at edge locations, before integrating with the Poisson solver, one
+     * washes out the texture of the selected region, giving its contents a flat aspect. Here Canny Edge %Detector is used.
+     *
+     * @param src Input 8-bit 3-channel image.
+     * @param mask Input 8-bit 1 or 3-channel image.
+     * @param dst Output image with the same size and type as src.
+     * @param low_threshold %Range from 0 to 100.
+     * @param high_threshold Value &gt; 100.
+     *
+     * <b>Note:</b>
+     * The algorithm assumes that the color of the source image is close to that of the destination. This
+     * assumption means that when the colors don't match, the source image color gets tinted toward the
+     * color of the destination image.
+     */
+    public static void textureFlattening(Mat src, Mat mask, Mat dst, float low_threshold, float high_threshold) {
+        textureFlattening_1(src.nativeObj, mask.nativeObj, dst.nativeObj, low_threshold, high_threshold);
+    }
+
+    /**
+     * By retaining only the gradients at edge locations, before integrating with the Poisson solver, one
+     * washes out the texture of the selected region, giving its contents a flat aspect. Here Canny Edge %Detector is used.
+     *
+     * @param src Input 8-bit 3-channel image.
+     * @param mask Input 8-bit 1 or 3-channel image.
+     * @param dst Output image with the same size and type as src.
+     * @param low_threshold %Range from 0 to 100.
+     *
+     * <b>Note:</b>
+     * The algorithm assumes that the color of the source image is close to that of the destination. This
+     * assumption means that when the colors don't match, the source image color gets tinted toward the
+     * color of the destination image.
+     */
+    public static void textureFlattening(Mat src, Mat mask, Mat dst, float low_threshold) {
+        textureFlattening_2(src.nativeObj, mask.nativeObj, dst.nativeObj, low_threshold);
+    }
+
+    /**
+     * By retaining only the gradients at edge locations, before integrating with the Poisson solver, one
+     * washes out the texture of the selected region, giving its contents a flat aspect. Here Canny Edge %Detector is used.
+     *
+     * @param src Input 8-bit 3-channel image.
+     * @param mask Input 8-bit 1 or 3-channel image.
+     * @param dst Output image with the same size and type as src.
+     *
+     * <b>Note:</b>
+     * The algorithm assumes that the color of the source image is close to that of the destination. This
+     * assumption means that when the colors don't match, the source image color gets tinted toward the
+     * color of the destination image.
+     */
+    public static void textureFlattening(Mat src, Mat mask, Mat dst) {
+        textureFlattening_3(src.nativeObj, mask.nativeObj, dst.nativeObj);
+    }
+
+
+
+
+    // C++:  Ptr_AlignMTB cv::createAlignMTB(int max_bits = 6, int exclude_range = 4, bool cut = true)
+    private static native long createAlignMTB_0(int max_bits, int exclude_range, boolean cut);
+    private static native long createAlignMTB_1(int max_bits, int exclude_range);
+    private static native long createAlignMTB_2(int max_bits);
+    private static native long createAlignMTB_3();
+
+    // C++:  Ptr_CalibrateDebevec cv::createCalibrateDebevec(int samples = 70, float lambda = 10.0f, bool random = false)
+    private static native long createCalibrateDebevec_0(int samples, float lambda, boolean random);
+    private static native long createCalibrateDebevec_1(int samples, float lambda);
+    private static native long createCalibrateDebevec_2(int samples);
+    private static native long createCalibrateDebevec_3();
+
+    // C++:  Ptr_CalibrateRobertson cv::createCalibrateRobertson(int max_iter = 30, float threshold = 0.01f)
+    private static native long createCalibrateRobertson_0(int max_iter, float threshold);
+    private static native long createCalibrateRobertson_1(int max_iter);
+    private static native long createCalibrateRobertson_2();
+
+    // C++:  Ptr_MergeDebevec cv::createMergeDebevec()
+    private static native long createMergeDebevec_0();
+
+    // C++:  Ptr_MergeMertens cv::createMergeMertens(float contrast_weight = 1.0f, float saturation_weight = 1.0f, float exposure_weight = 0.0f)
+    private static native long createMergeMertens_0(float contrast_weight, float saturation_weight, float exposure_weight);
+    private static native long createMergeMertens_1(float contrast_weight, float saturation_weight);
+    private static native long createMergeMertens_2(float contrast_weight);
+    private static native long createMergeMertens_3();
+
+    // C++:  Ptr_MergeRobertson cv::createMergeRobertson()
+    private static native long createMergeRobertson_0();
+
+    // C++:  Ptr_Tonemap cv::createTonemap(float gamma = 1.0f)
+    private static native long createTonemap_0(float gamma);
+    private static native long createTonemap_1();
+
+    // C++:  Ptr_TonemapDrago cv::createTonemapDrago(float gamma = 1.0f, float saturation = 1.0f, float bias = 0.85f)
+    private static native long createTonemapDrago_0(float gamma, float saturation, float bias);
+    private static native long createTonemapDrago_1(float gamma, float saturation);
+    private static native long createTonemapDrago_2(float gamma);
+    private static native long createTonemapDrago_3();
+
+    // C++:  Ptr_TonemapMantiuk cv::createTonemapMantiuk(float gamma = 1.0f, float scale = 0.7f, float saturation = 1.0f)
+    private static native long createTonemapMantiuk_0(float gamma, float scale, float saturation);
+    private static native long createTonemapMantiuk_1(float gamma, float scale);
+    private static native long createTonemapMantiuk_2(float gamma);
+    private static native long createTonemapMantiuk_3();
+
+    // C++:  Ptr_TonemapReinhard cv::createTonemapReinhard(float gamma = 1.0f, float intensity = 0.0f, float light_adapt = 1.0f, float color_adapt = 0.0f)
+    private static native long createTonemapReinhard_0(float gamma, float intensity, float light_adapt, float color_adapt);
+    private static native long createTonemapReinhard_1(float gamma, float intensity, float light_adapt);
+    private static native long createTonemapReinhard_2(float gamma, float intensity);
+    private static native long createTonemapReinhard_3(float gamma);
+    private static native long createTonemapReinhard_4();
+
+    // C++:  void cv::colorChange(Mat src, Mat mask, Mat& dst, float red_mul = 1.0f, float green_mul = 1.0f, float blue_mul = 1.0f)
+    private static native void colorChange_0(long src_nativeObj, long mask_nativeObj, long dst_nativeObj, float red_mul, float green_mul, float blue_mul);
+    private static native void colorChange_1(long src_nativeObj, long mask_nativeObj, long dst_nativeObj, float red_mul, float green_mul);
+    private static native void colorChange_2(long src_nativeObj, long mask_nativeObj, long dst_nativeObj, float red_mul);
+    private static native void colorChange_3(long src_nativeObj, long mask_nativeObj, long dst_nativeObj);
+
+    // C++:  void cv::decolor(Mat src, Mat& grayscale, Mat& color_boost)
+    private static native void decolor_0(long src_nativeObj, long grayscale_nativeObj, long color_boost_nativeObj);
+
+    // C++:  void cv::denoise_TVL1(vector_Mat observations, Mat result, double lambda = 1.0, int niters = 30)
+    private static native void denoise_TVL1_0(long observations_mat_nativeObj, long result_nativeObj, double lambda, int niters);
+    private static native void denoise_TVL1_1(long observations_mat_nativeObj, long result_nativeObj, double lambda);
+    private static native void denoise_TVL1_2(long observations_mat_nativeObj, long result_nativeObj);
+
+    // C++:  void cv::detailEnhance(Mat src, Mat& dst, float sigma_s = 10, float sigma_r = 0.15f)
+    private static native void detailEnhance_0(long src_nativeObj, long dst_nativeObj, float sigma_s, float sigma_r);
+    private static native void detailEnhance_1(long src_nativeObj, long dst_nativeObj, float sigma_s);
+    private static native void detailEnhance_2(long src_nativeObj, long dst_nativeObj);
+
+    // C++:  void cv::edgePreservingFilter(Mat src, Mat& dst, int flags = 1, float sigma_s = 60, float sigma_r = 0.4f)
+    private static native void edgePreservingFilter_0(long src_nativeObj, long dst_nativeObj, int flags, float sigma_s, float sigma_r);
+    private static native void edgePreservingFilter_1(long src_nativeObj, long dst_nativeObj, int flags, float sigma_s);
+    private static native void edgePreservingFilter_2(long src_nativeObj, long dst_nativeObj, int flags);
+    private static native void edgePreservingFilter_3(long src_nativeObj, long dst_nativeObj);
+
+    // C++:  void cv::fastNlMeansDenoising(Mat src, Mat& dst, float h = 3, int templateWindowSize = 7, int searchWindowSize = 21)
+    private static native void fastNlMeansDenoising_0(long src_nativeObj, long dst_nativeObj, float h, int templateWindowSize, int searchWindowSize);
+    private static native void fastNlMeansDenoising_1(long src_nativeObj, long dst_nativeObj, float h, int templateWindowSize);
+    private static native void fastNlMeansDenoising_2(long src_nativeObj, long dst_nativeObj, float h);
+    private static native void fastNlMeansDenoising_3(long src_nativeObj, long dst_nativeObj);
+
+    // C++:  void cv::fastNlMeansDenoising(Mat src, Mat& dst, vector_float h, int templateWindowSize = 7, int searchWindowSize = 21, int normType = NORM_L2)
+    private static native void fastNlMeansDenoising_4(long src_nativeObj, long dst_nativeObj, long h_mat_nativeObj, int templateWindowSize, int searchWindowSize, int normType);
+    private static native void fastNlMeansDenoising_5(long src_nativeObj, long dst_nativeObj, long h_mat_nativeObj, int templateWindowSize, int searchWindowSize);
+    private static native void fastNlMeansDenoising_6(long src_nativeObj, long dst_nativeObj, long h_mat_nativeObj, int templateWindowSize);
+    private static native void fastNlMeansDenoising_7(long src_nativeObj, long dst_nativeObj, long h_mat_nativeObj);
+
+    // C++:  void cv::fastNlMeansDenoisingColored(Mat src, Mat& dst, float h = 3, float hColor = 3, int templateWindowSize = 7, int searchWindowSize = 21)
+    private static native void fastNlMeansDenoisingColored_0(long src_nativeObj, long dst_nativeObj, float h, float hColor, int templateWindowSize, int searchWindowSize);
+    private static native void fastNlMeansDenoisingColored_1(long src_nativeObj, long dst_nativeObj, float h, float hColor, int templateWindowSize);
+    private static native void fastNlMeansDenoisingColored_2(long src_nativeObj, long dst_nativeObj, float h, float hColor);
+    private static native void fastNlMeansDenoisingColored_3(long src_nativeObj, long dst_nativeObj, float h);
+    private static native void fastNlMeansDenoisingColored_4(long src_nativeObj, long dst_nativeObj);
+
+    // C++:  void cv::fastNlMeansDenoisingColoredMulti(vector_Mat srcImgs, Mat& dst, int imgToDenoiseIndex, int temporalWindowSize, float h = 3, float hColor = 3, int templateWindowSize = 7, int searchWindowSize = 21)
+    private static native void fastNlMeansDenoisingColoredMulti_0(long srcImgs_mat_nativeObj, long dst_nativeObj, int imgToDenoiseIndex, int temporalWindowSize, float h, float hColor, int templateWindowSize, int searchWindowSize);
+    private static native void fastNlMeansDenoisingColoredMulti_1(long srcImgs_mat_nativeObj, long dst_nativeObj, int imgToDenoiseIndex, int temporalWindowSize, float h, float hColor, int templateWindowSize);
+    private static native void fastNlMeansDenoisingColoredMulti_2(long srcImgs_mat_nativeObj, long dst_nativeObj, int imgToDenoiseIndex, int temporalWindowSize, float h, float hColor);
+    private static native void fastNlMeansDenoisingColoredMulti_3(long srcImgs_mat_nativeObj, long dst_nativeObj, int imgToDenoiseIndex, int temporalWindowSize, float h);
+    private static native void fastNlMeansDenoisingColoredMulti_4(long srcImgs_mat_nativeObj, long dst_nativeObj, int imgToDenoiseIndex, int temporalWindowSize);
+
+    // C++:  void cv::fastNlMeansDenoisingMulti(vector_Mat srcImgs, Mat& dst, int imgToDenoiseIndex, int temporalWindowSize, float h = 3, int templateWindowSize = 7, int searchWindowSize = 21)
+    private static native void fastNlMeansDenoisingMulti_0(long srcImgs_mat_nativeObj, long dst_nativeObj, int imgToDenoiseIndex, int temporalWindowSize, float h, int templateWindowSize, int searchWindowSize);
+    private static native void fastNlMeansDenoisingMulti_1(long srcImgs_mat_nativeObj, long dst_nativeObj, int imgToDenoiseIndex, int temporalWindowSize, float h, int templateWindowSize);
+    private static native void fastNlMeansDenoisingMulti_2(long srcImgs_mat_nativeObj, long dst_nativeObj, int imgToDenoiseIndex, int temporalWindowSize, float h);
+    private static native void fastNlMeansDenoisingMulti_3(long srcImgs_mat_nativeObj, long dst_nativeObj, int imgToDenoiseIndex, int temporalWindowSize);
+
+    // C++:  void cv::fastNlMeansDenoisingMulti(vector_Mat srcImgs, Mat& dst, int imgToDenoiseIndex, int temporalWindowSize, vector_float h, int templateWindowSize = 7, int searchWindowSize = 21, int normType = NORM_L2)
+    private static native void fastNlMeansDenoisingMulti_4(long srcImgs_mat_nativeObj, long dst_nativeObj, int imgToDenoiseIndex, int temporalWindowSize, long h_mat_nativeObj, int templateWindowSize, int searchWindowSize, int normType);
+    private static native void fastNlMeansDenoisingMulti_5(long srcImgs_mat_nativeObj, long dst_nativeObj, int imgToDenoiseIndex, int temporalWindowSize, long h_mat_nativeObj, int templateWindowSize, int searchWindowSize);
+    private static native void fastNlMeansDenoisingMulti_6(long srcImgs_mat_nativeObj, long dst_nativeObj, int imgToDenoiseIndex, int temporalWindowSize, long h_mat_nativeObj, int templateWindowSize);
+    private static native void fastNlMeansDenoisingMulti_7(long srcImgs_mat_nativeObj, long dst_nativeObj, int imgToDenoiseIndex, int temporalWindowSize, long h_mat_nativeObj);
+
+    // C++:  void cv::illuminationChange(Mat src, Mat mask, Mat& dst, float alpha = 0.2f, float beta = 0.4f)
+    private static native void illuminationChange_0(long src_nativeObj, long mask_nativeObj, long dst_nativeObj, float alpha, float beta);
+    private static native void illuminationChange_1(long src_nativeObj, long mask_nativeObj, long dst_nativeObj, float alpha);
+    private static native void illuminationChange_2(long src_nativeObj, long mask_nativeObj, long dst_nativeObj);
+
+    // C++:  void cv::inpaint(Mat src, Mat inpaintMask, Mat& dst, double inpaintRadius, int flags)
+    private static native void inpaint_0(long src_nativeObj, long inpaintMask_nativeObj, long dst_nativeObj, double inpaintRadius, int flags);
+
+    // C++:  void cv::pencilSketch(Mat src, Mat& dst1, Mat& dst2, float sigma_s = 60, float sigma_r = 0.07f, float shade_factor = 0.02f)
+    private static native void pencilSketch_0(long src_nativeObj, long dst1_nativeObj, long dst2_nativeObj, float sigma_s, float sigma_r, float shade_factor);
+    private static native void pencilSketch_1(long src_nativeObj, long dst1_nativeObj, long dst2_nativeObj, float sigma_s, float sigma_r);
+    private static native void pencilSketch_2(long src_nativeObj, long dst1_nativeObj, long dst2_nativeObj, float sigma_s);
+    private static native void pencilSketch_3(long src_nativeObj, long dst1_nativeObj, long dst2_nativeObj);
+
+    // C++:  void cv::seamlessClone(Mat src, Mat dst, Mat mask, Point p, Mat& blend, int flags)
+    private static native void seamlessClone_0(long src_nativeObj, long dst_nativeObj, long mask_nativeObj, double p_x, double p_y, long blend_nativeObj, int flags);
+
+    // C++:  void cv::stylization(Mat src, Mat& dst, float sigma_s = 60, float sigma_r = 0.45f)
+    private static native void stylization_0(long src_nativeObj, long dst_nativeObj, float sigma_s, float sigma_r);
+    private static native void stylization_1(long src_nativeObj, long dst_nativeObj, float sigma_s);
+    private static native void stylization_2(long src_nativeObj, long dst_nativeObj);
+
+    // C++:  void cv::textureFlattening(Mat src, Mat mask, Mat& dst, float low_threshold = 30, float high_threshold = 45, int kernel_size = 3)
+    private static native void textureFlattening_0(long src_nativeObj, long mask_nativeObj, long dst_nativeObj, float low_threshold, float high_threshold, int kernel_size);
+    private static native void textureFlattening_1(long src_nativeObj, long mask_nativeObj, long dst_nativeObj, float low_threshold, float high_threshold);
+    private static native void textureFlattening_2(long src_nativeObj, long mask_nativeObj, long dst_nativeObj, float low_threshold);
+    private static native void textureFlattening_3(long src_nativeObj, long mask_nativeObj, long dst_nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/photo/Tonemap.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/photo/Tonemap.java	(date 1605830248152)
+++ openCVLibrary3411/src/main/java/org/opencv/photo/Tonemap.java	(date 1605830248152)
@@ -0,0 +1,72 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.photo;
+
+import org.opencv.core.Algorithm;
+import org.opencv.core.Mat;
+
+// C++: class Tonemap
+/**
+ * Base class for tonemapping algorithms - tools that are used to map HDR image to 8-bit range.
+ */
+public class Tonemap extends Algorithm {
+
+    protected Tonemap(long addr) { super(addr); }
+
+    // internal usage only
+    public static Tonemap __fromPtr__(long addr) { return new Tonemap(addr); }
+
+    //
+    // C++:  float cv::Tonemap::getGamma()
+    //
+
+    public float getGamma() {
+        return getGamma_0(nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::Tonemap::process(Mat src, Mat& dst)
+    //
+
+    /**
+     * Tonemaps image
+     *
+     *     @param src source image - CV_32FC3 Mat (float 32 bits 3 channels)
+     *     @param dst destination image - CV_32FC3 Mat with values in [0, 1] range
+     */
+    public void process(Mat src, Mat dst) {
+        process_0(nativeObj, src.nativeObj, dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::Tonemap::setGamma(float gamma)
+    //
+
+    public void setGamma(float gamma) {
+        setGamma_0(nativeObj, gamma);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++:  float cv::Tonemap::getGamma()
+    private static native float getGamma_0(long nativeObj);
+
+    // C++:  void cv::Tonemap::process(Mat src, Mat& dst)
+    private static native void process_0(long nativeObj, long src_nativeObj, long dst_nativeObj);
+
+    // C++:  void cv::Tonemap::setGamma(float gamma)
+    private static native void setGamma_0(long nativeObj, float gamma);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/photo/TonemapDrago.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/photo/TonemapDrago.java	(date 1605830248154)
+++ openCVLibrary3411/src/main/java/org/opencv/photo/TonemapDrago.java	(date 1605830248154)
@@ -0,0 +1,85 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.photo;
+
+import org.opencv.photo.Tonemap;
+
+// C++: class TonemapDrago
+/**
+ * Adaptive logarithmic mapping is a fast global tonemapping algorithm that scales the image in
+ * logarithmic domain.
+ *
+ * Since it's a global operator the same function is applied to all the pixels, it is controlled by the
+ * bias parameter.
+ *
+ * Optional saturation enhancement is possible as described in CITE: FL02 .
+ *
+ * For more information see CITE: DM03 .
+ */
+public class TonemapDrago extends Tonemap {
+
+    protected TonemapDrago(long addr) { super(addr); }
+
+    // internal usage only
+    public static TonemapDrago __fromPtr__(long addr) { return new TonemapDrago(addr); }
+
+    //
+    // C++:  float cv::TonemapDrago::getBias()
+    //
+
+    public float getBias() {
+        return getBias_0(nativeObj);
+    }
+
+
+    //
+    // C++:  float cv::TonemapDrago::getSaturation()
+    //
+
+    public float getSaturation() {
+        return getSaturation_0(nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::TonemapDrago::setBias(float bias)
+    //
+
+    public void setBias(float bias) {
+        setBias_0(nativeObj, bias);
+    }
+
+
+    //
+    // C++:  void cv::TonemapDrago::setSaturation(float saturation)
+    //
+
+    public void setSaturation(float saturation) {
+        setSaturation_0(nativeObj, saturation);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++:  float cv::TonemapDrago::getBias()
+    private static native float getBias_0(long nativeObj);
+
+    // C++:  float cv::TonemapDrago::getSaturation()
+    private static native float getSaturation_0(long nativeObj);
+
+    // C++:  void cv::TonemapDrago::setBias(float bias)
+    private static native void setBias_0(long nativeObj, float bias);
+
+    // C++:  void cv::TonemapDrago::setSaturation(float saturation)
+    private static native void setSaturation_0(long nativeObj, float saturation);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/photo/CalibrateRobertson.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/photo/CalibrateRobertson.java	(date 1605830248087)
+++ openCVLibrary3411/src/main/java/org/opencv/photo/CalibrateRobertson.java	(date 1605830248087)
@@ -0,0 +1,93 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.photo;
+
+import org.opencv.core.Mat;
+import org.opencv.photo.CalibrateCRF;
+
+// C++: class CalibrateRobertson
+/**
+ * Inverse camera response function is extracted for each brightness value by minimizing an objective
+ * function as linear system. This algorithm uses all image pixels.
+ *
+ * For more information see CITE: RB99 .
+ */
+public class CalibrateRobertson extends CalibrateCRF {
+
+    protected CalibrateRobertson(long addr) { super(addr); }
+
+    // internal usage only
+    public static CalibrateRobertson __fromPtr__(long addr) { return new CalibrateRobertson(addr); }
+
+    //
+    // C++:  Mat cv::CalibrateRobertson::getRadiance()
+    //
+
+    public Mat getRadiance() {
+        return new Mat(getRadiance_0(nativeObj));
+    }
+
+
+    //
+    // C++:  float cv::CalibrateRobertson::getThreshold()
+    //
+
+    public float getThreshold() {
+        return getThreshold_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::CalibrateRobertson::getMaxIter()
+    //
+
+    public int getMaxIter() {
+        return getMaxIter_0(nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::CalibrateRobertson::setMaxIter(int max_iter)
+    //
+
+    public void setMaxIter(int max_iter) {
+        setMaxIter_0(nativeObj, max_iter);
+    }
+
+
+    //
+    // C++:  void cv::CalibrateRobertson::setThreshold(float threshold)
+    //
+
+    public void setThreshold(float threshold) {
+        setThreshold_0(nativeObj, threshold);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++:  Mat cv::CalibrateRobertson::getRadiance()
+    private static native long getRadiance_0(long nativeObj);
+
+    // C++:  float cv::CalibrateRobertson::getThreshold()
+    private static native float getThreshold_0(long nativeObj);
+
+    // C++:  int cv::CalibrateRobertson::getMaxIter()
+    private static native int getMaxIter_0(long nativeObj);
+
+    // C++:  void cv::CalibrateRobertson::setMaxIter(int max_iter)
+    private static native void setMaxIter_0(long nativeObj, int max_iter);
+
+    // C++:  void cv::CalibrateRobertson::setThreshold(float threshold)
+    private static native void setThreshold_0(long nativeObj, float threshold);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/photo/MergeDebevec.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/photo/MergeDebevec.java	(date 1605830248101)
+++ openCVLibrary3411/src/main/java/org/opencv/photo/MergeDebevec.java	(date 1605830248101)
@@ -0,0 +1,62 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.photo;
+
+import java.util.ArrayList;
+import java.util.List;
+import org.opencv.core.Mat;
+import org.opencv.photo.MergeExposures;
+import org.opencv.utils.Converters;
+
+// C++: class MergeDebevec
+/**
+ * The resulting HDR image is calculated as weighted average of the exposures considering exposure
+ * values and camera response.
+ *
+ * For more information see CITE: DM97 .
+ */
+public class MergeDebevec extends MergeExposures {
+
+    protected MergeDebevec(long addr) { super(addr); }
+
+    // internal usage only
+    public static MergeDebevec __fromPtr__(long addr) { return new MergeDebevec(addr); }
+
+    //
+    // C++:  void cv::MergeDebevec::process(vector_Mat src, Mat& dst, Mat times, Mat response)
+    //
+
+    public void process(List<Mat> src, Mat dst, Mat times, Mat response) {
+        Mat src_mat = Converters.vector_Mat_to_Mat(src);
+        process_0(nativeObj, src_mat.nativeObj, dst.nativeObj, times.nativeObj, response.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::MergeDebevec::process(vector_Mat src, Mat& dst, Mat times)
+    //
+
+    public void process(List<Mat> src, Mat dst, Mat times) {
+        Mat src_mat = Converters.vector_Mat_to_Mat(src);
+        process_1(nativeObj, src_mat.nativeObj, dst.nativeObj, times.nativeObj);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++:  void cv::MergeDebevec::process(vector_Mat src, Mat& dst, Mat times, Mat response)
+    private static native void process_0(long nativeObj, long src_mat_nativeObj, long dst_nativeObj, long times_nativeObj, long response_nativeObj);
+
+    // C++:  void cv::MergeDebevec::process(vector_Mat src, Mat& dst, Mat times)
+    private static native void process_1(long nativeObj, long src_mat_nativeObj, long dst_nativeObj, long times_nativeObj);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/photo/MergeExposures.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/photo/MergeExposures.java	(date 1605830248103)
+++ openCVLibrary3411/src/main/java/org/opencv/photo/MergeExposures.java	(date 1605830248103)
@@ -0,0 +1,55 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.photo;
+
+import java.util.ArrayList;
+import java.util.List;
+import org.opencv.core.Algorithm;
+import org.opencv.core.Mat;
+import org.opencv.utils.Converters;
+
+// C++: class MergeExposures
+/**
+ * The base class algorithms that can merge exposure sequence to a single image.
+ */
+public class MergeExposures extends Algorithm {
+
+    protected MergeExposures(long addr) { super(addr); }
+
+    // internal usage only
+    public static MergeExposures __fromPtr__(long addr) { return new MergeExposures(addr); }
+
+    //
+    // C++:  void cv::MergeExposures::process(vector_Mat src, Mat& dst, Mat times, Mat response)
+    //
+
+    /**
+     * Merges images.
+     *
+     *     @param src vector of input images
+     *     @param dst result image
+     *     @param times vector of exposure time values for each image
+     *     @param response 256x1 matrix with inverse camera response function for each pixel value, it should
+     *     have the same number of channels as images.
+     */
+    public void process(List<Mat> src, Mat dst, Mat times, Mat response) {
+        Mat src_mat = Converters.vector_Mat_to_Mat(src);
+        process_0(nativeObj, src_mat.nativeObj, dst.nativeObj, times.nativeObj, response.nativeObj);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++:  void cv::MergeExposures::process(vector_Mat src, Mat& dst, Mat times, Mat response)
+    private static native void process_0(long nativeObj, long src_mat_nativeObj, long dst_nativeObj, long times_nativeObj, long response_nativeObj);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/photo/MergeMertens.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/photo/MergeMertens.java	(date 1605830248105)
+++ openCVLibrary3411/src/main/java/org/opencv/photo/MergeMertens.java	(date 1605830248105)
@@ -0,0 +1,146 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.photo;
+
+import java.util.ArrayList;
+import java.util.List;
+import org.opencv.core.Mat;
+import org.opencv.photo.MergeExposures;
+import org.opencv.utils.Converters;
+
+// C++: class MergeMertens
+/**
+ * Pixels are weighted using contrast, saturation and well-exposedness measures, than images are
+ * combined using laplacian pyramids.
+ *
+ * The resulting image weight is constructed as weighted average of contrast, saturation and
+ * well-exposedness measures.
+ *
+ * The resulting image doesn't require tonemapping and can be converted to 8-bit image by multiplying
+ * by 255, but it's recommended to apply gamma correction and/or linear tonemapping.
+ *
+ * For more information see CITE: MK07 .
+ */
+public class MergeMertens extends MergeExposures {
+
+    protected MergeMertens(long addr) { super(addr); }
+
+    // internal usage only
+    public static MergeMertens __fromPtr__(long addr) { return new MergeMertens(addr); }
+
+    //
+    // C++:  float cv::MergeMertens::getContrastWeight()
+    //
+
+    public float getContrastWeight() {
+        return getContrastWeight_0(nativeObj);
+    }
+
+
+    //
+    // C++:  float cv::MergeMertens::getExposureWeight()
+    //
+
+    public float getExposureWeight() {
+        return getExposureWeight_0(nativeObj);
+    }
+
+
+    //
+    // C++:  float cv::MergeMertens::getSaturationWeight()
+    //
+
+    public float getSaturationWeight() {
+        return getSaturationWeight_0(nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::MergeMertens::process(vector_Mat src, Mat& dst, Mat times, Mat response)
+    //
+
+    public void process(List<Mat> src, Mat dst, Mat times, Mat response) {
+        Mat src_mat = Converters.vector_Mat_to_Mat(src);
+        process_0(nativeObj, src_mat.nativeObj, dst.nativeObj, times.nativeObj, response.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::MergeMertens::process(vector_Mat src, Mat& dst)
+    //
+
+    /**
+     * Short version of process, that doesn't take extra arguments.
+     *
+     *     @param src vector of input images
+     *     @param dst result image
+     */
+    public void process(List<Mat> src, Mat dst) {
+        Mat src_mat = Converters.vector_Mat_to_Mat(src);
+        process_1(nativeObj, src_mat.nativeObj, dst.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::MergeMertens::setContrastWeight(float contrast_weiht)
+    //
+
+    public void setContrastWeight(float contrast_weiht) {
+        setContrastWeight_0(nativeObj, contrast_weiht);
+    }
+
+
+    //
+    // C++:  void cv::MergeMertens::setExposureWeight(float exposure_weight)
+    //
+
+    public void setExposureWeight(float exposure_weight) {
+        setExposureWeight_0(nativeObj, exposure_weight);
+    }
+
+
+    //
+    // C++:  void cv::MergeMertens::setSaturationWeight(float saturation_weight)
+    //
+
+    public void setSaturationWeight(float saturation_weight) {
+        setSaturationWeight_0(nativeObj, saturation_weight);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++:  float cv::MergeMertens::getContrastWeight()
+    private static native float getContrastWeight_0(long nativeObj);
+
+    // C++:  float cv::MergeMertens::getExposureWeight()
+    private static native float getExposureWeight_0(long nativeObj);
+
+    // C++:  float cv::MergeMertens::getSaturationWeight()
+    private static native float getSaturationWeight_0(long nativeObj);
+
+    // C++:  void cv::MergeMertens::process(vector_Mat src, Mat& dst, Mat times, Mat response)
+    private static native void process_0(long nativeObj, long src_mat_nativeObj, long dst_nativeObj, long times_nativeObj, long response_nativeObj);
+
+    // C++:  void cv::MergeMertens::process(vector_Mat src, Mat& dst)
+    private static native void process_1(long nativeObj, long src_mat_nativeObj, long dst_nativeObj);
+
+    // C++:  void cv::MergeMertens::setContrastWeight(float contrast_weiht)
+    private static native void setContrastWeight_0(long nativeObj, float contrast_weiht);
+
+    // C++:  void cv::MergeMertens::setExposureWeight(float exposure_weight)
+    private static native void setExposureWeight_0(long nativeObj, float exposure_weight);
+
+    // C++:  void cv::MergeMertens::setSaturationWeight(float saturation_weight)
+    private static native void setSaturationWeight_0(long nativeObj, float saturation_weight);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/photo/AlignExposures.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/photo/AlignExposures.java	(date 1605830248046)
+++ openCVLibrary3411/src/main/java/org/opencv/photo/AlignExposures.java	(date 1605830248046)
@@ -0,0 +1,56 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.photo;
+
+import java.util.ArrayList;
+import java.util.List;
+import org.opencv.core.Algorithm;
+import org.opencv.core.Mat;
+import org.opencv.utils.Converters;
+
+// C++: class AlignExposures
+/**
+ * The base class for algorithms that align images of the same scene with different exposures
+ */
+public class AlignExposures extends Algorithm {
+
+    protected AlignExposures(long addr) { super(addr); }
+
+    // internal usage only
+    public static AlignExposures __fromPtr__(long addr) { return new AlignExposures(addr); }
+
+    //
+    // C++:  void cv::AlignExposures::process(vector_Mat src, vector_Mat dst, Mat times, Mat response)
+    //
+
+    /**
+     * Aligns images
+     *
+     *     @param src vector of input images
+     *     @param dst vector of aligned images
+     *     @param times vector of exposure time values for each image
+     *     @param response 256x1 matrix with inverse camera response function for each pixel value, it should
+     *     have the same number of channels as images.
+     */
+    public void process(List<Mat> src, List<Mat> dst, Mat times, Mat response) {
+        Mat src_mat = Converters.vector_Mat_to_Mat(src);
+        Mat dst_mat = Converters.vector_Mat_to_Mat(dst);
+        process_0(nativeObj, src_mat.nativeObj, dst_mat.nativeObj, times.nativeObj, response.nativeObj);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++:  void cv::AlignExposures::process(vector_Mat src, vector_Mat dst, Mat times, Mat response)
+    private static native void process_0(long nativeObj, long src_mat_nativeObj, long dst_mat_nativeObj, long times_nativeObj, long response_nativeObj);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/photo/AlignMTB.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/photo/AlignMTB.java	(date 1605830248064)
+++ openCVLibrary3411/src/main/java/org/opencv/photo/AlignMTB.java	(date 1605830248064)
@@ -0,0 +1,205 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.photo;
+
+import java.util.ArrayList;
+import java.util.List;
+import org.opencv.core.Mat;
+import org.opencv.core.Point;
+import org.opencv.photo.AlignExposures;
+import org.opencv.utils.Converters;
+
+// C++: class AlignMTB
+/**
+ * This algorithm converts images to median threshold bitmaps (1 for pixels brighter than median
+ * luminance and 0 otherwise) and than aligns the resulting bitmaps using bit operations.
+ *
+ * It is invariant to exposure, so exposure values and camera response are not necessary.
+ *
+ * In this implementation new image regions are filled with zeros.
+ *
+ * For more information see CITE: GW03 .
+ */
+public class AlignMTB extends AlignExposures {
+
+    protected AlignMTB(long addr) { super(addr); }
+
+    // internal usage only
+    public static AlignMTB __fromPtr__(long addr) { return new AlignMTB(addr); }
+
+    //
+    // C++:  Point cv::AlignMTB::calculateShift(Mat img0, Mat img1)
+    //
+
+    /**
+     * Calculates shift between two images, i. e. how to shift the second image to correspond it with the
+     *     first.
+     *
+     *     @param img0 first image
+     *     @param img1 second image
+     * @return automatically generated
+     */
+    public Point calculateShift(Mat img0, Mat img1) {
+        return new Point(calculateShift_0(nativeObj, img0.nativeObj, img1.nativeObj));
+    }
+
+
+    //
+    // C++:  bool cv::AlignMTB::getCut()
+    //
+
+    public boolean getCut() {
+        return getCut_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::AlignMTB::getExcludeRange()
+    //
+
+    public int getExcludeRange() {
+        return getExcludeRange_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::AlignMTB::getMaxBits()
+    //
+
+    public int getMaxBits() {
+        return getMaxBits_0(nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::AlignMTB::computeBitmaps(Mat img, Mat& tb, Mat& eb)
+    //
+
+    /**
+     * Computes median threshold and exclude bitmaps of given image.
+     *
+     *     @param img input image
+     *     @param tb median threshold bitmap
+     *     @param eb exclude bitmap
+     */
+    public void computeBitmaps(Mat img, Mat tb, Mat eb) {
+        computeBitmaps_0(nativeObj, img.nativeObj, tb.nativeObj, eb.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::AlignMTB::process(vector_Mat src, vector_Mat dst, Mat times, Mat response)
+    //
+
+    public void process(List<Mat> src, List<Mat> dst, Mat times, Mat response) {
+        Mat src_mat = Converters.vector_Mat_to_Mat(src);
+        Mat dst_mat = Converters.vector_Mat_to_Mat(dst);
+        process_0(nativeObj, src_mat.nativeObj, dst_mat.nativeObj, times.nativeObj, response.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::AlignMTB::process(vector_Mat src, vector_Mat dst)
+    //
+
+    /**
+     * Short version of process, that doesn't take extra arguments.
+     *
+     *     @param src vector of input images
+     *     @param dst vector of aligned images
+     */
+    public void process(List<Mat> src, List<Mat> dst) {
+        Mat src_mat = Converters.vector_Mat_to_Mat(src);
+        Mat dst_mat = Converters.vector_Mat_to_Mat(dst);
+        process_1(nativeObj, src_mat.nativeObj, dst_mat.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::AlignMTB::setCut(bool value)
+    //
+
+    public void setCut(boolean value) {
+        setCut_0(nativeObj, value);
+    }
+
+
+    //
+    // C++:  void cv::AlignMTB::setExcludeRange(int exclude_range)
+    //
+
+    public void setExcludeRange(int exclude_range) {
+        setExcludeRange_0(nativeObj, exclude_range);
+    }
+
+
+    //
+    // C++:  void cv::AlignMTB::setMaxBits(int max_bits)
+    //
+
+    public void setMaxBits(int max_bits) {
+        setMaxBits_0(nativeObj, max_bits);
+    }
+
+
+    //
+    // C++:  void cv::AlignMTB::shiftMat(Mat src, Mat& dst, Point shift)
+    //
+
+    /**
+     * Helper function, that shift Mat filling new regions with zeros.
+     *
+     *     @param src input image
+     *     @param dst result image
+     *     @param shift shift value
+     */
+    public void shiftMat(Mat src, Mat dst, Point shift) {
+        shiftMat_0(nativeObj, src.nativeObj, dst.nativeObj, shift.x, shift.y);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++:  Point cv::AlignMTB::calculateShift(Mat img0, Mat img1)
+    private static native double[] calculateShift_0(long nativeObj, long img0_nativeObj, long img1_nativeObj);
+
+    // C++:  bool cv::AlignMTB::getCut()
+    private static native boolean getCut_0(long nativeObj);
+
+    // C++:  int cv::AlignMTB::getExcludeRange()
+    private static native int getExcludeRange_0(long nativeObj);
+
+    // C++:  int cv::AlignMTB::getMaxBits()
+    private static native int getMaxBits_0(long nativeObj);
+
+    // C++:  void cv::AlignMTB::computeBitmaps(Mat img, Mat& tb, Mat& eb)
+    private static native void computeBitmaps_0(long nativeObj, long img_nativeObj, long tb_nativeObj, long eb_nativeObj);
+
+    // C++:  void cv::AlignMTB::process(vector_Mat src, vector_Mat dst, Mat times, Mat response)
+    private static native void process_0(long nativeObj, long src_mat_nativeObj, long dst_mat_nativeObj, long times_nativeObj, long response_nativeObj);
+
+    // C++:  void cv::AlignMTB::process(vector_Mat src, vector_Mat dst)
+    private static native void process_1(long nativeObj, long src_mat_nativeObj, long dst_mat_nativeObj);
+
+    // C++:  void cv::AlignMTB::setCut(bool value)
+    private static native void setCut_0(long nativeObj, boolean value);
+
+    // C++:  void cv::AlignMTB::setExcludeRange(int exclude_range)
+    private static native void setExcludeRange_0(long nativeObj, int exclude_range);
+
+    // C++:  void cv::AlignMTB::setMaxBits(int max_bits)
+    private static native void setMaxBits_0(long nativeObj, int max_bits);
+
+    // C++:  void cv::AlignMTB::shiftMat(Mat src, Mat& dst, Point shift)
+    private static native void shiftMat_0(long nativeObj, long src_nativeObj, long dst_nativeObj, double shift_x, double shift_y);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/photo/CalibrateCRF.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/photo/CalibrateCRF.java	(date 1605830248066)
+++ openCVLibrary3411/src/main/java/org/opencv/photo/CalibrateCRF.java	(date 1605830248066)
@@ -0,0 +1,53 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.photo;
+
+import java.util.ArrayList;
+import java.util.List;
+import org.opencv.core.Algorithm;
+import org.opencv.core.Mat;
+import org.opencv.utils.Converters;
+
+// C++: class CalibrateCRF
+/**
+ * The base class for camera response calibration algorithms.
+ */
+public class CalibrateCRF extends Algorithm {
+
+    protected CalibrateCRF(long addr) { super(addr); }
+
+    // internal usage only
+    public static CalibrateCRF __fromPtr__(long addr) { return new CalibrateCRF(addr); }
+
+    //
+    // C++:  void cv::CalibrateCRF::process(vector_Mat src, Mat& dst, Mat times)
+    //
+
+    /**
+     * Recovers inverse camera response.
+     *
+     *     @param src vector of input images
+     *     @param dst 256x1 matrix with inverse camera response function
+     *     @param times vector of exposure time values for each image
+     */
+    public void process(List<Mat> src, Mat dst, Mat times) {
+        Mat src_mat = Converters.vector_Mat_to_Mat(src);
+        process_0(nativeObj, src_mat.nativeObj, dst.nativeObj, times.nativeObj);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++:  void cv::CalibrateCRF::process(vector_Mat src, Mat& dst, Mat times)
+    private static native void process_0(long nativeObj, long src_mat_nativeObj, long dst_nativeObj, long times_nativeObj);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/photo/CalibrateDebevec.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/photo/CalibrateDebevec.java	(date 1605830248074)
+++ openCVLibrary3411/src/main/java/org/opencv/photo/CalibrateDebevec.java	(date 1605830248074)
@@ -0,0 +1,105 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.photo;
+
+import org.opencv.photo.CalibrateCRF;
+
+// C++: class CalibrateDebevec
+/**
+ * Inverse camera response function is extracted for each brightness value by minimizing an objective
+ * function as linear system. Objective function is constructed using pixel values on the same position
+ * in all images, extra term is added to make the result smoother.
+ *
+ * For more information see CITE: DM97 .
+ */
+public class CalibrateDebevec extends CalibrateCRF {
+
+    protected CalibrateDebevec(long addr) { super(addr); }
+
+    // internal usage only
+    public static CalibrateDebevec __fromPtr__(long addr) { return new CalibrateDebevec(addr); }
+
+    //
+    // C++:  bool cv::CalibrateDebevec::getRandom()
+    //
+
+    public boolean getRandom() {
+        return getRandom_0(nativeObj);
+    }
+
+
+    //
+    // C++:  float cv::CalibrateDebevec::getLambda()
+    //
+
+    public float getLambda() {
+        return getLambda_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::CalibrateDebevec::getSamples()
+    //
+
+    public int getSamples() {
+        return getSamples_0(nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::CalibrateDebevec::setLambda(float lambda)
+    //
+
+    public void setLambda(float lambda) {
+        setLambda_0(nativeObj, lambda);
+    }
+
+
+    //
+    // C++:  void cv::CalibrateDebevec::setRandom(bool random)
+    //
+
+    public void setRandom(boolean random) {
+        setRandom_0(nativeObj, random);
+    }
+
+
+    //
+    // C++:  void cv::CalibrateDebevec::setSamples(int samples)
+    //
+
+    public void setSamples(int samples) {
+        setSamples_0(nativeObj, samples);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++:  bool cv::CalibrateDebevec::getRandom()
+    private static native boolean getRandom_0(long nativeObj);
+
+    // C++:  float cv::CalibrateDebevec::getLambda()
+    private static native float getLambda_0(long nativeObj);
+
+    // C++:  int cv::CalibrateDebevec::getSamples()
+    private static native int getSamples_0(long nativeObj);
+
+    // C++:  void cv::CalibrateDebevec::setLambda(float lambda)
+    private static native void setLambda_0(long nativeObj, float lambda);
+
+    // C++:  void cv::CalibrateDebevec::setRandom(bool random)
+    private static native void setRandom_0(long nativeObj, boolean random);
+
+    // C++:  void cv::CalibrateDebevec::setSamples(int samples)
+    private static native void setSamples_0(long nativeObj, int samples);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/core/TermCriteria.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/core/TermCriteria.java	(date 1605830247456)
+++ openCVLibrary3411/src/main/java/org/opencv/core/TermCriteria.java	(date 1605830247456)
@@ -0,0 +1,92 @@
+package org.opencv.core;
+
+//javadoc:TermCriteria
+public class TermCriteria {
+
+    /**
+     * The maximum number of iterations or elements to compute
+     */
+    public static final int COUNT = 1;
+    /**
+     * The maximum number of iterations or elements to compute
+     */
+    public static final int MAX_ITER = COUNT;
+    /**
+     * The desired accuracy threshold or change in parameters at which the iterative algorithm is terminated.
+     */
+    public static final int EPS = 2;
+
+    public int type;
+    public int maxCount;
+    public double epsilon;
+
+    /**
+     * Termination criteria for iterative algorithms.
+     *
+     * @param type
+     *            the type of termination criteria: COUNT, EPS or COUNT + EPS.
+     * @param maxCount
+     *            the maximum number of iterations/elements.
+     * @param epsilon
+     *            the desired accuracy.
+     */
+    public TermCriteria(int type, int maxCount, double epsilon) {
+        this.type = type;
+        this.maxCount = maxCount;
+        this.epsilon = epsilon;
+    }
+
+    /**
+     * Termination criteria for iterative algorithms.
+     */
+    public TermCriteria() {
+        this(0, 0, 0.0);
+    }
+
+    public TermCriteria(double[] vals) {
+        set(vals);
+    }
+
+    public void set(double[] vals) {
+        if (vals != null) {
+            type = vals.length > 0 ? (int) vals[0] : 0;
+            maxCount = vals.length > 1 ? (int) vals[1] : 0;
+            epsilon = vals.length > 2 ? (double) vals[2] : 0;
+        } else {
+            type = 0;
+            maxCount = 0;
+            epsilon = 0;
+        }
+    }
+
+    public TermCriteria clone() {
+        return new TermCriteria(type, maxCount, epsilon);
+    }
+
+    @Override
+    public int hashCode() {
+        final int prime = 31;
+        int result = 1;
+        long temp;
+        temp = Double.doubleToLongBits(type);
+        result = prime * result + (int) (temp ^ (temp >>> 32));
+        temp = Double.doubleToLongBits(maxCount);
+        result = prime * result + (int) (temp ^ (temp >>> 32));
+        temp = Double.doubleToLongBits(epsilon);
+        result = prime * result + (int) (temp ^ (temp >>> 32));
+        return result;
+    }
+
+    @Override
+    public boolean equals(Object obj) {
+        if (this == obj) return true;
+        if (!(obj instanceof TermCriteria)) return false;
+        TermCriteria it = (TermCriteria) obj;
+        return type == it.type && maxCount == it.maxCount && epsilon == it.epsilon;
+    }
+
+    @Override
+    public String toString() {
+        return "{ type: " + type + ", maxCount: " + maxCount + ", epsilon: " + epsilon + "}";
+    }
+}
Index: openCVLibrary3411/src/main/java/org/opencv/features2d/SIFT.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/features2d/SIFT.java	(date 1605830247698)
+++ openCVLibrary3411/src/main/java/org/opencv/features2d/SIFT.java	(date 1605830247698)
@@ -0,0 +1,197 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.features2d;
+
+import org.opencv.features2d.Feature2D;
+import org.opencv.features2d.SIFT;
+
+// C++: class SIFT
+/**
+ * Class for extracting keypoints and computing descriptors using the Scale Invariant Feature Transform
+ * (SIFT) algorithm by D. Lowe CITE: Lowe04 .
+ */
+public class SIFT extends Feature2D {
+
+    protected SIFT(long addr) { super(addr); }
+
+    // internal usage only
+    public static SIFT __fromPtr__(long addr) { return new SIFT(addr); }
+
+    //
+    // C++: static Ptr_SIFT cv::SIFT::create(int nfeatures = 0, int nOctaveLayers = 3, double contrastThreshold = 0.04, double edgeThreshold = 10, double sigma = 1.6)
+    //
+
+    /**
+     * @param nfeatures The number of best features to retain. The features are ranked by their scores
+     *     (measured in SIFT algorithm as the local contrast)
+     *
+     *     @param nOctaveLayers The number of layers in each octave. 3 is the value used in D. Lowe paper. The
+     *     number of octaves is computed automatically from the image resolution.
+     *
+     *     @param contrastThreshold The contrast threshold used to filter out weak features in semi-uniform
+     *     (low-contrast) regions. The larger the threshold, the less features are produced by the detector.
+     *
+     *     <b>Note:</b> The contrast threshold will be divided by nOctaveLayers when the filtering is applied. When
+     *     nOctaveLayers is set to default and if you want to use the value used in D. Lowe paper, 0.03, set
+     *     this argument to 0.09.
+     *
+     *     @param edgeThreshold The threshold used to filter out edge-like features. Note that the its meaning
+     *     is different from the contrastThreshold, i.e. the larger the edgeThreshold, the less features are
+     *     filtered out (more features are retained).
+     *
+     *     @param sigma The sigma of the Gaussian applied to the input image at the octave \#0. If your image
+     *     is captured with a weak camera with soft lenses, you might want to reduce the number.
+     * @return automatically generated
+     */
+    public static SIFT create(int nfeatures, int nOctaveLayers, double contrastThreshold, double edgeThreshold, double sigma) {
+        return SIFT.__fromPtr__(create_0(nfeatures, nOctaveLayers, contrastThreshold, edgeThreshold, sigma));
+    }
+
+    /**
+     * @param nfeatures The number of best features to retain. The features are ranked by their scores
+     *     (measured in SIFT algorithm as the local contrast)
+     *
+     *     @param nOctaveLayers The number of layers in each octave. 3 is the value used in D. Lowe paper. The
+     *     number of octaves is computed automatically from the image resolution.
+     *
+     *     @param contrastThreshold The contrast threshold used to filter out weak features in semi-uniform
+     *     (low-contrast) regions. The larger the threshold, the less features are produced by the detector.
+     *
+     *     <b>Note:</b> The contrast threshold will be divided by nOctaveLayers when the filtering is applied. When
+     *     nOctaveLayers is set to default and if you want to use the value used in D. Lowe paper, 0.03, set
+     *     this argument to 0.09.
+     *
+     *     @param edgeThreshold The threshold used to filter out edge-like features. Note that the its meaning
+     *     is different from the contrastThreshold, i.e. the larger the edgeThreshold, the less features are
+     *     filtered out (more features are retained).
+     *
+     *     is captured with a weak camera with soft lenses, you might want to reduce the number.
+     * @return automatically generated
+     */
+    public static SIFT create(int nfeatures, int nOctaveLayers, double contrastThreshold, double edgeThreshold) {
+        return SIFT.__fromPtr__(create_1(nfeatures, nOctaveLayers, contrastThreshold, edgeThreshold));
+    }
+
+    /**
+     * @param nfeatures The number of best features to retain. The features are ranked by their scores
+     *     (measured in SIFT algorithm as the local contrast)
+     *
+     *     @param nOctaveLayers The number of layers in each octave. 3 is the value used in D. Lowe paper. The
+     *     number of octaves is computed automatically from the image resolution.
+     *
+     *     @param contrastThreshold The contrast threshold used to filter out weak features in semi-uniform
+     *     (low-contrast) regions. The larger the threshold, the less features are produced by the detector.
+     *
+     *     <b>Note:</b> The contrast threshold will be divided by nOctaveLayers when the filtering is applied. When
+     *     nOctaveLayers is set to default and if you want to use the value used in D. Lowe paper, 0.03, set
+     *     this argument to 0.09.
+     *
+     *     is different from the contrastThreshold, i.e. the larger the edgeThreshold, the less features are
+     *     filtered out (more features are retained).
+     *
+     *     is captured with a weak camera with soft lenses, you might want to reduce the number.
+     * @return automatically generated
+     */
+    public static SIFT create(int nfeatures, int nOctaveLayers, double contrastThreshold) {
+        return SIFT.__fromPtr__(create_2(nfeatures, nOctaveLayers, contrastThreshold));
+    }
+
+    /**
+     * @param nfeatures The number of best features to retain. The features are ranked by their scores
+     *     (measured in SIFT algorithm as the local contrast)
+     *
+     *     @param nOctaveLayers The number of layers in each octave. 3 is the value used in D. Lowe paper. The
+     *     number of octaves is computed automatically from the image resolution.
+     *
+     *     (low-contrast) regions. The larger the threshold, the less features are produced by the detector.
+     *
+     *     <b>Note:</b> The contrast threshold will be divided by nOctaveLayers when the filtering is applied. When
+     *     nOctaveLayers is set to default and if you want to use the value used in D. Lowe paper, 0.03, set
+     *     this argument to 0.09.
+     *
+     *     is different from the contrastThreshold, i.e. the larger the edgeThreshold, the less features are
+     *     filtered out (more features are retained).
+     *
+     *     is captured with a weak camera with soft lenses, you might want to reduce the number.
+     * @return automatically generated
+     */
+    public static SIFT create(int nfeatures, int nOctaveLayers) {
+        return SIFT.__fromPtr__(create_3(nfeatures, nOctaveLayers));
+    }
+
+    /**
+     * @param nfeatures The number of best features to retain. The features are ranked by their scores
+     *     (measured in SIFT algorithm as the local contrast)
+     *
+     *     number of octaves is computed automatically from the image resolution.
+     *
+     *     (low-contrast) regions. The larger the threshold, the less features are produced by the detector.
+     *
+     *     <b>Note:</b> The contrast threshold will be divided by nOctaveLayers when the filtering is applied. When
+     *     nOctaveLayers is set to default and if you want to use the value used in D. Lowe paper, 0.03, set
+     *     this argument to 0.09.
+     *
+     *     is different from the contrastThreshold, i.e. the larger the edgeThreshold, the less features are
+     *     filtered out (more features are retained).
+     *
+     *     is captured with a weak camera with soft lenses, you might want to reduce the number.
+     * @return automatically generated
+     */
+    public static SIFT create(int nfeatures) {
+        return SIFT.__fromPtr__(create_4(nfeatures));
+    }
+
+    /**
+     *     (measured in SIFT algorithm as the local contrast)
+     *
+     *     number of octaves is computed automatically from the image resolution.
+     *
+     *     (low-contrast) regions. The larger the threshold, the less features are produced by the detector.
+     *
+     *     <b>Note:</b> The contrast threshold will be divided by nOctaveLayers when the filtering is applied. When
+     *     nOctaveLayers is set to default and if you want to use the value used in D. Lowe paper, 0.03, set
+     *     this argument to 0.09.
+     *
+     *     is different from the contrastThreshold, i.e. the larger the edgeThreshold, the less features are
+     *     filtered out (more features are retained).
+     *
+     *     is captured with a weak camera with soft lenses, you might want to reduce the number.
+     * @return automatically generated
+     */
+    public static SIFT create() {
+        return SIFT.__fromPtr__(create_5());
+    }
+
+
+    //
+    // C++:  String cv::SIFT::getDefaultName()
+    //
+
+    public String getDefaultName() {
+        return getDefaultName_0(nativeObj);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++: static Ptr_SIFT cv::SIFT::create(int nfeatures = 0, int nOctaveLayers = 3, double contrastThreshold = 0.04, double edgeThreshold = 10, double sigma = 1.6)
+    private static native long create_0(int nfeatures, int nOctaveLayers, double contrastThreshold, double edgeThreshold, double sigma);
+    private static native long create_1(int nfeatures, int nOctaveLayers, double contrastThreshold, double edgeThreshold);
+    private static native long create_2(int nfeatures, int nOctaveLayers, double contrastThreshold);
+    private static native long create_3(int nfeatures, int nOctaveLayers);
+    private static native long create_4(int nfeatures);
+    private static native long create_5();
+
+    // C++:  String cv::SIFT::getDefaultName()
+    private static native String getDefaultName_0(long nativeObj);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/core/TickMeter.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/core/TickMeter.java	(date 1605830247458)
+++ openCVLibrary3411/src/main/java/org/opencv/core/TickMeter.java	(date 1605830247458)
@@ -0,0 +1,185 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.core;
+
+
+
+// C++: class TickMeter
+/**
+ * a Class to measure passing time.
+ *
+ * The class computes passing time by counting the number of ticks per second. That is, the following code computes the
+ * execution time in seconds:
+ * SNIPPET: snippets/core_various.cpp TickMeter_total
+ *
+ * It is also possible to compute the average time over multiple runs:
+ * SNIPPET: snippets/core_various.cpp TickMeter_average
+ *
+ * SEE: getTickCount, getTickFrequency
+ */
+public class TickMeter {
+
+    protected final long nativeObj;
+    protected TickMeter(long addr) { nativeObj = addr; }
+
+    public long getNativeObjAddr() { return nativeObj; }
+
+    // internal usage only
+    public static TickMeter __fromPtr__(long addr) { return new TickMeter(addr); }
+
+    //
+    // C++:   cv::TickMeter::TickMeter()
+    //
+
+    public TickMeter() {
+        nativeObj = TickMeter_0();
+    }
+
+
+    //
+    // C++:  double cv::TickMeter::getAvgTimeMilli()
+    //
+
+    public double getAvgTimeMilli() {
+        return getAvgTimeMilli_0(nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::TickMeter::getAvgTimeSec()
+    //
+
+    public double getAvgTimeSec() {
+        return getAvgTimeSec_0(nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::TickMeter::getFPS()
+    //
+
+    public double getFPS() {
+        return getFPS_0(nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::TickMeter::getTimeMicro()
+    //
+
+    public double getTimeMicro() {
+        return getTimeMicro_0(nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::TickMeter::getTimeMilli()
+    //
+
+    public double getTimeMilli() {
+        return getTimeMilli_0(nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::TickMeter::getTimeSec()
+    //
+
+    public double getTimeSec() {
+        return getTimeSec_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int64 cv::TickMeter::getCounter()
+    //
+
+    public long getCounter() {
+        return getCounter_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int64 cv::TickMeter::getTimeTicks()
+    //
+
+    public long getTimeTicks() {
+        return getTimeTicks_0(nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::TickMeter::reset()
+    //
+
+    public void reset() {
+        reset_0(nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::TickMeter::start()
+    //
+
+    public void start() {
+        start_0(nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::TickMeter::stop()
+    //
+
+    public void stop() {
+        stop_0(nativeObj);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++:   cv::TickMeter::TickMeter()
+    private static native long TickMeter_0();
+
+    // C++:  double cv::TickMeter::getAvgTimeMilli()
+    private static native double getAvgTimeMilli_0(long nativeObj);
+
+    // C++:  double cv::TickMeter::getAvgTimeSec()
+    private static native double getAvgTimeSec_0(long nativeObj);
+
+    // C++:  double cv::TickMeter::getFPS()
+    private static native double getFPS_0(long nativeObj);
+
+    // C++:  double cv::TickMeter::getTimeMicro()
+    private static native double getTimeMicro_0(long nativeObj);
+
+    // C++:  double cv::TickMeter::getTimeMilli()
+    private static native double getTimeMilli_0(long nativeObj);
+
+    // C++:  double cv::TickMeter::getTimeSec()
+    private static native double getTimeSec_0(long nativeObj);
+
+    // C++:  int64 cv::TickMeter::getCounter()
+    private static native long getCounter_0(long nativeObj);
+
+    // C++:  int64 cv::TickMeter::getTimeTicks()
+    private static native long getTimeTicks_0(long nativeObj);
+
+    // C++:  void cv::TickMeter::reset()
+    private static native void reset_0(long nativeObj);
+
+    // C++:  void cv::TickMeter::start()
+    private static native void start_0(long nativeObj);
+
+    // C++:  void cv::TickMeter::stop()
+    private static native void stop_0(long nativeObj);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/features2d/SimpleBlobDetector.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/features2d/SimpleBlobDetector.java	(date 1605830247700)
+++ openCVLibrary3411/src/main/java/org/opencv/features2d/SimpleBlobDetector.java	(date 1605830247700)
@@ -0,0 +1,95 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.features2d;
+
+import org.opencv.features2d.Feature2D;
+import org.opencv.features2d.SimpleBlobDetector;
+
+// C++: class SimpleBlobDetector
+/**
+ * Class for extracting blobs from an image. :
+ *
+ * The class implements a simple algorithm for extracting blobs from an image:
+ *
+ * 1.  Convert the source image to binary images by applying thresholding with several thresholds from
+ *     minThreshold (inclusive) to maxThreshold (exclusive) with distance thresholdStep between
+ *     neighboring thresholds.
+ * 2.  Extract connected components from every binary image by findContours and calculate their
+ *     centers.
+ * 3.  Group centers from several binary images by their coordinates. Close centers form one group that
+ *     corresponds to one blob, which is controlled by the minDistBetweenBlobs parameter.
+ * 4.  From the groups, estimate final centers of blobs and their radiuses and return as locations and
+ *     sizes of keypoints.
+ *
+ * This class performs several filtrations of returned blobs. You should set filterBy\* to true/false
+ * to turn on/off corresponding filtration. Available filtrations:
+ *
+ * <ul>
+ *   <li>
+ *    <b>By color</b>. This filter compares the intensity of a binary image at the center of a blob to
+ * blobColor. If they differ, the blob is filtered out. Use blobColor = 0 to extract dark blobs
+ * and blobColor = 255 to extract light blobs.
+ *   </li>
+ *   <li>
+ *    <b>By area</b>. Extracted blobs have an area between minArea (inclusive) and maxArea (exclusive).
+ *   </li>
+ *   <li>
+ *    <b>By circularity</b>. Extracted blobs have circularity
+ * (\(\frac{4*\pi*Area}{perimeter * perimeter}\)) between minCircularity (inclusive) and
+ * maxCircularity (exclusive).
+ *   </li>
+ *   <li>
+ *    <b>By ratio of the minimum inertia to maximum inertia</b>. Extracted blobs have this ratio
+ * between minInertiaRatio (inclusive) and maxInertiaRatio (exclusive).
+ *   </li>
+ *   <li>
+ *    <b>By convexity</b>. Extracted blobs have convexity (area / area of blob convex hull) between
+ * minConvexity (inclusive) and maxConvexity (exclusive).
+ *   </li>
+ * </ul>
+ *
+ * Default values of parameters are tuned to extract dark circular blobs.
+ */
+public class SimpleBlobDetector extends Feature2D {
+
+    protected SimpleBlobDetector(long addr) { super(addr); }
+
+    // internal usage only
+    public static SimpleBlobDetector __fromPtr__(long addr) { return new SimpleBlobDetector(addr); }
+
+    //
+    // C++: static Ptr_SimpleBlobDetector cv::SimpleBlobDetector::create(SimpleBlobDetector_Params parameters = SimpleBlobDetector::Params())
+    //
+
+    public static SimpleBlobDetector create() {
+        return SimpleBlobDetector.__fromPtr__(create_0());
+    }
+
+
+    //
+    // C++:  String cv::SimpleBlobDetector::getDefaultName()
+    //
+
+    public String getDefaultName() {
+        return getDefaultName_0(nativeObj);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++: static Ptr_SimpleBlobDetector cv::SimpleBlobDetector::create(SimpleBlobDetector_Params parameters = SimpleBlobDetector::Params())
+    private static native long create_0();
+
+    // C++:  String cv::SimpleBlobDetector::getDefaultName()
+    private static native String getDefaultName_0(long nativeObj);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/osgi/OpenCVInterface.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/osgi/OpenCVInterface.java	(date 1605830248018)
+++ openCVLibrary3411/src/main/java/org/opencv/osgi/OpenCVInterface.java	(date 1605830248018)
@@ -0,0 +1,8 @@
+package org.opencv.osgi;
+
+/**
+ * Dummy interface to allow some integration testing within OSGi implementation.
+ */
+public interface OpenCVInterface
+{
+}
Index: openCVLibrary3411/src/main/java/org/opencv/osgi/OpenCVNativeLoader.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/osgi/OpenCVNativeLoader.java	(date 1605830248020)
+++ openCVLibrary3411/src/main/java/org/opencv/osgi/OpenCVNativeLoader.java	(date 1605830248020)
@@ -0,0 +1,18 @@
+package org.opencv.osgi;
+
+import java.util.logging.Level;
+import java.util.logging.Logger;
+
+/**
+ * This class is intended to provide a convenient way to load OpenCV's native
+ * library from the Java bundle. If Blueprint is enabled in the OSGi container
+ * this class will be instantiated automatically and the init() method called
+ * loading the native library.
+ */
+public class OpenCVNativeLoader implements OpenCVInterface {
+
+    public void init() {
+        System.loadLibrary("opencv_java3");
+        Logger.getLogger("org.opencv.osgi").log(Level.INFO, "Successfully loaded OpenCV native library.");
+    }
+}
Index: openCVLibrary3411/src/main/java/org/opencv/core/Rect2d.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/core/Rect2d.java	(date 1605830247436)
+++ openCVLibrary3411/src/main/java/org/opencv/core/Rect2d.java	(date 1605830247436)
@@ -0,0 +1,104 @@
+package org.opencv.core;
+
+//javadoc:Rect2d_
+public class Rect2d {
+
+    public double x, y, width, height;
+
+    public Rect2d(double x, double y, double width, double height) {
+        this.x = x;
+        this.y = y;
+        this.width = width;
+        this.height = height;
+    }
+
+    public Rect2d() {
+        this(0, 0, 0, 0);
+    }
+
+    public Rect2d(Point p1, Point p2) {
+        x = (double) (p1.x < p2.x ? p1.x : p2.x);
+        y = (double) (p1.y < p2.y ? p1.y : p2.y);
+        width = (double) (p1.x > p2.x ? p1.x : p2.x) - x;
+        height = (double) (p1.y > p2.y ? p1.y : p2.y) - y;
+    }
+
+    public Rect2d(Point p, Size s) {
+        this((double) p.x, (double) p.y, (double) s.width, (double) s.height);
+    }
+
+    public Rect2d(double[] vals) {
+        set(vals);
+    }
+
+    public void set(double[] vals) {
+        if (vals != null) {
+            x = vals.length > 0 ? (double) vals[0] : 0;
+            y = vals.length > 1 ? (double) vals[1] : 0;
+            width = vals.length > 2 ? (double) vals[2] : 0;
+            height = vals.length > 3 ? (double) vals[3] : 0;
+        } else {
+            x = 0;
+            y = 0;
+            width = 0;
+            height = 0;
+        }
+    }
+
+    public Rect2d clone() {
+        return new Rect2d(x, y, width, height);
+    }
+
+    public Point tl() {
+        return new Point(x, y);
+    }
+
+    public Point br() {
+        return new Point(x + width, y + height);
+    }
+
+    public Size size() {
+        return new Size(width, height);
+    }
+
+    public double area() {
+        return width * height;
+    }
+
+    public boolean empty() {
+        return width <= 0 || height <= 0;
+    }
+
+    public boolean contains(Point p) {
+        return x <= p.x && p.x < x + width && y <= p.y && p.y < y + height;
+    }
+
+    @Override
+    public int hashCode() {
+        final int prime = 31;
+        int result = 1;
+        long temp;
+        temp = Double.doubleToLongBits(height);
+        result = prime * result + (int) (temp ^ (temp >>> 32));
+        temp = Double.doubleToLongBits(width);
+        result = prime * result + (int) (temp ^ (temp >>> 32));
+        temp = Double.doubleToLongBits(x);
+        result = prime * result + (int) (temp ^ (temp >>> 32));
+        temp = Double.doubleToLongBits(y);
+        result = prime * result + (int) (temp ^ (temp >>> 32));
+        return result;
+    }
+
+    @Override
+    public boolean equals(Object obj) {
+        if (this == obj) return true;
+        if (!(obj instanceof Rect2d)) return false;
+        Rect2d it = (Rect2d) obj;
+        return x == it.x && y == it.y && width == it.width && height == it.height;
+    }
+
+    @Override
+    public String toString() {
+        return "{" + x + ", " + y + ", " + width + "x" + height + "}";
+    }
+}
Index: openCVLibrary3411/src/main/java/org/opencv/features2d/KAZE.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/features2d/KAZE.java	(date 1605830247678)
+++ openCVLibrary3411/src/main/java/org/opencv/features2d/KAZE.java	(date 1605830247678)
@@ -0,0 +1,303 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.features2d;
+
+import org.opencv.features2d.Feature2D;
+import org.opencv.features2d.KAZE;
+
+// C++: class KAZE
+/**
+ * Class implementing the KAZE keypoint detector and descriptor extractor, described in CITE: ABD12 .
+ *
+ * <b>Note:</b> AKAZE descriptor can only be used with KAZE or AKAZE keypoints .. [ABD12] KAZE Features. Pablo
+ * F. Alcantarilla, Adrien Bartoli and Andrew J. Davison. In European Conference on Computer Vision
+ * (ECCV), Fiorenze, Italy, October 2012.
+ */
+public class KAZE extends Feature2D {
+
+    protected KAZE(long addr) { super(addr); }
+
+    // internal usage only
+    public static KAZE __fromPtr__(long addr) { return new KAZE(addr); }
+
+    // C++: enum <unnamed>
+    public static final int
+            DIFF_PM_G1 = 0,
+            DIFF_PM_G2 = 1,
+            DIFF_WEICKERT = 2,
+            DIFF_CHARBONNIER = 3;
+
+
+    //
+    // C++: static Ptr_KAZE cv::KAZE::create(bool extended = false, bool upright = false, float threshold = 0.001f, int nOctaves = 4, int nOctaveLayers = 4, int diffusivity = KAZE::DIFF_PM_G2)
+    //
+
+    /**
+     * The KAZE constructor
+     *
+     *     @param extended Set to enable extraction of extended (128-byte) descriptor.
+     *     @param upright Set to enable use of upright descriptors (non rotation-invariant).
+     *     @param threshold Detector response threshold to accept point
+     *     @param nOctaves Maximum octave evolution of the image
+     *     @param nOctaveLayers Default number of sublevels per scale level
+     *     @param diffusivity Diffusivity type. DIFF_PM_G1, DIFF_PM_G2, DIFF_WEICKERT or
+     *     DIFF_CHARBONNIER
+     * @return automatically generated
+     */
+    public static KAZE create(boolean extended, boolean upright, float threshold, int nOctaves, int nOctaveLayers, int diffusivity) {
+        return KAZE.__fromPtr__(create_0(extended, upright, threshold, nOctaves, nOctaveLayers, diffusivity));
+    }
+
+    /**
+     * The KAZE constructor
+     *
+     *     @param extended Set to enable extraction of extended (128-byte) descriptor.
+     *     @param upright Set to enable use of upright descriptors (non rotation-invariant).
+     *     @param threshold Detector response threshold to accept point
+     *     @param nOctaves Maximum octave evolution of the image
+     *     @param nOctaveLayers Default number of sublevels per scale level
+     *     DIFF_CHARBONNIER
+     * @return automatically generated
+     */
+    public static KAZE create(boolean extended, boolean upright, float threshold, int nOctaves, int nOctaveLayers) {
+        return KAZE.__fromPtr__(create_1(extended, upright, threshold, nOctaves, nOctaveLayers));
+    }
+
+    /**
+     * The KAZE constructor
+     *
+     *     @param extended Set to enable extraction of extended (128-byte) descriptor.
+     *     @param upright Set to enable use of upright descriptors (non rotation-invariant).
+     *     @param threshold Detector response threshold to accept point
+     *     @param nOctaves Maximum octave evolution of the image
+     *     DIFF_CHARBONNIER
+     * @return automatically generated
+     */
+    public static KAZE create(boolean extended, boolean upright, float threshold, int nOctaves) {
+        return KAZE.__fromPtr__(create_2(extended, upright, threshold, nOctaves));
+    }
+
+    /**
+     * The KAZE constructor
+     *
+     *     @param extended Set to enable extraction of extended (128-byte) descriptor.
+     *     @param upright Set to enable use of upright descriptors (non rotation-invariant).
+     *     @param threshold Detector response threshold to accept point
+     *     DIFF_CHARBONNIER
+     * @return automatically generated
+     */
+    public static KAZE create(boolean extended, boolean upright, float threshold) {
+        return KAZE.__fromPtr__(create_3(extended, upright, threshold));
+    }
+
+    /**
+     * The KAZE constructor
+     *
+     *     @param extended Set to enable extraction of extended (128-byte) descriptor.
+     *     @param upright Set to enable use of upright descriptors (non rotation-invariant).
+     *     DIFF_CHARBONNIER
+     * @return automatically generated
+     */
+    public static KAZE create(boolean extended, boolean upright) {
+        return KAZE.__fromPtr__(create_4(extended, upright));
+    }
+
+    /**
+     * The KAZE constructor
+     *
+     *     @param extended Set to enable extraction of extended (128-byte) descriptor.
+     *     DIFF_CHARBONNIER
+     * @return automatically generated
+     */
+    public static KAZE create(boolean extended) {
+        return KAZE.__fromPtr__(create_5(extended));
+    }
+
+    /**
+     * The KAZE constructor
+     *
+     *     DIFF_CHARBONNIER
+     * @return automatically generated
+     */
+    public static KAZE create() {
+        return KAZE.__fromPtr__(create_6());
+    }
+
+
+    //
+    // C++:  String cv::KAZE::getDefaultName()
+    //
+
+    public String getDefaultName() {
+        return getDefaultName_0(nativeObj);
+    }
+
+
+    //
+    // C++:  bool cv::KAZE::getExtended()
+    //
+
+    public boolean getExtended() {
+        return getExtended_0(nativeObj);
+    }
+
+
+    //
+    // C++:  bool cv::KAZE::getUpright()
+    //
+
+    public boolean getUpright() {
+        return getUpright_0(nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::KAZE::getThreshold()
+    //
+
+    public double getThreshold() {
+        return getThreshold_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::KAZE::getDiffusivity()
+    //
+
+    public int getDiffusivity() {
+        return getDiffusivity_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::KAZE::getNOctaveLayers()
+    //
+
+    public int getNOctaveLayers() {
+        return getNOctaveLayers_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::KAZE::getNOctaves()
+    //
+
+    public int getNOctaves() {
+        return getNOctaves_0(nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::KAZE::setDiffusivity(int diff)
+    //
+
+    public void setDiffusivity(int diff) {
+        setDiffusivity_0(nativeObj, diff);
+    }
+
+
+    //
+    // C++:  void cv::KAZE::setExtended(bool extended)
+    //
+
+    public void setExtended(boolean extended) {
+        setExtended_0(nativeObj, extended);
+    }
+
+
+    //
+    // C++:  void cv::KAZE::setNOctaveLayers(int octaveLayers)
+    //
+
+    public void setNOctaveLayers(int octaveLayers) {
+        setNOctaveLayers_0(nativeObj, octaveLayers);
+    }
+
+
+    //
+    // C++:  void cv::KAZE::setNOctaves(int octaves)
+    //
+
+    public void setNOctaves(int octaves) {
+        setNOctaves_0(nativeObj, octaves);
+    }
+
+
+    //
+    // C++:  void cv::KAZE::setThreshold(double threshold)
+    //
+
+    public void setThreshold(double threshold) {
+        setThreshold_0(nativeObj, threshold);
+    }
+
+
+    //
+    // C++:  void cv::KAZE::setUpright(bool upright)
+    //
+
+    public void setUpright(boolean upright) {
+        setUpright_0(nativeObj, upright);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++: static Ptr_KAZE cv::KAZE::create(bool extended = false, bool upright = false, float threshold = 0.001f, int nOctaves = 4, int nOctaveLayers = 4, int diffusivity = KAZE::DIFF_PM_G2)
+    private static native long create_0(boolean extended, boolean upright, float threshold, int nOctaves, int nOctaveLayers, int diffusivity);
+    private static native long create_1(boolean extended, boolean upright, float threshold, int nOctaves, int nOctaveLayers);
+    private static native long create_2(boolean extended, boolean upright, float threshold, int nOctaves);
+    private static native long create_3(boolean extended, boolean upright, float threshold);
+    private static native long create_4(boolean extended, boolean upright);
+    private static native long create_5(boolean extended);
+    private static native long create_6();
+
+    // C++:  String cv::KAZE::getDefaultName()
+    private static native String getDefaultName_0(long nativeObj);
+
+    // C++:  bool cv::KAZE::getExtended()
+    private static native boolean getExtended_0(long nativeObj);
+
+    // C++:  bool cv::KAZE::getUpright()
+    private static native boolean getUpright_0(long nativeObj);
+
+    // C++:  double cv::KAZE::getThreshold()
+    private static native double getThreshold_0(long nativeObj);
+
+    // C++:  int cv::KAZE::getDiffusivity()
+    private static native int getDiffusivity_0(long nativeObj);
+
+    // C++:  int cv::KAZE::getNOctaveLayers()
+    private static native int getNOctaveLayers_0(long nativeObj);
+
+    // C++:  int cv::KAZE::getNOctaves()
+    private static native int getNOctaves_0(long nativeObj);
+
+    // C++:  void cv::KAZE::setDiffusivity(int diff)
+    private static native void setDiffusivity_0(long nativeObj, int diff);
+
+    // C++:  void cv::KAZE::setExtended(bool extended)
+    private static native void setExtended_0(long nativeObj, boolean extended);
+
+    // C++:  void cv::KAZE::setNOctaveLayers(int octaveLayers)
+    private static native void setNOctaveLayers_0(long nativeObj, int octaveLayers);
+
+    // C++:  void cv::KAZE::setNOctaves(int octaves)
+    private static native void setNOctaves_0(long nativeObj, int octaves);
+
+    // C++:  void cv::KAZE::setThreshold(double threshold)
+    private static native void setThreshold_0(long nativeObj, double threshold);
+
+    // C++:  void cv::KAZE::setUpright(bool upright)
+    private static native void setUpright_0(long nativeObj, boolean upright);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/core/RotatedRect.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/core/RotatedRect.java	(date 1605830247452)
+++ openCVLibrary3411/src/main/java/org/opencv/core/RotatedRect.java	(date 1605830247452)
@@ -0,0 +1,113 @@
+package org.opencv.core;
+
+//javadoc:RotatedRect_
+public class RotatedRect {
+
+    public Point center;
+    public Size size;
+    public double angle;
+
+    public RotatedRect() {
+        this.center = new Point();
+        this.size = new Size();
+        this.angle = 0;
+    }
+
+    public RotatedRect(Point c, Size s, double a) {
+        this.center = c.clone();
+        this.size = s.clone();
+        this.angle = a;
+    }
+
+    public RotatedRect(double[] vals) {
+        this();
+        set(vals);
+    }
+
+    public void set(double[] vals) {
+        if (vals != null) {
+            center.x = vals.length > 0 ? (double) vals[0] : 0;
+            center.y = vals.length > 1 ? (double) vals[1] : 0;
+            size.width = vals.length > 2 ? (double) vals[2] : 0;
+            size.height = vals.length > 3 ? (double) vals[3] : 0;
+            angle = vals.length > 4 ? (double) vals[4] : 0;
+        } else {
+            center.x = 0;
+            center.y = 0;
+            size.width = 0;
+            size.height = 0;
+            angle = 0;
+        }
+    }
+
+    public void points(Point pt[])
+    {
+        double _angle = angle * Math.PI / 180.0;
+        double b = (double) Math.cos(_angle) * 0.5f;
+        double a = (double) Math.sin(_angle) * 0.5f;
+
+        pt[0] = new Point(
+                center.x - a * size.height - b * size.width,
+                center.y + b * size.height - a * size.width);
+
+        pt[1] = new Point(
+                center.x + a * size.height - b * size.width,
+                center.y - b * size.height - a * size.width);
+
+        pt[2] = new Point(
+                2 * center.x - pt[0].x,
+                2 * center.y - pt[0].y);
+
+        pt[3] = new Point(
+                2 * center.x - pt[1].x,
+                2 * center.y - pt[1].y);
+    }
+
+    public Rect boundingRect()
+    {
+        Point pt[] = new Point[4];
+        points(pt);
+        Rect r = new Rect((int) Math.floor(Math.min(Math.min(Math.min(pt[0].x, pt[1].x), pt[2].x), pt[3].x)),
+                (int) Math.floor(Math.min(Math.min(Math.min(pt[0].y, pt[1].y), pt[2].y), pt[3].y)),
+                (int) Math.ceil(Math.max(Math.max(Math.max(pt[0].x, pt[1].x), pt[2].x), pt[3].x)),
+                (int) Math.ceil(Math.max(Math.max(Math.max(pt[0].y, pt[1].y), pt[2].y), pt[3].y)));
+        r.width -= r.x - 1;
+        r.height -= r.y - 1;
+        return r;
+    }
+
+    public RotatedRect clone() {
+        return new RotatedRect(center, size, angle);
+    }
+
+    @Override
+    public int hashCode() {
+        final int prime = 31;
+        int result = 1;
+        long temp;
+        temp = Double.doubleToLongBits(center.x);
+        result = prime * result + (int) (temp ^ (temp >>> 32));
+        temp = Double.doubleToLongBits(center.y);
+        result = prime * result + (int) (temp ^ (temp >>> 32));
+        temp = Double.doubleToLongBits(size.width);
+        result = prime * result + (int) (temp ^ (temp >>> 32));
+        temp = Double.doubleToLongBits(size.height);
+        result = prime * result + (int) (temp ^ (temp >>> 32));
+        temp = Double.doubleToLongBits(angle);
+        result = prime * result + (int) (temp ^ (temp >>> 32));
+        return result;
+    }
+
+    @Override
+    public boolean equals(Object obj) {
+        if (this == obj) return true;
+        if (!(obj instanceof RotatedRect)) return false;
+        RotatedRect it = (RotatedRect) obj;
+        return center.equals(it.center) && size.equals(it.size) && angle == it.angle;
+    }
+
+    @Override
+    public String toString() {
+        return "{ " + center + " " + size + " * " + angle + " }";
+    }
+}
Index: openCVLibrary3411/src/main/java/org/opencv/features2d/MSER.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/features2d/MSER.java	(date 1605830247680)
+++ openCVLibrary3411/src/main/java/org/opencv/features2d/MSER.java	(date 1605830247680)
@@ -0,0 +1,349 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.features2d;
+
+import java.util.ArrayList;
+import java.util.List;
+import org.opencv.core.Mat;
+import org.opencv.core.MatOfPoint;
+import org.opencv.core.MatOfRect;
+import org.opencv.features2d.Feature2D;
+import org.opencv.features2d.MSER;
+import org.opencv.utils.Converters;
+
+// C++: class MSER
+/**
+ * Maximally stable extremal region extractor
+ *
+ * The class encapsulates all the parameters of the %MSER extraction algorithm (see [wiki
+ * article](http://en.wikipedia.org/wiki/Maximally_stable_extremal_regions)).
+ *
+ * <ul>
+ *   <li>
+ *  there are two different implementation of %MSER: one for grey image, one for color image
+ *   </li>
+ * </ul>
+ *
+ * <ul>
+ *   <li>
+ *  the grey image algorithm is taken from: CITE: nister2008linear ;  the paper claims to be faster
+ * than union-find method; it actually get 1.5~2m/s on my centrino L7200 1.2GHz laptop.
+ *   </li>
+ * </ul>
+ *
+ * <ul>
+ *   <li>
+ *  the color image algorithm is taken from: CITE: forssen2007maximally ; it should be much slower
+ * than grey image method ( 3~4 times ); the chi_table.h file is taken directly from paper's source
+ * code which is distributed under GPL.
+ *   </li>
+ * </ul>
+ *
+ * <ul>
+ *   <li>
+ *  (Python) A complete example showing the use of the %MSER detector can be found at samples/python/mser.py
+ *   </li>
+ * </ul>
+ */
+public class MSER extends Feature2D {
+
+    protected MSER(long addr) { super(addr); }
+
+    // internal usage only
+    public static MSER __fromPtr__(long addr) { return new MSER(addr); }
+
+    //
+    // C++: static Ptr_MSER cv::MSER::create(int _delta = 5, int _min_area = 60, int _max_area = 14400, double _max_variation = 0.25, double _min_diversity = .2, int _max_evolution = 200, double _area_threshold = 1.01, double _min_margin = 0.003, int _edge_blur_size = 5)
+    //
+
+    /**
+     * Full constructor for %MSER detector
+     *
+     *     @param _delta it compares \((size_{i}-size_{i-delta})/size_{i-delta}\)
+     *     @param _min_area prune the area which smaller than minArea
+     *     @param _max_area prune the area which bigger than maxArea
+     *     @param _max_variation prune the area have similar size to its children
+     *     @param _min_diversity for color image, trace back to cut off mser with diversity less than min_diversity
+     *     @param _max_evolution  for color image, the evolution steps
+     *     @param _area_threshold for color image, the area threshold to cause re-initialize
+     *     @param _min_margin for color image, ignore too small margin
+     *     @param _edge_blur_size for color image, the aperture size for edge blur
+     * @return automatically generated
+     */
+    public static MSER create(int _delta, int _min_area, int _max_area, double _max_variation, double _min_diversity, int _max_evolution, double _area_threshold, double _min_margin, int _edge_blur_size) {
+        return MSER.__fromPtr__(create_0(_delta, _min_area, _max_area, _max_variation, _min_diversity, _max_evolution, _area_threshold, _min_margin, _edge_blur_size));
+    }
+
+    /**
+     * Full constructor for %MSER detector
+     *
+     *     @param _delta it compares \((size_{i}-size_{i-delta})/size_{i-delta}\)
+     *     @param _min_area prune the area which smaller than minArea
+     *     @param _max_area prune the area which bigger than maxArea
+     *     @param _max_variation prune the area have similar size to its children
+     *     @param _min_diversity for color image, trace back to cut off mser with diversity less than min_diversity
+     *     @param _max_evolution  for color image, the evolution steps
+     *     @param _area_threshold for color image, the area threshold to cause re-initialize
+     *     @param _min_margin for color image, ignore too small margin
+     * @return automatically generated
+     */
+    public static MSER create(int _delta, int _min_area, int _max_area, double _max_variation, double _min_diversity, int _max_evolution, double _area_threshold, double _min_margin) {
+        return MSER.__fromPtr__(create_1(_delta, _min_area, _max_area, _max_variation, _min_diversity, _max_evolution, _area_threshold, _min_margin));
+    }
+
+    /**
+     * Full constructor for %MSER detector
+     *
+     *     @param _delta it compares \((size_{i}-size_{i-delta})/size_{i-delta}\)
+     *     @param _min_area prune the area which smaller than minArea
+     *     @param _max_area prune the area which bigger than maxArea
+     *     @param _max_variation prune the area have similar size to its children
+     *     @param _min_diversity for color image, trace back to cut off mser with diversity less than min_diversity
+     *     @param _max_evolution  for color image, the evolution steps
+     *     @param _area_threshold for color image, the area threshold to cause re-initialize
+     * @return automatically generated
+     */
+    public static MSER create(int _delta, int _min_area, int _max_area, double _max_variation, double _min_diversity, int _max_evolution, double _area_threshold) {
+        return MSER.__fromPtr__(create_2(_delta, _min_area, _max_area, _max_variation, _min_diversity, _max_evolution, _area_threshold));
+    }
+
+    /**
+     * Full constructor for %MSER detector
+     *
+     *     @param _delta it compares \((size_{i}-size_{i-delta})/size_{i-delta}\)
+     *     @param _min_area prune the area which smaller than minArea
+     *     @param _max_area prune the area which bigger than maxArea
+     *     @param _max_variation prune the area have similar size to its children
+     *     @param _min_diversity for color image, trace back to cut off mser with diversity less than min_diversity
+     *     @param _max_evolution  for color image, the evolution steps
+     * @return automatically generated
+     */
+    public static MSER create(int _delta, int _min_area, int _max_area, double _max_variation, double _min_diversity, int _max_evolution) {
+        return MSER.__fromPtr__(create_3(_delta, _min_area, _max_area, _max_variation, _min_diversity, _max_evolution));
+    }
+
+    /**
+     * Full constructor for %MSER detector
+     *
+     *     @param _delta it compares \((size_{i}-size_{i-delta})/size_{i-delta}\)
+     *     @param _min_area prune the area which smaller than minArea
+     *     @param _max_area prune the area which bigger than maxArea
+     *     @param _max_variation prune the area have similar size to its children
+     *     @param _min_diversity for color image, trace back to cut off mser with diversity less than min_diversity
+     * @return automatically generated
+     */
+    public static MSER create(int _delta, int _min_area, int _max_area, double _max_variation, double _min_diversity) {
+        return MSER.__fromPtr__(create_4(_delta, _min_area, _max_area, _max_variation, _min_diversity));
+    }
+
+    /**
+     * Full constructor for %MSER detector
+     *
+     *     @param _delta it compares \((size_{i}-size_{i-delta})/size_{i-delta}\)
+     *     @param _min_area prune the area which smaller than minArea
+     *     @param _max_area prune the area which bigger than maxArea
+     *     @param _max_variation prune the area have similar size to its children
+     * @return automatically generated
+     */
+    public static MSER create(int _delta, int _min_area, int _max_area, double _max_variation) {
+        return MSER.__fromPtr__(create_5(_delta, _min_area, _max_area, _max_variation));
+    }
+
+    /**
+     * Full constructor for %MSER detector
+     *
+     *     @param _delta it compares \((size_{i}-size_{i-delta})/size_{i-delta}\)
+     *     @param _min_area prune the area which smaller than minArea
+     *     @param _max_area prune the area which bigger than maxArea
+     * @return automatically generated
+     */
+    public static MSER create(int _delta, int _min_area, int _max_area) {
+        return MSER.__fromPtr__(create_6(_delta, _min_area, _max_area));
+    }
+
+    /**
+     * Full constructor for %MSER detector
+     *
+     *     @param _delta it compares \((size_{i}-size_{i-delta})/size_{i-delta}\)
+     *     @param _min_area prune the area which smaller than minArea
+     * @return automatically generated
+     */
+    public static MSER create(int _delta, int _min_area) {
+        return MSER.__fromPtr__(create_7(_delta, _min_area));
+    }
+
+    /**
+     * Full constructor for %MSER detector
+     *
+     *     @param _delta it compares \((size_{i}-size_{i-delta})/size_{i-delta}\)
+     * @return automatically generated
+     */
+    public static MSER create(int _delta) {
+        return MSER.__fromPtr__(create_8(_delta));
+    }
+
+    /**
+     * Full constructor for %MSER detector
+     *
+     * @return automatically generated
+     */
+    public static MSER create() {
+        return MSER.__fromPtr__(create_9());
+    }
+
+
+    //
+    // C++:  String cv::MSER::getDefaultName()
+    //
+
+    public String getDefaultName() {
+        return getDefaultName_0(nativeObj);
+    }
+
+
+    //
+    // C++:  bool cv::MSER::getPass2Only()
+    //
+
+    public boolean getPass2Only() {
+        return getPass2Only_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::MSER::getDelta()
+    //
+
+    public int getDelta() {
+        return getDelta_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::MSER::getMaxArea()
+    //
+
+    public int getMaxArea() {
+        return getMaxArea_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::MSER::getMinArea()
+    //
+
+    public int getMinArea() {
+        return getMinArea_0(nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::MSER::detectRegions(Mat image, vector_vector_Point& msers, vector_Rect& bboxes)
+    //
+
+    /**
+     * Detect %MSER regions
+     *
+     *     @param image input image (8UC1, 8UC3 or 8UC4, must be greater or equal than 3x3)
+     *     @param msers resulting list of point sets
+     *     @param bboxes resulting bounding boxes
+     */
+    public void detectRegions(Mat image, List<MatOfPoint> msers, MatOfRect bboxes) {
+        Mat msers_mat = new Mat();
+        Mat bboxes_mat = bboxes;
+        detectRegions_0(nativeObj, image.nativeObj, msers_mat.nativeObj, bboxes_mat.nativeObj);
+        Converters.Mat_to_vector_vector_Point(msers_mat, msers);
+        msers_mat.release();
+    }
+
+
+    //
+    // C++:  void cv::MSER::setDelta(int delta)
+    //
+
+    public void setDelta(int delta) {
+        setDelta_0(nativeObj, delta);
+    }
+
+
+    //
+    // C++:  void cv::MSER::setMaxArea(int maxArea)
+    //
+
+    public void setMaxArea(int maxArea) {
+        setMaxArea_0(nativeObj, maxArea);
+    }
+
+
+    //
+    // C++:  void cv::MSER::setMinArea(int minArea)
+    //
+
+    public void setMinArea(int minArea) {
+        setMinArea_0(nativeObj, minArea);
+    }
+
+
+    //
+    // C++:  void cv::MSER::setPass2Only(bool f)
+    //
+
+    public void setPass2Only(boolean f) {
+        setPass2Only_0(nativeObj, f);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++: static Ptr_MSER cv::MSER::create(int _delta = 5, int _min_area = 60, int _max_area = 14400, double _max_variation = 0.25, double _min_diversity = .2, int _max_evolution = 200, double _area_threshold = 1.01, double _min_margin = 0.003, int _edge_blur_size = 5)
+    private static native long create_0(int _delta, int _min_area, int _max_area, double _max_variation, double _min_diversity, int _max_evolution, double _area_threshold, double _min_margin, int _edge_blur_size);
+    private static native long create_1(int _delta, int _min_area, int _max_area, double _max_variation, double _min_diversity, int _max_evolution, double _area_threshold, double _min_margin);
+    private static native long create_2(int _delta, int _min_area, int _max_area, double _max_variation, double _min_diversity, int _max_evolution, double _area_threshold);
+    private static native long create_3(int _delta, int _min_area, int _max_area, double _max_variation, double _min_diversity, int _max_evolution);
+    private static native long create_4(int _delta, int _min_area, int _max_area, double _max_variation, double _min_diversity);
+    private static native long create_5(int _delta, int _min_area, int _max_area, double _max_variation);
+    private static native long create_6(int _delta, int _min_area, int _max_area);
+    private static native long create_7(int _delta, int _min_area);
+    private static native long create_8(int _delta);
+    private static native long create_9();
+
+    // C++:  String cv::MSER::getDefaultName()
+    private static native String getDefaultName_0(long nativeObj);
+
+    // C++:  bool cv::MSER::getPass2Only()
+    private static native boolean getPass2Only_0(long nativeObj);
+
+    // C++:  int cv::MSER::getDelta()
+    private static native int getDelta_0(long nativeObj);
+
+    // C++:  int cv::MSER::getMaxArea()
+    private static native int getMaxArea_0(long nativeObj);
+
+    // C++:  int cv::MSER::getMinArea()
+    private static native int getMinArea_0(long nativeObj);
+
+    // C++:  void cv::MSER::detectRegions(Mat image, vector_vector_Point& msers, vector_Rect& bboxes)
+    private static native void detectRegions_0(long nativeObj, long image_nativeObj, long msers_mat_nativeObj, long bboxes_mat_nativeObj);
+
+    // C++:  void cv::MSER::setDelta(int delta)
+    private static native void setDelta_0(long nativeObj, int delta);
+
+    // C++:  void cv::MSER::setMaxArea(int maxArea)
+    private static native void setMaxArea_0(long nativeObj, int maxArea);
+
+    // C++:  void cv::MSER::setMinArea(int minArea)
+    private static native void setMinArea_0(long nativeObj, int minArea);
+
+    // C++:  void cv::MSER::setPass2Only(bool f)
+    private static native void setPass2Only_0(long nativeObj, boolean f);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/core/Scalar.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/core/Scalar.java	(date 1605830247454)
+++ openCVLibrary3411/src/main/java/org/opencv/core/Scalar.java	(date 1605830247454)
@@ -0,0 +1,90 @@
+package org.opencv.core;
+
+//javadoc:Scalar_
+public class Scalar {
+
+    public double val[];
+
+    public Scalar(double v0, double v1, double v2, double v3) {
+        val = new double[] { v0, v1, v2, v3 };
+    }
+
+    public Scalar(double v0, double v1, double v2) {
+        val = new double[] { v0, v1, v2, 0 };
+    }
+
+    public Scalar(double v0, double v1) {
+        val = new double[] { v0, v1, 0, 0 };
+    }
+
+    public Scalar(double v0) {
+        val = new double[] { v0, 0, 0, 0 };
+    }
+
+    public Scalar(double[] vals) {
+        if (vals != null && vals.length == 4)
+            val = vals.clone();
+        else {
+            val = new double[4];
+            set(vals);
+        }
+    }
+
+    public void set(double[] vals) {
+        if (vals != null) {
+            val[0] = vals.length > 0 ? vals[0] : 0;
+            val[1] = vals.length > 1 ? vals[1] : 0;
+            val[2] = vals.length > 2 ? vals[2] : 0;
+            val[3] = vals.length > 3 ? vals[3] : 0;
+        } else
+            val[0] = val[1] = val[2] = val[3] = 0;
+    }
+
+    public static Scalar all(double v) {
+        return new Scalar(v, v, v, v);
+    }
+
+    public Scalar clone() {
+        return new Scalar(val);
+    }
+
+    public Scalar mul(Scalar it, double scale) {
+        return new Scalar(val[0] * it.val[0] * scale, val[1] * it.val[1] * scale,
+                val[2] * it.val[2] * scale, val[3] * it.val[3] * scale);
+    }
+
+    public Scalar mul(Scalar it) {
+        return mul(it, 1);
+    }
+
+    public Scalar conj() {
+        return new Scalar(val[0], -val[1], -val[2], -val[3]);
+    }
+
+    public boolean isReal() {
+        return val[1] == 0 && val[2] == 0 && val[3] == 0;
+    }
+
+    @Override
+    public int hashCode() {
+        final int prime = 31;
+        int result = 1;
+        result = prime * result + java.util.Arrays.hashCode(val);
+        return result;
+    }
+
+    @Override
+    public boolean equals(Object obj) {
+        if (this == obj) return true;
+        if (!(obj instanceof Scalar)) return false;
+        Scalar it = (Scalar) obj;
+        if (!java.util.Arrays.equals(val, it.val)) return false;
+        return true;
+    }
+
+    @Override
+    public String toString() {
+        return "[" + val[0] + ", " + val[1] + ", " + val[2] + ", " + val[3] + "]";
+    }
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/features2d/ORB.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/features2d/ORB.java	(date 1605830247693)
+++ openCVLibrary3411/src/main/java/org/opencv/features2d/ORB.java	(date 1605830247693)
@@ -0,0 +1,612 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.features2d;
+
+import org.opencv.features2d.Feature2D;
+import org.opencv.features2d.ORB;
+
+// C++: class ORB
+/**
+ * Class implementing the ORB (*oriented BRIEF*) keypoint detector and descriptor extractor
+ *
+ * described in CITE: RRKB11 . The algorithm uses FAST in pyramids to detect stable keypoints, selects
+ * the strongest features using FAST or Harris response, finds their orientation using first-order
+ * moments and computes the descriptors using BRIEF (where the coordinates of random point pairs (or
+ * k-tuples) are rotated according to the measured orientation).
+ */
+public class ORB extends Feature2D {
+
+    protected ORB(long addr) { super(addr); }
+
+    // internal usage only
+    public static ORB __fromPtr__(long addr) { return new ORB(addr); }
+
+    // C++: enum <unnamed>
+    public static final int
+            kBytes = 32,
+            HARRIS_SCORE = 0,
+            FAST_SCORE = 1;
+
+
+    //
+    // C++: static Ptr_ORB cv::ORB::create(int nfeatures = 500, float scaleFactor = 1.2f, int nlevels = 8, int edgeThreshold = 31, int firstLevel = 0, int WTA_K = 2, int scoreType = ORB::HARRIS_SCORE, int patchSize = 31, int fastThreshold = 20)
+    //
+
+    /**
+     * The ORB constructor
+     *
+     *     @param nfeatures The maximum number of features to retain.
+     *     @param scaleFactor Pyramid decimation ratio, greater than 1. scaleFactor==2 means the classical
+     *     pyramid, where each next level has 4x less pixels than the previous, but such a big scale factor
+     *     will degrade feature matching scores dramatically. On the other hand, too close to 1 scale factor
+     *     will mean that to cover certain scale range you will need more pyramid levels and so the speed
+     *     will suffer.
+     *     @param nlevels The number of pyramid levels. The smallest level will have linear size equal to
+     *     input_image_linear_size/pow(scaleFactor, nlevels - firstLevel).
+     *     @param edgeThreshold This is size of the border where the features are not detected. It should
+     *     roughly match the patchSize parameter.
+     *     @param firstLevel The level of pyramid to put source image to. Previous layers are filled
+     *     with upscaled source image.
+     *     @param WTA_K The number of points that produce each element of the oriented BRIEF descriptor. The
+     *     default value 2 means the BRIEF where we take a random point pair and compare their brightnesses,
+     *     so we get 0/1 response. Other possible values are 3 and 4. For example, 3 means that we take 3
+     *     random points (of course, those point coordinates are random, but they are generated from the
+     *     pre-defined seed, so each element of BRIEF descriptor is computed deterministically from the pixel
+     *     rectangle), find point of maximum brightness and output index of the winner (0, 1 or 2). Such
+     *     output will occupy 2 bits, and therefore it will need a special variant of Hamming distance,
+     *     denoted as NORM_HAMMING2 (2 bits per bin). When WTA_K=4, we take 4 random points to compute each
+     *     bin (that will also occupy 2 bits with possible values 0, 1, 2 or 3).
+     *     @param scoreType The default HARRIS_SCORE means that Harris algorithm is used to rank features
+     *     (the score is written to KeyPoint::score and is used to retain best nfeatures features);
+     *     FAST_SCORE is alternative value of the parameter that produces slightly less stable keypoints,
+     *     but it is a little faster to compute.
+     *     @param patchSize size of the patch used by the oriented BRIEF descriptor. Of course, on smaller
+     *     pyramid layers the perceived image area covered by a feature will be larger.
+     *     @param fastThreshold the fast threshold
+     * @return automatically generated
+     */
+    public static ORB create(int nfeatures, float scaleFactor, int nlevels, int edgeThreshold, int firstLevel, int WTA_K, int scoreType, int patchSize, int fastThreshold) {
+        return ORB.__fromPtr__(create_0(nfeatures, scaleFactor, nlevels, edgeThreshold, firstLevel, WTA_K, scoreType, patchSize, fastThreshold));
+    }
+
+    /**
+     * The ORB constructor
+     *
+     *     @param nfeatures The maximum number of features to retain.
+     *     @param scaleFactor Pyramid decimation ratio, greater than 1. scaleFactor==2 means the classical
+     *     pyramid, where each next level has 4x less pixels than the previous, but such a big scale factor
+     *     will degrade feature matching scores dramatically. On the other hand, too close to 1 scale factor
+     *     will mean that to cover certain scale range you will need more pyramid levels and so the speed
+     *     will suffer.
+     *     @param nlevels The number of pyramid levels. The smallest level will have linear size equal to
+     *     input_image_linear_size/pow(scaleFactor, nlevels - firstLevel).
+     *     @param edgeThreshold This is size of the border where the features are not detected. It should
+     *     roughly match the patchSize parameter.
+     *     @param firstLevel The level of pyramid to put source image to. Previous layers are filled
+     *     with upscaled source image.
+     *     @param WTA_K The number of points that produce each element of the oriented BRIEF descriptor. The
+     *     default value 2 means the BRIEF where we take a random point pair and compare their brightnesses,
+     *     so we get 0/1 response. Other possible values are 3 and 4. For example, 3 means that we take 3
+     *     random points (of course, those point coordinates are random, but they are generated from the
+     *     pre-defined seed, so each element of BRIEF descriptor is computed deterministically from the pixel
+     *     rectangle), find point of maximum brightness and output index of the winner (0, 1 or 2). Such
+     *     output will occupy 2 bits, and therefore it will need a special variant of Hamming distance,
+     *     denoted as NORM_HAMMING2 (2 bits per bin). When WTA_K=4, we take 4 random points to compute each
+     *     bin (that will also occupy 2 bits with possible values 0, 1, 2 or 3).
+     *     @param scoreType The default HARRIS_SCORE means that Harris algorithm is used to rank features
+     *     (the score is written to KeyPoint::score and is used to retain best nfeatures features);
+     *     FAST_SCORE is alternative value of the parameter that produces slightly less stable keypoints,
+     *     but it is a little faster to compute.
+     *     @param patchSize size of the patch used by the oriented BRIEF descriptor. Of course, on smaller
+     *     pyramid layers the perceived image area covered by a feature will be larger.
+     * @return automatically generated
+     */
+    public static ORB create(int nfeatures, float scaleFactor, int nlevels, int edgeThreshold, int firstLevel, int WTA_K, int scoreType, int patchSize) {
+        return ORB.__fromPtr__(create_1(nfeatures, scaleFactor, nlevels, edgeThreshold, firstLevel, WTA_K, scoreType, patchSize));
+    }
+
+    /**
+     * The ORB constructor
+     *
+     *     @param nfeatures The maximum number of features to retain.
+     *     @param scaleFactor Pyramid decimation ratio, greater than 1. scaleFactor==2 means the classical
+     *     pyramid, where each next level has 4x less pixels than the previous, but such a big scale factor
+     *     will degrade feature matching scores dramatically. On the other hand, too close to 1 scale factor
+     *     will mean that to cover certain scale range you will need more pyramid levels and so the speed
+     *     will suffer.
+     *     @param nlevels The number of pyramid levels. The smallest level will have linear size equal to
+     *     input_image_linear_size/pow(scaleFactor, nlevels - firstLevel).
+     *     @param edgeThreshold This is size of the border where the features are not detected. It should
+     *     roughly match the patchSize parameter.
+     *     @param firstLevel The level of pyramid to put source image to. Previous layers are filled
+     *     with upscaled source image.
+     *     @param WTA_K The number of points that produce each element of the oriented BRIEF descriptor. The
+     *     default value 2 means the BRIEF where we take a random point pair and compare their brightnesses,
+     *     so we get 0/1 response. Other possible values are 3 and 4. For example, 3 means that we take 3
+     *     random points (of course, those point coordinates are random, but they are generated from the
+     *     pre-defined seed, so each element of BRIEF descriptor is computed deterministically from the pixel
+     *     rectangle), find point of maximum brightness and output index of the winner (0, 1 or 2). Such
+     *     output will occupy 2 bits, and therefore it will need a special variant of Hamming distance,
+     *     denoted as NORM_HAMMING2 (2 bits per bin). When WTA_K=4, we take 4 random points to compute each
+     *     bin (that will also occupy 2 bits with possible values 0, 1, 2 or 3).
+     *     @param scoreType The default HARRIS_SCORE means that Harris algorithm is used to rank features
+     *     (the score is written to KeyPoint::score and is used to retain best nfeatures features);
+     *     FAST_SCORE is alternative value of the parameter that produces slightly less stable keypoints,
+     *     but it is a little faster to compute.
+     *     pyramid layers the perceived image area covered by a feature will be larger.
+     * @return automatically generated
+     */
+    public static ORB create(int nfeatures, float scaleFactor, int nlevels, int edgeThreshold, int firstLevel, int WTA_K, int scoreType) {
+        return ORB.__fromPtr__(create_2(nfeatures, scaleFactor, nlevels, edgeThreshold, firstLevel, WTA_K, scoreType));
+    }
+
+    /**
+     * The ORB constructor
+     *
+     *     @param nfeatures The maximum number of features to retain.
+     *     @param scaleFactor Pyramid decimation ratio, greater than 1. scaleFactor==2 means the classical
+     *     pyramid, where each next level has 4x less pixels than the previous, but such a big scale factor
+     *     will degrade feature matching scores dramatically. On the other hand, too close to 1 scale factor
+     *     will mean that to cover certain scale range you will need more pyramid levels and so the speed
+     *     will suffer.
+     *     @param nlevels The number of pyramid levels. The smallest level will have linear size equal to
+     *     input_image_linear_size/pow(scaleFactor, nlevels - firstLevel).
+     *     @param edgeThreshold This is size of the border where the features are not detected. It should
+     *     roughly match the patchSize parameter.
+     *     @param firstLevel The level of pyramid to put source image to. Previous layers are filled
+     *     with upscaled source image.
+     *     @param WTA_K The number of points that produce each element of the oriented BRIEF descriptor. The
+     *     default value 2 means the BRIEF where we take a random point pair and compare their brightnesses,
+     *     so we get 0/1 response. Other possible values are 3 and 4. For example, 3 means that we take 3
+     *     random points (of course, those point coordinates are random, but they are generated from the
+     *     pre-defined seed, so each element of BRIEF descriptor is computed deterministically from the pixel
+     *     rectangle), find point of maximum brightness and output index of the winner (0, 1 or 2). Such
+     *     output will occupy 2 bits, and therefore it will need a special variant of Hamming distance,
+     *     denoted as NORM_HAMMING2 (2 bits per bin). When WTA_K=4, we take 4 random points to compute each
+     *     bin (that will also occupy 2 bits with possible values 0, 1, 2 or 3).
+     *     (the score is written to KeyPoint::score and is used to retain best nfeatures features);
+     *     FAST_SCORE is alternative value of the parameter that produces slightly less stable keypoints,
+     *     but it is a little faster to compute.
+     *     pyramid layers the perceived image area covered by a feature will be larger.
+     * @return automatically generated
+     */
+    public static ORB create(int nfeatures, float scaleFactor, int nlevels, int edgeThreshold, int firstLevel, int WTA_K) {
+        return ORB.__fromPtr__(create_3(nfeatures, scaleFactor, nlevels, edgeThreshold, firstLevel, WTA_K));
+    }
+
+    /**
+     * The ORB constructor
+     *
+     *     @param nfeatures The maximum number of features to retain.
+     *     @param scaleFactor Pyramid decimation ratio, greater than 1. scaleFactor==2 means the classical
+     *     pyramid, where each next level has 4x less pixels than the previous, but such a big scale factor
+     *     will degrade feature matching scores dramatically. On the other hand, too close to 1 scale factor
+     *     will mean that to cover certain scale range you will need more pyramid levels and so the speed
+     *     will suffer.
+     *     @param nlevels The number of pyramid levels. The smallest level will have linear size equal to
+     *     input_image_linear_size/pow(scaleFactor, nlevels - firstLevel).
+     *     @param edgeThreshold This is size of the border where the features are not detected. It should
+     *     roughly match the patchSize parameter.
+     *     @param firstLevel The level of pyramid to put source image to. Previous layers are filled
+     *     with upscaled source image.
+     *     default value 2 means the BRIEF where we take a random point pair and compare their brightnesses,
+     *     so we get 0/1 response. Other possible values are 3 and 4. For example, 3 means that we take 3
+     *     random points (of course, those point coordinates are random, but they are generated from the
+     *     pre-defined seed, so each element of BRIEF descriptor is computed deterministically from the pixel
+     *     rectangle), find point of maximum brightness and output index of the winner (0, 1 or 2). Such
+     *     output will occupy 2 bits, and therefore it will need a special variant of Hamming distance,
+     *     denoted as NORM_HAMMING2 (2 bits per bin). When WTA_K=4, we take 4 random points to compute each
+     *     bin (that will also occupy 2 bits with possible values 0, 1, 2 or 3).
+     *     (the score is written to KeyPoint::score and is used to retain best nfeatures features);
+     *     FAST_SCORE is alternative value of the parameter that produces slightly less stable keypoints,
+     *     but it is a little faster to compute.
+     *     pyramid layers the perceived image area covered by a feature will be larger.
+     * @return automatically generated
+     */
+    public static ORB create(int nfeatures, float scaleFactor, int nlevels, int edgeThreshold, int firstLevel) {
+        return ORB.__fromPtr__(create_4(nfeatures, scaleFactor, nlevels, edgeThreshold, firstLevel));
+    }
+
+    /**
+     * The ORB constructor
+     *
+     *     @param nfeatures The maximum number of features to retain.
+     *     @param scaleFactor Pyramid decimation ratio, greater than 1. scaleFactor==2 means the classical
+     *     pyramid, where each next level has 4x less pixels than the previous, but such a big scale factor
+     *     will degrade feature matching scores dramatically. On the other hand, too close to 1 scale factor
+     *     will mean that to cover certain scale range you will need more pyramid levels and so the speed
+     *     will suffer.
+     *     @param nlevels The number of pyramid levels. The smallest level will have linear size equal to
+     *     input_image_linear_size/pow(scaleFactor, nlevels - firstLevel).
+     *     @param edgeThreshold This is size of the border where the features are not detected. It should
+     *     roughly match the patchSize parameter.
+     *     with upscaled source image.
+     *     default value 2 means the BRIEF where we take a random point pair and compare their brightnesses,
+     *     so we get 0/1 response. Other possible values are 3 and 4. For example, 3 means that we take 3
+     *     random points (of course, those point coordinates are random, but they are generated from the
+     *     pre-defined seed, so each element of BRIEF descriptor is computed deterministically from the pixel
+     *     rectangle), find point of maximum brightness and output index of the winner (0, 1 or 2). Such
+     *     output will occupy 2 bits, and therefore it will need a special variant of Hamming distance,
+     *     denoted as NORM_HAMMING2 (2 bits per bin). When WTA_K=4, we take 4 random points to compute each
+     *     bin (that will also occupy 2 bits with possible values 0, 1, 2 or 3).
+     *     (the score is written to KeyPoint::score and is used to retain best nfeatures features);
+     *     FAST_SCORE is alternative value of the parameter that produces slightly less stable keypoints,
+     *     but it is a little faster to compute.
+     *     pyramid layers the perceived image area covered by a feature will be larger.
+     * @return automatically generated
+     */
+    public static ORB create(int nfeatures, float scaleFactor, int nlevels, int edgeThreshold) {
+        return ORB.__fromPtr__(create_5(nfeatures, scaleFactor, nlevels, edgeThreshold));
+    }
+
+    /**
+     * The ORB constructor
+     *
+     *     @param nfeatures The maximum number of features to retain.
+     *     @param scaleFactor Pyramid decimation ratio, greater than 1. scaleFactor==2 means the classical
+     *     pyramid, where each next level has 4x less pixels than the previous, but such a big scale factor
+     *     will degrade feature matching scores dramatically. On the other hand, too close to 1 scale factor
+     *     will mean that to cover certain scale range you will need more pyramid levels and so the speed
+     *     will suffer.
+     *     @param nlevels The number of pyramid levels. The smallest level will have linear size equal to
+     *     input_image_linear_size/pow(scaleFactor, nlevels - firstLevel).
+     *     roughly match the patchSize parameter.
+     *     with upscaled source image.
+     *     default value 2 means the BRIEF where we take a random point pair and compare their brightnesses,
+     *     so we get 0/1 response. Other possible values are 3 and 4. For example, 3 means that we take 3
+     *     random points (of course, those point coordinates are random, but they are generated from the
+     *     pre-defined seed, so each element of BRIEF descriptor is computed deterministically from the pixel
+     *     rectangle), find point of maximum brightness and output index of the winner (0, 1 or 2). Such
+     *     output will occupy 2 bits, and therefore it will need a special variant of Hamming distance,
+     *     denoted as NORM_HAMMING2 (2 bits per bin). When WTA_K=4, we take 4 random points to compute each
+     *     bin (that will also occupy 2 bits with possible values 0, 1, 2 or 3).
+     *     (the score is written to KeyPoint::score and is used to retain best nfeatures features);
+     *     FAST_SCORE is alternative value of the parameter that produces slightly less stable keypoints,
+     *     but it is a little faster to compute.
+     *     pyramid layers the perceived image area covered by a feature will be larger.
+     * @return automatically generated
+     */
+    public static ORB create(int nfeatures, float scaleFactor, int nlevels) {
+        return ORB.__fromPtr__(create_6(nfeatures, scaleFactor, nlevels));
+    }
+
+    /**
+     * The ORB constructor
+     *
+     *     @param nfeatures The maximum number of features to retain.
+     *     @param scaleFactor Pyramid decimation ratio, greater than 1. scaleFactor==2 means the classical
+     *     pyramid, where each next level has 4x less pixels than the previous, but such a big scale factor
+     *     will degrade feature matching scores dramatically. On the other hand, too close to 1 scale factor
+     *     will mean that to cover certain scale range you will need more pyramid levels and so the speed
+     *     will suffer.
+     *     input_image_linear_size/pow(scaleFactor, nlevels - firstLevel).
+     *     roughly match the patchSize parameter.
+     *     with upscaled source image.
+     *     default value 2 means the BRIEF where we take a random point pair and compare their brightnesses,
+     *     so we get 0/1 response. Other possible values are 3 and 4. For example, 3 means that we take 3
+     *     random points (of course, those point coordinates are random, but they are generated from the
+     *     pre-defined seed, so each element of BRIEF descriptor is computed deterministically from the pixel
+     *     rectangle), find point of maximum brightness and output index of the winner (0, 1 or 2). Such
+     *     output will occupy 2 bits, and therefore it will need a special variant of Hamming distance,
+     *     denoted as NORM_HAMMING2 (2 bits per bin). When WTA_K=4, we take 4 random points to compute each
+     *     bin (that will also occupy 2 bits with possible values 0, 1, 2 or 3).
+     *     (the score is written to KeyPoint::score and is used to retain best nfeatures features);
+     *     FAST_SCORE is alternative value of the parameter that produces slightly less stable keypoints,
+     *     but it is a little faster to compute.
+     *     pyramid layers the perceived image area covered by a feature will be larger.
+     * @return automatically generated
+     */
+    public static ORB create(int nfeatures, float scaleFactor) {
+        return ORB.__fromPtr__(create_7(nfeatures, scaleFactor));
+    }
+
+    /**
+     * The ORB constructor
+     *
+     *     @param nfeatures The maximum number of features to retain.
+     *     pyramid, where each next level has 4x less pixels than the previous, but such a big scale factor
+     *     will degrade feature matching scores dramatically. On the other hand, too close to 1 scale factor
+     *     will mean that to cover certain scale range you will need more pyramid levels and so the speed
+     *     will suffer.
+     *     input_image_linear_size/pow(scaleFactor, nlevels - firstLevel).
+     *     roughly match the patchSize parameter.
+     *     with upscaled source image.
+     *     default value 2 means the BRIEF where we take a random point pair and compare their brightnesses,
+     *     so we get 0/1 response. Other possible values are 3 and 4. For example, 3 means that we take 3
+     *     random points (of course, those point coordinates are random, but they are generated from the
+     *     pre-defined seed, so each element of BRIEF descriptor is computed deterministically from the pixel
+     *     rectangle), find point of maximum brightness and output index of the winner (0, 1 or 2). Such
+     *     output will occupy 2 bits, and therefore it will need a special variant of Hamming distance,
+     *     denoted as NORM_HAMMING2 (2 bits per bin). When WTA_K=4, we take 4 random points to compute each
+     *     bin (that will also occupy 2 bits with possible values 0, 1, 2 or 3).
+     *     (the score is written to KeyPoint::score and is used to retain best nfeatures features);
+     *     FAST_SCORE is alternative value of the parameter that produces slightly less stable keypoints,
+     *     but it is a little faster to compute.
+     *     pyramid layers the perceived image area covered by a feature will be larger.
+     * @return automatically generated
+     */
+    public static ORB create(int nfeatures) {
+        return ORB.__fromPtr__(create_8(nfeatures));
+    }
+
+    /**
+     * The ORB constructor
+     *
+     *     pyramid, where each next level has 4x less pixels than the previous, but such a big scale factor
+     *     will degrade feature matching scores dramatically. On the other hand, too close to 1 scale factor
+     *     will mean that to cover certain scale range you will need more pyramid levels and so the speed
+     *     will suffer.
+     *     input_image_linear_size/pow(scaleFactor, nlevels - firstLevel).
+     *     roughly match the patchSize parameter.
+     *     with upscaled source image.
+     *     default value 2 means the BRIEF where we take a random point pair and compare their brightnesses,
+     *     so we get 0/1 response. Other possible values are 3 and 4. For example, 3 means that we take 3
+     *     random points (of course, those point coordinates are random, but they are generated from the
+     *     pre-defined seed, so each element of BRIEF descriptor is computed deterministically from the pixel
+     *     rectangle), find point of maximum brightness and output index of the winner (0, 1 or 2). Such
+     *     output will occupy 2 bits, and therefore it will need a special variant of Hamming distance,
+     *     denoted as NORM_HAMMING2 (2 bits per bin). When WTA_K=4, we take 4 random points to compute each
+     *     bin (that will also occupy 2 bits with possible values 0, 1, 2 or 3).
+     *     (the score is written to KeyPoint::score and is used to retain best nfeatures features);
+     *     FAST_SCORE is alternative value of the parameter that produces slightly less stable keypoints,
+     *     but it is a little faster to compute.
+     *     pyramid layers the perceived image area covered by a feature will be larger.
+     * @return automatically generated
+     */
+    public static ORB create() {
+        return ORB.__fromPtr__(create_9());
+    }
+
+
+    //
+    // C++:  String cv::ORB::getDefaultName()
+    //
+
+    public String getDefaultName() {
+        return getDefaultName_0(nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::ORB::getScaleFactor()
+    //
+
+    public double getScaleFactor() {
+        return getScaleFactor_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::ORB::getEdgeThreshold()
+    //
+
+    public int getEdgeThreshold() {
+        return getEdgeThreshold_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::ORB::getFastThreshold()
+    //
+
+    public int getFastThreshold() {
+        return getFastThreshold_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::ORB::getFirstLevel()
+    //
+
+    public int getFirstLevel() {
+        return getFirstLevel_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::ORB::getMaxFeatures()
+    //
+
+    public int getMaxFeatures() {
+        return getMaxFeatures_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::ORB::getNLevels()
+    //
+
+    public int getNLevels() {
+        return getNLevels_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::ORB::getPatchSize()
+    //
+
+    public int getPatchSize() {
+        return getPatchSize_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::ORB::getScoreType()
+    //
+
+    public int getScoreType() {
+        return getScoreType_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::ORB::getWTA_K()
+    //
+
+    public int getWTA_K() {
+        return getWTA_K_0(nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::ORB::setEdgeThreshold(int edgeThreshold)
+    //
+
+    public void setEdgeThreshold(int edgeThreshold) {
+        setEdgeThreshold_0(nativeObj, edgeThreshold);
+    }
+
+
+    //
+    // C++:  void cv::ORB::setFastThreshold(int fastThreshold)
+    //
+
+    public void setFastThreshold(int fastThreshold) {
+        setFastThreshold_0(nativeObj, fastThreshold);
+    }
+
+
+    //
+    // C++:  void cv::ORB::setFirstLevel(int firstLevel)
+    //
+
+    public void setFirstLevel(int firstLevel) {
+        setFirstLevel_0(nativeObj, firstLevel);
+    }
+
+
+    //
+    // C++:  void cv::ORB::setMaxFeatures(int maxFeatures)
+    //
+
+    public void setMaxFeatures(int maxFeatures) {
+        setMaxFeatures_0(nativeObj, maxFeatures);
+    }
+
+
+    //
+    // C++:  void cv::ORB::setNLevels(int nlevels)
+    //
+
+    public void setNLevels(int nlevels) {
+        setNLevels_0(nativeObj, nlevels);
+    }
+
+
+    //
+    // C++:  void cv::ORB::setPatchSize(int patchSize)
+    //
+
+    public void setPatchSize(int patchSize) {
+        setPatchSize_0(nativeObj, patchSize);
+    }
+
+
+    //
+    // C++:  void cv::ORB::setScaleFactor(double scaleFactor)
+    //
+
+    public void setScaleFactor(double scaleFactor) {
+        setScaleFactor_0(nativeObj, scaleFactor);
+    }
+
+
+    //
+    // C++:  void cv::ORB::setScoreType(int scoreType)
+    //
+
+    public void setScoreType(int scoreType) {
+        setScoreType_0(nativeObj, scoreType);
+    }
+
+
+    //
+    // C++:  void cv::ORB::setWTA_K(int wta_k)
+    //
+
+    public void setWTA_K(int wta_k) {
+        setWTA_K_0(nativeObj, wta_k);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++: static Ptr_ORB cv::ORB::create(int nfeatures = 500, float scaleFactor = 1.2f, int nlevels = 8, int edgeThreshold = 31, int firstLevel = 0, int WTA_K = 2, int scoreType = ORB::HARRIS_SCORE, int patchSize = 31, int fastThreshold = 20)
+    private static native long create_0(int nfeatures, float scaleFactor, int nlevels, int edgeThreshold, int firstLevel, int WTA_K, int scoreType, int patchSize, int fastThreshold);
+    private static native long create_1(int nfeatures, float scaleFactor, int nlevels, int edgeThreshold, int firstLevel, int WTA_K, int scoreType, int patchSize);
+    private static native long create_2(int nfeatures, float scaleFactor, int nlevels, int edgeThreshold, int firstLevel, int WTA_K, int scoreType);
+    private static native long create_3(int nfeatures, float scaleFactor, int nlevels, int edgeThreshold, int firstLevel, int WTA_K);
+    private static native long create_4(int nfeatures, float scaleFactor, int nlevels, int edgeThreshold, int firstLevel);
+    private static native long create_5(int nfeatures, float scaleFactor, int nlevels, int edgeThreshold);
+    private static native long create_6(int nfeatures, float scaleFactor, int nlevels);
+    private static native long create_7(int nfeatures, float scaleFactor);
+    private static native long create_8(int nfeatures);
+    private static native long create_9();
+
+    // C++:  String cv::ORB::getDefaultName()
+    private static native String getDefaultName_0(long nativeObj);
+
+    // C++:  double cv::ORB::getScaleFactor()
+    private static native double getScaleFactor_0(long nativeObj);
+
+    // C++:  int cv::ORB::getEdgeThreshold()
+    private static native int getEdgeThreshold_0(long nativeObj);
+
+    // C++:  int cv::ORB::getFastThreshold()
+    private static native int getFastThreshold_0(long nativeObj);
+
+    // C++:  int cv::ORB::getFirstLevel()
+    private static native int getFirstLevel_0(long nativeObj);
+
+    // C++:  int cv::ORB::getMaxFeatures()
+    private static native int getMaxFeatures_0(long nativeObj);
+
+    // C++:  int cv::ORB::getNLevels()
+    private static native int getNLevels_0(long nativeObj);
+
+    // C++:  int cv::ORB::getPatchSize()
+    private static native int getPatchSize_0(long nativeObj);
+
+    // C++:  int cv::ORB::getScoreType()
+    private static native int getScoreType_0(long nativeObj);
+
+    // C++:  int cv::ORB::getWTA_K()
+    private static native int getWTA_K_0(long nativeObj);
+
+    // C++:  void cv::ORB::setEdgeThreshold(int edgeThreshold)
+    private static native void setEdgeThreshold_0(long nativeObj, int edgeThreshold);
+
+    // C++:  void cv::ORB::setFastThreshold(int fastThreshold)
+    private static native void setFastThreshold_0(long nativeObj, int fastThreshold);
+
+    // C++:  void cv::ORB::setFirstLevel(int firstLevel)
+    private static native void setFirstLevel_0(long nativeObj, int firstLevel);
+
+    // C++:  void cv::ORB::setMaxFeatures(int maxFeatures)
+    private static native void setMaxFeatures_0(long nativeObj, int maxFeatures);
+
+    // C++:  void cv::ORB::setNLevels(int nlevels)
+    private static native void setNLevels_0(long nativeObj, int nlevels);
+
+    // C++:  void cv::ORB::setPatchSize(int patchSize)
+    private static native void setPatchSize_0(long nativeObj, int patchSize);
+
+    // C++:  void cv::ORB::setScaleFactor(double scaleFactor)
+    private static native void setScaleFactor_0(long nativeObj, double scaleFactor);
+
+    // C++:  void cv::ORB::setScoreType(int scoreType)
+    private static native void setScoreType_0(long nativeObj, int scoreType);
+
+    // C++:  void cv::ORB::setWTA_K(int wta_k)
+    private static native void setWTA_K_0(long nativeObj, int wta_k);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/core/Size.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/core/Size.java	(date 1605830247455)
+++ openCVLibrary3411/src/main/java/org/opencv/core/Size.java	(date 1605830247455)
@@ -0,0 +1,73 @@
+package org.opencv.core;
+
+//javadoc:Size_
+public class Size {
+
+    public double width, height;
+
+    public Size(double width, double height) {
+        this.width = width;
+        this.height = height;
+    }
+
+    public Size() {
+        this(0, 0);
+    }
+
+    public Size(Point p) {
+        width = p.x;
+        height = p.y;
+    }
+
+    public Size(double[] vals) {
+        set(vals);
+    }
+
+    public void set(double[] vals) {
+        if (vals != null) {
+            width = vals.length > 0 ? vals[0] : 0;
+            height = vals.length > 1 ? vals[1] : 0;
+        } else {
+            width = 0;
+            height = 0;
+        }
+    }
+
+    public double area() {
+        return width * height;
+    }
+
+    public boolean empty() {
+        return width <= 0 || height <= 0;
+    }
+
+    public Size clone() {
+        return new Size(width, height);
+    }
+
+    @Override
+    public int hashCode() {
+        final int prime = 31;
+        int result = 1;
+        long temp;
+        temp = Double.doubleToLongBits(height);
+        result = prime * result + (int) (temp ^ (temp >>> 32));
+        temp = Double.doubleToLongBits(width);
+        result = prime * result + (int) (temp ^ (temp >>> 32));
+        return result;
+    }
+
+    @Override
+    public boolean equals(Object obj) {
+        if (this == obj) return true;
+        if (!(obj instanceof Size)) return false;
+        Size it = (Size) obj;
+        return width == it.width && height == it.height;
+    }
+
+    @Override
+    public String toString() {
+        return (int)width + "x" + (int)height;
+    }
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/features2d/Params.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/features2d/Params.java	(date 1605830247696)
+++ openCVLibrary3411/src/main/java/org/opencv/features2d/Params.java	(date 1605830247696)
@@ -0,0 +1,488 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.features2d;
+
+
+
+// C++: class Params
+
+public class Params {
+
+    protected final long nativeObj;
+    protected Params(long addr) { nativeObj = addr; }
+
+    public long getNativeObjAddr() { return nativeObj; }
+
+    // internal usage only
+    public static Params __fromPtr__(long addr) { return new Params(addr); }
+
+    //
+    // C++:   cv::SimpleBlobDetector::Params::Params()
+    //
+
+    public Params() {
+        nativeObj = Params_0();
+    }
+
+
+    //
+    // C++: float Params::thresholdStep
+    //
+
+    public float get_thresholdStep() {
+        return get_thresholdStep_0(nativeObj);
+    }
+
+
+    //
+    // C++: void Params::thresholdStep
+    //
+
+    public void set_thresholdStep(float thresholdStep) {
+        set_thresholdStep_0(nativeObj, thresholdStep);
+    }
+
+
+    //
+    // C++: float Params::minThreshold
+    //
+
+    public float get_minThreshold() {
+        return get_minThreshold_0(nativeObj);
+    }
+
+
+    //
+    // C++: void Params::minThreshold
+    //
+
+    public void set_minThreshold(float minThreshold) {
+        set_minThreshold_0(nativeObj, minThreshold);
+    }
+
+
+    //
+    // C++: float Params::maxThreshold
+    //
+
+    public float get_maxThreshold() {
+        return get_maxThreshold_0(nativeObj);
+    }
+
+
+    //
+    // C++: void Params::maxThreshold
+    //
+
+    public void set_maxThreshold(float maxThreshold) {
+        set_maxThreshold_0(nativeObj, maxThreshold);
+    }
+
+
+    //
+    // C++: size_t Params::minRepeatability
+    //
+
+    public long get_minRepeatability() {
+        return get_minRepeatability_0(nativeObj);
+    }
+
+
+    //
+    // C++: void Params::minRepeatability
+    //
+
+    public void set_minRepeatability(long minRepeatability) {
+        set_minRepeatability_0(nativeObj, minRepeatability);
+    }
+
+
+    //
+    // C++: float Params::minDistBetweenBlobs
+    //
+
+    public float get_minDistBetweenBlobs() {
+        return get_minDistBetweenBlobs_0(nativeObj);
+    }
+
+
+    //
+    // C++: void Params::minDistBetweenBlobs
+    //
+
+    public void set_minDistBetweenBlobs(float minDistBetweenBlobs) {
+        set_minDistBetweenBlobs_0(nativeObj, minDistBetweenBlobs);
+    }
+
+
+    //
+    // C++: bool Params::filterByColor
+    //
+
+    public boolean get_filterByColor() {
+        return get_filterByColor_0(nativeObj);
+    }
+
+
+    //
+    // C++: void Params::filterByColor
+    //
+
+    public void set_filterByColor(boolean filterByColor) {
+        set_filterByColor_0(nativeObj, filterByColor);
+    }
+
+
+    //
+    // C++: uchar Params::blobColor
+    //
+
+    // Return type 'uchar' is not supported, skipping the function
+
+
+    //
+    // C++: void Params::blobColor
+    //
+
+    // Unknown type 'uchar' (I), skipping the function
+
+
+    //
+    // C++: bool Params::filterByArea
+    //
+
+    public boolean get_filterByArea() {
+        return get_filterByArea_0(nativeObj);
+    }
+
+
+    //
+    // C++: void Params::filterByArea
+    //
+
+    public void set_filterByArea(boolean filterByArea) {
+        set_filterByArea_0(nativeObj, filterByArea);
+    }
+
+
+    //
+    // C++: float Params::minArea
+    //
+
+    public float get_minArea() {
+        return get_minArea_0(nativeObj);
+    }
+
+
+    //
+    // C++: void Params::minArea
+    //
+
+    public void set_minArea(float minArea) {
+        set_minArea_0(nativeObj, minArea);
+    }
+
+
+    //
+    // C++: float Params::maxArea
+    //
+
+    public float get_maxArea() {
+        return get_maxArea_0(nativeObj);
+    }
+
+
+    //
+    // C++: void Params::maxArea
+    //
+
+    public void set_maxArea(float maxArea) {
+        set_maxArea_0(nativeObj, maxArea);
+    }
+
+
+    //
+    // C++: bool Params::filterByCircularity
+    //
+
+    public boolean get_filterByCircularity() {
+        return get_filterByCircularity_0(nativeObj);
+    }
+
+
+    //
+    // C++: void Params::filterByCircularity
+    //
+
+    public void set_filterByCircularity(boolean filterByCircularity) {
+        set_filterByCircularity_0(nativeObj, filterByCircularity);
+    }
+
+
+    //
+    // C++: float Params::minCircularity
+    //
+
+    public float get_minCircularity() {
+        return get_minCircularity_0(nativeObj);
+    }
+
+
+    //
+    // C++: void Params::minCircularity
+    //
+
+    public void set_minCircularity(float minCircularity) {
+        set_minCircularity_0(nativeObj, minCircularity);
+    }
+
+
+    //
+    // C++: float Params::maxCircularity
+    //
+
+    public float get_maxCircularity() {
+        return get_maxCircularity_0(nativeObj);
+    }
+
+
+    //
+    // C++: void Params::maxCircularity
+    //
+
+    public void set_maxCircularity(float maxCircularity) {
+        set_maxCircularity_0(nativeObj, maxCircularity);
+    }
+
+
+    //
+    // C++: bool Params::filterByInertia
+    //
+
+    public boolean get_filterByInertia() {
+        return get_filterByInertia_0(nativeObj);
+    }
+
+
+    //
+    // C++: void Params::filterByInertia
+    //
+
+    public void set_filterByInertia(boolean filterByInertia) {
+        set_filterByInertia_0(nativeObj, filterByInertia);
+    }
+
+
+    //
+    // C++: float Params::minInertiaRatio
+    //
+
+    public float get_minInertiaRatio() {
+        return get_minInertiaRatio_0(nativeObj);
+    }
+
+
+    //
+    // C++: void Params::minInertiaRatio
+    //
+
+    public void set_minInertiaRatio(float minInertiaRatio) {
+        set_minInertiaRatio_0(nativeObj, minInertiaRatio);
+    }
+
+
+    //
+    // C++: float Params::maxInertiaRatio
+    //
+
+    public float get_maxInertiaRatio() {
+        return get_maxInertiaRatio_0(nativeObj);
+    }
+
+
+    //
+    // C++: void Params::maxInertiaRatio
+    //
+
+    public void set_maxInertiaRatio(float maxInertiaRatio) {
+        set_maxInertiaRatio_0(nativeObj, maxInertiaRatio);
+    }
+
+
+    //
+    // C++: bool Params::filterByConvexity
+    //
+
+    public boolean get_filterByConvexity() {
+        return get_filterByConvexity_0(nativeObj);
+    }
+
+
+    //
+    // C++: void Params::filterByConvexity
+    //
+
+    public void set_filterByConvexity(boolean filterByConvexity) {
+        set_filterByConvexity_0(nativeObj, filterByConvexity);
+    }
+
+
+    //
+    // C++: float Params::minConvexity
+    //
+
+    public float get_minConvexity() {
+        return get_minConvexity_0(nativeObj);
+    }
+
+
+    //
+    // C++: void Params::minConvexity
+    //
+
+    public void set_minConvexity(float minConvexity) {
+        set_minConvexity_0(nativeObj, minConvexity);
+    }
+
+
+    //
+    // C++: float Params::maxConvexity
+    //
+
+    public float get_maxConvexity() {
+        return get_maxConvexity_0(nativeObj);
+    }
+
+
+    //
+    // C++: void Params::maxConvexity
+    //
+
+    public void set_maxConvexity(float maxConvexity) {
+        set_maxConvexity_0(nativeObj, maxConvexity);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++:   cv::SimpleBlobDetector::Params::Params()
+    private static native long Params_0();
+
+    // C++: float Params::thresholdStep
+    private static native float get_thresholdStep_0(long nativeObj);
+
+    // C++: void Params::thresholdStep
+    private static native void set_thresholdStep_0(long nativeObj, float thresholdStep);
+
+    // C++: float Params::minThreshold
+    private static native float get_minThreshold_0(long nativeObj);
+
+    // C++: void Params::minThreshold
+    private static native void set_minThreshold_0(long nativeObj, float minThreshold);
+
+    // C++: float Params::maxThreshold
+    private static native float get_maxThreshold_0(long nativeObj);
+
+    // C++: void Params::maxThreshold
+    private static native void set_maxThreshold_0(long nativeObj, float maxThreshold);
+
+    // C++: size_t Params::minRepeatability
+    private static native long get_minRepeatability_0(long nativeObj);
+
+    // C++: void Params::minRepeatability
+    private static native void set_minRepeatability_0(long nativeObj, long minRepeatability);
+
+    // C++: float Params::minDistBetweenBlobs
+    private static native float get_minDistBetweenBlobs_0(long nativeObj);
+
+    // C++: void Params::minDistBetweenBlobs
+    private static native void set_minDistBetweenBlobs_0(long nativeObj, float minDistBetweenBlobs);
+
+    // C++: bool Params::filterByColor
+    private static native boolean get_filterByColor_0(long nativeObj);
+
+    // C++: void Params::filterByColor
+    private static native void set_filterByColor_0(long nativeObj, boolean filterByColor);
+
+    // C++: bool Params::filterByArea
+    private static native boolean get_filterByArea_0(long nativeObj);
+
+    // C++: void Params::filterByArea
+    private static native void set_filterByArea_0(long nativeObj, boolean filterByArea);
+
+    // C++: float Params::minArea
+    private static native float get_minArea_0(long nativeObj);
+
+    // C++: void Params::minArea
+    private static native void set_minArea_0(long nativeObj, float minArea);
+
+    // C++: float Params::maxArea
+    private static native float get_maxArea_0(long nativeObj);
+
+    // C++: void Params::maxArea
+    private static native void set_maxArea_0(long nativeObj, float maxArea);
+
+    // C++: bool Params::filterByCircularity
+    private static native boolean get_filterByCircularity_0(long nativeObj);
+
+    // C++: void Params::filterByCircularity
+    private static native void set_filterByCircularity_0(long nativeObj, boolean filterByCircularity);
+
+    // C++: float Params::minCircularity
+    private static native float get_minCircularity_0(long nativeObj);
+
+    // C++: void Params::minCircularity
+    private static native void set_minCircularity_0(long nativeObj, float minCircularity);
+
+    // C++: float Params::maxCircularity
+    private static native float get_maxCircularity_0(long nativeObj);
+
+    // C++: void Params::maxCircularity
+    private static native void set_maxCircularity_0(long nativeObj, float maxCircularity);
+
+    // C++: bool Params::filterByInertia
+    private static native boolean get_filterByInertia_0(long nativeObj);
+
+    // C++: void Params::filterByInertia
+    private static native void set_filterByInertia_0(long nativeObj, boolean filterByInertia);
+
+    // C++: float Params::minInertiaRatio
+    private static native float get_minInertiaRatio_0(long nativeObj);
+
+    // C++: void Params::minInertiaRatio
+    private static native void set_minInertiaRatio_0(long nativeObj, float minInertiaRatio);
+
+    // C++: float Params::maxInertiaRatio
+    private static native float get_maxInertiaRatio_0(long nativeObj);
+
+    // C++: void Params::maxInertiaRatio
+    private static native void set_maxInertiaRatio_0(long nativeObj, float maxInertiaRatio);
+
+    // C++: bool Params::filterByConvexity
+    private static native boolean get_filterByConvexity_0(long nativeObj);
+
+    // C++: void Params::filterByConvexity
+    private static native void set_filterByConvexity_0(long nativeObj, boolean filterByConvexity);
+
+    // C++: float Params::minConvexity
+    private static native float get_minConvexity_0(long nativeObj);
+
+    // C++: void Params::minConvexity
+    private static native void set_minConvexity_0(long nativeObj, float minConvexity);
+
+    // C++: float Params::maxConvexity
+    private static native float get_maxConvexity_0(long nativeObj);
+
+    // C++: void Params::maxConvexity
+    private static native void set_maxConvexity_0(long nativeObj, float maxConvexity);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/core/Point.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/core/Point.java	(date 1605830247420)
+++ openCVLibrary3411/src/main/java/org/opencv/core/Point.java	(date 1605830247420)
@@ -0,0 +1,68 @@
+package org.opencv.core;
+
+//javadoc:Point_
+public class Point {
+
+    public double x, y;
+
+    public Point(double x, double y) {
+        this.x = x;
+        this.y = y;
+    }
+
+    public Point() {
+        this(0, 0);
+    }
+
+    public Point(double[] vals) {
+        this();
+        set(vals);
+    }
+
+    public void set(double[] vals) {
+        if (vals != null) {
+            x = vals.length > 0 ? vals[0] : 0;
+            y = vals.length > 1 ? vals[1] : 0;
+        } else {
+            x = 0;
+            y = 0;
+        }
+    }
+
+    public Point clone() {
+        return new Point(x, y);
+    }
+
+    public double dot(Point p) {
+        return x * p.x + y * p.y;
+    }
+
+    @Override
+    public int hashCode() {
+        final int prime = 31;
+        int result = 1;
+        long temp;
+        temp = Double.doubleToLongBits(x);
+        result = prime * result + (int) (temp ^ (temp >>> 32));
+        temp = Double.doubleToLongBits(y);
+        result = prime * result + (int) (temp ^ (temp >>> 32));
+        return result;
+    }
+
+    @Override
+    public boolean equals(Object obj) {
+        if (this == obj) return true;
+        if (!(obj instanceof Point)) return false;
+        Point it = (Point) obj;
+        return x == it.x && y == it.y;
+    }
+
+    public boolean inside(Rect r) {
+        return r.contains(this);
+    }
+
+    @Override
+    public String toString() {
+        return "{" + x + ", " + y + "}";
+    }
+}
Index: openCVLibrary3411/src/main/java/org/opencv/features2d/FeatureDetector.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/features2d/FeatureDetector.java	(date 1605830247635)
+++ openCVLibrary3411/src/main/java/org/opencv/features2d/FeatureDetector.java	(date 1605830247635)
@@ -0,0 +1,197 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.features2d;
+
+import java.util.ArrayList;
+import java.util.List;
+import org.opencv.core.Mat;
+import org.opencv.core.MatOfKeyPoint;
+import org.opencv.features2d.FeatureDetector;
+import org.opencv.utils.Converters;
+
+// C++: class javaFeatureDetector
+/**
+ * @deprecated Please use direct instantiation of Feature2D classes
+ */
+@Deprecated
+public class FeatureDetector {
+
+    protected final long nativeObj;
+    protected FeatureDetector(long addr) { nativeObj = addr; }
+
+    public long getNativeObjAddr() { return nativeObj; }
+
+    // internal usage only
+    public static FeatureDetector __fromPtr__(long addr) { return new FeatureDetector(addr); }
+
+    private static final int
+            GRIDDETECTOR = 1000,
+            PYRAMIDDETECTOR = 2000,
+            DYNAMICDETECTOR = 3000;
+
+
+    // C++: enum <unnamed>
+    public static final int
+            FAST = 1,
+            STAR = 2,
+            SIFT = 3,
+            SURF = 4,
+            ORB = 5,
+            MSER = 6,
+            GFTT = 7,
+            HARRIS = 8,
+            SIMPLEBLOB = 9,
+            DENSE = 10,
+            BRISK = 11,
+            AKAZE = 12,
+            GRID_FAST = GRIDDETECTOR + FAST,
+            GRID_STAR = GRIDDETECTOR + STAR,
+            GRID_SIFT = GRIDDETECTOR + SIFT,
+            GRID_SURF = GRIDDETECTOR + SURF,
+            GRID_ORB = GRIDDETECTOR + ORB,
+            GRID_MSER = GRIDDETECTOR + MSER,
+            GRID_GFTT = GRIDDETECTOR + GFTT,
+            GRID_HARRIS = GRIDDETECTOR + HARRIS,
+            GRID_SIMPLEBLOB = GRIDDETECTOR + SIMPLEBLOB,
+            GRID_DENSE = GRIDDETECTOR + DENSE,
+            GRID_BRISK = GRIDDETECTOR + BRISK,
+            GRID_AKAZE = GRIDDETECTOR + AKAZE,
+            PYRAMID_FAST = PYRAMIDDETECTOR + FAST,
+            PYRAMID_STAR = PYRAMIDDETECTOR + STAR,
+            PYRAMID_SIFT = PYRAMIDDETECTOR + SIFT,
+            PYRAMID_SURF = PYRAMIDDETECTOR + SURF,
+            PYRAMID_ORB = PYRAMIDDETECTOR + ORB,
+            PYRAMID_MSER = PYRAMIDDETECTOR + MSER,
+            PYRAMID_GFTT = PYRAMIDDETECTOR + GFTT,
+            PYRAMID_HARRIS = PYRAMIDDETECTOR + HARRIS,
+            PYRAMID_SIMPLEBLOB = PYRAMIDDETECTOR + SIMPLEBLOB,
+            PYRAMID_DENSE = PYRAMIDDETECTOR + DENSE,
+            PYRAMID_BRISK = PYRAMIDDETECTOR + BRISK,
+            PYRAMID_AKAZE = PYRAMIDDETECTOR + AKAZE,
+            DYNAMIC_FAST = DYNAMICDETECTOR + FAST,
+            DYNAMIC_STAR = DYNAMICDETECTOR + STAR,
+            DYNAMIC_SIFT = DYNAMICDETECTOR + SIFT,
+            DYNAMIC_SURF = DYNAMICDETECTOR + SURF,
+            DYNAMIC_ORB = DYNAMICDETECTOR + ORB,
+            DYNAMIC_MSER = DYNAMICDETECTOR + MSER,
+            DYNAMIC_GFTT = DYNAMICDETECTOR + GFTT,
+            DYNAMIC_HARRIS = DYNAMICDETECTOR + HARRIS,
+            DYNAMIC_SIMPLEBLOB = DYNAMICDETECTOR + SIMPLEBLOB,
+            DYNAMIC_DENSE = DYNAMICDETECTOR + DENSE,
+            DYNAMIC_BRISK = DYNAMICDETECTOR + BRISK,
+            DYNAMIC_AKAZE = DYNAMICDETECTOR + AKAZE;
+
+
+    //
+    // C++: static Ptr_javaFeatureDetector cv::javaFeatureDetector::create(int detectorType)
+    //
+
+    /**
+     * supported: FAST STAR SIFT SURF ORB MSER GFTT HARRIS BRISK AKAZE Grid(XXXX) Pyramid(XXXX) Dynamic(XXXX)
+     * not supported: SimpleBlob, Dense
+     * @deprecated
+     * @param detectorType automatically generated
+     * @return automatically generated
+     */
+    @Deprecated
+    public static FeatureDetector create(int detectorType) {
+        return FeatureDetector.__fromPtr__(create_0(detectorType));
+    }
+
+
+    //
+    // C++:  bool cv::javaFeatureDetector::empty()
+    //
+
+    public boolean empty() {
+        return empty_0(nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::javaFeatureDetector::detect(Mat image, vector_KeyPoint& keypoints, Mat mask = Mat())
+    //
+
+    public void detect(Mat image, MatOfKeyPoint keypoints, Mat mask) {
+        Mat keypoints_mat = keypoints;
+        detect_0(nativeObj, image.nativeObj, keypoints_mat.nativeObj, mask.nativeObj);
+    }
+
+    public void detect(Mat image, MatOfKeyPoint keypoints) {
+        Mat keypoints_mat = keypoints;
+        detect_1(nativeObj, image.nativeObj, keypoints_mat.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::javaFeatureDetector::detect(vector_Mat images, vector_vector_KeyPoint& keypoints, vector_Mat masks = std::vector<Mat>())
+    //
+
+    public void detect(List<Mat> images, List<MatOfKeyPoint> keypoints, List<Mat> masks) {
+        Mat images_mat = Converters.vector_Mat_to_Mat(images);
+        Mat keypoints_mat = new Mat();
+        Mat masks_mat = Converters.vector_Mat_to_Mat(masks);
+        detect_2(nativeObj, images_mat.nativeObj, keypoints_mat.nativeObj, masks_mat.nativeObj);
+        Converters.Mat_to_vector_vector_KeyPoint(keypoints_mat, keypoints);
+        keypoints_mat.release();
+    }
+
+    public void detect(List<Mat> images, List<MatOfKeyPoint> keypoints) {
+        Mat images_mat = Converters.vector_Mat_to_Mat(images);
+        Mat keypoints_mat = new Mat();
+        detect_3(nativeObj, images_mat.nativeObj, keypoints_mat.nativeObj);
+        Converters.Mat_to_vector_vector_KeyPoint(keypoints_mat, keypoints);
+        keypoints_mat.release();
+    }
+
+
+    //
+    // C++:  void cv::javaFeatureDetector::read(String fileName)
+    //
+
+    public void read(String fileName) {
+        read_0(nativeObj, fileName);
+    }
+
+
+    //
+    // C++:  void cv::javaFeatureDetector::write(String fileName)
+    //
+
+    public void write(String fileName) {
+        write_0(nativeObj, fileName);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++: static Ptr_javaFeatureDetector cv::javaFeatureDetector::create(int detectorType)
+    private static native long create_0(int detectorType);
+
+    // C++:  bool cv::javaFeatureDetector::empty()
+    private static native boolean empty_0(long nativeObj);
+
+    // C++:  void cv::javaFeatureDetector::detect(Mat image, vector_KeyPoint& keypoints, Mat mask = Mat())
+    private static native void detect_0(long nativeObj, long image_nativeObj, long keypoints_mat_nativeObj, long mask_nativeObj);
+    private static native void detect_1(long nativeObj, long image_nativeObj, long keypoints_mat_nativeObj);
+
+    // C++:  void cv::javaFeatureDetector::detect(vector_Mat images, vector_vector_KeyPoint& keypoints, vector_Mat masks = std::vector<Mat>())
+    private static native void detect_2(long nativeObj, long images_mat_nativeObj, long keypoints_mat_nativeObj, long masks_mat_nativeObj);
+    private static native void detect_3(long nativeObj, long images_mat_nativeObj, long keypoints_mat_nativeObj);
+
+    // C++:  void cv::javaFeatureDetector::read(String fileName)
+    private static native void read_0(long nativeObj, String fileName);
+
+    // C++:  void cv::javaFeatureDetector::write(String fileName)
+    private static native void write_0(long nativeObj, String fileName);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/core/Point3.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/core/Point3.java	(date 1605830247421)
+++ openCVLibrary3411/src/main/java/org/opencv/core/Point3.java	(date 1605830247421)
@@ -0,0 +1,79 @@
+package org.opencv.core;
+
+//javadoc:Point3_
+public class Point3 {
+
+    public double x, y, z;
+
+    public Point3(double x, double y, double z) {
+        this.x = x;
+        this.y = y;
+        this.z = z;
+    }
+
+    public Point3() {
+        this(0, 0, 0);
+    }
+
+    public Point3(Point p) {
+        x = p.x;
+        y = p.y;
+        z = 0;
+    }
+
+    public Point3(double[] vals) {
+        this();
+        set(vals);
+    }
+
+    public void set(double[] vals) {
+        if (vals != null) {
+            x = vals.length > 0 ? vals[0] : 0;
+            y = vals.length > 1 ? vals[1] : 0;
+            z = vals.length > 2 ? vals[2] : 0;
+        } else {
+            x = 0;
+            y = 0;
+            z = 0;
+        }
+    }
+
+    public Point3 clone() {
+        return new Point3(x, y, z);
+    }
+
+    public double dot(Point3 p) {
+        return x * p.x + y * p.y + z * p.z;
+    }
+
+    public Point3 cross(Point3 p) {
+        return new Point3(y * p.z - z * p.y, z * p.x - x * p.z, x * p.y - y * p.x);
+    }
+
+    @Override
+    public int hashCode() {
+        final int prime = 31;
+        int result = 1;
+        long temp;
+        temp = Double.doubleToLongBits(x);
+        result = prime * result + (int) (temp ^ (temp >>> 32));
+        temp = Double.doubleToLongBits(y);
+        result = prime * result + (int) (temp ^ (temp >>> 32));
+        temp = Double.doubleToLongBits(z);
+        result = prime * result + (int) (temp ^ (temp >>> 32));
+        return result;
+    }
+
+    @Override
+    public boolean equals(Object obj) {
+        if (this == obj) return true;
+        if (!(obj instanceof Point3)) return false;
+        Point3 it = (Point3) obj;
+        return x == it.x && y == it.y && z == it.z;
+    }
+
+    @Override
+    public String toString() {
+        return "{" + x + ", " + y + ", " + z + "}";
+    }
+}
Index: openCVLibrary3411/src/main/java/org/opencv/features2d/Features2d.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/features2d/Features2d.java	(date 1605830247665)
+++ openCVLibrary3411/src/main/java/org/opencv/features2d/Features2d.java	(date 1605830247665)
@@ -0,0 +1,368 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.features2d;
+
+import java.util.ArrayList;
+import java.util.List;
+import org.opencv.core.Mat;
+import org.opencv.core.MatOfByte;
+import org.opencv.core.MatOfDMatch;
+import org.opencv.core.MatOfKeyPoint;
+import org.opencv.core.Scalar;
+import org.opencv.utils.Converters;
+
+// C++: class Features2d
+
+public class Features2d {
+
+    // C++: enum <unnamed>
+    public static final int
+            DRAW_OVER_OUTIMG = 1,
+            NOT_DRAW_SINGLE_POINTS = 2,
+            DRAW_RICH_KEYPOINTS = 4,
+            DrawMatchesFlags_DEFAULT = 0,
+            DrawMatchesFlags_DRAW_OVER_OUTIMG = 1,
+            DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS = 2,
+            DrawMatchesFlags_DRAW_RICH_KEYPOINTS = 4;
+
+
+    //
+    // C++:  void cv::drawKeypoints(Mat image, vector_KeyPoint keypoints, Mat& outImage, Scalar color = Scalar::all(-1), int flags = DrawMatchesFlags::DEFAULT)
+    //
+
+    /**
+     * Draws keypoints.
+     *
+     * @param image Source image.
+     * @param keypoints Keypoints from the source image.
+     * @param outImage Output image. Its content depends on the flags value defining what is drawn in the
+     * output image. See possible flags bit values below.
+     * @param color Color of keypoints.
+     * @param flags Flags setting drawing features. Possible flags bit values are defined by
+     * DrawMatchesFlags. See details above in drawMatches .
+     *
+     * <b>Note:</b>
+     * For Python API, flags are modified as cv2.DRAW_MATCHES_FLAGS_DEFAULT,
+     * cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS, cv2.DRAW_MATCHES_FLAGS_DRAW_OVER_OUTIMG,
+     * cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS
+     */
+    public static void drawKeypoints(Mat image, MatOfKeyPoint keypoints, Mat outImage, Scalar color, int flags) {
+        Mat keypoints_mat = keypoints;
+        drawKeypoints_0(image.nativeObj, keypoints_mat.nativeObj, outImage.nativeObj, color.val[0], color.val[1], color.val[2], color.val[3], flags);
+    }
+
+    /**
+     * Draws keypoints.
+     *
+     * @param image Source image.
+     * @param keypoints Keypoints from the source image.
+     * @param outImage Output image. Its content depends on the flags value defining what is drawn in the
+     * output image. See possible flags bit values below.
+     * @param color Color of keypoints.
+     * DrawMatchesFlags. See details above in drawMatches .
+     *
+     * <b>Note:</b>
+     * For Python API, flags are modified as cv2.DRAW_MATCHES_FLAGS_DEFAULT,
+     * cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS, cv2.DRAW_MATCHES_FLAGS_DRAW_OVER_OUTIMG,
+     * cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS
+     */
+    public static void drawKeypoints(Mat image, MatOfKeyPoint keypoints, Mat outImage, Scalar color) {
+        Mat keypoints_mat = keypoints;
+        drawKeypoints_1(image.nativeObj, keypoints_mat.nativeObj, outImage.nativeObj, color.val[0], color.val[1], color.val[2], color.val[3]);
+    }
+
+    /**
+     * Draws keypoints.
+     *
+     * @param image Source image.
+     * @param keypoints Keypoints from the source image.
+     * @param outImage Output image. Its content depends on the flags value defining what is drawn in the
+     * output image. See possible flags bit values below.
+     * DrawMatchesFlags. See details above in drawMatches .
+     *
+     * <b>Note:</b>
+     * For Python API, flags are modified as cv2.DRAW_MATCHES_FLAGS_DEFAULT,
+     * cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS, cv2.DRAW_MATCHES_FLAGS_DRAW_OVER_OUTIMG,
+     * cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS
+     */
+    public static void drawKeypoints(Mat image, MatOfKeyPoint keypoints, Mat outImage) {
+        Mat keypoints_mat = keypoints;
+        drawKeypoints_2(image.nativeObj, keypoints_mat.nativeObj, outImage.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::drawMatches(Mat img1, vector_KeyPoint keypoints1, Mat img2, vector_KeyPoint keypoints2, vector_DMatch matches1to2, Mat& outImg, Scalar matchColor = Scalar::all(-1), Scalar singlePointColor = Scalar::all(-1), vector_char matchesMask = std::vector<char>(), int flags = DrawMatchesFlags::DEFAULT)
+    //
+
+    /**
+     * Draws the found matches of keypoints from two images.
+     *
+     * @param img1 First source image.
+     * @param keypoints1 Keypoints from the first source image.
+     * @param img2 Second source image.
+     * @param keypoints2 Keypoints from the second source image.
+     * @param matches1to2 Matches from the first image to the second one, which means that keypoints1[i]
+     * has a corresponding point in keypoints2[matches[i]] .
+     * @param outImg Output image. Its content depends on the flags value defining what is drawn in the
+     * output image. See possible flags bit values below.
+     * @param matchColor Color of matches (lines and connected keypoints). If matchColor==Scalar::all(-1)
+     * , the color is generated randomly.
+     * @param singlePointColor Color of single keypoints (circles), which means that keypoints do not
+     * have the matches. If singlePointColor==Scalar::all(-1) , the color is generated randomly.
+     * @param matchesMask Mask determining which matches are drawn. If the mask is empty, all matches are
+     * drawn.
+     * @param flags Flags setting drawing features. Possible flags bit values are defined by
+     * DrawMatchesFlags.
+     *
+     * This function draws matches of keypoints from two images in the output image. Match is a line
+     * connecting two keypoints (circles). See cv::DrawMatchesFlags.
+     */
+    public static void drawMatches(Mat img1, MatOfKeyPoint keypoints1, Mat img2, MatOfKeyPoint keypoints2, MatOfDMatch matches1to2, Mat outImg, Scalar matchColor, Scalar singlePointColor, MatOfByte matchesMask, int flags) {
+        Mat keypoints1_mat = keypoints1;
+        Mat keypoints2_mat = keypoints2;
+        Mat matches1to2_mat = matches1to2;
+        Mat matchesMask_mat = matchesMask;
+        drawMatches_0(img1.nativeObj, keypoints1_mat.nativeObj, img2.nativeObj, keypoints2_mat.nativeObj, matches1to2_mat.nativeObj, outImg.nativeObj, matchColor.val[0], matchColor.val[1], matchColor.val[2], matchColor.val[3], singlePointColor.val[0], singlePointColor.val[1], singlePointColor.val[2], singlePointColor.val[3], matchesMask_mat.nativeObj, flags);
+    }
+
+    /**
+     * Draws the found matches of keypoints from two images.
+     *
+     * @param img1 First source image.
+     * @param keypoints1 Keypoints from the first source image.
+     * @param img2 Second source image.
+     * @param keypoints2 Keypoints from the second source image.
+     * @param matches1to2 Matches from the first image to the second one, which means that keypoints1[i]
+     * has a corresponding point in keypoints2[matches[i]] .
+     * @param outImg Output image. Its content depends on the flags value defining what is drawn in the
+     * output image. See possible flags bit values below.
+     * @param matchColor Color of matches (lines and connected keypoints). If matchColor==Scalar::all(-1)
+     * , the color is generated randomly.
+     * @param singlePointColor Color of single keypoints (circles), which means that keypoints do not
+     * have the matches. If singlePointColor==Scalar::all(-1) , the color is generated randomly.
+     * @param matchesMask Mask determining which matches are drawn. If the mask is empty, all matches are
+     * drawn.
+     * DrawMatchesFlags.
+     *
+     * This function draws matches of keypoints from two images in the output image. Match is a line
+     * connecting two keypoints (circles). See cv::DrawMatchesFlags.
+     */
+    public static void drawMatches(Mat img1, MatOfKeyPoint keypoints1, Mat img2, MatOfKeyPoint keypoints2, MatOfDMatch matches1to2, Mat outImg, Scalar matchColor, Scalar singlePointColor, MatOfByte matchesMask) {
+        Mat keypoints1_mat = keypoints1;
+        Mat keypoints2_mat = keypoints2;
+        Mat matches1to2_mat = matches1to2;
+        Mat matchesMask_mat = matchesMask;
+        drawMatches_1(img1.nativeObj, keypoints1_mat.nativeObj, img2.nativeObj, keypoints2_mat.nativeObj, matches1to2_mat.nativeObj, outImg.nativeObj, matchColor.val[0], matchColor.val[1], matchColor.val[2], matchColor.val[3], singlePointColor.val[0], singlePointColor.val[1], singlePointColor.val[2], singlePointColor.val[3], matchesMask_mat.nativeObj);
+    }
+
+    /**
+     * Draws the found matches of keypoints from two images.
+     *
+     * @param img1 First source image.
+     * @param keypoints1 Keypoints from the first source image.
+     * @param img2 Second source image.
+     * @param keypoints2 Keypoints from the second source image.
+     * @param matches1to2 Matches from the first image to the second one, which means that keypoints1[i]
+     * has a corresponding point in keypoints2[matches[i]] .
+     * @param outImg Output image. Its content depends on the flags value defining what is drawn in the
+     * output image. See possible flags bit values below.
+     * @param matchColor Color of matches (lines and connected keypoints). If matchColor==Scalar::all(-1)
+     * , the color is generated randomly.
+     * @param singlePointColor Color of single keypoints (circles), which means that keypoints do not
+     * have the matches. If singlePointColor==Scalar::all(-1) , the color is generated randomly.
+     * drawn.
+     * DrawMatchesFlags.
+     *
+     * This function draws matches of keypoints from two images in the output image. Match is a line
+     * connecting two keypoints (circles). See cv::DrawMatchesFlags.
+     */
+    public static void drawMatches(Mat img1, MatOfKeyPoint keypoints1, Mat img2, MatOfKeyPoint keypoints2, MatOfDMatch matches1to2, Mat outImg, Scalar matchColor, Scalar singlePointColor) {
+        Mat keypoints1_mat = keypoints1;
+        Mat keypoints2_mat = keypoints2;
+        Mat matches1to2_mat = matches1to2;
+        drawMatches_2(img1.nativeObj, keypoints1_mat.nativeObj, img2.nativeObj, keypoints2_mat.nativeObj, matches1to2_mat.nativeObj, outImg.nativeObj, matchColor.val[0], matchColor.val[1], matchColor.val[2], matchColor.val[3], singlePointColor.val[0], singlePointColor.val[1], singlePointColor.val[2], singlePointColor.val[3]);
+    }
+
+    /**
+     * Draws the found matches of keypoints from two images.
+     *
+     * @param img1 First source image.
+     * @param keypoints1 Keypoints from the first source image.
+     * @param img2 Second source image.
+     * @param keypoints2 Keypoints from the second source image.
+     * @param matches1to2 Matches from the first image to the second one, which means that keypoints1[i]
+     * has a corresponding point in keypoints2[matches[i]] .
+     * @param outImg Output image. Its content depends on the flags value defining what is drawn in the
+     * output image. See possible flags bit values below.
+     * @param matchColor Color of matches (lines and connected keypoints). If matchColor==Scalar::all(-1)
+     * , the color is generated randomly.
+     * have the matches. If singlePointColor==Scalar::all(-1) , the color is generated randomly.
+     * drawn.
+     * DrawMatchesFlags.
+     *
+     * This function draws matches of keypoints from two images in the output image. Match is a line
+     * connecting two keypoints (circles). See cv::DrawMatchesFlags.
+     */
+    public static void drawMatches(Mat img1, MatOfKeyPoint keypoints1, Mat img2, MatOfKeyPoint keypoints2, MatOfDMatch matches1to2, Mat outImg, Scalar matchColor) {
+        Mat keypoints1_mat = keypoints1;
+        Mat keypoints2_mat = keypoints2;
+        Mat matches1to2_mat = matches1to2;
+        drawMatches_3(img1.nativeObj, keypoints1_mat.nativeObj, img2.nativeObj, keypoints2_mat.nativeObj, matches1to2_mat.nativeObj, outImg.nativeObj, matchColor.val[0], matchColor.val[1], matchColor.val[2], matchColor.val[3]);
+    }
+
+    /**
+     * Draws the found matches of keypoints from two images.
+     *
+     * @param img1 First source image.
+     * @param keypoints1 Keypoints from the first source image.
+     * @param img2 Second source image.
+     * @param keypoints2 Keypoints from the second source image.
+     * @param matches1to2 Matches from the first image to the second one, which means that keypoints1[i]
+     * has a corresponding point in keypoints2[matches[i]] .
+     * @param outImg Output image. Its content depends on the flags value defining what is drawn in the
+     * output image. See possible flags bit values below.
+     * , the color is generated randomly.
+     * have the matches. If singlePointColor==Scalar::all(-1) , the color is generated randomly.
+     * drawn.
+     * DrawMatchesFlags.
+     *
+     * This function draws matches of keypoints from two images in the output image. Match is a line
+     * connecting two keypoints (circles). See cv::DrawMatchesFlags.
+     */
+    public static void drawMatches(Mat img1, MatOfKeyPoint keypoints1, Mat img2, MatOfKeyPoint keypoints2, MatOfDMatch matches1to2, Mat outImg) {
+        Mat keypoints1_mat = keypoints1;
+        Mat keypoints2_mat = keypoints2;
+        Mat matches1to2_mat = matches1to2;
+        drawMatches_4(img1.nativeObj, keypoints1_mat.nativeObj, img2.nativeObj, keypoints2_mat.nativeObj, matches1to2_mat.nativeObj, outImg.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::drawMatches(Mat img1, vector_KeyPoint keypoints1, Mat img2, vector_KeyPoint keypoints2, vector_vector_DMatch matches1to2, Mat outImg, Scalar matchColor = Scalar::all(-1), Scalar singlePointColor = Scalar::all(-1), vector_vector_char matchesMask = std::vector<std::vector<char> >(), int flags = 0)
+    //
+
+    public static void drawMatches2(Mat img1, MatOfKeyPoint keypoints1, Mat img2, MatOfKeyPoint keypoints2, List<MatOfDMatch> matches1to2, Mat outImg, Scalar matchColor, Scalar singlePointColor, List<MatOfByte> matchesMask, int flags) {
+        Mat keypoints1_mat = keypoints1;
+        Mat keypoints2_mat = keypoints2;
+        List<Mat> matches1to2_tmplm = new ArrayList<Mat>((matches1to2 != null) ? matches1to2.size() : 0);
+        Mat matches1to2_mat = Converters.vector_vector_DMatch_to_Mat(matches1to2, matches1to2_tmplm);
+        List<Mat> matchesMask_tmplm = new ArrayList<Mat>((matchesMask != null) ? matchesMask.size() : 0);
+        Mat matchesMask_mat = Converters.vector_vector_char_to_Mat(matchesMask, matchesMask_tmplm);
+        drawMatches2_0(img1.nativeObj, keypoints1_mat.nativeObj, img2.nativeObj, keypoints2_mat.nativeObj, matches1to2_mat.nativeObj, outImg.nativeObj, matchColor.val[0], matchColor.val[1], matchColor.val[2], matchColor.val[3], singlePointColor.val[0], singlePointColor.val[1], singlePointColor.val[2], singlePointColor.val[3], matchesMask_mat.nativeObj, flags);
+    }
+
+    public static void drawMatches2(Mat img1, MatOfKeyPoint keypoints1, Mat img2, MatOfKeyPoint keypoints2, List<MatOfDMatch> matches1to2, Mat outImg, Scalar matchColor, Scalar singlePointColor, List<MatOfByte> matchesMask) {
+        Mat keypoints1_mat = keypoints1;
+        Mat keypoints2_mat = keypoints2;
+        List<Mat> matches1to2_tmplm = new ArrayList<Mat>((matches1to2 != null) ? matches1to2.size() : 0);
+        Mat matches1to2_mat = Converters.vector_vector_DMatch_to_Mat(matches1to2, matches1to2_tmplm);
+        List<Mat> matchesMask_tmplm = new ArrayList<Mat>((matchesMask != null) ? matchesMask.size() : 0);
+        Mat matchesMask_mat = Converters.vector_vector_char_to_Mat(matchesMask, matchesMask_tmplm);
+        drawMatches2_1(img1.nativeObj, keypoints1_mat.nativeObj, img2.nativeObj, keypoints2_mat.nativeObj, matches1to2_mat.nativeObj, outImg.nativeObj, matchColor.val[0], matchColor.val[1], matchColor.val[2], matchColor.val[3], singlePointColor.val[0], singlePointColor.val[1], singlePointColor.val[2], singlePointColor.val[3], matchesMask_mat.nativeObj);
+    }
+
+    public static void drawMatches2(Mat img1, MatOfKeyPoint keypoints1, Mat img2, MatOfKeyPoint keypoints2, List<MatOfDMatch> matches1to2, Mat outImg, Scalar matchColor, Scalar singlePointColor) {
+        Mat keypoints1_mat = keypoints1;
+        Mat keypoints2_mat = keypoints2;
+        List<Mat> matches1to2_tmplm = new ArrayList<Mat>((matches1to2 != null) ? matches1to2.size() : 0);
+        Mat matches1to2_mat = Converters.vector_vector_DMatch_to_Mat(matches1to2, matches1to2_tmplm);
+        drawMatches2_2(img1.nativeObj, keypoints1_mat.nativeObj, img2.nativeObj, keypoints2_mat.nativeObj, matches1to2_mat.nativeObj, outImg.nativeObj, matchColor.val[0], matchColor.val[1], matchColor.val[2], matchColor.val[3], singlePointColor.val[0], singlePointColor.val[1], singlePointColor.val[2], singlePointColor.val[3]);
+    }
+
+    public static void drawMatches2(Mat img1, MatOfKeyPoint keypoints1, Mat img2, MatOfKeyPoint keypoints2, List<MatOfDMatch> matches1to2, Mat outImg, Scalar matchColor) {
+        Mat keypoints1_mat = keypoints1;
+        Mat keypoints2_mat = keypoints2;
+        List<Mat> matches1to2_tmplm = new ArrayList<Mat>((matches1to2 != null) ? matches1to2.size() : 0);
+        Mat matches1to2_mat = Converters.vector_vector_DMatch_to_Mat(matches1to2, matches1to2_tmplm);
+        drawMatches2_3(img1.nativeObj, keypoints1_mat.nativeObj, img2.nativeObj, keypoints2_mat.nativeObj, matches1to2_mat.nativeObj, outImg.nativeObj, matchColor.val[0], matchColor.val[1], matchColor.val[2], matchColor.val[3]);
+    }
+
+    public static void drawMatches2(Mat img1, MatOfKeyPoint keypoints1, Mat img2, MatOfKeyPoint keypoints2, List<MatOfDMatch> matches1to2, Mat outImg) {
+        Mat keypoints1_mat = keypoints1;
+        Mat keypoints2_mat = keypoints2;
+        List<Mat> matches1to2_tmplm = new ArrayList<Mat>((matches1to2 != null) ? matches1to2.size() : 0);
+        Mat matches1to2_mat = Converters.vector_vector_DMatch_to_Mat(matches1to2, matches1to2_tmplm);
+        drawMatches2_4(img1.nativeObj, keypoints1_mat.nativeObj, img2.nativeObj, keypoints2_mat.nativeObj, matches1to2_mat.nativeObj, outImg.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::drawMatches(Mat img1, vector_KeyPoint keypoints1, Mat img2, vector_KeyPoint keypoints2, vector_vector_DMatch matches1to2, Mat& outImg, Scalar matchColor = Scalar::all(-1), Scalar singlePointColor = Scalar::all(-1), vector_vector_char matchesMask = std::vector<std::vector<char> >(), int flags = DrawMatchesFlags::DEFAULT)
+    //
+
+    public static void drawMatchesKnn(Mat img1, MatOfKeyPoint keypoints1, Mat img2, MatOfKeyPoint keypoints2, List<MatOfDMatch> matches1to2, Mat outImg, Scalar matchColor, Scalar singlePointColor, List<MatOfByte> matchesMask, int flags) {
+        Mat keypoints1_mat = keypoints1;
+        Mat keypoints2_mat = keypoints2;
+        List<Mat> matches1to2_tmplm = new ArrayList<Mat>((matches1to2 != null) ? matches1to2.size() : 0);
+        Mat matches1to2_mat = Converters.vector_vector_DMatch_to_Mat(matches1to2, matches1to2_tmplm);
+        List<Mat> matchesMask_tmplm = new ArrayList<Mat>((matchesMask != null) ? matchesMask.size() : 0);
+        Mat matchesMask_mat = Converters.vector_vector_char_to_Mat(matchesMask, matchesMask_tmplm);
+        drawMatchesKnn_0(img1.nativeObj, keypoints1_mat.nativeObj, img2.nativeObj, keypoints2_mat.nativeObj, matches1to2_mat.nativeObj, outImg.nativeObj, matchColor.val[0], matchColor.val[1], matchColor.val[2], matchColor.val[3], singlePointColor.val[0], singlePointColor.val[1], singlePointColor.val[2], singlePointColor.val[3], matchesMask_mat.nativeObj, flags);
+    }
+
+    public static void drawMatchesKnn(Mat img1, MatOfKeyPoint keypoints1, Mat img2, MatOfKeyPoint keypoints2, List<MatOfDMatch> matches1to2, Mat outImg, Scalar matchColor, Scalar singlePointColor, List<MatOfByte> matchesMask) {
+        Mat keypoints1_mat = keypoints1;
+        Mat keypoints2_mat = keypoints2;
+        List<Mat> matches1to2_tmplm = new ArrayList<Mat>((matches1to2 != null) ? matches1to2.size() : 0);
+        Mat matches1to2_mat = Converters.vector_vector_DMatch_to_Mat(matches1to2, matches1to2_tmplm);
+        List<Mat> matchesMask_tmplm = new ArrayList<Mat>((matchesMask != null) ? matchesMask.size() : 0);
+        Mat matchesMask_mat = Converters.vector_vector_char_to_Mat(matchesMask, matchesMask_tmplm);
+        drawMatchesKnn_1(img1.nativeObj, keypoints1_mat.nativeObj, img2.nativeObj, keypoints2_mat.nativeObj, matches1to2_mat.nativeObj, outImg.nativeObj, matchColor.val[0], matchColor.val[1], matchColor.val[2], matchColor.val[3], singlePointColor.val[0], singlePointColor.val[1], singlePointColor.val[2], singlePointColor.val[3], matchesMask_mat.nativeObj);
+    }
+
+    public static void drawMatchesKnn(Mat img1, MatOfKeyPoint keypoints1, Mat img2, MatOfKeyPoint keypoints2, List<MatOfDMatch> matches1to2, Mat outImg, Scalar matchColor, Scalar singlePointColor) {
+        Mat keypoints1_mat = keypoints1;
+        Mat keypoints2_mat = keypoints2;
+        List<Mat> matches1to2_tmplm = new ArrayList<Mat>((matches1to2 != null) ? matches1to2.size() : 0);
+        Mat matches1to2_mat = Converters.vector_vector_DMatch_to_Mat(matches1to2, matches1to2_tmplm);
+        drawMatchesKnn_2(img1.nativeObj, keypoints1_mat.nativeObj, img2.nativeObj, keypoints2_mat.nativeObj, matches1to2_mat.nativeObj, outImg.nativeObj, matchColor.val[0], matchColor.val[1], matchColor.val[2], matchColor.val[3], singlePointColor.val[0], singlePointColor.val[1], singlePointColor.val[2], singlePointColor.val[3]);
+    }
+
+    public static void drawMatchesKnn(Mat img1, MatOfKeyPoint keypoints1, Mat img2, MatOfKeyPoint keypoints2, List<MatOfDMatch> matches1to2, Mat outImg, Scalar matchColor) {
+        Mat keypoints1_mat = keypoints1;
+        Mat keypoints2_mat = keypoints2;
+        List<Mat> matches1to2_tmplm = new ArrayList<Mat>((matches1to2 != null) ? matches1to2.size() : 0);
+        Mat matches1to2_mat = Converters.vector_vector_DMatch_to_Mat(matches1to2, matches1to2_tmplm);
+        drawMatchesKnn_3(img1.nativeObj, keypoints1_mat.nativeObj, img2.nativeObj, keypoints2_mat.nativeObj, matches1to2_mat.nativeObj, outImg.nativeObj, matchColor.val[0], matchColor.val[1], matchColor.val[2], matchColor.val[3]);
+    }
+
+    public static void drawMatchesKnn(Mat img1, MatOfKeyPoint keypoints1, Mat img2, MatOfKeyPoint keypoints2, List<MatOfDMatch> matches1to2, Mat outImg) {
+        Mat keypoints1_mat = keypoints1;
+        Mat keypoints2_mat = keypoints2;
+        List<Mat> matches1to2_tmplm = new ArrayList<Mat>((matches1to2 != null) ? matches1to2.size() : 0);
+        Mat matches1to2_mat = Converters.vector_vector_DMatch_to_Mat(matches1to2, matches1to2_tmplm);
+        drawMatchesKnn_4(img1.nativeObj, keypoints1_mat.nativeObj, img2.nativeObj, keypoints2_mat.nativeObj, matches1to2_mat.nativeObj, outImg.nativeObj);
+    }
+
+
+
+
+    // C++:  void cv::drawKeypoints(Mat image, vector_KeyPoint keypoints, Mat& outImage, Scalar color = Scalar::all(-1), int flags = DrawMatchesFlags::DEFAULT)
+    private static native void drawKeypoints_0(long image_nativeObj, long keypoints_mat_nativeObj, long outImage_nativeObj, double color_val0, double color_val1, double color_val2, double color_val3, int flags);
+    private static native void drawKeypoints_1(long image_nativeObj, long keypoints_mat_nativeObj, long outImage_nativeObj, double color_val0, double color_val1, double color_val2, double color_val3);
+    private static native void drawKeypoints_2(long image_nativeObj, long keypoints_mat_nativeObj, long outImage_nativeObj);
+
+    // C++:  void cv::drawMatches(Mat img1, vector_KeyPoint keypoints1, Mat img2, vector_KeyPoint keypoints2, vector_DMatch matches1to2, Mat& outImg, Scalar matchColor = Scalar::all(-1), Scalar singlePointColor = Scalar::all(-1), vector_char matchesMask = std::vector<char>(), int flags = DrawMatchesFlags::DEFAULT)
+    private static native void drawMatches_0(long img1_nativeObj, long keypoints1_mat_nativeObj, long img2_nativeObj, long keypoints2_mat_nativeObj, long matches1to2_mat_nativeObj, long outImg_nativeObj, double matchColor_val0, double matchColor_val1, double matchColor_val2, double matchColor_val3, double singlePointColor_val0, double singlePointColor_val1, double singlePointColor_val2, double singlePointColor_val3, long matchesMask_mat_nativeObj, int flags);
+    private static native void drawMatches_1(long img1_nativeObj, long keypoints1_mat_nativeObj, long img2_nativeObj, long keypoints2_mat_nativeObj, long matches1to2_mat_nativeObj, long outImg_nativeObj, double matchColor_val0, double matchColor_val1, double matchColor_val2, double matchColor_val3, double singlePointColor_val0, double singlePointColor_val1, double singlePointColor_val2, double singlePointColor_val3, long matchesMask_mat_nativeObj);
+    private static native void drawMatches_2(long img1_nativeObj, long keypoints1_mat_nativeObj, long img2_nativeObj, long keypoints2_mat_nativeObj, long matches1to2_mat_nativeObj, long outImg_nativeObj, double matchColor_val0, double matchColor_val1, double matchColor_val2, double matchColor_val3, double singlePointColor_val0, double singlePointColor_val1, double singlePointColor_val2, double singlePointColor_val3);
+    private static native void drawMatches_3(long img1_nativeObj, long keypoints1_mat_nativeObj, long img2_nativeObj, long keypoints2_mat_nativeObj, long matches1to2_mat_nativeObj, long outImg_nativeObj, double matchColor_val0, double matchColor_val1, double matchColor_val2, double matchColor_val3);
+    private static native void drawMatches_4(long img1_nativeObj, long keypoints1_mat_nativeObj, long img2_nativeObj, long keypoints2_mat_nativeObj, long matches1to2_mat_nativeObj, long outImg_nativeObj);
+
+    // C++:  void cv::drawMatches(Mat img1, vector_KeyPoint keypoints1, Mat img2, vector_KeyPoint keypoints2, vector_vector_DMatch matches1to2, Mat outImg, Scalar matchColor = Scalar::all(-1), Scalar singlePointColor = Scalar::all(-1), vector_vector_char matchesMask = std::vector<std::vector<char> >(), int flags = 0)
+    private static native void drawMatches2_0(long img1_nativeObj, long keypoints1_mat_nativeObj, long img2_nativeObj, long keypoints2_mat_nativeObj, long matches1to2_mat_nativeObj, long outImg_nativeObj, double matchColor_val0, double matchColor_val1, double matchColor_val2, double matchColor_val3, double singlePointColor_val0, double singlePointColor_val1, double singlePointColor_val2, double singlePointColor_val3, long matchesMask_mat_nativeObj, int flags);
+    private static native void drawMatches2_1(long img1_nativeObj, long keypoints1_mat_nativeObj, long img2_nativeObj, long keypoints2_mat_nativeObj, long matches1to2_mat_nativeObj, long outImg_nativeObj, double matchColor_val0, double matchColor_val1, double matchColor_val2, double matchColor_val3, double singlePointColor_val0, double singlePointColor_val1, double singlePointColor_val2, double singlePointColor_val3, long matchesMask_mat_nativeObj);
+    private static native void drawMatches2_2(long img1_nativeObj, long keypoints1_mat_nativeObj, long img2_nativeObj, long keypoints2_mat_nativeObj, long matches1to2_mat_nativeObj, long outImg_nativeObj, double matchColor_val0, double matchColor_val1, double matchColor_val2, double matchColor_val3, double singlePointColor_val0, double singlePointColor_val1, double singlePointColor_val2, double singlePointColor_val3);
+    private static native void drawMatches2_3(long img1_nativeObj, long keypoints1_mat_nativeObj, long img2_nativeObj, long keypoints2_mat_nativeObj, long matches1to2_mat_nativeObj, long outImg_nativeObj, double matchColor_val0, double matchColor_val1, double matchColor_val2, double matchColor_val3);
+    private static native void drawMatches2_4(long img1_nativeObj, long keypoints1_mat_nativeObj, long img2_nativeObj, long keypoints2_mat_nativeObj, long matches1to2_mat_nativeObj, long outImg_nativeObj);
+
+    // C++:  void cv::drawMatches(Mat img1, vector_KeyPoint keypoints1, Mat img2, vector_KeyPoint keypoints2, vector_vector_DMatch matches1to2, Mat& outImg, Scalar matchColor = Scalar::all(-1), Scalar singlePointColor = Scalar::all(-1), vector_vector_char matchesMask = std::vector<std::vector<char> >(), int flags = DrawMatchesFlags::DEFAULT)
+    private static native void drawMatchesKnn_0(long img1_nativeObj, long keypoints1_mat_nativeObj, long img2_nativeObj, long keypoints2_mat_nativeObj, long matches1to2_mat_nativeObj, long outImg_nativeObj, double matchColor_val0, double matchColor_val1, double matchColor_val2, double matchColor_val3, double singlePointColor_val0, double singlePointColor_val1, double singlePointColor_val2, double singlePointColor_val3, long matchesMask_mat_nativeObj, int flags);
+    private static native void drawMatchesKnn_1(long img1_nativeObj, long keypoints1_mat_nativeObj, long img2_nativeObj, long keypoints2_mat_nativeObj, long matches1to2_mat_nativeObj, long outImg_nativeObj, double matchColor_val0, double matchColor_val1, double matchColor_val2, double matchColor_val3, double singlePointColor_val0, double singlePointColor_val1, double singlePointColor_val2, double singlePointColor_val3, long matchesMask_mat_nativeObj);
+    private static native void drawMatchesKnn_2(long img1_nativeObj, long keypoints1_mat_nativeObj, long img2_nativeObj, long keypoints2_mat_nativeObj, long matches1to2_mat_nativeObj, long outImg_nativeObj, double matchColor_val0, double matchColor_val1, double matchColor_val2, double matchColor_val3, double singlePointColor_val0, double singlePointColor_val1, double singlePointColor_val2, double singlePointColor_val3);
+    private static native void drawMatchesKnn_3(long img1_nativeObj, long keypoints1_mat_nativeObj, long img2_nativeObj, long keypoints2_mat_nativeObj, long matches1to2_mat_nativeObj, long outImg_nativeObj, double matchColor_val0, double matchColor_val1, double matchColor_val2, double matchColor_val3);
+    private static native void drawMatchesKnn_4(long img1_nativeObj, long keypoints1_mat_nativeObj, long img2_nativeObj, long keypoints2_mat_nativeObj, long matches1to2_mat_nativeObj, long outImg_nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/core/Range.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/core/Range.java	(date 1605830247423)
+++ openCVLibrary3411/src/main/java/org/opencv/core/Range.java	(date 1605830247423)
@@ -0,0 +1,82 @@
+package org.opencv.core;
+
+//javadoc:Range
+public class Range {
+
+    public int start, end;
+
+    public Range(int s, int e) {
+        this.start = s;
+        this.end = e;
+    }
+
+    public Range() {
+        this(0, 0);
+    }
+
+    public Range(double[] vals) {
+        set(vals);
+    }
+
+    public void set(double[] vals) {
+        if (vals != null) {
+            start = vals.length > 0 ? (int) vals[0] : 0;
+            end = vals.length > 1 ? (int) vals[1] : 0;
+        } else {
+            start = 0;
+            end = 0;
+        }
+
+    }
+
+    public int size() {
+        return empty() ? 0 : end - start;
+    }
+
+    public boolean empty() {
+        return end <= start;
+    }
+
+    public static Range all() {
+        return new Range(Integer.MIN_VALUE, Integer.MAX_VALUE);
+    }
+
+    public Range intersection(Range r1) {
+        Range r = new Range(Math.max(r1.start, this.start), Math.min(r1.end, this.end));
+        r.end = Math.max(r.end, r.start);
+        return r;
+    }
+
+    public Range shift(int delta) {
+        return new Range(start + delta, end + delta);
+    }
+
+    public Range clone() {
+        return new Range(start, end);
+    }
+
+    @Override
+    public int hashCode() {
+        final int prime = 31;
+        int result = 1;
+        long temp;
+        temp = Double.doubleToLongBits(start);
+        result = prime * result + (int) (temp ^ (temp >>> 32));
+        temp = Double.doubleToLongBits(end);
+        result = prime * result + (int) (temp ^ (temp >>> 32));
+        return result;
+    }
+
+    @Override
+    public boolean equals(Object obj) {
+        if (this == obj) return true;
+        if (!(obj instanceof Range)) return false;
+        Range it = (Range) obj;
+        return start == it.start && end == it.end;
+    }
+
+    @Override
+    public String toString() {
+        return "[" + start + ", " + end + ")";
+    }
+}
Index: openCVLibrary3411/src/main/java/org/opencv/features2d/FlannBasedMatcher.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/features2d/FlannBasedMatcher.java	(date 1605830247667)
+++ openCVLibrary3411/src/main/java/org/opencv/features2d/FlannBasedMatcher.java	(date 1605830247667)
@@ -0,0 +1,59 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.features2d;
+
+import org.opencv.features2d.DescriptorMatcher;
+import org.opencv.features2d.FlannBasedMatcher;
+
+// C++: class FlannBasedMatcher
+/**
+ * Flann-based descriptor matcher.
+ *
+ * This matcher trains cv::flann::Index on a train descriptor collection and calls its nearest search
+ * methods to find the best matches. So, this matcher may be faster when matching a large train
+ * collection than the brute force matcher. FlannBasedMatcher does not support masking permissible
+ * matches of descriptor sets because flann::Index does not support this. :
+ */
+public class FlannBasedMatcher extends DescriptorMatcher {
+
+    protected FlannBasedMatcher(long addr) { super(addr); }
+
+    // internal usage only
+    public static FlannBasedMatcher __fromPtr__(long addr) { return new FlannBasedMatcher(addr); }
+
+    //
+    // C++:   cv::FlannBasedMatcher::FlannBasedMatcher(Ptr_flann_IndexParams indexParams = makePtr<flann::KDTreeIndexParams>(), Ptr_flann_SearchParams searchParams = makePtr<flann::SearchParams>())
+    //
+
+    public FlannBasedMatcher() {
+        super(FlannBasedMatcher_0());
+    }
+
+
+    //
+    // C++: static Ptr_FlannBasedMatcher cv::FlannBasedMatcher::create()
+    //
+
+    public static FlannBasedMatcher create() {
+        return FlannBasedMatcher.__fromPtr__(create_0());
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++:   cv::FlannBasedMatcher::FlannBasedMatcher(Ptr_flann_IndexParams indexParams = makePtr<flann::KDTreeIndexParams>(), Ptr_flann_SearchParams searchParams = makePtr<flann::SearchParams>())
+    private static native long FlannBasedMatcher_0();
+
+    // C++: static Ptr_FlannBasedMatcher cv::FlannBasedMatcher::create()
+    private static native long create_0();
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/core/Rect.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/core/Rect.java	(date 1605830247430)
+++ openCVLibrary3411/src/main/java/org/opencv/core/Rect.java	(date 1605830247430)
@@ -0,0 +1,104 @@
+package org.opencv.core;
+
+//javadoc:Rect_
+public class Rect {
+
+    public int x, y, width, height;
+
+    public Rect(int x, int y, int width, int height) {
+        this.x = x;
+        this.y = y;
+        this.width = width;
+        this.height = height;
+    }
+
+    public Rect() {
+        this(0, 0, 0, 0);
+    }
+
+    public Rect(Point p1, Point p2) {
+        x = (int) (p1.x < p2.x ? p1.x : p2.x);
+        y = (int) (p1.y < p2.y ? p1.y : p2.y);
+        width = (int) (p1.x > p2.x ? p1.x : p2.x) - x;
+        height = (int) (p1.y > p2.y ? p1.y : p2.y) - y;
+    }
+
+    public Rect(Point p, Size s) {
+        this((int) p.x, (int) p.y, (int) s.width, (int) s.height);
+    }
+
+    public Rect(double[] vals) {
+        set(vals);
+    }
+
+    public void set(double[] vals) {
+        if (vals != null) {
+            x = vals.length > 0 ? (int) vals[0] : 0;
+            y = vals.length > 1 ? (int) vals[1] : 0;
+            width = vals.length > 2 ? (int) vals[2] : 0;
+            height = vals.length > 3 ? (int) vals[3] : 0;
+        } else {
+            x = 0;
+            y = 0;
+            width = 0;
+            height = 0;
+        }
+    }
+
+    public Rect clone() {
+        return new Rect(x, y, width, height);
+    }
+
+    public Point tl() {
+        return new Point(x, y);
+    }
+
+    public Point br() {
+        return new Point(x + width, y + height);
+    }
+
+    public Size size() {
+        return new Size(width, height);
+    }
+
+    public double area() {
+        return width * height;
+    }
+
+    public boolean empty() {
+        return width <= 0 || height <= 0;
+    }
+
+    public boolean contains(Point p) {
+        return x <= p.x && p.x < x + width && y <= p.y && p.y < y + height;
+    }
+
+    @Override
+    public int hashCode() {
+        final int prime = 31;
+        int result = 1;
+        long temp;
+        temp = Double.doubleToLongBits(height);
+        result = prime * result + (int) (temp ^ (temp >>> 32));
+        temp = Double.doubleToLongBits(width);
+        result = prime * result + (int) (temp ^ (temp >>> 32));
+        temp = Double.doubleToLongBits(x);
+        result = prime * result + (int) (temp ^ (temp >>> 32));
+        temp = Double.doubleToLongBits(y);
+        result = prime * result + (int) (temp ^ (temp >>> 32));
+        return result;
+    }
+
+    @Override
+    public boolean equals(Object obj) {
+        if (this == obj) return true;
+        if (!(obj instanceof Rect)) return false;
+        Rect it = (Rect) obj;
+        return x == it.x && y == it.y && width == it.width && height == it.height;
+    }
+
+    @Override
+    public String toString() {
+        return "{" + x + ", " + y + ", " + width + "x" + height + "}";
+    }
+}
Index: openCVLibrary3411/src/main/java/org/opencv/features2d/GFTTDetector.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/features2d/GFTTDetector.java	(date 1605830247677)
+++ openCVLibrary3411/src/main/java/org/opencv/features2d/GFTTDetector.java	(date 1605830247677)
@@ -0,0 +1,250 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.features2d;
+
+import org.opencv.features2d.Feature2D;
+import org.opencv.features2d.GFTTDetector;
+
+// C++: class GFTTDetector
+/**
+ * Wrapping class for feature detection using the goodFeaturesToTrack function. :
+ */
+public class GFTTDetector extends Feature2D {
+
+    protected GFTTDetector(long addr) { super(addr); }
+
+    // internal usage only
+    public static GFTTDetector __fromPtr__(long addr) { return new GFTTDetector(addr); }
+
+    //
+    // C++: static Ptr_GFTTDetector cv::GFTTDetector::create(int maxCorners, double qualityLevel, double minDistance, int blockSize, int gradiantSize, bool useHarrisDetector = false, double k = 0.04)
+    //
+
+    public static GFTTDetector create(int maxCorners, double qualityLevel, double minDistance, int blockSize, int gradiantSize, boolean useHarrisDetector, double k) {
+        return GFTTDetector.__fromPtr__(create_0(maxCorners, qualityLevel, minDistance, blockSize, gradiantSize, useHarrisDetector, k));
+    }
+
+    public static GFTTDetector create(int maxCorners, double qualityLevel, double minDistance, int blockSize, int gradiantSize, boolean useHarrisDetector) {
+        return GFTTDetector.__fromPtr__(create_1(maxCorners, qualityLevel, minDistance, blockSize, gradiantSize, useHarrisDetector));
+    }
+
+    public static GFTTDetector create(int maxCorners, double qualityLevel, double minDistance, int blockSize, int gradiantSize) {
+        return GFTTDetector.__fromPtr__(create_2(maxCorners, qualityLevel, minDistance, blockSize, gradiantSize));
+    }
+
+
+    //
+    // C++: static Ptr_GFTTDetector cv::GFTTDetector::create(int maxCorners = 1000, double qualityLevel = 0.01, double minDistance = 1, int blockSize = 3, bool useHarrisDetector = false, double k = 0.04)
+    //
+
+    public static GFTTDetector create(int maxCorners, double qualityLevel, double minDistance, int blockSize, boolean useHarrisDetector, double k) {
+        return GFTTDetector.__fromPtr__(create_3(maxCorners, qualityLevel, minDistance, blockSize, useHarrisDetector, k));
+    }
+
+    public static GFTTDetector create(int maxCorners, double qualityLevel, double minDistance, int blockSize, boolean useHarrisDetector) {
+        return GFTTDetector.__fromPtr__(create_4(maxCorners, qualityLevel, minDistance, blockSize, useHarrisDetector));
+    }
+
+    public static GFTTDetector create(int maxCorners, double qualityLevel, double minDistance, int blockSize) {
+        return GFTTDetector.__fromPtr__(create_5(maxCorners, qualityLevel, minDistance, blockSize));
+    }
+
+    public static GFTTDetector create(int maxCorners, double qualityLevel, double minDistance) {
+        return GFTTDetector.__fromPtr__(create_6(maxCorners, qualityLevel, minDistance));
+    }
+
+    public static GFTTDetector create(int maxCorners, double qualityLevel) {
+        return GFTTDetector.__fromPtr__(create_7(maxCorners, qualityLevel));
+    }
+
+    public static GFTTDetector create(int maxCorners) {
+        return GFTTDetector.__fromPtr__(create_8(maxCorners));
+    }
+
+    public static GFTTDetector create() {
+        return GFTTDetector.__fromPtr__(create_9());
+    }
+
+
+    //
+    // C++:  String cv::GFTTDetector::getDefaultName()
+    //
+
+    public String getDefaultName() {
+        return getDefaultName_0(nativeObj);
+    }
+
+
+    //
+    // C++:  bool cv::GFTTDetector::getHarrisDetector()
+    //
+
+    public boolean getHarrisDetector() {
+        return getHarrisDetector_0(nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::GFTTDetector::getK()
+    //
+
+    public double getK() {
+        return getK_0(nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::GFTTDetector::getMinDistance()
+    //
+
+    public double getMinDistance() {
+        return getMinDistance_0(nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::GFTTDetector::getQualityLevel()
+    //
+
+    public double getQualityLevel() {
+        return getQualityLevel_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::GFTTDetector::getBlockSize()
+    //
+
+    public int getBlockSize() {
+        return getBlockSize_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::GFTTDetector::getMaxFeatures()
+    //
+
+    public int getMaxFeatures() {
+        return getMaxFeatures_0(nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::GFTTDetector::setBlockSize(int blockSize)
+    //
+
+    public void setBlockSize(int blockSize) {
+        setBlockSize_0(nativeObj, blockSize);
+    }
+
+
+    //
+    // C++:  void cv::GFTTDetector::setHarrisDetector(bool val)
+    //
+
+    public void setHarrisDetector(boolean val) {
+        setHarrisDetector_0(nativeObj, val);
+    }
+
+
+    //
+    // C++:  void cv::GFTTDetector::setK(double k)
+    //
+
+    public void setK(double k) {
+        setK_0(nativeObj, k);
+    }
+
+
+    //
+    // C++:  void cv::GFTTDetector::setMaxFeatures(int maxFeatures)
+    //
+
+    public void setMaxFeatures(int maxFeatures) {
+        setMaxFeatures_0(nativeObj, maxFeatures);
+    }
+
+
+    //
+    // C++:  void cv::GFTTDetector::setMinDistance(double minDistance)
+    //
+
+    public void setMinDistance(double minDistance) {
+        setMinDistance_0(nativeObj, minDistance);
+    }
+
+
+    //
+    // C++:  void cv::GFTTDetector::setQualityLevel(double qlevel)
+    //
+
+    public void setQualityLevel(double qlevel) {
+        setQualityLevel_0(nativeObj, qlevel);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++: static Ptr_GFTTDetector cv::GFTTDetector::create(int maxCorners, double qualityLevel, double minDistance, int blockSize, int gradiantSize, bool useHarrisDetector = false, double k = 0.04)
+    private static native long create_0(int maxCorners, double qualityLevel, double minDistance, int blockSize, int gradiantSize, boolean useHarrisDetector, double k);
+    private static native long create_1(int maxCorners, double qualityLevel, double minDistance, int blockSize, int gradiantSize, boolean useHarrisDetector);
+    private static native long create_2(int maxCorners, double qualityLevel, double minDistance, int blockSize, int gradiantSize);
+
+    // C++: static Ptr_GFTTDetector cv::GFTTDetector::create(int maxCorners = 1000, double qualityLevel = 0.01, double minDistance = 1, int blockSize = 3, bool useHarrisDetector = false, double k = 0.04)
+    private static native long create_3(int maxCorners, double qualityLevel, double minDistance, int blockSize, boolean useHarrisDetector, double k);
+    private static native long create_4(int maxCorners, double qualityLevel, double minDistance, int blockSize, boolean useHarrisDetector);
+    private static native long create_5(int maxCorners, double qualityLevel, double minDistance, int blockSize);
+    private static native long create_6(int maxCorners, double qualityLevel, double minDistance);
+    private static native long create_7(int maxCorners, double qualityLevel);
+    private static native long create_8(int maxCorners);
+    private static native long create_9();
+
+    // C++:  String cv::GFTTDetector::getDefaultName()
+    private static native String getDefaultName_0(long nativeObj);
+
+    // C++:  bool cv::GFTTDetector::getHarrisDetector()
+    private static native boolean getHarrisDetector_0(long nativeObj);
+
+    // C++:  double cv::GFTTDetector::getK()
+    private static native double getK_0(long nativeObj);
+
+    // C++:  double cv::GFTTDetector::getMinDistance()
+    private static native double getMinDistance_0(long nativeObj);
+
+    // C++:  double cv::GFTTDetector::getQualityLevel()
+    private static native double getQualityLevel_0(long nativeObj);
+
+    // C++:  int cv::GFTTDetector::getBlockSize()
+    private static native int getBlockSize_0(long nativeObj);
+
+    // C++:  int cv::GFTTDetector::getMaxFeatures()
+    private static native int getMaxFeatures_0(long nativeObj);
+
+    // C++:  void cv::GFTTDetector::setBlockSize(int blockSize)
+    private static native void setBlockSize_0(long nativeObj, int blockSize);
+
+    // C++:  void cv::GFTTDetector::setHarrisDetector(bool val)
+    private static native void setHarrisDetector_0(long nativeObj, boolean val);
+
+    // C++:  void cv::GFTTDetector::setK(double k)
+    private static native void setK_0(long nativeObj, double k);
+
+    // C++:  void cv::GFTTDetector::setMaxFeatures(int maxFeatures)
+    private static native void setMaxFeatures_0(long nativeObj, int maxFeatures);
+
+    // C++:  void cv::GFTTDetector::setMinDistance(double minDistance)
+    private static native void setMinDistance_0(long nativeObj, double minDistance);
+
+    // C++:  void cv::GFTTDetector::setQualityLevel(double qlevel)
+    private static native void setQualityLevel_0(long nativeObj, double qlevel);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/core/MatOfPoint3f.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/core/MatOfPoint3f.java	(date 1605830247413)
+++ openCVLibrary3411/src/main/java/org/opencv/core/MatOfPoint3f.java	(date 1605830247413)
@@ -0,0 +1,79 @@
+package org.opencv.core;
+
+import java.util.Arrays;
+import java.util.List;
+
+public class MatOfPoint3f extends Mat {
+    // 32FC3
+    private static final int _depth = CvType.CV_32F;
+    private static final int _channels = 3;
+
+    public MatOfPoint3f() {
+        super();
+    }
+
+    protected MatOfPoint3f(long addr) {
+        super(addr);
+        if( !empty() && checkVector(_channels, _depth) < 0 )
+            throw new IllegalArgumentException("Incompatible Mat");
+        //FIXME: do we need release() here?
+    }
+
+    public static MatOfPoint3f fromNativeAddr(long addr) {
+        return new MatOfPoint3f(addr);
+    }
+
+    public MatOfPoint3f(Mat m) {
+        super(m, Range.all());
+        if( !empty() && checkVector(_channels, _depth) < 0 )
+            throw new IllegalArgumentException("Incompatible Mat");
+        //FIXME: do we need release() here?
+    }
+
+    public MatOfPoint3f(Point3...a) {
+        super();
+        fromArray(a);
+    }
+
+    public void alloc(int elemNumber) {
+        if(elemNumber>0)
+            super.create(elemNumber, 1, CvType.makeType(_depth, _channels));
+    }
+
+    public void fromArray(Point3...a) {
+        if(a==null || a.length==0)
+            return;
+        int num = a.length;
+        alloc(num);
+        float buff[] = new float[num * _channels];
+        for(int i=0; i<num; i++) {
+            Point3 p = a[i];
+            buff[_channels*i+0] = (float) p.x;
+            buff[_channels*i+1] = (float) p.y;
+            buff[_channels*i+2] = (float) p.z;
+        }
+        put(0, 0, buff); //TODO: check ret val!
+    }
+
+    public Point3[] toArray() {
+        int num = (int) total();
+        Point3[] ap = new Point3[num];
+        if(num == 0)
+            return ap;
+        float buff[] = new float[num * _channels];
+        get(0, 0, buff); //TODO: check ret val!
+        for(int i=0; i<num; i++)
+            ap[i] = new Point3(buff[i*_channels], buff[i*_channels+1], buff[i*_channels+2]);
+        return ap;
+    }
+
+    public void fromList(List<Point3> lp) {
+        Point3 ap[] = lp.toArray(new Point3[0]);
+        fromArray(ap);
+    }
+
+    public List<Point3> toList() {
+        Point3[] ap = toArray();
+        return Arrays.asList(ap);
+    }
+}
Index: openCVLibrary3411/src/main/java/org/opencv/features2d/DescriptorExtractor.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/features2d/DescriptorExtractor.java	(date 1605830247600)
+++ openCVLibrary3411/src/main/java/org/opencv/features2d/DescriptorExtractor.java	(date 1605830247600)
@@ -0,0 +1,165 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.features2d;
+
+import java.util.ArrayList;
+import java.util.List;
+import org.opencv.core.Mat;
+import org.opencv.core.MatOfKeyPoint;
+import org.opencv.features2d.DescriptorExtractor;
+import org.opencv.utils.Converters;
+
+// C++: class javaDescriptorExtractor
+/**
+ * @deprecated
+ */
+@Deprecated
+public class DescriptorExtractor {
+
+    protected final long nativeObj;
+    protected DescriptorExtractor(long addr) { nativeObj = addr; }
+
+    public long getNativeObjAddr() { return nativeObj; }
+
+    // internal usage only
+    public static DescriptorExtractor __fromPtr__(long addr) { return new DescriptorExtractor(addr); }
+
+    private static final int
+            OPPONENTEXTRACTOR = 1000;
+
+
+    // C++: enum <unnamed>
+    public static final int
+            SIFT = 1,
+            SURF = 2,
+            ORB = 3,
+            BRIEF = 4,
+            BRISK = 5,
+            FREAK = 6,
+            AKAZE = 7,
+            OPPONENT_SIFT = OPPONENTEXTRACTOR + SIFT,
+            OPPONENT_SURF = OPPONENTEXTRACTOR + SURF,
+            OPPONENT_ORB = OPPONENTEXTRACTOR + ORB,
+            OPPONENT_BRIEF = OPPONENTEXTRACTOR + BRIEF,
+            OPPONENT_BRISK = OPPONENTEXTRACTOR + BRISK,
+            OPPONENT_FREAK = OPPONENTEXTRACTOR + FREAK,
+            OPPONENT_AKAZE = OPPONENTEXTRACTOR + AKAZE;
+
+
+    //
+    // C++: static Ptr_javaDescriptorExtractor cv::javaDescriptorExtractor::create(int extractorType)
+    //
+
+    public static DescriptorExtractor create(int extractorType) {
+        return DescriptorExtractor.__fromPtr__(create_0(extractorType));
+    }
+
+
+    //
+    // C++:  bool cv::javaDescriptorExtractor::empty()
+    //
+
+    public boolean empty() {
+        return empty_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::javaDescriptorExtractor::descriptorSize()
+    //
+
+    public int descriptorSize() {
+        return descriptorSize_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::javaDescriptorExtractor::descriptorType()
+    //
+
+    public int descriptorType() {
+        return descriptorType_0(nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::javaDescriptorExtractor::compute(Mat image, vector_KeyPoint& keypoints, Mat descriptors)
+    //
+
+    public void compute(Mat image, MatOfKeyPoint keypoints, Mat descriptors) {
+        Mat keypoints_mat = keypoints;
+        compute_0(nativeObj, image.nativeObj, keypoints_mat.nativeObj, descriptors.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::javaDescriptorExtractor::compute(vector_Mat images, vector_vector_KeyPoint& keypoints, vector_Mat& descriptors)
+    //
+
+    public void compute(List<Mat> images, List<MatOfKeyPoint> keypoints, List<Mat> descriptors) {
+        Mat images_mat = Converters.vector_Mat_to_Mat(images);
+        List<Mat> keypoints_tmplm = new ArrayList<Mat>((keypoints != null) ? keypoints.size() : 0);
+        Mat keypoints_mat = Converters.vector_vector_KeyPoint_to_Mat(keypoints, keypoints_tmplm);
+        Mat descriptors_mat = new Mat();
+        compute_1(nativeObj, images_mat.nativeObj, keypoints_mat.nativeObj, descriptors_mat.nativeObj);
+        Converters.Mat_to_vector_vector_KeyPoint(keypoints_mat, keypoints);
+        keypoints_mat.release();
+        Converters.Mat_to_vector_Mat(descriptors_mat, descriptors);
+        descriptors_mat.release();
+    }
+
+
+    //
+    // C++:  void cv::javaDescriptorExtractor::read(String fileName)
+    //
+
+    public void read(String fileName) {
+        read_0(nativeObj, fileName);
+    }
+
+
+    //
+    // C++:  void cv::javaDescriptorExtractor::write(String fileName)
+    //
+
+    public void write(String fileName) {
+        write_0(nativeObj, fileName);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++: static Ptr_javaDescriptorExtractor cv::javaDescriptorExtractor::create(int extractorType)
+    private static native long create_0(int extractorType);
+
+    // C++:  bool cv::javaDescriptorExtractor::empty()
+    private static native boolean empty_0(long nativeObj);
+
+    // C++:  int cv::javaDescriptorExtractor::descriptorSize()
+    private static native int descriptorSize_0(long nativeObj);
+
+    // C++:  int cv::javaDescriptorExtractor::descriptorType()
+    private static native int descriptorType_0(long nativeObj);
+
+    // C++:  void cv::javaDescriptorExtractor::compute(Mat image, vector_KeyPoint& keypoints, Mat descriptors)
+    private static native void compute_0(long nativeObj, long image_nativeObj, long keypoints_mat_nativeObj, long descriptors_nativeObj);
+
+    // C++:  void cv::javaDescriptorExtractor::compute(vector_Mat images, vector_vector_KeyPoint& keypoints, vector_Mat& descriptors)
+    private static native void compute_1(long nativeObj, long images_mat_nativeObj, long keypoints_mat_nativeObj, long descriptors_mat_nativeObj);
+
+    // C++:  void cv::javaDescriptorExtractor::read(String fileName)
+    private static native void read_0(long nativeObj, String fileName);
+
+    // C++:  void cv::javaDescriptorExtractor::write(String fileName)
+    private static native void write_0(long nativeObj, String fileName);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/core/MatOfRect.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/core/MatOfRect.java	(date 1605830247414)
+++ openCVLibrary3411/src/main/java/org/opencv/core/MatOfRect.java	(date 1605830247414)
@@ -0,0 +1,81 @@
+package org.opencv.core;
+
+import java.util.Arrays;
+import java.util.List;
+
+
+public class MatOfRect extends Mat {
+    // 32SC4
+    private static final int _depth = CvType.CV_32S;
+    private static final int _channels = 4;
+
+    public MatOfRect() {
+        super();
+    }
+
+    protected MatOfRect(long addr) {
+        super(addr);
+        if( !empty() && checkVector(_channels, _depth) < 0 )
+            throw new IllegalArgumentException("Incompatible Mat");
+        //FIXME: do we need release() here?
+    }
+
+    public static MatOfRect fromNativeAddr(long addr) {
+        return new MatOfRect(addr);
+    }
+
+    public MatOfRect(Mat m) {
+        super(m, Range.all());
+        if( !empty() && checkVector(_channels, _depth) < 0 )
+            throw new IllegalArgumentException("Incompatible Mat");
+        //FIXME: do we need release() here?
+    }
+
+    public MatOfRect(Rect...a) {
+        super();
+        fromArray(a);
+    }
+
+    public void alloc(int elemNumber) {
+        if(elemNumber>0)
+            super.create(elemNumber, 1, CvType.makeType(_depth, _channels));
+    }
+
+    public void fromArray(Rect...a) {
+        if(a==null || a.length==0)
+            return;
+        int num = a.length;
+        alloc(num);
+        int buff[] = new int[num * _channels];
+        for(int i=0; i<num; i++) {
+            Rect r = a[i];
+            buff[_channels*i+0] = (int) r.x;
+            buff[_channels*i+1] = (int) r.y;
+            buff[_channels*i+2] = (int) r.width;
+            buff[_channels*i+3] = (int) r.height;
+        }
+        put(0, 0, buff); //TODO: check ret val!
+    }
+
+
+    public Rect[] toArray() {
+        int num = (int) total();
+        Rect[] a = new Rect[num];
+        if(num == 0)
+            return a;
+        int buff[] = new int[num * _channels];
+        get(0, 0, buff); //TODO: check ret val!
+        for(int i=0; i<num; i++)
+            a[i] = new Rect(buff[i*_channels], buff[i*_channels+1], buff[i*_channels+2], buff[i*_channels+3]);
+        return a;
+    }
+    public void fromList(List<Rect> lr) {
+        Rect ap[] = lr.toArray(new Rect[0]);
+        fromArray(ap);
+    }
+
+    public List<Rect> toList() {
+        Rect[] ar = toArray();
+        return Arrays.asList(ar);
+    }
+}
Index: openCVLibrary3411/src/main/java/org/opencv/features2d/DescriptorMatcher.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/features2d/DescriptorMatcher.java	(date 1605830247615)
+++ openCVLibrary3411/src/main/java/org/opencv/features2d/DescriptorMatcher.java	(date 1605830247615)
@@ -0,0 +1,670 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.features2d;
+
+import java.util.ArrayList;
+import java.util.List;
+import org.opencv.core.Algorithm;
+import org.opencv.core.Mat;
+import org.opencv.core.MatOfDMatch;
+import org.opencv.features2d.DescriptorMatcher;
+import org.opencv.utils.Converters;
+
+// C++: class DescriptorMatcher
+/**
+ * Abstract base class for matching keypoint descriptors.
+ *
+ * It has two groups of match methods: for matching descriptors of an image with another image or with
+ * an image set.
+ */
+public class DescriptorMatcher extends Algorithm {
+
+    protected DescriptorMatcher(long addr) { super(addr); }
+
+    // internal usage only
+    public static DescriptorMatcher __fromPtr__(long addr) { return new DescriptorMatcher(addr); }
+
+    // C++: enum <unnamed>
+    public static final int
+            FLANNBASED = 1,
+            BRUTEFORCE = 2,
+            BRUTEFORCE_L1 = 3,
+            BRUTEFORCE_HAMMING = 4,
+            BRUTEFORCE_HAMMINGLUT = 5,
+            BRUTEFORCE_SL2 = 6;
+
+
+    //
+    // C++:  Ptr_DescriptorMatcher cv::DescriptorMatcher::clone(bool emptyTrainData = false)
+    //
+
+    /**
+     * Clones the matcher.
+     *
+     *     @param emptyTrainData If emptyTrainData is false, the method creates a deep copy of the object,
+     *     that is, copies both parameters and train data. If emptyTrainData is true, the method creates an
+     *     object copy with the current parameters but with empty train data.
+     * @return automatically generated
+     */
+    public DescriptorMatcher clone(boolean emptyTrainData) {
+        return DescriptorMatcher.__fromPtr__(clone_0(nativeObj, emptyTrainData));
+    }
+
+    /**
+     * Clones the matcher.
+     *
+     *     that is, copies both parameters and train data. If emptyTrainData is true, the method creates an
+     *     object copy with the current parameters but with empty train data.
+     * @return automatically generated
+     */
+    public DescriptorMatcher clone() {
+        return DescriptorMatcher.__fromPtr__(clone_1(nativeObj));
+    }
+
+
+    //
+    // C++: static Ptr_DescriptorMatcher cv::DescriptorMatcher::create(String descriptorMatcherType)
+    //
+
+    /**
+     * Creates a descriptor matcher of a given type with the default parameters (using default
+     *     constructor).
+     *
+     *     @param descriptorMatcherType Descriptor matcher type. Now the following matcher types are
+     *     supported:
+     * <ul>
+     *   <li>
+     *        {@code BruteForce} (it uses L2 )
+     *   </li>
+     *   <li>
+     *        {@code BruteForce-L1}
+     *   </li>
+     *   <li>
+     *        {@code BruteForce-Hamming}
+     *   </li>
+     *   <li>
+     *        {@code BruteForce-Hamming(2)}
+     *   </li>
+     *   <li>
+     *        {@code FlannBased}
+     *   </li>
+     * </ul>
+     * @return automatically generated
+     */
+    public static DescriptorMatcher create(String descriptorMatcherType) {
+        return DescriptorMatcher.__fromPtr__(create_0(descriptorMatcherType));
+    }
+
+
+    //
+    // C++: static Ptr_DescriptorMatcher cv::DescriptorMatcher::create(int matcherType)
+    //
+
+    public static DescriptorMatcher create(int matcherType) {
+        return DescriptorMatcher.__fromPtr__(create_1(matcherType));
+    }
+
+
+    //
+    // C++:  bool cv::DescriptorMatcher::empty()
+    //
+
+    /**
+     * Returns true if there are no train descriptors in the both collections.
+     * @return automatically generated
+     */
+    public boolean empty() {
+        return empty_0(nativeObj);
+    }
+
+
+    //
+    // C++:  bool cv::DescriptorMatcher::isMaskSupported()
+    //
+
+    /**
+     * Returns true if the descriptor matcher supports masking permissible matches.
+     * @return automatically generated
+     */
+    public boolean isMaskSupported() {
+        return isMaskSupported_0(nativeObj);
+    }
+
+
+    //
+    // C++:  vector_Mat cv::DescriptorMatcher::getTrainDescriptors()
+    //
+
+    /**
+     * Returns a constant link to the train descriptor collection trainDescCollection .
+     * @return automatically generated
+     */
+    public List<Mat> getTrainDescriptors() {
+        List<Mat> retVal = new ArrayList<Mat>();
+        Mat retValMat = new Mat(getTrainDescriptors_0(nativeObj));
+        Converters.Mat_to_vector_Mat(retValMat, retVal);
+        return retVal;
+    }
+
+
+    //
+    // C++:  void cv::DescriptorMatcher::add(vector_Mat descriptors)
+    //
+
+    /**
+     * Adds descriptors to train a CPU(trainDescCollectionis) or GPU(utrainDescCollectionis) descriptor
+     *     collection.
+     *
+     *     If the collection is not empty, the new descriptors are added to existing train descriptors.
+     *
+     *     @param descriptors Descriptors to add. Each descriptors[i] is a set of descriptors from the same
+     *     train image.
+     */
+    public void add(List<Mat> descriptors) {
+        Mat descriptors_mat = Converters.vector_Mat_to_Mat(descriptors);
+        add_0(nativeObj, descriptors_mat.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::DescriptorMatcher::clear()
+    //
+
+    /**
+     * Clears the train descriptor collections.
+     */
+    public void clear() {
+        clear_0(nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::DescriptorMatcher::knnMatch(Mat queryDescriptors, Mat trainDescriptors, vector_vector_DMatch& matches, int k, Mat mask = Mat(), bool compactResult = false)
+    //
+
+    /**
+     * Finds the k best matches for each descriptor from a query set.
+     *
+     *     @param queryDescriptors Query set of descriptors.
+     *     @param trainDescriptors Train set of descriptors. This set is not added to the train descriptors
+     *     collection stored in the class object.
+     *     @param mask Mask specifying permissible matches between an input query and train matrices of
+     *     descriptors.
+     *     @param matches Matches. Each matches[i] is k or less matches for the same query descriptor.
+     *     @param k Count of best matches found per each query descriptor or less if a query descriptor has
+     *     less than k possible matches in total.
+     *     @param compactResult Parameter used when the mask (or masks) is not empty. If compactResult is
+     *     false, the matches vector has the same size as queryDescriptors rows. If compactResult is true,
+     *     the matches vector does not contain matches for fully masked-out query descriptors.
+     *
+     *     These extended variants of DescriptorMatcher::match methods find several best matches for each query
+     *     descriptor. The matches are returned in the distance increasing order. See DescriptorMatcher::match
+     *     for the details about query and train descriptors.
+     */
+    public void knnMatch(Mat queryDescriptors, Mat trainDescriptors, List<MatOfDMatch> matches, int k, Mat mask, boolean compactResult) {
+        Mat matches_mat = new Mat();
+        knnMatch_0(nativeObj, queryDescriptors.nativeObj, trainDescriptors.nativeObj, matches_mat.nativeObj, k, mask.nativeObj, compactResult);
+        Converters.Mat_to_vector_vector_DMatch(matches_mat, matches);
+        matches_mat.release();
+    }
+
+    /**
+     * Finds the k best matches for each descriptor from a query set.
+     *
+     *     @param queryDescriptors Query set of descriptors.
+     *     @param trainDescriptors Train set of descriptors. This set is not added to the train descriptors
+     *     collection stored in the class object.
+     *     @param mask Mask specifying permissible matches between an input query and train matrices of
+     *     descriptors.
+     *     @param matches Matches. Each matches[i] is k or less matches for the same query descriptor.
+     *     @param k Count of best matches found per each query descriptor or less if a query descriptor has
+     *     less than k possible matches in total.
+     *     false, the matches vector has the same size as queryDescriptors rows. If compactResult is true,
+     *     the matches vector does not contain matches for fully masked-out query descriptors.
+     *
+     *     These extended variants of DescriptorMatcher::match methods find several best matches for each query
+     *     descriptor. The matches are returned in the distance increasing order. See DescriptorMatcher::match
+     *     for the details about query and train descriptors.
+     */
+    public void knnMatch(Mat queryDescriptors, Mat trainDescriptors, List<MatOfDMatch> matches, int k, Mat mask) {
+        Mat matches_mat = new Mat();
+        knnMatch_1(nativeObj, queryDescriptors.nativeObj, trainDescriptors.nativeObj, matches_mat.nativeObj, k, mask.nativeObj);
+        Converters.Mat_to_vector_vector_DMatch(matches_mat, matches);
+        matches_mat.release();
+    }
+
+    /**
+     * Finds the k best matches for each descriptor from a query set.
+     *
+     *     @param queryDescriptors Query set of descriptors.
+     *     @param trainDescriptors Train set of descriptors. This set is not added to the train descriptors
+     *     collection stored in the class object.
+     *     descriptors.
+     *     @param matches Matches. Each matches[i] is k or less matches for the same query descriptor.
+     *     @param k Count of best matches found per each query descriptor or less if a query descriptor has
+     *     less than k possible matches in total.
+     *     false, the matches vector has the same size as queryDescriptors rows. If compactResult is true,
+     *     the matches vector does not contain matches for fully masked-out query descriptors.
+     *
+     *     These extended variants of DescriptorMatcher::match methods find several best matches for each query
+     *     descriptor. The matches are returned in the distance increasing order. See DescriptorMatcher::match
+     *     for the details about query and train descriptors.
+     */
+    public void knnMatch(Mat queryDescriptors, Mat trainDescriptors, List<MatOfDMatch> matches, int k) {
+        Mat matches_mat = new Mat();
+        knnMatch_2(nativeObj, queryDescriptors.nativeObj, trainDescriptors.nativeObj, matches_mat.nativeObj, k);
+        Converters.Mat_to_vector_vector_DMatch(matches_mat, matches);
+        matches_mat.release();
+    }
+
+
+    //
+    // C++:  void cv::DescriptorMatcher::knnMatch(Mat queryDescriptors, vector_vector_DMatch& matches, int k, vector_Mat masks = vector_Mat(), bool compactResult = false)
+    //
+
+    /**
+     *
+     *     @param queryDescriptors Query set of descriptors.
+     *     @param matches Matches. Each matches[i] is k or less matches for the same query descriptor.
+     *     @param k Count of best matches found per each query descriptor or less if a query descriptor has
+     *     less than k possible matches in total.
+     *     @param masks Set of masks. Each masks[i] specifies permissible matches between the input query
+     *     descriptors and stored train descriptors from the i-th image trainDescCollection[i].
+     *     @param compactResult Parameter used when the mask (or masks) is not empty. If compactResult is
+     *     false, the matches vector has the same size as queryDescriptors rows. If compactResult is true,
+     *     the matches vector does not contain matches for fully masked-out query descriptors.
+     */
+    public void knnMatch(Mat queryDescriptors, List<MatOfDMatch> matches, int k, List<Mat> masks, boolean compactResult) {
+        Mat matches_mat = new Mat();
+        Mat masks_mat = Converters.vector_Mat_to_Mat(masks);
+        knnMatch_3(nativeObj, queryDescriptors.nativeObj, matches_mat.nativeObj, k, masks_mat.nativeObj, compactResult);
+        Converters.Mat_to_vector_vector_DMatch(matches_mat, matches);
+        matches_mat.release();
+    }
+
+    /**
+     *
+     *     @param queryDescriptors Query set of descriptors.
+     *     @param matches Matches. Each matches[i] is k or less matches for the same query descriptor.
+     *     @param k Count of best matches found per each query descriptor or less if a query descriptor has
+     *     less than k possible matches in total.
+     *     @param masks Set of masks. Each masks[i] specifies permissible matches between the input query
+     *     descriptors and stored train descriptors from the i-th image trainDescCollection[i].
+     *     false, the matches vector has the same size as queryDescriptors rows. If compactResult is true,
+     *     the matches vector does not contain matches for fully masked-out query descriptors.
+     */
+    public void knnMatch(Mat queryDescriptors, List<MatOfDMatch> matches, int k, List<Mat> masks) {
+        Mat matches_mat = new Mat();
+        Mat masks_mat = Converters.vector_Mat_to_Mat(masks);
+        knnMatch_4(nativeObj, queryDescriptors.nativeObj, matches_mat.nativeObj, k, masks_mat.nativeObj);
+        Converters.Mat_to_vector_vector_DMatch(matches_mat, matches);
+        matches_mat.release();
+    }
+
+    /**
+     *
+     *     @param queryDescriptors Query set of descriptors.
+     *     @param matches Matches. Each matches[i] is k or less matches for the same query descriptor.
+     *     @param k Count of best matches found per each query descriptor or less if a query descriptor has
+     *     less than k possible matches in total.
+     *     descriptors and stored train descriptors from the i-th image trainDescCollection[i].
+     *     false, the matches vector has the same size as queryDescriptors rows. If compactResult is true,
+     *     the matches vector does not contain matches for fully masked-out query descriptors.
+     */
+    public void knnMatch(Mat queryDescriptors, List<MatOfDMatch> matches, int k) {
+        Mat matches_mat = new Mat();
+        knnMatch_5(nativeObj, queryDescriptors.nativeObj, matches_mat.nativeObj, k);
+        Converters.Mat_to_vector_vector_DMatch(matches_mat, matches);
+        matches_mat.release();
+    }
+
+
+    //
+    // C++:  void cv::DescriptorMatcher::match(Mat queryDescriptors, Mat trainDescriptors, vector_DMatch& matches, Mat mask = Mat())
+    //
+
+    /**
+     * Finds the best match for each descriptor from a query set.
+     *
+     *     @param queryDescriptors Query set of descriptors.
+     *     @param trainDescriptors Train set of descriptors. This set is not added to the train descriptors
+     *     collection stored in the class object.
+     *     @param matches Matches. If a query descriptor is masked out in mask , no match is added for this
+     *     descriptor. So, matches size may be smaller than the query descriptors count.
+     *     @param mask Mask specifying permissible matches between an input query and train matrices of
+     *     descriptors.
+     *
+     *     In the first variant of this method, the train descriptors are passed as an input argument. In the
+     *     second variant of the method, train descriptors collection that was set by DescriptorMatcher::add is
+     *     used. Optional mask (or masks) can be passed to specify which query and training descriptors can be
+     *     matched. Namely, queryDescriptors[i] can be matched with trainDescriptors[j] only if
+     *     mask.at&lt;uchar&gt;(i,j) is non-zero.
+     */
+    public void match(Mat queryDescriptors, Mat trainDescriptors, MatOfDMatch matches, Mat mask) {
+        Mat matches_mat = matches;
+        match_0(nativeObj, queryDescriptors.nativeObj, trainDescriptors.nativeObj, matches_mat.nativeObj, mask.nativeObj);
+    }
+
+    /**
+     * Finds the best match for each descriptor from a query set.
+     *
+     *     @param queryDescriptors Query set of descriptors.
+     *     @param trainDescriptors Train set of descriptors. This set is not added to the train descriptors
+     *     collection stored in the class object.
+     *     @param matches Matches. If a query descriptor is masked out in mask , no match is added for this
+     *     descriptor. So, matches size may be smaller than the query descriptors count.
+     *     descriptors.
+     *
+     *     In the first variant of this method, the train descriptors are passed as an input argument. In the
+     *     second variant of the method, train descriptors collection that was set by DescriptorMatcher::add is
+     *     used. Optional mask (or masks) can be passed to specify which query and training descriptors can be
+     *     matched. Namely, queryDescriptors[i] can be matched with trainDescriptors[j] only if
+     *     mask.at&lt;uchar&gt;(i,j) is non-zero.
+     */
+    public void match(Mat queryDescriptors, Mat trainDescriptors, MatOfDMatch matches) {
+        Mat matches_mat = matches;
+        match_1(nativeObj, queryDescriptors.nativeObj, trainDescriptors.nativeObj, matches_mat.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::DescriptorMatcher::match(Mat queryDescriptors, vector_DMatch& matches, vector_Mat masks = vector_Mat())
+    //
+
+    /**
+     *
+     *     @param queryDescriptors Query set of descriptors.
+     *     @param matches Matches. If a query descriptor is masked out in mask , no match is added for this
+     *     descriptor. So, matches size may be smaller than the query descriptors count.
+     *     @param masks Set of masks. Each masks[i] specifies permissible matches between the input query
+     *     descriptors and stored train descriptors from the i-th image trainDescCollection[i].
+     */
+    public void match(Mat queryDescriptors, MatOfDMatch matches, List<Mat> masks) {
+        Mat matches_mat = matches;
+        Mat masks_mat = Converters.vector_Mat_to_Mat(masks);
+        match_2(nativeObj, queryDescriptors.nativeObj, matches_mat.nativeObj, masks_mat.nativeObj);
+    }
+
+    /**
+     *
+     *     @param queryDescriptors Query set of descriptors.
+     *     @param matches Matches. If a query descriptor is masked out in mask , no match is added for this
+     *     descriptor. So, matches size may be smaller than the query descriptors count.
+     *     descriptors and stored train descriptors from the i-th image trainDescCollection[i].
+     */
+    public void match(Mat queryDescriptors, MatOfDMatch matches) {
+        Mat matches_mat = matches;
+        match_3(nativeObj, queryDescriptors.nativeObj, matches_mat.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::DescriptorMatcher::radiusMatch(Mat queryDescriptors, Mat trainDescriptors, vector_vector_DMatch& matches, float maxDistance, Mat mask = Mat(), bool compactResult = false)
+    //
+
+    /**
+     * For each query descriptor, finds the training descriptors not farther than the specified distance.
+     *
+     *     @param queryDescriptors Query set of descriptors.
+     *     @param trainDescriptors Train set of descriptors. This set is not added to the train descriptors
+     *     collection stored in the class object.
+     *     @param matches Found matches.
+     *     @param compactResult Parameter used when the mask (or masks) is not empty. If compactResult is
+     *     false, the matches vector has the same size as queryDescriptors rows. If compactResult is true,
+     *     the matches vector does not contain matches for fully masked-out query descriptors.
+     *     @param maxDistance Threshold for the distance between matched descriptors. Distance means here
+     *     metric distance (e.g. Hamming distance), not the distance between coordinates (which is measured
+     *     in Pixels)!
+     *     @param mask Mask specifying permissible matches between an input query and train matrices of
+     *     descriptors.
+     *
+     *     For each query descriptor, the methods find such training descriptors that the distance between the
+     *     query descriptor and the training descriptor is equal or smaller than maxDistance. Found matches are
+     *     returned in the distance increasing order.
+     */
+    public void radiusMatch(Mat queryDescriptors, Mat trainDescriptors, List<MatOfDMatch> matches, float maxDistance, Mat mask, boolean compactResult) {
+        Mat matches_mat = new Mat();
+        radiusMatch_0(nativeObj, queryDescriptors.nativeObj, trainDescriptors.nativeObj, matches_mat.nativeObj, maxDistance, mask.nativeObj, compactResult);
+        Converters.Mat_to_vector_vector_DMatch(matches_mat, matches);
+        matches_mat.release();
+    }
+
+    /**
+     * For each query descriptor, finds the training descriptors not farther than the specified distance.
+     *
+     *     @param queryDescriptors Query set of descriptors.
+     *     @param trainDescriptors Train set of descriptors. This set is not added to the train descriptors
+     *     collection stored in the class object.
+     *     @param matches Found matches.
+     *     false, the matches vector has the same size as queryDescriptors rows. If compactResult is true,
+     *     the matches vector does not contain matches for fully masked-out query descriptors.
+     *     @param maxDistance Threshold for the distance between matched descriptors. Distance means here
+     *     metric distance (e.g. Hamming distance), not the distance between coordinates (which is measured
+     *     in Pixels)!
+     *     @param mask Mask specifying permissible matches between an input query and train matrices of
+     *     descriptors.
+     *
+     *     For each query descriptor, the methods find such training descriptors that the distance between the
+     *     query descriptor and the training descriptor is equal or smaller than maxDistance. Found matches are
+     *     returned in the distance increasing order.
+     */
+    public void radiusMatch(Mat queryDescriptors, Mat trainDescriptors, List<MatOfDMatch> matches, float maxDistance, Mat mask) {
+        Mat matches_mat = new Mat();
+        radiusMatch_1(nativeObj, queryDescriptors.nativeObj, trainDescriptors.nativeObj, matches_mat.nativeObj, maxDistance, mask.nativeObj);
+        Converters.Mat_to_vector_vector_DMatch(matches_mat, matches);
+        matches_mat.release();
+    }
+
+    /**
+     * For each query descriptor, finds the training descriptors not farther than the specified distance.
+     *
+     *     @param queryDescriptors Query set of descriptors.
+     *     @param trainDescriptors Train set of descriptors. This set is not added to the train descriptors
+     *     collection stored in the class object.
+     *     @param matches Found matches.
+     *     false, the matches vector has the same size as queryDescriptors rows. If compactResult is true,
+     *     the matches vector does not contain matches for fully masked-out query descriptors.
+     *     @param maxDistance Threshold for the distance between matched descriptors. Distance means here
+     *     metric distance (e.g. Hamming distance), not the distance between coordinates (which is measured
+     *     in Pixels)!
+     *     descriptors.
+     *
+     *     For each query descriptor, the methods find such training descriptors that the distance between the
+     *     query descriptor and the training descriptor is equal or smaller than maxDistance. Found matches are
+     *     returned in the distance increasing order.
+     */
+    public void radiusMatch(Mat queryDescriptors, Mat trainDescriptors, List<MatOfDMatch> matches, float maxDistance) {
+        Mat matches_mat = new Mat();
+        radiusMatch_2(nativeObj, queryDescriptors.nativeObj, trainDescriptors.nativeObj, matches_mat.nativeObj, maxDistance);
+        Converters.Mat_to_vector_vector_DMatch(matches_mat, matches);
+        matches_mat.release();
+    }
+
+
+    //
+    // C++:  void cv::DescriptorMatcher::radiusMatch(Mat queryDescriptors, vector_vector_DMatch& matches, float maxDistance, vector_Mat masks = vector_Mat(), bool compactResult = false)
+    //
+
+    /**
+     *
+     *     @param queryDescriptors Query set of descriptors.
+     *     @param matches Found matches.
+     *     @param maxDistance Threshold for the distance between matched descriptors. Distance means here
+     *     metric distance (e.g. Hamming distance), not the distance between coordinates (which is measured
+     *     in Pixels)!
+     *     @param masks Set of masks. Each masks[i] specifies permissible matches between the input query
+     *     descriptors and stored train descriptors from the i-th image trainDescCollection[i].
+     *     @param compactResult Parameter used when the mask (or masks) is not empty. If compactResult is
+     *     false, the matches vector has the same size as queryDescriptors rows. If compactResult is true,
+     *     the matches vector does not contain matches for fully masked-out query descriptors.
+     */
+    public void radiusMatch(Mat queryDescriptors, List<MatOfDMatch> matches, float maxDistance, List<Mat> masks, boolean compactResult) {
+        Mat matches_mat = new Mat();
+        Mat masks_mat = Converters.vector_Mat_to_Mat(masks);
+        radiusMatch_3(nativeObj, queryDescriptors.nativeObj, matches_mat.nativeObj, maxDistance, masks_mat.nativeObj, compactResult);
+        Converters.Mat_to_vector_vector_DMatch(matches_mat, matches);
+        matches_mat.release();
+    }
+
+    /**
+     *
+     *     @param queryDescriptors Query set of descriptors.
+     *     @param matches Found matches.
+     *     @param maxDistance Threshold for the distance between matched descriptors. Distance means here
+     *     metric distance (e.g. Hamming distance), not the distance between coordinates (which is measured
+     *     in Pixels)!
+     *     @param masks Set of masks. Each masks[i] specifies permissible matches between the input query
+     *     descriptors and stored train descriptors from the i-th image trainDescCollection[i].
+     *     false, the matches vector has the same size as queryDescriptors rows. If compactResult is true,
+     *     the matches vector does not contain matches for fully masked-out query descriptors.
+     */
+    public void radiusMatch(Mat queryDescriptors, List<MatOfDMatch> matches, float maxDistance, List<Mat> masks) {
+        Mat matches_mat = new Mat();
+        Mat masks_mat = Converters.vector_Mat_to_Mat(masks);
+        radiusMatch_4(nativeObj, queryDescriptors.nativeObj, matches_mat.nativeObj, maxDistance, masks_mat.nativeObj);
+        Converters.Mat_to_vector_vector_DMatch(matches_mat, matches);
+        matches_mat.release();
+    }
+
+    /**
+     *
+     *     @param queryDescriptors Query set of descriptors.
+     *     @param matches Found matches.
+     *     @param maxDistance Threshold for the distance between matched descriptors. Distance means here
+     *     metric distance (e.g. Hamming distance), not the distance between coordinates (which is measured
+     *     in Pixels)!
+     *     descriptors and stored train descriptors from the i-th image trainDescCollection[i].
+     *     false, the matches vector has the same size as queryDescriptors rows. If compactResult is true,
+     *     the matches vector does not contain matches for fully masked-out query descriptors.
+     */
+    public void radiusMatch(Mat queryDescriptors, List<MatOfDMatch> matches, float maxDistance) {
+        Mat matches_mat = new Mat();
+        radiusMatch_5(nativeObj, queryDescriptors.nativeObj, matches_mat.nativeObj, maxDistance);
+        Converters.Mat_to_vector_vector_DMatch(matches_mat, matches);
+        matches_mat.release();
+    }
+
+
+    //
+    // C++:  void cv::DescriptorMatcher::read(FileNode arg1)
+    //
+
+    // Unknown type 'FileNode' (I), skipping the function
+
+
+    //
+    // C++:  void cv::DescriptorMatcher::read(String fileName)
+    //
+
+    public void read(String fileName) {
+        read_0(nativeObj, fileName);
+    }
+
+
+    //
+    // C++:  void cv::DescriptorMatcher::train()
+    //
+
+    /**
+     * Trains a descriptor matcher
+     *
+     *     Trains a descriptor matcher (for example, the flann index). In all methods to match, the method
+     *     train() is run every time before matching. Some descriptor matchers (for example, BruteForceMatcher)
+     *     have an empty implementation of this method. Other matchers really train their inner structures (for
+     *     example, FlannBasedMatcher trains flann::Index ).
+     */
+    public void train() {
+        train_0(nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::DescriptorMatcher::write(Ptr_FileStorage fs, String name = String())
+    //
+
+    // Unknown type 'Ptr_FileStorage' (I), skipping the function
+
+
+    //
+    // C++:  void cv::DescriptorMatcher::write(String fileName)
+    //
+
+    public void write(String fileName) {
+        write_0(nativeObj, fileName);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++:  Ptr_DescriptorMatcher cv::DescriptorMatcher::clone(bool emptyTrainData = false)
+    private static native long clone_0(long nativeObj, boolean emptyTrainData);
+    private static native long clone_1(long nativeObj);
+
+    // C++: static Ptr_DescriptorMatcher cv::DescriptorMatcher::create(String descriptorMatcherType)
+    private static native long create_0(String descriptorMatcherType);
+
+    // C++: static Ptr_DescriptorMatcher cv::DescriptorMatcher::create(int matcherType)
+    private static native long create_1(int matcherType);
+
+    // C++:  bool cv::DescriptorMatcher::empty()
+    private static native boolean empty_0(long nativeObj);
+
+    // C++:  bool cv::DescriptorMatcher::isMaskSupported()
+    private static native boolean isMaskSupported_0(long nativeObj);
+
+    // C++:  vector_Mat cv::DescriptorMatcher::getTrainDescriptors()
+    private static native long getTrainDescriptors_0(long nativeObj);
+
+    // C++:  void cv::DescriptorMatcher::add(vector_Mat descriptors)
+    private static native void add_0(long nativeObj, long descriptors_mat_nativeObj);
+
+    // C++:  void cv::DescriptorMatcher::clear()
+    private static native void clear_0(long nativeObj);
+
+    // C++:  void cv::DescriptorMatcher::knnMatch(Mat queryDescriptors, Mat trainDescriptors, vector_vector_DMatch& matches, int k, Mat mask = Mat(), bool compactResult = false)
+    private static native void knnMatch_0(long nativeObj, long queryDescriptors_nativeObj, long trainDescriptors_nativeObj, long matches_mat_nativeObj, int k, long mask_nativeObj, boolean compactResult);
+    private static native void knnMatch_1(long nativeObj, long queryDescriptors_nativeObj, long trainDescriptors_nativeObj, long matches_mat_nativeObj, int k, long mask_nativeObj);
+    private static native void knnMatch_2(long nativeObj, long queryDescriptors_nativeObj, long trainDescriptors_nativeObj, long matches_mat_nativeObj, int k);
+
+    // C++:  void cv::DescriptorMatcher::knnMatch(Mat queryDescriptors, vector_vector_DMatch& matches, int k, vector_Mat masks = vector_Mat(), bool compactResult = false)
+    private static native void knnMatch_3(long nativeObj, long queryDescriptors_nativeObj, long matches_mat_nativeObj, int k, long masks_mat_nativeObj, boolean compactResult);
+    private static native void knnMatch_4(long nativeObj, long queryDescriptors_nativeObj, long matches_mat_nativeObj, int k, long masks_mat_nativeObj);
+    private static native void knnMatch_5(long nativeObj, long queryDescriptors_nativeObj, long matches_mat_nativeObj, int k);
+
+    // C++:  void cv::DescriptorMatcher::match(Mat queryDescriptors, Mat trainDescriptors, vector_DMatch& matches, Mat mask = Mat())
+    private static native void match_0(long nativeObj, long queryDescriptors_nativeObj, long trainDescriptors_nativeObj, long matches_mat_nativeObj, long mask_nativeObj);
+    private static native void match_1(long nativeObj, long queryDescriptors_nativeObj, long trainDescriptors_nativeObj, long matches_mat_nativeObj);
+
+    // C++:  void cv::DescriptorMatcher::match(Mat queryDescriptors, vector_DMatch& matches, vector_Mat masks = vector_Mat())
+    private static native void match_2(long nativeObj, long queryDescriptors_nativeObj, long matches_mat_nativeObj, long masks_mat_nativeObj);
+    private static native void match_3(long nativeObj, long queryDescriptors_nativeObj, long matches_mat_nativeObj);
+
+    // C++:  void cv::DescriptorMatcher::radiusMatch(Mat queryDescriptors, Mat trainDescriptors, vector_vector_DMatch& matches, float maxDistance, Mat mask = Mat(), bool compactResult = false)
+    private static native void radiusMatch_0(long nativeObj, long queryDescriptors_nativeObj, long trainDescriptors_nativeObj, long matches_mat_nativeObj, float maxDistance, long mask_nativeObj, boolean compactResult);
+    private static native void radiusMatch_1(long nativeObj, long queryDescriptors_nativeObj, long trainDescriptors_nativeObj, long matches_mat_nativeObj, float maxDistance, long mask_nativeObj);
+    private static native void radiusMatch_2(long nativeObj, long queryDescriptors_nativeObj, long trainDescriptors_nativeObj, long matches_mat_nativeObj, float maxDistance);
+
+    // C++:  void cv::DescriptorMatcher::radiusMatch(Mat queryDescriptors, vector_vector_DMatch& matches, float maxDistance, vector_Mat masks = vector_Mat(), bool compactResult = false)
+    private static native void radiusMatch_3(long nativeObj, long queryDescriptors_nativeObj, long matches_mat_nativeObj, float maxDistance, long masks_mat_nativeObj, boolean compactResult);
+    private static native void radiusMatch_4(long nativeObj, long queryDescriptors_nativeObj, long matches_mat_nativeObj, float maxDistance, long masks_mat_nativeObj);
+    private static native void radiusMatch_5(long nativeObj, long queryDescriptors_nativeObj, long matches_mat_nativeObj, float maxDistance);
+
+    // C++:  void cv::DescriptorMatcher::read(String fileName)
+    private static native void read_0(long nativeObj, String fileName);
+
+    // C++:  void cv::DescriptorMatcher::train()
+    private static native void train_0(long nativeObj);
+
+    // C++:  void cv::DescriptorMatcher::write(String fileName)
+    private static native void write_0(long nativeObj, String fileName);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/core/MatOfRect2d.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/core/MatOfRect2d.java	(date 1605830247416)
+++ openCVLibrary3411/src/main/java/org/opencv/core/MatOfRect2d.java	(date 1605830247416)
@@ -0,0 +1,81 @@
+package org.opencv.core;
+
+import java.util.Arrays;
+import java.util.List;
+
+
+public class MatOfRect2d extends Mat {
+    // 64FC4
+    private static final int _depth = CvType.CV_64F;
+    private static final int _channels = 4;
+
+    public MatOfRect2d() {
+        super();
+    }
+
+    protected MatOfRect2d(long addr) {
+        super(addr);
+        if( !empty() && checkVector(_channels, _depth) < 0 )
+            throw new IllegalArgumentException("Incompatible Mat");
+        //FIXME: do we need release() here?
+    }
+
+    public static MatOfRect2d fromNativeAddr(long addr) {
+        return new MatOfRect2d(addr);
+    }
+
+    public MatOfRect2d(Mat m) {
+        super(m, Range.all());
+        if( !empty() && checkVector(_channels, _depth) < 0 )
+            throw new IllegalArgumentException("Incompatible Mat");
+        //FIXME: do we need release() here?
+    }
+
+    public MatOfRect2d(Rect2d...a) {
+        super();
+        fromArray(a);
+    }
+
+    public void alloc(int elemNumber) {
+        if(elemNumber>0)
+            super.create(elemNumber, 1, CvType.makeType(_depth, _channels));
+    }
+
+    public void fromArray(Rect2d...a) {
+        if(a==null || a.length==0)
+            return;
+        int num = a.length;
+        alloc(num);
+        double buff[] = new double[num * _channels];
+        for(int i=0; i<num; i++) {
+            Rect2d r = a[i];
+            buff[_channels*i+0] = (double) r.x;
+            buff[_channels*i+1] = (double) r.y;
+            buff[_channels*i+2] = (double) r.width;
+            buff[_channels*i+3] = (double) r.height;
+        }
+        put(0, 0, buff); //TODO: check ret val!
+    }
+
+
+    public Rect2d[] toArray() {
+        int num = (int) total();
+        Rect2d[] a = new Rect2d[num];
+        if(num == 0)
+            return a;
+        double buff[] = new double[num * _channels];
+        get(0, 0, buff); //TODO: check ret val!
+        for(int i=0; i<num; i++)
+            a[i] = new Rect2d(buff[i*_channels], buff[i*_channels+1], buff[i*_channels+2], buff[i*_channels+3]);
+        return a;
+    }
+    public void fromList(List<Rect2d> lr) {
+        Rect2d ap[] = lr.toArray(new Rect2d[0]);
+        fromArray(ap);
+    }
+
+    public List<Rect2d> toList() {
+        Rect2d[] ar = toArray();
+        return Arrays.asList(ar);
+    }
+}
Index: openCVLibrary3411/src/main/java/org/opencv/features2d/FastFeatureDetector.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/features2d/FastFeatureDetector.java	(date 1605830247617)
+++ openCVLibrary3411/src/main/java/org/opencv/features2d/FastFeatureDetector.java	(date 1605830247617)
@@ -0,0 +1,151 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.features2d;
+
+import org.opencv.features2d.FastFeatureDetector;
+import org.opencv.features2d.Feature2D;
+
+// C++: class FastFeatureDetector
+/**
+ * Wrapping class for feature detection using the FAST method. :
+ */
+public class FastFeatureDetector extends Feature2D {
+
+    protected FastFeatureDetector(long addr) { super(addr); }
+
+    // internal usage only
+    public static FastFeatureDetector __fromPtr__(long addr) { return new FastFeatureDetector(addr); }
+
+    // C++: enum <unnamed>
+    public static final int
+            TYPE_5_8 = 0,
+            TYPE_7_12 = 1,
+            TYPE_9_16 = 2,
+            THRESHOLD = 10000,
+            NONMAX_SUPPRESSION = 10001,
+            FAST_N = 10002;
+
+
+    //
+    // C++: static Ptr_FastFeatureDetector cv::FastFeatureDetector::create(int threshold = 10, bool nonmaxSuppression = true, int type = FastFeatureDetector::TYPE_9_16)
+    //
+
+    public static FastFeatureDetector create(int threshold, boolean nonmaxSuppression, int type) {
+        return FastFeatureDetector.__fromPtr__(create_0(threshold, nonmaxSuppression, type));
+    }
+
+    public static FastFeatureDetector create(int threshold, boolean nonmaxSuppression) {
+        return FastFeatureDetector.__fromPtr__(create_1(threshold, nonmaxSuppression));
+    }
+
+    public static FastFeatureDetector create(int threshold) {
+        return FastFeatureDetector.__fromPtr__(create_2(threshold));
+    }
+
+    public static FastFeatureDetector create() {
+        return FastFeatureDetector.__fromPtr__(create_3());
+    }
+
+
+    //
+    // C++:  String cv::FastFeatureDetector::getDefaultName()
+    //
+
+    public String getDefaultName() {
+        return getDefaultName_0(nativeObj);
+    }
+
+
+    //
+    // C++:  bool cv::FastFeatureDetector::getNonmaxSuppression()
+    //
+
+    public boolean getNonmaxSuppression() {
+        return getNonmaxSuppression_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::FastFeatureDetector::getThreshold()
+    //
+
+    public int getThreshold() {
+        return getThreshold_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::FastFeatureDetector::getType()
+    //
+
+    public int getType() {
+        return getType_0(nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::FastFeatureDetector::setNonmaxSuppression(bool f)
+    //
+
+    public void setNonmaxSuppression(boolean f) {
+        setNonmaxSuppression_0(nativeObj, f);
+    }
+
+
+    //
+    // C++:  void cv::FastFeatureDetector::setThreshold(int threshold)
+    //
+
+    public void setThreshold(int threshold) {
+        setThreshold_0(nativeObj, threshold);
+    }
+
+
+    //
+    // C++:  void cv::FastFeatureDetector::setType(int type)
+    //
+
+    public void setType(int type) {
+        setType_0(nativeObj, type);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++: static Ptr_FastFeatureDetector cv::FastFeatureDetector::create(int threshold = 10, bool nonmaxSuppression = true, int type = FastFeatureDetector::TYPE_9_16)
+    private static native long create_0(int threshold, boolean nonmaxSuppression, int type);
+    private static native long create_1(int threshold, boolean nonmaxSuppression);
+    private static native long create_2(int threshold);
+    private static native long create_3();
+
+    // C++:  String cv::FastFeatureDetector::getDefaultName()
+    private static native String getDefaultName_0(long nativeObj);
+
+    // C++:  bool cv::FastFeatureDetector::getNonmaxSuppression()
+    private static native boolean getNonmaxSuppression_0(long nativeObj);
+
+    // C++:  int cv::FastFeatureDetector::getThreshold()
+    private static native int getThreshold_0(long nativeObj);
+
+    // C++:  int cv::FastFeatureDetector::getType()
+    private static native int getType_0(long nativeObj);
+
+    // C++:  void cv::FastFeatureDetector::setNonmaxSuppression(bool f)
+    private static native void setNonmaxSuppression_0(long nativeObj, boolean f);
+
+    // C++:  void cv::FastFeatureDetector::setThreshold(int threshold)
+    private static native void setThreshold_0(long nativeObj, int threshold);
+
+    // C++:  void cv::FastFeatureDetector::setType(int type)
+    private static native void setType_0(long nativeObj, int type);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/core/MatOfRotatedRect.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/core/MatOfRotatedRect.java	(date 1605830247418)
+++ openCVLibrary3411/src/main/java/org/opencv/core/MatOfRotatedRect.java	(date 1605830247418)
@@ -0,0 +1,86 @@
+package org.opencv.core;
+
+import java.util.Arrays;
+import java.util.List;
+
+import org.opencv.core.RotatedRect;
+
+
+
+public class MatOfRotatedRect extends Mat {
+    // 32FC5
+    private static final int _depth = CvType.CV_32F;
+    private static final int _channels = 5;
+
+    public MatOfRotatedRect() {
+        super();
+    }
+
+    protected MatOfRotatedRect(long addr) {
+        super(addr);
+        if( !empty() && checkVector(_channels, _depth) < 0 )
+            throw new IllegalArgumentException("Incompatible Mat");
+        //FIXME: do we need release() here?
+    }
+
+    public static MatOfRotatedRect fromNativeAddr(long addr) {
+        return new MatOfRotatedRect(addr);
+    }
+
+    public MatOfRotatedRect(Mat m) {
+        super(m, Range.all());
+        if( !empty() && checkVector(_channels, _depth) < 0 )
+            throw new IllegalArgumentException("Incompatible Mat");
+        //FIXME: do we need release() here?
+    }
+
+    public MatOfRotatedRect(RotatedRect...a) {
+        super();
+        fromArray(a);
+    }
+
+    public void alloc(int elemNumber) {
+        if(elemNumber>0)
+            super.create(elemNumber, 1, CvType.makeType(_depth, _channels));
+    }
+
+    public void fromArray(RotatedRect...a) {
+        if(a==null || a.length==0)
+            return;
+        int num = a.length;
+        alloc(num);
+        float buff[] = new float[num * _channels];
+        for(int i=0; i<num; i++) {
+            RotatedRect r = a[i];
+            buff[_channels*i+0] = (float) r.center.x;
+            buff[_channels*i+1] = (float) r.center.y;
+            buff[_channels*i+2] = (float) r.size.width;
+            buff[_channels*i+3] = (float) r.size.height;
+            buff[_channels*i+4] = (float) r.angle;
+        }
+        put(0, 0, buff); //TODO: check ret val!
+    }
+
+    public RotatedRect[] toArray() {
+        int num = (int) total();
+        RotatedRect[] a = new RotatedRect[num];
+        if(num == 0)
+            return a;
+        float buff[] = new float[_channels];
+        for(int i=0; i<num; i++) {
+            get(i, 0, buff); //TODO: check ret val!
+            a[i] = new RotatedRect(new Point(buff[0],buff[1]),new Size(buff[2],buff[3]),buff[4]);
+        }
+        return a;
+    }
+
+    public void fromList(List<RotatedRect> lr) {
+        RotatedRect ap[] = lr.toArray(new RotatedRect[0]);
+        fromArray(ap);
+    }
+
+    public List<RotatedRect> toList() {
+        RotatedRect[] ar = toArray();
+        return Arrays.asList(ar);
+    }
+}
Index: openCVLibrary3411/src/main/java/org/opencv/features2d/Feature2D.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/features2d/Feature2D.java	(date 1605830247633)
+++ openCVLibrary3411/src/main/java/org/opencv/features2d/Feature2D.java	(date 1605830247633)
@@ -0,0 +1,299 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.features2d;
+
+import java.util.ArrayList;
+import java.util.List;
+import org.opencv.core.Algorithm;
+import org.opencv.core.Mat;
+import org.opencv.core.MatOfKeyPoint;
+import org.opencv.utils.Converters;
+
+// C++: class Feature2D
+/**
+ * Abstract base class for 2D image feature detectors and descriptor extractors
+ */
+public class Feature2D extends Algorithm {
+
+    protected Feature2D(long addr) { super(addr); }
+
+    // internal usage only
+    public static Feature2D __fromPtr__(long addr) { return new Feature2D(addr); }
+
+    //
+    // C++:  String cv::Feature2D::getDefaultName()
+    //
+
+    public String getDefaultName() {
+        return getDefaultName_0(nativeObj);
+    }
+
+
+    //
+    // C++:  bool cv::Feature2D::empty()
+    //
+
+    public boolean empty() {
+        return empty_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::Feature2D::defaultNorm()
+    //
+
+    public int defaultNorm() {
+        return defaultNorm_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::Feature2D::descriptorSize()
+    //
+
+    public int descriptorSize() {
+        return descriptorSize_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::Feature2D::descriptorType()
+    //
+
+    public int descriptorType() {
+        return descriptorType_0(nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::Feature2D::compute(Mat image, vector_KeyPoint& keypoints, Mat& descriptors)
+    //
+
+    /**
+     * Computes the descriptors for a set of keypoints detected in an image (first variant) or image set
+     *     (second variant).
+     *
+     *     @param image Image.
+     *     @param keypoints Input collection of keypoints. Keypoints for which a descriptor cannot be
+     *     computed are removed. Sometimes new keypoints can be added, for example: SIFT duplicates keypoint
+     *     with several dominant orientations (for each orientation).
+     *     @param descriptors Computed descriptors. In the second variant of the method descriptors[i] are
+     *     descriptors computed for a keypoints[i]. Row j is the keypoints (or keypoints[i]) is the
+     *     descriptor for keypoint j-th keypoint.
+     */
+    public void compute(Mat image, MatOfKeyPoint keypoints, Mat descriptors) {
+        Mat keypoints_mat = keypoints;
+        compute_0(nativeObj, image.nativeObj, keypoints_mat.nativeObj, descriptors.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::Feature2D::compute(vector_Mat images, vector_vector_KeyPoint& keypoints, vector_Mat& descriptors)
+    //
+
+    /**
+     *
+     *
+     *     @param images Image set.
+     *     @param keypoints Input collection of keypoints. Keypoints for which a descriptor cannot be
+     *     computed are removed. Sometimes new keypoints can be added, for example: SIFT duplicates keypoint
+     *     with several dominant orientations (for each orientation).
+     *     @param descriptors Computed descriptors. In the second variant of the method descriptors[i] are
+     *     descriptors computed for a keypoints[i]. Row j is the keypoints (or keypoints[i]) is the
+     *     descriptor for keypoint j-th keypoint.
+     */
+    public void compute(List<Mat> images, List<MatOfKeyPoint> keypoints, List<Mat> descriptors) {
+        Mat images_mat = Converters.vector_Mat_to_Mat(images);
+        List<Mat> keypoints_tmplm = new ArrayList<Mat>((keypoints != null) ? keypoints.size() : 0);
+        Mat keypoints_mat = Converters.vector_vector_KeyPoint_to_Mat(keypoints, keypoints_tmplm);
+        Mat descriptors_mat = new Mat();
+        compute_1(nativeObj, images_mat.nativeObj, keypoints_mat.nativeObj, descriptors_mat.nativeObj);
+        Converters.Mat_to_vector_vector_KeyPoint(keypoints_mat, keypoints);
+        keypoints_mat.release();
+        Converters.Mat_to_vector_Mat(descriptors_mat, descriptors);
+        descriptors_mat.release();
+    }
+
+
+    //
+    // C++:  void cv::Feature2D::detect(Mat image, vector_KeyPoint& keypoints, Mat mask = Mat())
+    //
+
+    /**
+     * Detects keypoints in an image (first variant) or image set (second variant).
+     *
+     *     @param image Image.
+     *     @param keypoints The detected keypoints. In the second variant of the method keypoints[i] is a set
+     *     of keypoints detected in images[i] .
+     *     @param mask Mask specifying where to look for keypoints (optional). It must be a 8-bit integer
+     *     matrix with non-zero values in the region of interest.
+     */
+    public void detect(Mat image, MatOfKeyPoint keypoints, Mat mask) {
+        Mat keypoints_mat = keypoints;
+        detect_0(nativeObj, image.nativeObj, keypoints_mat.nativeObj, mask.nativeObj);
+    }
+
+    /**
+     * Detects keypoints in an image (first variant) or image set (second variant).
+     *
+     *     @param image Image.
+     *     @param keypoints The detected keypoints. In the second variant of the method keypoints[i] is a set
+     *     of keypoints detected in images[i] .
+     *     matrix with non-zero values in the region of interest.
+     */
+    public void detect(Mat image, MatOfKeyPoint keypoints) {
+        Mat keypoints_mat = keypoints;
+        detect_1(nativeObj, image.nativeObj, keypoints_mat.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::Feature2D::detect(vector_Mat images, vector_vector_KeyPoint& keypoints, vector_Mat masks = vector_Mat())
+    //
+
+    /**
+     *
+     *     @param images Image set.
+     *     @param keypoints The detected keypoints. In the second variant of the method keypoints[i] is a set
+     *     of keypoints detected in images[i] .
+     *     @param masks Masks for each input image specifying where to look for keypoints (optional).
+     *     masks[i] is a mask for images[i].
+     */
+    public void detect(List<Mat> images, List<MatOfKeyPoint> keypoints, List<Mat> masks) {
+        Mat images_mat = Converters.vector_Mat_to_Mat(images);
+        Mat keypoints_mat = new Mat();
+        Mat masks_mat = Converters.vector_Mat_to_Mat(masks);
+        detect_2(nativeObj, images_mat.nativeObj, keypoints_mat.nativeObj, masks_mat.nativeObj);
+        Converters.Mat_to_vector_vector_KeyPoint(keypoints_mat, keypoints);
+        keypoints_mat.release();
+    }
+
+    /**
+     *
+     *     @param images Image set.
+     *     @param keypoints The detected keypoints. In the second variant of the method keypoints[i] is a set
+     *     of keypoints detected in images[i] .
+     *     masks[i] is a mask for images[i].
+     */
+    public void detect(List<Mat> images, List<MatOfKeyPoint> keypoints) {
+        Mat images_mat = Converters.vector_Mat_to_Mat(images);
+        Mat keypoints_mat = new Mat();
+        detect_3(nativeObj, images_mat.nativeObj, keypoints_mat.nativeObj);
+        Converters.Mat_to_vector_vector_KeyPoint(keypoints_mat, keypoints);
+        keypoints_mat.release();
+    }
+
+
+    //
+    // C++:  void cv::Feature2D::detectAndCompute(Mat image, Mat mask, vector_KeyPoint& keypoints, Mat& descriptors, bool useProvidedKeypoints = false)
+    //
+
+    /**
+     * Detects keypoints and computes the descriptors
+     * @param image automatically generated
+     * @param mask automatically generated
+     * @param keypoints automatically generated
+     * @param descriptors automatically generated
+     * @param useProvidedKeypoints automatically generated
+     */
+    public void detectAndCompute(Mat image, Mat mask, MatOfKeyPoint keypoints, Mat descriptors, boolean useProvidedKeypoints) {
+        Mat keypoints_mat = keypoints;
+        detectAndCompute_0(nativeObj, image.nativeObj, mask.nativeObj, keypoints_mat.nativeObj, descriptors.nativeObj, useProvidedKeypoints);
+    }
+
+    /**
+     * Detects keypoints and computes the descriptors
+     * @param image automatically generated
+     * @param mask automatically generated
+     * @param keypoints automatically generated
+     * @param descriptors automatically generated
+     */
+    public void detectAndCompute(Mat image, Mat mask, MatOfKeyPoint keypoints, Mat descriptors) {
+        Mat keypoints_mat = keypoints;
+        detectAndCompute_1(nativeObj, image.nativeObj, mask.nativeObj, keypoints_mat.nativeObj, descriptors.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::Feature2D::read(FileNode arg1)
+    //
+
+    // Unknown type 'FileNode' (I), skipping the function
+
+
+    //
+    // C++:  void cv::Feature2D::read(String fileName)
+    //
+
+    public void read(String fileName) {
+        read_0(nativeObj, fileName);
+    }
+
+
+    //
+    // C++:  void cv::Feature2D::write(Ptr_FileStorage fs, String name = String())
+    //
+
+    // Unknown type 'Ptr_FileStorage' (I), skipping the function
+
+
+    //
+    // C++:  void cv::Feature2D::write(String fileName)
+    //
+
+    public void write(String fileName) {
+        write_0(nativeObj, fileName);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++:  String cv::Feature2D::getDefaultName()
+    private static native String getDefaultName_0(long nativeObj);
+
+    // C++:  bool cv::Feature2D::empty()
+    private static native boolean empty_0(long nativeObj);
+
+    // C++:  int cv::Feature2D::defaultNorm()
+    private static native int defaultNorm_0(long nativeObj);
+
+    // C++:  int cv::Feature2D::descriptorSize()
+    private static native int descriptorSize_0(long nativeObj);
+
+    // C++:  int cv::Feature2D::descriptorType()
+    private static native int descriptorType_0(long nativeObj);
+
+    // C++:  void cv::Feature2D::compute(Mat image, vector_KeyPoint& keypoints, Mat& descriptors)
+    private static native void compute_0(long nativeObj, long image_nativeObj, long keypoints_mat_nativeObj, long descriptors_nativeObj);
+
+    // C++:  void cv::Feature2D::compute(vector_Mat images, vector_vector_KeyPoint& keypoints, vector_Mat& descriptors)
+    private static native void compute_1(long nativeObj, long images_mat_nativeObj, long keypoints_mat_nativeObj, long descriptors_mat_nativeObj);
+
+    // C++:  void cv::Feature2D::detect(Mat image, vector_KeyPoint& keypoints, Mat mask = Mat())
+    private static native void detect_0(long nativeObj, long image_nativeObj, long keypoints_mat_nativeObj, long mask_nativeObj);
+    private static native void detect_1(long nativeObj, long image_nativeObj, long keypoints_mat_nativeObj);
+
+    // C++:  void cv::Feature2D::detect(vector_Mat images, vector_vector_KeyPoint& keypoints, vector_Mat masks = vector_Mat())
+    private static native void detect_2(long nativeObj, long images_mat_nativeObj, long keypoints_mat_nativeObj, long masks_mat_nativeObj);
+    private static native void detect_3(long nativeObj, long images_mat_nativeObj, long keypoints_mat_nativeObj);
+
+    // C++:  void cv::Feature2D::detectAndCompute(Mat image, Mat mask, vector_KeyPoint& keypoints, Mat& descriptors, bool useProvidedKeypoints = false)
+    private static native void detectAndCompute_0(long nativeObj, long image_nativeObj, long mask_nativeObj, long keypoints_mat_nativeObj, long descriptors_nativeObj, boolean useProvidedKeypoints);
+    private static native void detectAndCompute_1(long nativeObj, long image_nativeObj, long mask_nativeObj, long keypoints_mat_nativeObj, long descriptors_nativeObj);
+
+    // C++:  void cv::Feature2D::read(String fileName)
+    private static native void read_0(long nativeObj, String fileName);
+
+    // C++:  void cv::Feature2D::write(String fileName)
+    private static native void write_0(long nativeObj, String fileName);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/features2d/BOWImgDescriptorExtractor.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/features2d/BOWImgDescriptorExtractor.java	(date 1605830247533)
+++ openCVLibrary3411/src/main/java/org/opencv/features2d/BOWImgDescriptorExtractor.java	(date 1605830247533)
@@ -0,0 +1,138 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.features2d;
+
+import java.util.ArrayList;
+import java.util.List;
+import org.opencv.core.Mat;
+import org.opencv.core.MatOfKeyPoint;
+import org.opencv.utils.Converters;
+
+// C++: class BOWImgDescriptorExtractor
+/**
+ * Class to compute an image descriptor using the *bag of visual words*.
+ *
+ * Such a computation consists of the following steps:
+ *
+ * 1.  Compute descriptors for a given image and its keypoints set.
+ * 2.  Find the nearest visual words from the vocabulary for each keypoint descriptor.
+ * 3.  Compute the bag-of-words image descriptor as is a normalized histogram of vocabulary words
+ * encountered in the image. The i-th bin of the histogram is a frequency of i-th word of the
+ * vocabulary in the given image.
+ */
+public class BOWImgDescriptorExtractor {
+
+    protected final long nativeObj;
+    protected BOWImgDescriptorExtractor(long addr) { nativeObj = addr; }
+
+    public long getNativeObjAddr() { return nativeObj; }
+
+    // internal usage only
+    public static BOWImgDescriptorExtractor __fromPtr__(long addr) { return new BOWImgDescriptorExtractor(addr); }
+
+    //
+    // C++:   cv::BOWImgDescriptorExtractor::BOWImgDescriptorExtractor(Ptr_DescriptorExtractor dextractor, Ptr_DescriptorMatcher dmatcher)
+    //
+
+    // Unknown type 'Ptr_DescriptorExtractor' (I), skipping the function
+
+
+    //
+    // C++:  Mat cv::BOWImgDescriptorExtractor::getVocabulary()
+    //
+
+    /**
+     * Returns the set vocabulary.
+     * @return automatically generated
+     */
+    public Mat getVocabulary() {
+        return new Mat(getVocabulary_0(nativeObj));
+    }
+
+
+    //
+    // C++:  int cv::BOWImgDescriptorExtractor::descriptorSize()
+    //
+
+    /**
+     * Returns an image descriptor size if the vocabulary is set. Otherwise, it returns 0.
+     * @return automatically generated
+     */
+    public int descriptorSize() {
+        return descriptorSize_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::BOWImgDescriptorExtractor::descriptorType()
+    //
+
+    /**
+     * Returns an image descriptor type.
+     * @return automatically generated
+     */
+    public int descriptorType() {
+        return descriptorType_0(nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::BOWImgDescriptorExtractor::compute2(Mat image, vector_KeyPoint keypoints, Mat& imgDescriptor)
+    //
+
+    /**
+     *
+     *     @param imgDescriptor Computed output image descriptor.
+     *     pointIdxsOfClusters[i] are keypoint indices that belong to the i -th cluster (word of vocabulary)
+     *     returned if it is non-zero.
+     * @param image automatically generated
+     * @param keypoints automatically generated
+     */
+    public void compute(Mat image, MatOfKeyPoint keypoints, Mat imgDescriptor) {
+        Mat keypoints_mat = keypoints;
+        compute_0(nativeObj, image.nativeObj, keypoints_mat.nativeObj, imgDescriptor.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::BOWImgDescriptorExtractor::setVocabulary(Mat vocabulary)
+    //
+
+    /**
+     * Sets a visual vocabulary.
+     *
+     *     @param vocabulary Vocabulary (can be trained using the inheritor of BOWTrainer ). Each row of the
+     *     vocabulary is a visual word (cluster center).
+     */
+    public void setVocabulary(Mat vocabulary) {
+        setVocabulary_0(nativeObj, vocabulary.nativeObj);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++:  Mat cv::BOWImgDescriptorExtractor::getVocabulary()
+    private static native long getVocabulary_0(long nativeObj);
+
+    // C++:  int cv::BOWImgDescriptorExtractor::descriptorSize()
+    private static native int descriptorSize_0(long nativeObj);
+
+    // C++:  int cv::BOWImgDescriptorExtractor::descriptorType()
+    private static native int descriptorType_0(long nativeObj);
+
+    // C++:  void cv::BOWImgDescriptorExtractor::compute2(Mat image, vector_KeyPoint keypoints, Mat& imgDescriptor)
+    private static native void compute_0(long nativeObj, long image_nativeObj, long keypoints_mat_nativeObj, long imgDescriptor_nativeObj);
+
+    // C++:  void cv::BOWImgDescriptorExtractor::setVocabulary(Mat vocabulary)
+    private static native void setVocabulary_0(long nativeObj, long vocabulary_nativeObj);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/features2d/BOWKMeansTrainer.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/features2d/BOWKMeansTrainer.java	(date 1605830247535)
+++ openCVLibrary3411/src/main/java/org/opencv/features2d/BOWKMeansTrainer.java	(date 1605830247535)
@@ -0,0 +1,112 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.features2d;
+
+import org.opencv.core.Mat;
+import org.opencv.core.TermCriteria;
+import org.opencv.features2d.BOWTrainer;
+
+// C++: class BOWKMeansTrainer
+/**
+ * kmeans -based class to train visual vocabulary using the *bag of visual words* approach. :
+ */
+public class BOWKMeansTrainer extends BOWTrainer {
+
+    protected BOWKMeansTrainer(long addr) { super(addr); }
+
+    // internal usage only
+    public static BOWKMeansTrainer __fromPtr__(long addr) { return new BOWKMeansTrainer(addr); }
+
+    //
+    // C++:   cv::BOWKMeansTrainer::BOWKMeansTrainer(int clusterCount, TermCriteria termcrit = TermCriteria(), int attempts = 3, int flags = KMEANS_PP_CENTERS)
+    //
+
+    /**
+     * The constructor.
+     *
+     *     SEE: cv::kmeans
+     * @param clusterCount automatically generated
+     * @param termcrit automatically generated
+     * @param attempts automatically generated
+     * @param flags automatically generated
+     */
+    public BOWKMeansTrainer(int clusterCount, TermCriteria termcrit, int attempts, int flags) {
+        super(BOWKMeansTrainer_0(clusterCount, termcrit.type, termcrit.maxCount, termcrit.epsilon, attempts, flags));
+    }
+
+    /**
+     * The constructor.
+     *
+     *     SEE: cv::kmeans
+     * @param clusterCount automatically generated
+     * @param termcrit automatically generated
+     * @param attempts automatically generated
+     */
+    public BOWKMeansTrainer(int clusterCount, TermCriteria termcrit, int attempts) {
+        super(BOWKMeansTrainer_1(clusterCount, termcrit.type, termcrit.maxCount, termcrit.epsilon, attempts));
+    }
+
+    /**
+     * The constructor.
+     *
+     *     SEE: cv::kmeans
+     * @param clusterCount automatically generated
+     * @param termcrit automatically generated
+     */
+    public BOWKMeansTrainer(int clusterCount, TermCriteria termcrit) {
+        super(BOWKMeansTrainer_2(clusterCount, termcrit.type, termcrit.maxCount, termcrit.epsilon));
+    }
+
+    /**
+     * The constructor.
+     *
+     *     SEE: cv::kmeans
+     * @param clusterCount automatically generated
+     */
+    public BOWKMeansTrainer(int clusterCount) {
+        super(BOWKMeansTrainer_3(clusterCount));
+    }
+
+
+    //
+    // C++:  Mat cv::BOWKMeansTrainer::cluster(Mat descriptors)
+    //
+
+    public Mat cluster(Mat descriptors) {
+        return new Mat(cluster_0(nativeObj, descriptors.nativeObj));
+    }
+
+
+    //
+    // C++:  Mat cv::BOWKMeansTrainer::cluster()
+    //
+
+    public Mat cluster() {
+        return new Mat(cluster_1(nativeObj));
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++:   cv::BOWKMeansTrainer::BOWKMeansTrainer(int clusterCount, TermCriteria termcrit = TermCriteria(), int attempts = 3, int flags = KMEANS_PP_CENTERS)
+    private static native long BOWKMeansTrainer_0(int clusterCount, int termcrit_type, int termcrit_maxCount, double termcrit_epsilon, int attempts, int flags);
+    private static native long BOWKMeansTrainer_1(int clusterCount, int termcrit_type, int termcrit_maxCount, double termcrit_epsilon, int attempts);
+    private static native long BOWKMeansTrainer_2(int clusterCount, int termcrit_type, int termcrit_maxCount, double termcrit_epsilon);
+    private static native long BOWKMeansTrainer_3(int clusterCount);
+
+    // C++:  Mat cv::BOWKMeansTrainer::cluster(Mat descriptors)
+    private static native long cluster_0(long nativeObj, long descriptors_nativeObj);
+
+    // C++:  Mat cv::BOWKMeansTrainer::cluster()
+    private static native long cluster_1(long nativeObj);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/core/MatOfPoint2f.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/core/MatOfPoint2f.java	(date 1605830247410)
+++ openCVLibrary3411/src/main/java/org/opencv/core/MatOfPoint2f.java	(date 1605830247410)
@@ -0,0 +1,78 @@
+package org.opencv.core;
+
+import java.util.Arrays;
+import java.util.List;
+
+public class MatOfPoint2f extends Mat {
+    // 32FC2
+    private static final int _depth = CvType.CV_32F;
+    private static final int _channels = 2;
+
+    public MatOfPoint2f() {
+        super();
+    }
+
+    protected MatOfPoint2f(long addr) {
+        super(addr);
+        if( !empty() && checkVector(_channels, _depth) < 0 )
+            throw new IllegalArgumentException("Incompatible Mat");
+        //FIXME: do we need release() here?
+    }
+
+    public static MatOfPoint2f fromNativeAddr(long addr) {
+        return new MatOfPoint2f(addr);
+    }
+
+    public MatOfPoint2f(Mat m) {
+        super(m, Range.all());
+        if( !empty() && checkVector(_channels, _depth) < 0 )
+            throw new IllegalArgumentException("Incompatible Mat");
+        //FIXME: do we need release() here?
+    }
+
+    public MatOfPoint2f(Point...a) {
+        super();
+        fromArray(a);
+    }
+
+    public void alloc(int elemNumber) {
+        if(elemNumber>0)
+            super.create(elemNumber, 1, CvType.makeType(_depth, _channels));
+    }
+
+    public void fromArray(Point...a) {
+        if(a==null || a.length==0)
+            return;
+        int num = a.length;
+        alloc(num);
+        float buff[] = new float[num * _channels];
+        for(int i=0; i<num; i++) {
+            Point p = a[i];
+            buff[_channels*i+0] = (float) p.x;
+            buff[_channels*i+1] = (float) p.y;
+        }
+        put(0, 0, buff); //TODO: check ret val!
+    }
+
+    public Point[] toArray() {
+        int num = (int) total();
+        Point[] ap = new Point[num];
+        if(num == 0)
+            return ap;
+        float buff[] = new float[num * _channels];
+        get(0, 0, buff); //TODO: check ret val!
+        for(int i=0; i<num; i++)
+            ap[i] = new Point(buff[i*_channels], buff[i*_channels+1]);
+        return ap;
+    }
+
+    public void fromList(List<Point> lp) {
+        Point ap[] = lp.toArray(new Point[0]);
+        fromArray(ap);
+    }
+
+    public List<Point> toList() {
+        Point[] ap = toArray();
+        return Arrays.asList(ap);
+    }
+}
Index: openCVLibrary3411/src/main/java/org/opencv/features2d/BOWTrainer.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/features2d/BOWTrainer.java	(date 1605830247538)
+++ openCVLibrary3411/src/main/java/org/opencv/features2d/BOWTrainer.java	(date 1605830247538)
@@ -0,0 +1,140 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.features2d;
+
+import java.util.ArrayList;
+import java.util.List;
+import org.opencv.core.Mat;
+import org.opencv.utils.Converters;
+
+// C++: class BOWTrainer
+/**
+ * Abstract base class for training the *bag of visual words* vocabulary from a set of descriptors.
+ *
+ * For details, see, for example, *Visual Categorization with Bags of Keypoints* by Gabriella Csurka,
+ * Christopher R. Dance, Lixin Fan, Jutta Willamowski, Cedric Bray, 2004. :
+ */
+public class BOWTrainer {
+
+    protected final long nativeObj;
+    protected BOWTrainer(long addr) { nativeObj = addr; }
+
+    public long getNativeObjAddr() { return nativeObj; }
+
+    // internal usage only
+    public static BOWTrainer __fromPtr__(long addr) { return new BOWTrainer(addr); }
+
+    //
+    // C++:  Mat cv::BOWTrainer::cluster(Mat descriptors)
+    //
+
+    /**
+     * Clusters train descriptors.
+     *
+     *     @param descriptors Descriptors to cluster. Each row of the descriptors matrix is a descriptor.
+     *     Descriptors are not added to the inner train descriptor set.
+     *
+     *     The vocabulary consists of cluster centers. So, this method returns the vocabulary. In the first
+     *     variant of the method, train descriptors stored in the object are clustered. In the second variant,
+     *     input descriptors are clustered.
+     * @return automatically generated
+     */
+    public Mat cluster(Mat descriptors) {
+        return new Mat(cluster_0(nativeObj, descriptors.nativeObj));
+    }
+
+
+    //
+    // C++:  Mat cv::BOWTrainer::cluster()
+    //
+
+    public Mat cluster() {
+        return new Mat(cluster_1(nativeObj));
+    }
+
+
+    //
+    // C++:  int cv::BOWTrainer::descriptorsCount()
+    //
+
+    /**
+     * Returns the count of all descriptors stored in the training set.
+     * @return automatically generated
+     */
+    public int descriptorsCount() {
+        return descriptorsCount_0(nativeObj);
+    }
+
+
+    //
+    // C++:  vector_Mat cv::BOWTrainer::getDescriptors()
+    //
+
+    /**
+     * Returns a training set of descriptors.
+     * @return automatically generated
+     */
+    public List<Mat> getDescriptors() {
+        List<Mat> retVal = new ArrayList<Mat>();
+        Mat retValMat = new Mat(getDescriptors_0(nativeObj));
+        Converters.Mat_to_vector_Mat(retValMat, retVal);
+        return retVal;
+    }
+
+
+    //
+    // C++:  void cv::BOWTrainer::add(Mat descriptors)
+    //
+
+    /**
+     * Adds descriptors to a training set.
+     *
+     *     @param descriptors Descriptors to add to a training set. Each row of the descriptors matrix is a
+     *     descriptor.
+     *
+     *     The training set is clustered using clustermethod to construct the vocabulary.
+     */
+    public void add(Mat descriptors) {
+        add_0(nativeObj, descriptors.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::BOWTrainer::clear()
+    //
+
+    public void clear() {
+        clear_0(nativeObj);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++:  Mat cv::BOWTrainer::cluster(Mat descriptors)
+    private static native long cluster_0(long nativeObj, long descriptors_nativeObj);
+
+    // C++:  Mat cv::BOWTrainer::cluster()
+    private static native long cluster_1(long nativeObj);
+
+    // C++:  int cv::BOWTrainer::descriptorsCount()
+    private static native int descriptorsCount_0(long nativeObj);
+
+    // C++:  vector_Mat cv::BOWTrainer::getDescriptors()
+    private static native long getDescriptors_0(long nativeObj);
+
+    // C++:  void cv::BOWTrainer::add(Mat descriptors)
+    private static native void add_0(long nativeObj, long descriptors_nativeObj);
+
+    // C++:  void cv::BOWTrainer::clear()
+    private static native void clear_0(long nativeObj);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/core/MatOfPoint3.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/core/MatOfPoint3.java	(date 1605830247411)
+++ openCVLibrary3411/src/main/java/org/opencv/core/MatOfPoint3.java	(date 1605830247411)
@@ -0,0 +1,79 @@
+package org.opencv.core;
+
+import java.util.Arrays;
+import java.util.List;
+
+public class MatOfPoint3 extends Mat {
+    // 32SC3
+    private static final int _depth = CvType.CV_32S;
+    private static final int _channels = 3;
+
+    public MatOfPoint3() {
+        super();
+    }
+
+    protected MatOfPoint3(long addr) {
+        super(addr);
+        if( !empty() && checkVector(_channels, _depth) < 0 )
+            throw new IllegalArgumentException("Incompatible Mat");
+        //FIXME: do we need release() here?
+    }
+
+    public static MatOfPoint3 fromNativeAddr(long addr) {
+        return new MatOfPoint3(addr);
+    }
+
+    public MatOfPoint3(Mat m) {
+        super(m, Range.all());
+        if( !empty() && checkVector(_channels, _depth) < 0 )
+            throw new IllegalArgumentException("Incompatible Mat");
+        //FIXME: do we need release() here?
+    }
+
+    public MatOfPoint3(Point3...a) {
+        super();
+        fromArray(a);
+    }
+
+    public void alloc(int elemNumber) {
+        if(elemNumber>0)
+            super.create(elemNumber, 1, CvType.makeType(_depth, _channels));
+    }
+
+    public void fromArray(Point3...a) {
+        if(a==null || a.length==0)
+            return;
+        int num = a.length;
+        alloc(num);
+        int buff[] = new int[num * _channels];
+        for(int i=0; i<num; i++) {
+            Point3 p = a[i];
+            buff[_channels*i+0] = (int) p.x;
+            buff[_channels*i+1] = (int) p.y;
+            buff[_channels*i+2] = (int) p.z;
+        }
+        put(0, 0, buff); //TODO: check ret val!
+    }
+
+    public Point3[] toArray() {
+        int num = (int) total();
+        Point3[] ap = new Point3[num];
+        if(num == 0)
+            return ap;
+        int buff[] = new int[num * _channels];
+        get(0, 0, buff); //TODO: check ret val!
+        for(int i=0; i<num; i++)
+            ap[i] = new Point3(buff[i*_channels], buff[i*_channels+1], buff[i*_channels+2]);
+        return ap;
+    }
+
+    public void fromList(List<Point3> lp) {
+        Point3 ap[] = lp.toArray(new Point3[0]);
+        fromArray(ap);
+    }
+
+    public List<Point3> toList() {
+        Point3[] ap = toArray();
+        return Arrays.asList(ap);
+    }
+}
Index: openCVLibrary3411/src/main/java/org/opencv/features2d/BRISK.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/features2d/BRISK.java	(date 1605830247541)
+++ openCVLibrary3411/src/main/java/org/opencv/features2d/BRISK.java	(date 1605830247541)
@@ -0,0 +1,285 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.features2d;
+
+import java.util.ArrayList;
+import java.util.List;
+import org.opencv.core.Mat;
+import org.opencv.core.MatOfFloat;
+import org.opencv.core.MatOfInt;
+import org.opencv.features2d.BRISK;
+import org.opencv.features2d.Feature2D;
+import org.opencv.utils.Converters;
+
+// C++: class BRISK
+/**
+ * Class implementing the BRISK keypoint detector and descriptor extractor, described in CITE: LCS11 .
+ */
+public class BRISK extends Feature2D {
+
+    protected BRISK(long addr) { super(addr); }
+
+    // internal usage only
+    public static BRISK __fromPtr__(long addr) { return new BRISK(addr); }
+
+    //
+    // C++: static Ptr_BRISK cv::BRISK::create(int thresh, int octaves, vector_float radiusList, vector_int numberList, float dMax = 5.85f, float dMin = 8.2f, vector_int indexChange = std::vector<int>())
+    //
+
+    /**
+     * The BRISK constructor for a custom pattern, detection threshold and octaves
+     *
+     *     @param thresh AGAST detection threshold score.
+     *     @param octaves detection octaves. Use 0 to do single scale.
+     *     @param radiusList defines the radii (in pixels) where the samples around a keypoint are taken (for
+     *     keypoint scale 1).
+     *     @param numberList defines the number of sampling points on the sampling circle. Must be the same
+     *     size as radiusList..
+     *     @param dMax threshold for the short pairings used for descriptor formation (in pixels for keypoint
+     *     scale 1).
+     *     @param dMin threshold for the long pairings used for orientation determination (in pixels for
+     *     keypoint scale 1).
+     * @param indexChange index remapping of the bits.
+     * @return automatically generated
+     */
+    public static BRISK create(int thresh, int octaves, MatOfFloat radiusList, MatOfInt numberList, float dMax, float dMin, MatOfInt indexChange) {
+        Mat radiusList_mat = radiusList;
+        Mat numberList_mat = numberList;
+        Mat indexChange_mat = indexChange;
+        return BRISK.__fromPtr__(create_0(thresh, octaves, radiusList_mat.nativeObj, numberList_mat.nativeObj, dMax, dMin, indexChange_mat.nativeObj));
+    }
+
+    /**
+     * The BRISK constructor for a custom pattern, detection threshold and octaves
+     *
+     *     @param thresh AGAST detection threshold score.
+     *     @param octaves detection octaves. Use 0 to do single scale.
+     *     @param radiusList defines the radii (in pixels) where the samples around a keypoint are taken (for
+     *     keypoint scale 1).
+     *     @param numberList defines the number of sampling points on the sampling circle. Must be the same
+     *     size as radiusList..
+     *     @param dMax threshold for the short pairings used for descriptor formation (in pixels for keypoint
+     *     scale 1).
+     *     @param dMin threshold for the long pairings used for orientation determination (in pixels for
+     *     keypoint scale 1).
+     * @return automatically generated
+     */
+    public static BRISK create(int thresh, int octaves, MatOfFloat radiusList, MatOfInt numberList, float dMax, float dMin) {
+        Mat radiusList_mat = radiusList;
+        Mat numberList_mat = numberList;
+        return BRISK.__fromPtr__(create_1(thresh, octaves, radiusList_mat.nativeObj, numberList_mat.nativeObj, dMax, dMin));
+    }
+
+    /**
+     * The BRISK constructor for a custom pattern, detection threshold and octaves
+     *
+     *     @param thresh AGAST detection threshold score.
+     *     @param octaves detection octaves. Use 0 to do single scale.
+     *     @param radiusList defines the radii (in pixels) where the samples around a keypoint are taken (for
+     *     keypoint scale 1).
+     *     @param numberList defines the number of sampling points on the sampling circle. Must be the same
+     *     size as radiusList..
+     *     @param dMax threshold for the short pairings used for descriptor formation (in pixels for keypoint
+     *     scale 1).
+     *     keypoint scale 1).
+     * @return automatically generated
+     */
+    public static BRISK create(int thresh, int octaves, MatOfFloat radiusList, MatOfInt numberList, float dMax) {
+        Mat radiusList_mat = radiusList;
+        Mat numberList_mat = numberList;
+        return BRISK.__fromPtr__(create_2(thresh, octaves, radiusList_mat.nativeObj, numberList_mat.nativeObj, dMax));
+    }
+
+    /**
+     * The BRISK constructor for a custom pattern, detection threshold and octaves
+     *
+     *     @param thresh AGAST detection threshold score.
+     *     @param octaves detection octaves. Use 0 to do single scale.
+     *     @param radiusList defines the radii (in pixels) where the samples around a keypoint are taken (for
+     *     keypoint scale 1).
+     *     @param numberList defines the number of sampling points on the sampling circle. Must be the same
+     *     size as radiusList..
+     *     scale 1).
+     *     keypoint scale 1).
+     * @return automatically generated
+     */
+    public static BRISK create(int thresh, int octaves, MatOfFloat radiusList, MatOfInt numberList) {
+        Mat radiusList_mat = radiusList;
+        Mat numberList_mat = numberList;
+        return BRISK.__fromPtr__(create_3(thresh, octaves, radiusList_mat.nativeObj, numberList_mat.nativeObj));
+    }
+
+
+    //
+    // C++: static Ptr_BRISK cv::BRISK::create(int thresh = 30, int octaves = 3, float patternScale = 1.0f)
+    //
+
+    /**
+     * The BRISK constructor
+     *
+     *     @param thresh AGAST detection threshold score.
+     *     @param octaves detection octaves. Use 0 to do single scale.
+     *     @param patternScale apply this scale to the pattern used for sampling the neighbourhood of a
+     *     keypoint.
+     * @return automatically generated
+     */
+    public static BRISK create(int thresh, int octaves, float patternScale) {
+        return BRISK.__fromPtr__(create_4(thresh, octaves, patternScale));
+    }
+
+    /**
+     * The BRISK constructor
+     *
+     *     @param thresh AGAST detection threshold score.
+     *     @param octaves detection octaves. Use 0 to do single scale.
+     *     keypoint.
+     * @return automatically generated
+     */
+    public static BRISK create(int thresh, int octaves) {
+        return BRISK.__fromPtr__(create_5(thresh, octaves));
+    }
+
+    /**
+     * The BRISK constructor
+     *
+     *     @param thresh AGAST detection threshold score.
+     *     keypoint.
+     * @return automatically generated
+     */
+    public static BRISK create(int thresh) {
+        return BRISK.__fromPtr__(create_6(thresh));
+    }
+
+    /**
+     * The BRISK constructor
+     *
+     *     keypoint.
+     * @return automatically generated
+     */
+    public static BRISK create() {
+        return BRISK.__fromPtr__(create_7());
+    }
+
+
+    //
+    // C++: static Ptr_BRISK cv::BRISK::create(vector_float radiusList, vector_int numberList, float dMax = 5.85f, float dMin = 8.2f, vector_int indexChange = std::vector<int>())
+    //
+
+    /**
+     * The BRISK constructor for a custom pattern
+     *
+     *     @param radiusList defines the radii (in pixels) where the samples around a keypoint are taken (for
+     *     keypoint scale 1).
+     *     @param numberList defines the number of sampling points on the sampling circle. Must be the same
+     *     size as radiusList..
+     *     @param dMax threshold for the short pairings used for descriptor formation (in pixels for keypoint
+     *     scale 1).
+     *     @param dMin threshold for the long pairings used for orientation determination (in pixels for
+     *     keypoint scale 1).
+     * @param indexChange index remapping of the bits.
+     * @return automatically generated
+     */
+    public static BRISK create(MatOfFloat radiusList, MatOfInt numberList, float dMax, float dMin, MatOfInt indexChange) {
+        Mat radiusList_mat = radiusList;
+        Mat numberList_mat = numberList;
+        Mat indexChange_mat = indexChange;
+        return BRISK.__fromPtr__(create_8(radiusList_mat.nativeObj, numberList_mat.nativeObj, dMax, dMin, indexChange_mat.nativeObj));
+    }
+
+    /**
+     * The BRISK constructor for a custom pattern
+     *
+     *     @param radiusList defines the radii (in pixels) where the samples around a keypoint are taken (for
+     *     keypoint scale 1).
+     *     @param numberList defines the number of sampling points on the sampling circle. Must be the same
+     *     size as radiusList..
+     *     @param dMax threshold for the short pairings used for descriptor formation (in pixels for keypoint
+     *     scale 1).
+     *     @param dMin threshold for the long pairings used for orientation determination (in pixels for
+     *     keypoint scale 1).
+     * @return automatically generated
+     */
+    public static BRISK create(MatOfFloat radiusList, MatOfInt numberList, float dMax, float dMin) {
+        Mat radiusList_mat = radiusList;
+        Mat numberList_mat = numberList;
+        return BRISK.__fromPtr__(create_9(radiusList_mat.nativeObj, numberList_mat.nativeObj, dMax, dMin));
+    }
+
+    /**
+     * The BRISK constructor for a custom pattern
+     *
+     *     @param radiusList defines the radii (in pixels) where the samples around a keypoint are taken (for
+     *     keypoint scale 1).
+     *     @param numberList defines the number of sampling points on the sampling circle. Must be the same
+     *     size as radiusList..
+     *     @param dMax threshold for the short pairings used for descriptor formation (in pixels for keypoint
+     *     scale 1).
+     *     keypoint scale 1).
+     * @return automatically generated
+     */
+    public static BRISK create(MatOfFloat radiusList, MatOfInt numberList, float dMax) {
+        Mat radiusList_mat = radiusList;
+        Mat numberList_mat = numberList;
+        return BRISK.__fromPtr__(create_10(radiusList_mat.nativeObj, numberList_mat.nativeObj, dMax));
+    }
+
+    /**
+     * The BRISK constructor for a custom pattern
+     *
+     *     @param radiusList defines the radii (in pixels) where the samples around a keypoint are taken (for
+     *     keypoint scale 1).
+     *     @param numberList defines the number of sampling points on the sampling circle. Must be the same
+     *     size as radiusList..
+     *     scale 1).
+     *     keypoint scale 1).
+     * @return automatically generated
+     */
+    public static BRISK create(MatOfFloat radiusList, MatOfInt numberList) {
+        Mat radiusList_mat = radiusList;
+        Mat numberList_mat = numberList;
+        return BRISK.__fromPtr__(create_11(radiusList_mat.nativeObj, numberList_mat.nativeObj));
+    }
+
+
+    //
+    // C++:  String cv::BRISK::getDefaultName()
+    //
+
+    public String getDefaultName() {
+        return getDefaultName_0(nativeObj);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++: static Ptr_BRISK cv::BRISK::create(int thresh, int octaves, vector_float radiusList, vector_int numberList, float dMax = 5.85f, float dMin = 8.2f, vector_int indexChange = std::vector<int>())
+    private static native long create_0(int thresh, int octaves, long radiusList_mat_nativeObj, long numberList_mat_nativeObj, float dMax, float dMin, long indexChange_mat_nativeObj);
+    private static native long create_1(int thresh, int octaves, long radiusList_mat_nativeObj, long numberList_mat_nativeObj, float dMax, float dMin);
+    private static native long create_2(int thresh, int octaves, long radiusList_mat_nativeObj, long numberList_mat_nativeObj, float dMax);
+    private static native long create_3(int thresh, int octaves, long radiusList_mat_nativeObj, long numberList_mat_nativeObj);
+
+    // C++: static Ptr_BRISK cv::BRISK::create(int thresh = 30, int octaves = 3, float patternScale = 1.0f)
+    private static native long create_4(int thresh, int octaves, float patternScale);
+    private static native long create_5(int thresh, int octaves);
+    private static native long create_6(int thresh);
+    private static native long create_7();
+
+    // C++: static Ptr_BRISK cv::BRISK::create(vector_float radiusList, vector_int numberList, float dMax = 5.85f, float dMin = 8.2f, vector_int indexChange = std::vector<int>())
+    private static native long create_8(long radiusList_mat_nativeObj, long numberList_mat_nativeObj, float dMax, float dMin, long indexChange_mat_nativeObj);
+    private static native long create_9(long radiusList_mat_nativeObj, long numberList_mat_nativeObj, float dMax, float dMin);
+    private static native long create_10(long radiusList_mat_nativeObj, long numberList_mat_nativeObj, float dMax);
+    private static native long create_11(long radiusList_mat_nativeObj, long numberList_mat_nativeObj);
+
+    // C++:  String cv::BRISK::getDefaultName()
+    private static native String getDefaultName_0(long nativeObj);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/objdetect/QRCodeDetector.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/objdetect/QRCodeDetector.java	(date 1605830247996)
+++ openCVLibrary3411/src/main/java/org/opencv/objdetect/QRCodeDetector.java	(date 1605830247996)
@@ -0,0 +1,281 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.objdetect;
+
+import java.util.ArrayList;
+import java.util.List;
+import org.opencv.core.Mat;
+import org.opencv.utils.Converters;
+
+// C++: class QRCodeDetector
+/**
+ * Groups the object candidate rectangles.
+ *     rectList  Input/output vector of rectangles. Output vector includes retained and grouped rectangles. (The Python list is not modified in place.)
+ *     weights Input/output vector of weights of rectangles. Output vector includes weights of retained and grouped rectangles. (The Python list is not modified in place.)
+ *     groupThreshold Minimum possible number of rectangles minus 1. The threshold is used in a group of rectangles to retain it.
+ *     eps Relative difference between sides of the rectangles to merge them into a group.
+ */
+public class QRCodeDetector {
+
+    protected final long nativeObj;
+    protected QRCodeDetector(long addr) { nativeObj = addr; }
+
+    public long getNativeObjAddr() { return nativeObj; }
+
+    // internal usage only
+    public static QRCodeDetector __fromPtr__(long addr) { return new QRCodeDetector(addr); }
+
+    //
+    // C++:   cv::QRCodeDetector::QRCodeDetector()
+    //
+
+    public QRCodeDetector() {
+        nativeObj = QRCodeDetector_0();
+    }
+
+
+    //
+    // C++:  String cv::QRCodeDetector::decode(Mat img, Mat points, Mat& straight_qrcode = Mat())
+    //
+
+    /**
+     * Decodes QR code in image once it's found by the detect() method.
+     *
+     *      Returns UTF8-encoded output string or empty string if the code cannot be decoded.
+     *      @param img grayscale or color (BGR) image containing QR code.
+     *      @param points Quadrangle vertices found by detect() method (or some other algorithm).
+     *      @param straight_qrcode The optional output image containing rectified and binarized QR code
+     * @return automatically generated
+     */
+    public String decode(Mat img, Mat points, Mat straight_qrcode) {
+        return decode_0(nativeObj, img.nativeObj, points.nativeObj, straight_qrcode.nativeObj);
+    }
+
+    /**
+     * Decodes QR code in image once it's found by the detect() method.
+     *
+     *      Returns UTF8-encoded output string or empty string if the code cannot be decoded.
+     *      @param img grayscale or color (BGR) image containing QR code.
+     *      @param points Quadrangle vertices found by detect() method (or some other algorithm).
+     * @return automatically generated
+     */
+    public String decode(Mat img, Mat points) {
+        return decode_1(nativeObj, img.nativeObj, points.nativeObj);
+    }
+
+
+    //
+    // C++:  String cv::QRCodeDetector::detectAndDecode(Mat img, Mat& points = Mat(), Mat& straight_qrcode = Mat())
+    //
+
+    /**
+     * Both detects and decodes QR code
+     *
+     *      @param img grayscale or color (BGR) image containing QR code.
+     *      @param points optional output array of vertices of the found QR code quadrangle. Will be empty if not found.
+     *      @param straight_qrcode The optional output image containing rectified and binarized QR code
+     * @return automatically generated
+     */
+    public String detectAndDecode(Mat img, Mat points, Mat straight_qrcode) {
+        return detectAndDecode_0(nativeObj, img.nativeObj, points.nativeObj, straight_qrcode.nativeObj);
+    }
+
+    /**
+     * Both detects and decodes QR code
+     *
+     *      @param img grayscale or color (BGR) image containing QR code.
+     *      @param points optional output array of vertices of the found QR code quadrangle. Will be empty if not found.
+     * @return automatically generated
+     */
+    public String detectAndDecode(Mat img, Mat points) {
+        return detectAndDecode_1(nativeObj, img.nativeObj, points.nativeObj);
+    }
+
+    /**
+     * Both detects and decodes QR code
+     *
+     *      @param img grayscale or color (BGR) image containing QR code.
+     * @return automatically generated
+     */
+    public String detectAndDecode(Mat img) {
+        return detectAndDecode_2(nativeObj, img.nativeObj);
+    }
+
+
+    //
+    // C++:  bool cv::QRCodeDetector::decodeMulti(Mat img, Mat points, vector_String& decoded_info, vector_Mat& straight_qrcode = vector_Mat())
+    //
+
+    /**
+     * Decodes QR codes in image once it's found by the detect() method.
+     *      @param img grayscale or color (BGR) image containing QR codes.
+     *      @param decoded_info UTF8-encoded output vector of string or empty vector of string if the codes cannot be decoded.
+     *      @param points vector of Quadrangle vertices found by detect() method (or some other algorithm).
+     *      @param straight_qrcode The optional output vector of images containing rectified and binarized QR codes
+     * @return automatically generated
+     */
+    public boolean decodeMulti(Mat img, Mat points, List<String> decoded_info, List<Mat> straight_qrcode) {
+        Mat straight_qrcode_mat = new Mat();
+        boolean retVal = decodeMulti_0(nativeObj, img.nativeObj, points.nativeObj, decoded_info, straight_qrcode_mat.nativeObj);
+        Converters.Mat_to_vector_Mat(straight_qrcode_mat, straight_qrcode);
+        straight_qrcode_mat.release();
+        return retVal;
+    }
+
+    /**
+     * Decodes QR codes in image once it's found by the detect() method.
+     *      @param img grayscale or color (BGR) image containing QR codes.
+     *      @param decoded_info UTF8-encoded output vector of string or empty vector of string if the codes cannot be decoded.
+     *      @param points vector of Quadrangle vertices found by detect() method (or some other algorithm).
+     * @return automatically generated
+     */
+    public boolean decodeMulti(Mat img, Mat points, List<String> decoded_info) {
+        return decodeMulti_1(nativeObj, img.nativeObj, points.nativeObj, decoded_info);
+    }
+
+
+    //
+    // C++:  bool cv::QRCodeDetector::detect(Mat img, Mat& points)
+    //
+
+    /**
+     * Detects QR code in image and returns the quadrangle containing the code.
+     *      @param img grayscale or color (BGR) image containing (or not) QR code.
+     *      @param points Output vector of vertices of the minimum-area quadrangle containing the code.
+     * @return automatically generated
+     */
+    public boolean detect(Mat img, Mat points) {
+        return detect_0(nativeObj, img.nativeObj, points.nativeObj);
+    }
+
+
+    //
+    // C++:  bool cv::QRCodeDetector::detectAndDecodeMulti(Mat img, vector_String& decoded_info, Mat& points = Mat(), vector_Mat& straight_qrcode = vector_Mat())
+    //
+
+    /**
+     * Both detects and decodes QR codes
+     *     @param img grayscale or color (BGR) image containing QR codes.
+     *     @param decoded_info UTF8-encoded output vector of string or empty vector of string if the codes cannot be decoded.
+     *     @param points optional output vector of vertices of the found QR code quadrangles. Will be empty if not found.
+     *     @param straight_qrcode The optional output vector of images containing rectified and binarized QR codes
+     * @return automatically generated
+     */
+    public boolean detectAndDecodeMulti(Mat img, List<String> decoded_info, Mat points, List<Mat> straight_qrcode) {
+        Mat straight_qrcode_mat = new Mat();
+        boolean retVal = detectAndDecodeMulti_0(nativeObj, img.nativeObj, decoded_info, points.nativeObj, straight_qrcode_mat.nativeObj);
+        Converters.Mat_to_vector_Mat(straight_qrcode_mat, straight_qrcode);
+        straight_qrcode_mat.release();
+        return retVal;
+    }
+
+    /**
+     * Both detects and decodes QR codes
+     *     @param img grayscale or color (BGR) image containing QR codes.
+     *     @param decoded_info UTF8-encoded output vector of string or empty vector of string if the codes cannot be decoded.
+     *     @param points optional output vector of vertices of the found QR code quadrangles. Will be empty if not found.
+     * @return automatically generated
+     */
+    public boolean detectAndDecodeMulti(Mat img, List<String> decoded_info, Mat points) {
+        return detectAndDecodeMulti_1(nativeObj, img.nativeObj, decoded_info, points.nativeObj);
+    }
+
+    /**
+     * Both detects and decodes QR codes
+     *     @param img grayscale or color (BGR) image containing QR codes.
+     *     @param decoded_info UTF8-encoded output vector of string or empty vector of string if the codes cannot be decoded.
+     * @return automatically generated
+     */
+    public boolean detectAndDecodeMulti(Mat img, List<String> decoded_info) {
+        return detectAndDecodeMulti_2(nativeObj, img.nativeObj, decoded_info);
+    }
+
+
+    //
+    // C++:  bool cv::QRCodeDetector::detectMulti(Mat img, Mat& points)
+    //
+
+    /**
+     * Detects QR codes in image and returns the vector of the quadrangles containing the codes.
+     *      @param img grayscale or color (BGR) image containing (or not) QR codes.
+     *      @param points Output vector of vector of vertices of the minimum-area quadrangle containing the codes.
+     * @return automatically generated
+     */
+    public boolean detectMulti(Mat img, Mat points) {
+        return detectMulti_0(nativeObj, img.nativeObj, points.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::QRCodeDetector::setEpsX(double epsX)
+    //
+
+    /**
+     * sets the epsilon used during the horizontal scan of QR code stop marker detection.
+     *      @param epsX Epsilon neighborhood, which allows you to determine the horizontal pattern
+     *      of the scheme 1:1:3:1:1 according to QR code standard.
+     */
+    public void setEpsX(double epsX) {
+        setEpsX_0(nativeObj, epsX);
+    }
+
+
+    //
+    // C++:  void cv::QRCodeDetector::setEpsY(double epsY)
+    //
+
+    /**
+     * sets the epsilon used during the vertical scan of QR code stop marker detection.
+     *      @param epsY Epsilon neighborhood, which allows you to determine the vertical pattern
+     *      of the scheme 1:1:3:1:1 according to QR code standard.
+     */
+    public void setEpsY(double epsY) {
+        setEpsY_0(nativeObj, epsY);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++:   cv::QRCodeDetector::QRCodeDetector()
+    private static native long QRCodeDetector_0();
+
+    // C++:  String cv::QRCodeDetector::decode(Mat img, Mat points, Mat& straight_qrcode = Mat())
+    private static native String decode_0(long nativeObj, long img_nativeObj, long points_nativeObj, long straight_qrcode_nativeObj);
+    private static native String decode_1(long nativeObj, long img_nativeObj, long points_nativeObj);
+
+    // C++:  String cv::QRCodeDetector::detectAndDecode(Mat img, Mat& points = Mat(), Mat& straight_qrcode = Mat())
+    private static native String detectAndDecode_0(long nativeObj, long img_nativeObj, long points_nativeObj, long straight_qrcode_nativeObj);
+    private static native String detectAndDecode_1(long nativeObj, long img_nativeObj, long points_nativeObj);
+    private static native String detectAndDecode_2(long nativeObj, long img_nativeObj);
+
+    // C++:  bool cv::QRCodeDetector::decodeMulti(Mat img, Mat points, vector_String& decoded_info, vector_Mat& straight_qrcode = vector_Mat())
+    private static native boolean decodeMulti_0(long nativeObj, long img_nativeObj, long points_nativeObj, List<String> decoded_info, long straight_qrcode_mat_nativeObj);
+    private static native boolean decodeMulti_1(long nativeObj, long img_nativeObj, long points_nativeObj, List<String> decoded_info);
+
+    // C++:  bool cv::QRCodeDetector::detect(Mat img, Mat& points)
+    private static native boolean detect_0(long nativeObj, long img_nativeObj, long points_nativeObj);
+
+    // C++:  bool cv::QRCodeDetector::detectAndDecodeMulti(Mat img, vector_String& decoded_info, Mat& points = Mat(), vector_Mat& straight_qrcode = vector_Mat())
+    private static native boolean detectAndDecodeMulti_0(long nativeObj, long img_nativeObj, List<String> decoded_info, long points_nativeObj, long straight_qrcode_mat_nativeObj);
+    private static native boolean detectAndDecodeMulti_1(long nativeObj, long img_nativeObj, List<String> decoded_info, long points_nativeObj);
+    private static native boolean detectAndDecodeMulti_2(long nativeObj, long img_nativeObj, List<String> decoded_info);
+
+    // C++:  bool cv::QRCodeDetector::detectMulti(Mat img, Mat& points)
+    private static native boolean detectMulti_0(long nativeObj, long img_nativeObj, long points_nativeObj);
+
+    // C++:  void cv::QRCodeDetector::setEpsX(double epsX)
+    private static native void setEpsX_0(long nativeObj, double epsX);
+
+    // C++:  void cv::QRCodeDetector::setEpsY(double epsY)
+    private static native void setEpsY_0(long nativeObj, double epsY);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/features2d/AgastFeatureDetector.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/features2d/AgastFeatureDetector.java	(date 1605830247519)
+++ openCVLibrary3411/src/main/java/org/opencv/features2d/AgastFeatureDetector.java	(date 1605830247519)
@@ -0,0 +1,151 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.features2d;
+
+import org.opencv.features2d.AgastFeatureDetector;
+import org.opencv.features2d.Feature2D;
+
+// C++: class AgastFeatureDetector
+/**
+ * Wrapping class for feature detection using the AGAST method. :
+ */
+public class AgastFeatureDetector extends Feature2D {
+
+    protected AgastFeatureDetector(long addr) { super(addr); }
+
+    // internal usage only
+    public static AgastFeatureDetector __fromPtr__(long addr) { return new AgastFeatureDetector(addr); }
+
+    // C++: enum <unnamed>
+    public static final int
+            AGAST_5_8 = 0,
+            AGAST_7_12d = 1,
+            AGAST_7_12s = 2,
+            OAST_9_16 = 3,
+            THRESHOLD = 10000,
+            NONMAX_SUPPRESSION = 10001;
+
+
+    //
+    // C++: static Ptr_AgastFeatureDetector cv::AgastFeatureDetector::create(int threshold = 10, bool nonmaxSuppression = true, int type = AgastFeatureDetector::OAST_9_16)
+    //
+
+    public static AgastFeatureDetector create(int threshold, boolean nonmaxSuppression, int type) {
+        return AgastFeatureDetector.__fromPtr__(create_0(threshold, nonmaxSuppression, type));
+    }
+
+    public static AgastFeatureDetector create(int threshold, boolean nonmaxSuppression) {
+        return AgastFeatureDetector.__fromPtr__(create_1(threshold, nonmaxSuppression));
+    }
+
+    public static AgastFeatureDetector create(int threshold) {
+        return AgastFeatureDetector.__fromPtr__(create_2(threshold));
+    }
+
+    public static AgastFeatureDetector create() {
+        return AgastFeatureDetector.__fromPtr__(create_3());
+    }
+
+
+    //
+    // C++:  String cv::AgastFeatureDetector::getDefaultName()
+    //
+
+    public String getDefaultName() {
+        return getDefaultName_0(nativeObj);
+    }
+
+
+    //
+    // C++:  bool cv::AgastFeatureDetector::getNonmaxSuppression()
+    //
+
+    public boolean getNonmaxSuppression() {
+        return getNonmaxSuppression_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::AgastFeatureDetector::getThreshold()
+    //
+
+    public int getThreshold() {
+        return getThreshold_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::AgastFeatureDetector::getType()
+    //
+
+    public int getType() {
+        return getType_0(nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::AgastFeatureDetector::setNonmaxSuppression(bool f)
+    //
+
+    public void setNonmaxSuppression(boolean f) {
+        setNonmaxSuppression_0(nativeObj, f);
+    }
+
+
+    //
+    // C++:  void cv::AgastFeatureDetector::setThreshold(int threshold)
+    //
+
+    public void setThreshold(int threshold) {
+        setThreshold_0(nativeObj, threshold);
+    }
+
+
+    //
+    // C++:  void cv::AgastFeatureDetector::setType(int type)
+    //
+
+    public void setType(int type) {
+        setType_0(nativeObj, type);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++: static Ptr_AgastFeatureDetector cv::AgastFeatureDetector::create(int threshold = 10, bool nonmaxSuppression = true, int type = AgastFeatureDetector::OAST_9_16)
+    private static native long create_0(int threshold, boolean nonmaxSuppression, int type);
+    private static native long create_1(int threshold, boolean nonmaxSuppression);
+    private static native long create_2(int threshold);
+    private static native long create_3();
+
+    // C++:  String cv::AgastFeatureDetector::getDefaultName()
+    private static native String getDefaultName_0(long nativeObj);
+
+    // C++:  bool cv::AgastFeatureDetector::getNonmaxSuppression()
+    private static native boolean getNonmaxSuppression_0(long nativeObj);
+
+    // C++:  int cv::AgastFeatureDetector::getThreshold()
+    private static native int getThreshold_0(long nativeObj);
+
+    // C++:  int cv::AgastFeatureDetector::getType()
+    private static native int getType_0(long nativeObj);
+
+    // C++:  void cv::AgastFeatureDetector::setNonmaxSuppression(bool f)
+    private static native void setNonmaxSuppression_0(long nativeObj, boolean f);
+
+    // C++:  void cv::AgastFeatureDetector::setThreshold(int threshold)
+    private static native void setThreshold_0(long nativeObj, int threshold);
+
+    // C++:  void cv::AgastFeatureDetector::setType(int type)
+    private static native void setType_0(long nativeObj, int type);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/features2d/AKAZE.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/features2d/AKAZE.java	(date 1605830247529)
+++ openCVLibrary3411/src/main/java/org/opencv/features2d/AKAZE.java	(date 1605830247529)
@@ -0,0 +1,362 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.features2d;
+
+import org.opencv.features2d.AKAZE;
+import org.opencv.features2d.Feature2D;
+
+// C++: class AKAZE
+/**
+ * Class implementing the AKAZE keypoint detector and descriptor extractor, described in CITE: ANB13.
+ *
+ * AKAZE descriptors can only be used with KAZE or AKAZE keypoints. This class is thread-safe.
+ *
+ * <b>Note:</b> When you need descriptors use Feature2D::detectAndCompute, which
+ * provides better performance. When using Feature2D::detect followed by
+ * Feature2D::compute scale space pyramid is computed twice.
+ *
+ * <b>Note:</b> AKAZE implements T-API. When image is passed as UMat some parts of the algorithm
+ * will use OpenCL.
+ *
+ * <b>Note:</b> [ANB13] Fast Explicit Diffusion for Accelerated Features in Nonlinear
+ * Scale Spaces. Pablo F. Alcantarilla, Jess Nuevo and Adrien Bartoli. In
+ * British Machine Vision Conference (BMVC), Bristol, UK, September 2013.
+ */
+public class AKAZE extends Feature2D {
+
+    protected AKAZE(long addr) { super(addr); }
+
+    // internal usage only
+    public static AKAZE __fromPtr__(long addr) { return new AKAZE(addr); }
+
+    // C++: enum <unnamed>
+    public static final int
+            DESCRIPTOR_KAZE_UPRIGHT = 2,
+            DESCRIPTOR_KAZE = 3,
+            DESCRIPTOR_MLDB_UPRIGHT = 4,
+            DESCRIPTOR_MLDB = 5;
+
+
+    //
+    // C++: static Ptr_AKAZE cv::AKAZE::create(int descriptor_type = AKAZE::DESCRIPTOR_MLDB, int descriptor_size = 0, int descriptor_channels = 3, float threshold = 0.001f, int nOctaves = 4, int nOctaveLayers = 4, int diffusivity = KAZE::DIFF_PM_G2)
+    //
+
+    /**
+     * The AKAZE constructor
+     *
+     *     @param descriptor_type Type of the extracted descriptor: DESCRIPTOR_KAZE,
+     *     DESCRIPTOR_KAZE_UPRIGHT, DESCRIPTOR_MLDB or DESCRIPTOR_MLDB_UPRIGHT.
+     *     @param descriptor_size Size of the descriptor in bits. 0 -&gt; Full size
+     *     @param descriptor_channels Number of channels in the descriptor (1, 2, 3)
+     *     @param threshold Detector response threshold to accept point
+     *     @param nOctaves Maximum octave evolution of the image
+     *     @param nOctaveLayers Default number of sublevels per scale level
+     *     @param diffusivity Diffusivity type. DIFF_PM_G1, DIFF_PM_G2, DIFF_WEICKERT or
+     *     DIFF_CHARBONNIER
+     * @return automatically generated
+     */
+    public static AKAZE create(int descriptor_type, int descriptor_size, int descriptor_channels, float threshold, int nOctaves, int nOctaveLayers, int diffusivity) {
+        return AKAZE.__fromPtr__(create_0(descriptor_type, descriptor_size, descriptor_channels, threshold, nOctaves, nOctaveLayers, diffusivity));
+    }
+
+    /**
+     * The AKAZE constructor
+     *
+     *     @param descriptor_type Type of the extracted descriptor: DESCRIPTOR_KAZE,
+     *     DESCRIPTOR_KAZE_UPRIGHT, DESCRIPTOR_MLDB or DESCRIPTOR_MLDB_UPRIGHT.
+     *     @param descriptor_size Size of the descriptor in bits. 0 -&gt; Full size
+     *     @param descriptor_channels Number of channels in the descriptor (1, 2, 3)
+     *     @param threshold Detector response threshold to accept point
+     *     @param nOctaves Maximum octave evolution of the image
+     *     @param nOctaveLayers Default number of sublevels per scale level
+     *     DIFF_CHARBONNIER
+     * @return automatically generated
+     */
+    public static AKAZE create(int descriptor_type, int descriptor_size, int descriptor_channels, float threshold, int nOctaves, int nOctaveLayers) {
+        return AKAZE.__fromPtr__(create_1(descriptor_type, descriptor_size, descriptor_channels, threshold, nOctaves, nOctaveLayers));
+    }
+
+    /**
+     * The AKAZE constructor
+     *
+     *     @param descriptor_type Type of the extracted descriptor: DESCRIPTOR_KAZE,
+     *     DESCRIPTOR_KAZE_UPRIGHT, DESCRIPTOR_MLDB or DESCRIPTOR_MLDB_UPRIGHT.
+     *     @param descriptor_size Size of the descriptor in bits. 0 -&gt; Full size
+     *     @param descriptor_channels Number of channels in the descriptor (1, 2, 3)
+     *     @param threshold Detector response threshold to accept point
+     *     @param nOctaves Maximum octave evolution of the image
+     *     DIFF_CHARBONNIER
+     * @return automatically generated
+     */
+    public static AKAZE create(int descriptor_type, int descriptor_size, int descriptor_channels, float threshold, int nOctaves) {
+        return AKAZE.__fromPtr__(create_2(descriptor_type, descriptor_size, descriptor_channels, threshold, nOctaves));
+    }
+
+    /**
+     * The AKAZE constructor
+     *
+     *     @param descriptor_type Type of the extracted descriptor: DESCRIPTOR_KAZE,
+     *     DESCRIPTOR_KAZE_UPRIGHT, DESCRIPTOR_MLDB or DESCRIPTOR_MLDB_UPRIGHT.
+     *     @param descriptor_size Size of the descriptor in bits. 0 -&gt; Full size
+     *     @param descriptor_channels Number of channels in the descriptor (1, 2, 3)
+     *     @param threshold Detector response threshold to accept point
+     *     DIFF_CHARBONNIER
+     * @return automatically generated
+     */
+    public static AKAZE create(int descriptor_type, int descriptor_size, int descriptor_channels, float threshold) {
+        return AKAZE.__fromPtr__(create_3(descriptor_type, descriptor_size, descriptor_channels, threshold));
+    }
+
+    /**
+     * The AKAZE constructor
+     *
+     *     @param descriptor_type Type of the extracted descriptor: DESCRIPTOR_KAZE,
+     *     DESCRIPTOR_KAZE_UPRIGHT, DESCRIPTOR_MLDB or DESCRIPTOR_MLDB_UPRIGHT.
+     *     @param descriptor_size Size of the descriptor in bits. 0 -&gt; Full size
+     *     @param descriptor_channels Number of channels in the descriptor (1, 2, 3)
+     *     DIFF_CHARBONNIER
+     * @return automatically generated
+     */
+    public static AKAZE create(int descriptor_type, int descriptor_size, int descriptor_channels) {
+        return AKAZE.__fromPtr__(create_4(descriptor_type, descriptor_size, descriptor_channels));
+    }
+
+    /**
+     * The AKAZE constructor
+     *
+     *     @param descriptor_type Type of the extracted descriptor: DESCRIPTOR_KAZE,
+     *     DESCRIPTOR_KAZE_UPRIGHT, DESCRIPTOR_MLDB or DESCRIPTOR_MLDB_UPRIGHT.
+     *     @param descriptor_size Size of the descriptor in bits. 0 -&gt; Full size
+     *     DIFF_CHARBONNIER
+     * @return automatically generated
+     */
+    public static AKAZE create(int descriptor_type, int descriptor_size) {
+        return AKAZE.__fromPtr__(create_5(descriptor_type, descriptor_size));
+    }
+
+    /**
+     * The AKAZE constructor
+     *
+     *     @param descriptor_type Type of the extracted descriptor: DESCRIPTOR_KAZE,
+     *     DESCRIPTOR_KAZE_UPRIGHT, DESCRIPTOR_MLDB or DESCRIPTOR_MLDB_UPRIGHT.
+     *     DIFF_CHARBONNIER
+     * @return automatically generated
+     */
+    public static AKAZE create(int descriptor_type) {
+        return AKAZE.__fromPtr__(create_6(descriptor_type));
+    }
+
+    /**
+     * The AKAZE constructor
+     *
+     *     DESCRIPTOR_KAZE_UPRIGHT, DESCRIPTOR_MLDB or DESCRIPTOR_MLDB_UPRIGHT.
+     *     DIFF_CHARBONNIER
+     * @return automatically generated
+     */
+    public static AKAZE create() {
+        return AKAZE.__fromPtr__(create_7());
+    }
+
+
+    //
+    // C++:  String cv::AKAZE::getDefaultName()
+    //
+
+    public String getDefaultName() {
+        return getDefaultName_0(nativeObj);
+    }
+
+
+    //
+    // C++:  double cv::AKAZE::getThreshold()
+    //
+
+    public double getThreshold() {
+        return getThreshold_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::AKAZE::getDescriptorChannels()
+    //
+
+    public int getDescriptorChannels() {
+        return getDescriptorChannels_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::AKAZE::getDescriptorSize()
+    //
+
+    public int getDescriptorSize() {
+        return getDescriptorSize_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::AKAZE::getDescriptorType()
+    //
+
+    public int getDescriptorType() {
+        return getDescriptorType_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::AKAZE::getDiffusivity()
+    //
+
+    public int getDiffusivity() {
+        return getDiffusivity_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::AKAZE::getNOctaveLayers()
+    //
+
+    public int getNOctaveLayers() {
+        return getNOctaveLayers_0(nativeObj);
+    }
+
+
+    //
+    // C++:  int cv::AKAZE::getNOctaves()
+    //
+
+    public int getNOctaves() {
+        return getNOctaves_0(nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::AKAZE::setDescriptorChannels(int dch)
+    //
+
+    public void setDescriptorChannels(int dch) {
+        setDescriptorChannels_0(nativeObj, dch);
+    }
+
+
+    //
+    // C++:  void cv::AKAZE::setDescriptorSize(int dsize)
+    //
+
+    public void setDescriptorSize(int dsize) {
+        setDescriptorSize_0(nativeObj, dsize);
+    }
+
+
+    //
+    // C++:  void cv::AKAZE::setDescriptorType(int dtype)
+    //
+
+    public void setDescriptorType(int dtype) {
+        setDescriptorType_0(nativeObj, dtype);
+    }
+
+
+    //
+    // C++:  void cv::AKAZE::setDiffusivity(int diff)
+    //
+
+    public void setDiffusivity(int diff) {
+        setDiffusivity_0(nativeObj, diff);
+    }
+
+
+    //
+    // C++:  void cv::AKAZE::setNOctaveLayers(int octaveLayers)
+    //
+
+    public void setNOctaveLayers(int octaveLayers) {
+        setNOctaveLayers_0(nativeObj, octaveLayers);
+    }
+
+
+    //
+    // C++:  void cv::AKAZE::setNOctaves(int octaves)
+    //
+
+    public void setNOctaves(int octaves) {
+        setNOctaves_0(nativeObj, octaves);
+    }
+
+
+    //
+    // C++:  void cv::AKAZE::setThreshold(double threshold)
+    //
+
+    public void setThreshold(double threshold) {
+        setThreshold_0(nativeObj, threshold);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++: static Ptr_AKAZE cv::AKAZE::create(int descriptor_type = AKAZE::DESCRIPTOR_MLDB, int descriptor_size = 0, int descriptor_channels = 3, float threshold = 0.001f, int nOctaves = 4, int nOctaveLayers = 4, int diffusivity = KAZE::DIFF_PM_G2)
+    private static native long create_0(int descriptor_type, int descriptor_size, int descriptor_channels, float threshold, int nOctaves, int nOctaveLayers, int diffusivity);
+    private static native long create_1(int descriptor_type, int descriptor_size, int descriptor_channels, float threshold, int nOctaves, int nOctaveLayers);
+    private static native long create_2(int descriptor_type, int descriptor_size, int descriptor_channels, float threshold, int nOctaves);
+    private static native long create_3(int descriptor_type, int descriptor_size, int descriptor_channels, float threshold);
+    private static native long create_4(int descriptor_type, int descriptor_size, int descriptor_channels);
+    private static native long create_5(int descriptor_type, int descriptor_size);
+    private static native long create_6(int descriptor_type);
+    private static native long create_7();
+
+    // C++:  String cv::AKAZE::getDefaultName()
+    private static native String getDefaultName_0(long nativeObj);
+
+    // C++:  double cv::AKAZE::getThreshold()
+    private static native double getThreshold_0(long nativeObj);
+
+    // C++:  int cv::AKAZE::getDescriptorChannels()
+    private static native int getDescriptorChannels_0(long nativeObj);
+
+    // C++:  int cv::AKAZE::getDescriptorSize()
+    private static native int getDescriptorSize_0(long nativeObj);
+
+    // C++:  int cv::AKAZE::getDescriptorType()
+    private static native int getDescriptorType_0(long nativeObj);
+
+    // C++:  int cv::AKAZE::getDiffusivity()
+    private static native int getDiffusivity_0(long nativeObj);
+
+    // C++:  int cv::AKAZE::getNOctaveLayers()
+    private static native int getNOctaveLayers_0(long nativeObj);
+
+    // C++:  int cv::AKAZE::getNOctaves()
+    private static native int getNOctaves_0(long nativeObj);
+
+    // C++:  void cv::AKAZE::setDescriptorChannels(int dch)
+    private static native void setDescriptorChannels_0(long nativeObj, int dch);
+
+    // C++:  void cv::AKAZE::setDescriptorSize(int dsize)
+    private static native void setDescriptorSize_0(long nativeObj, int dsize);
+
+    // C++:  void cv::AKAZE::setDescriptorType(int dtype)
+    private static native void setDescriptorType_0(long nativeObj, int dtype);
+
+    // C++:  void cv::AKAZE::setDiffusivity(int diff)
+    private static native void setDiffusivity_0(long nativeObj, int diff);
+
+    // C++:  void cv::AKAZE::setNOctaveLayers(int octaveLayers)
+    private static native void setNOctaveLayers_0(long nativeObj, int octaveLayers);
+
+    // C++:  void cv::AKAZE::setNOctaves(int octaves)
+    private static native void setNOctaves_0(long nativeObj, int octaves);
+
+    // C++:  void cv::AKAZE::setThreshold(double threshold)
+    private static native void setThreshold_0(long nativeObj, double threshold);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/features2d/BFMatcher.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/features2d/BFMatcher.java	(date 1605830247531)
+++ openCVLibrary3411/src/main/java/org/opencv/features2d/BFMatcher.java	(date 1605830247531)
@@ -0,0 +1,135 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.features2d;
+
+import org.opencv.features2d.BFMatcher;
+import org.opencv.features2d.DescriptorMatcher;
+
+// C++: class BFMatcher
+/**
+ * Brute-force descriptor matcher.
+ *
+ * For each descriptor in the first set, this matcher finds the closest descriptor in the second set
+ * by trying each one. This descriptor matcher supports masking permissible matches of descriptor
+ * sets.
+ */
+public class BFMatcher extends DescriptorMatcher {
+
+    protected BFMatcher(long addr) { super(addr); }
+
+    // internal usage only
+    public static BFMatcher __fromPtr__(long addr) { return new BFMatcher(addr); }
+
+    //
+    // C++:   cv::BFMatcher::BFMatcher(int normType = NORM_L2, bool crossCheck = false)
+    //
+
+    /**
+     * Brute-force matcher constructor (obsolete). Please use BFMatcher.create()
+     *
+     *
+     * @param normType automatically generated
+     * @param crossCheck automatically generated
+     */
+    public BFMatcher(int normType, boolean crossCheck) {
+        super(BFMatcher_0(normType, crossCheck));
+    }
+
+    /**
+     * Brute-force matcher constructor (obsolete). Please use BFMatcher.create()
+     *
+     *
+     * @param normType automatically generated
+     */
+    public BFMatcher(int normType) {
+        super(BFMatcher_1(normType));
+    }
+
+    /**
+     * Brute-force matcher constructor (obsolete). Please use BFMatcher.create()
+     *
+     *
+     */
+    public BFMatcher() {
+        super(BFMatcher_2());
+    }
+
+
+    //
+    // C++: static Ptr_BFMatcher cv::BFMatcher::create(int normType = NORM_L2, bool crossCheck = false)
+    //
+
+    /**
+     * Brute-force matcher create method.
+     *     @param normType One of NORM_L1, NORM_L2, NORM_HAMMING, NORM_HAMMING2. L1 and L2 norms are
+     *     preferable choices for SIFT and SURF descriptors, NORM_HAMMING should be used with ORB, BRISK and
+     *     BRIEF, NORM_HAMMING2 should be used with ORB when WTA_K==3 or 4 (see ORB::ORB constructor
+     *     description).
+     *     @param crossCheck If it is false, this is will be default BFMatcher behaviour when it finds the k
+     *     nearest neighbors for each query descriptor. If crossCheck==true, then the knnMatch() method with
+     *     k=1 will only return pairs (i,j) such that for i-th query descriptor the j-th descriptor in the
+     *     matcher's collection is the nearest and vice versa, i.e. the BFMatcher will only return consistent
+     *     pairs. Such technique usually produces best results with minimal number of outliers when there are
+     *     enough matches. This is alternative to the ratio test, used by D. Lowe in SIFT paper.
+     * @return automatically generated
+     */
+    public static BFMatcher create(int normType, boolean crossCheck) {
+        return BFMatcher.__fromPtr__(create_0(normType, crossCheck));
+    }
+
+    /**
+     * Brute-force matcher create method.
+     *     @param normType One of NORM_L1, NORM_L2, NORM_HAMMING, NORM_HAMMING2. L1 and L2 norms are
+     *     preferable choices for SIFT and SURF descriptors, NORM_HAMMING should be used with ORB, BRISK and
+     *     BRIEF, NORM_HAMMING2 should be used with ORB when WTA_K==3 or 4 (see ORB::ORB constructor
+     *     description).
+     *     nearest neighbors for each query descriptor. If crossCheck==true, then the knnMatch() method with
+     *     k=1 will only return pairs (i,j) such that for i-th query descriptor the j-th descriptor in the
+     *     matcher's collection is the nearest and vice versa, i.e. the BFMatcher will only return consistent
+     *     pairs. Such technique usually produces best results with minimal number of outliers when there are
+     *     enough matches. This is alternative to the ratio test, used by D. Lowe in SIFT paper.
+     * @return automatically generated
+     */
+    public static BFMatcher create(int normType) {
+        return BFMatcher.__fromPtr__(create_1(normType));
+    }
+
+    /**
+     * Brute-force matcher create method.
+     *     preferable choices for SIFT and SURF descriptors, NORM_HAMMING should be used with ORB, BRISK and
+     *     BRIEF, NORM_HAMMING2 should be used with ORB when WTA_K==3 or 4 (see ORB::ORB constructor
+     *     description).
+     *     nearest neighbors for each query descriptor. If crossCheck==true, then the knnMatch() method with
+     *     k=1 will only return pairs (i,j) such that for i-th query descriptor the j-th descriptor in the
+     *     matcher's collection is the nearest and vice versa, i.e. the BFMatcher will only return consistent
+     *     pairs. Such technique usually produces best results with minimal number of outliers when there are
+     *     enough matches. This is alternative to the ratio test, used by D. Lowe in SIFT paper.
+     * @return automatically generated
+     */
+    public static BFMatcher create() {
+        return BFMatcher.__fromPtr__(create_2());
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++:   cv::BFMatcher::BFMatcher(int normType = NORM_L2, bool crossCheck = false)
+    private static native long BFMatcher_0(int normType, boolean crossCheck);
+    private static native long BFMatcher_1(int normType);
+    private static native long BFMatcher_2();
+
+    // C++: static Ptr_BFMatcher cv::BFMatcher::create(int normType = NORM_L2, bool crossCheck = false)
+    private static native long create_0(int normType, boolean crossCheck);
+    private static native long create_1(int normType);
+    private static native long create_2();
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/objdetect/Objdetect.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/objdetect/Objdetect.java	(date 1605830247985)
+++ openCVLibrary3411/src/main/java/org/opencv/objdetect/Objdetect.java	(date 1605830247985)
@@ -0,0 +1,56 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.objdetect;
+
+import java.util.ArrayList;
+import java.util.List;
+import org.opencv.core.Mat;
+import org.opencv.core.MatOfInt;
+import org.opencv.core.MatOfRect;
+import org.opencv.utils.Converters;
+
+// C++: class Objdetect
+
+public class Objdetect {
+
+    // C++: enum <unnamed>
+    public static final int
+            CASCADE_DO_CANNY_PRUNING = 1,
+            CASCADE_SCALE_IMAGE = 2,
+            CASCADE_FIND_BIGGEST_OBJECT = 4,
+            CASCADE_DO_ROUGH_SEARCH = 8;
+
+
+    // C++: enum ObjectStatus
+    public static final int
+            DetectionBasedTracker_DETECTED_NOT_SHOWN_YET = 0,
+            DetectionBasedTracker_DETECTED = 1,
+            DetectionBasedTracker_DETECTED_TEMPORARY_LOST = 2,
+            DetectionBasedTracker_WRONG_OBJECT = 3;
+
+
+    //
+    // C++:  void cv::groupRectangles(vector_Rect& rectList, vector_int& weights, int groupThreshold, double eps = 0.2)
+    //
+
+    public static void groupRectangles(MatOfRect rectList, MatOfInt weights, int groupThreshold, double eps) {
+        Mat rectList_mat = rectList;
+        Mat weights_mat = weights;
+        groupRectangles_0(rectList_mat.nativeObj, weights_mat.nativeObj, groupThreshold, eps);
+    }
+
+    public static void groupRectangles(MatOfRect rectList, MatOfInt weights, int groupThreshold) {
+        Mat rectList_mat = rectList;
+        Mat weights_mat = weights;
+        groupRectangles_1(rectList_mat.nativeObj, weights_mat.nativeObj, groupThreshold);
+    }
+
+
+
+
+    // C++:  void cv::groupRectangles(vector_Rect& rectList, vector_int& weights, int groupThreshold, double eps = 0.2)
+    private static native void groupRectangles_0(long rectList_mat_nativeObj, long weights_mat_nativeObj, int groupThreshold, double eps);
+    private static native void groupRectangles_1(long rectList_mat_nativeObj, long weights_mat_nativeObj, int groupThreshold);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/imgcodecs/Imgcodecs.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/imgcodecs/Imgcodecs.java	(date 1605830247712)
+++ openCVLibrary3411/src/main/java/org/opencv/imgcodecs/Imgcodecs.java	(date 1605830247712)
@@ -0,0 +1,525 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.imgcodecs;
+
+import java.util.ArrayList;
+import java.util.List;
+import org.opencv.core.Mat;
+import org.opencv.core.MatOfByte;
+import org.opencv.core.MatOfInt;
+import org.opencv.utils.Converters;
+
+// C++: class Imgcodecs
+
+public class Imgcodecs {
+
+    // C++: enum <unnamed>
+    public static final int
+            CV_LOAD_IMAGE_UNCHANGED = -1,
+            CV_LOAD_IMAGE_GRAYSCALE = 0,
+            CV_LOAD_IMAGE_COLOR = 1,
+            CV_LOAD_IMAGE_ANYDEPTH = 2,
+            CV_LOAD_IMAGE_ANYCOLOR = 4,
+            CV_LOAD_IMAGE_IGNORE_ORIENTATION = 128,
+            CV_IMWRITE_JPEG_QUALITY = 1,
+            CV_IMWRITE_JPEG_PROGRESSIVE = 2,
+            CV_IMWRITE_JPEG_OPTIMIZE = 3,
+            CV_IMWRITE_JPEG_RST_INTERVAL = 4,
+            CV_IMWRITE_JPEG_LUMA_QUALITY = 5,
+            CV_IMWRITE_JPEG_CHROMA_QUALITY = 6,
+            CV_IMWRITE_PNG_COMPRESSION = 16,
+            CV_IMWRITE_PNG_STRATEGY = 17,
+            CV_IMWRITE_PNG_BILEVEL = 18,
+            CV_IMWRITE_PNG_STRATEGY_DEFAULT = 0,
+            CV_IMWRITE_PNG_STRATEGY_FILTERED = 1,
+            CV_IMWRITE_PNG_STRATEGY_HUFFMAN_ONLY = 2,
+            CV_IMWRITE_PNG_STRATEGY_RLE = 3,
+            CV_IMWRITE_PNG_STRATEGY_FIXED = 4,
+            CV_IMWRITE_PXM_BINARY = 32,
+            CV_IMWRITE_EXR_TYPE = 48,
+            CV_IMWRITE_WEBP_QUALITY = 64,
+            CV_IMWRITE_PAM_TUPLETYPE = 128,
+            CV_IMWRITE_PAM_FORMAT_NULL = 0,
+            CV_IMWRITE_PAM_FORMAT_BLACKANDWHITE = 1,
+            CV_IMWRITE_PAM_FORMAT_GRAYSCALE = 2,
+            CV_IMWRITE_PAM_FORMAT_GRAYSCALE_ALPHA = 3,
+            CV_IMWRITE_PAM_FORMAT_RGB = 4,
+            CV_IMWRITE_PAM_FORMAT_RGB_ALPHA = 5,
+            CV_CVTIMG_FLIP = 1,
+            CV_CVTIMG_SWAP_RB = 2;
+
+
+    // C++: enum ImwriteEXRTypeFlags
+    public static final int
+            IMWRITE_EXR_TYPE_HALF = 1,
+            IMWRITE_EXR_TYPE_FLOAT = 2;
+
+
+    // C++: enum ImwritePNGFlags
+    public static final int
+            IMWRITE_PNG_STRATEGY_DEFAULT = 0,
+            IMWRITE_PNG_STRATEGY_FILTERED = 1,
+            IMWRITE_PNG_STRATEGY_HUFFMAN_ONLY = 2,
+            IMWRITE_PNG_STRATEGY_RLE = 3,
+            IMWRITE_PNG_STRATEGY_FIXED = 4;
+
+
+    // C++: enum ImwriteFlags
+    public static final int
+            IMWRITE_JPEG_QUALITY = 1,
+            IMWRITE_JPEG_PROGRESSIVE = 2,
+            IMWRITE_JPEG_OPTIMIZE = 3,
+            IMWRITE_JPEG_RST_INTERVAL = 4,
+            IMWRITE_JPEG_LUMA_QUALITY = 5,
+            IMWRITE_JPEG_CHROMA_QUALITY = 6,
+            IMWRITE_PNG_COMPRESSION = 16,
+            IMWRITE_PNG_STRATEGY = 17,
+            IMWRITE_PNG_BILEVEL = 18,
+            IMWRITE_PXM_BINARY = 32,
+            IMWRITE_EXR_TYPE = (3 << 4) + 0,
+            IMWRITE_WEBP_QUALITY = 64,
+            IMWRITE_PAM_TUPLETYPE = 128,
+            IMWRITE_TIFF_RESUNIT = 256,
+            IMWRITE_TIFF_XDPI = 257,
+            IMWRITE_TIFF_YDPI = 258,
+            IMWRITE_TIFF_COMPRESSION = 259;
+
+
+    // C++: enum ImreadModes
+    public static final int
+            IMREAD_UNCHANGED = -1,
+            IMREAD_GRAYSCALE = 0,
+            IMREAD_COLOR = 1,
+            IMREAD_ANYDEPTH = 2,
+            IMREAD_ANYCOLOR = 4,
+            IMREAD_LOAD_GDAL = 8,
+            IMREAD_REDUCED_GRAYSCALE_2 = 16,
+            IMREAD_REDUCED_COLOR_2 = 17,
+            IMREAD_REDUCED_GRAYSCALE_4 = 32,
+            IMREAD_REDUCED_COLOR_4 = 33,
+            IMREAD_REDUCED_GRAYSCALE_8 = 64,
+            IMREAD_REDUCED_COLOR_8 = 65,
+            IMREAD_IGNORE_ORIENTATION = 128;
+
+
+    // C++: enum ImwritePAMFlags
+    public static final int
+            IMWRITE_PAM_FORMAT_NULL = 0,
+            IMWRITE_PAM_FORMAT_BLACKANDWHITE = 1,
+            IMWRITE_PAM_FORMAT_GRAYSCALE = 2,
+            IMWRITE_PAM_FORMAT_GRAYSCALE_ALPHA = 3,
+            IMWRITE_PAM_FORMAT_RGB = 4,
+            IMWRITE_PAM_FORMAT_RGB_ALPHA = 5;
+
+
+    //
+    // C++:  Mat cv::imdecode(Mat buf, int flags)
+    //
+
+    /**
+     * Reads an image from a buffer in memory.
+     *
+     * The function imdecode reads an image from the specified buffer in the memory. If the buffer is too short or
+     * contains invalid data, the function returns an empty matrix ( Mat::data==NULL ).
+     *
+     * See cv::imread for the list of supported formats and flags description.
+     *
+     * <b>Note:</b> In the case of color images, the decoded images will have the channels stored in <b>B G R</b> order.
+     * @param buf Input array or vector of bytes.
+     * @param flags The same flags as in cv::imread, see cv::ImreadModes.
+     * @return automatically generated
+     */
+    public static Mat imdecode(Mat buf, int flags) {
+        return new Mat(imdecode_0(buf.nativeObj, flags));
+    }
+
+
+    //
+    // C++:  Mat cv::imread(String filename, int flags = IMREAD_COLOR)
+    //
+
+    /**
+     * Loads an image from a file.
+     *
+     *  imread
+     *
+     * The function imread loads an image from the specified file and returns it. If the image cannot be
+     * read (because of missing file, improper permissions, unsupported or invalid format), the function
+     * returns an empty matrix ( Mat::data==NULL ).
+     *
+     * Currently, the following file formats are supported:
+     *
+     * <ul>
+     *   <li>
+     *    Windows bitmaps - \*.bmp, \*.dib (always supported)
+     *   </li>
+     *   <li>
+     *    JPEG files - \*.jpeg, \*.jpg, \*.jpe (see the *Note* section)
+     *   </li>
+     *   <li>
+     *    JPEG 2000 files - \*.jp2 (see the *Note* section)
+     *   </li>
+     *   <li>
+     *    Portable Network Graphics - \*.png (see the *Note* section)
+     *   </li>
+     *   <li>
+     *    WebP - \*.webp (see the *Note* section)
+     *   </li>
+     *   <li>
+     *    Portable image format - \*.pbm, \*.pgm, \*.ppm \*.pxm, \*.pnm (always supported)
+     *   </li>
+     *   <li>
+     *    Sun rasters - \*.sr, \*.ras (always supported)
+     *   </li>
+     *   <li>
+     *    TIFF files - \*.tiff, \*.tif (see the *Note* section)
+     *   </li>
+     *   <li>
+     *    OpenEXR Image files - \*.exr (see the *Note* section)
+     *   </li>
+     *   <li>
+     *    Radiance HDR - \*.hdr, \*.pic (always supported)
+     *   </li>
+     *   <li>
+     *    Raster and Vector geospatial data supported by GDAL (see the *Note* section)
+     *   </li>
+     * </ul>
+     *
+     * <b>Note:</b>
+     * <ul>
+     *   <li>
+     *    The function determines the type of an image by the content, not by the file extension.
+     *   </li>
+     *   <li>
+     *    In the case of color images, the decoded images will have the channels stored in <b>B G R</b> order.
+     *   </li>
+     *   <li>
+     *    When using IMREAD_GRAYSCALE, the codec's internal grayscale conversion will be used, if available.
+     *     Results may differ to the output of cvtColor()
+     *   </li>
+     *   <li>
+     *    On Microsoft Windows\* OS and MacOSX\*, the codecs shipped with an OpenCV image (libjpeg,
+     *     libpng, libtiff, and libjasper) are used by default. So, OpenCV can always read JPEGs, PNGs,
+     *     and TIFFs. On MacOSX, there is also an option to use native MacOSX image readers. But beware
+     *     that currently these native image loaders give images with different pixel values because of
+     *     the color management embedded into MacOSX.
+     *   </li>
+     *   <li>
+     *    On Linux\*, BSD flavors and other Unix-like open-source operating systems, OpenCV looks for
+     *     codecs supplied with an OS image. Install the relevant packages (do not forget the development
+     *     files, for example, "libjpeg-dev", in Debian\* and Ubuntu\*) to get the codec support or turn
+     *     on the OPENCV_BUILD_3RDPARTY_LIBS flag in CMake.
+     *   </li>
+     *   <li>
+     *    In the case you set *WITH_GDAL* flag to true in CMake and REF: IMREAD_LOAD_GDAL to load the image,
+     *     then the [GDAL](http://www.gdal.org) driver will be used in order to decode the image, supporting
+     *     the following formats: [Raster](http://www.gdal.org/formats_list.html),
+     *     [Vector](http://www.gdal.org/ogr_formats.html).
+     *   </li>
+     *   <li>
+     *    If EXIF information is embedded in the image file, the EXIF orientation will be taken into account
+     *     and thus the image will be rotated accordingly except if the flags REF: IMREAD_IGNORE_ORIENTATION
+     *     or REF: IMREAD_UNCHANGED are passed.
+     *   </li>
+     *   <li>
+     *    By default number of pixels must be less than 2^30. Limit can be set using system
+     *     variable OPENCV_IO_MAX_IMAGE_PIXELS
+     *   </li>
+     * </ul>
+     *
+     * @param filename Name of file to be loaded.
+     * @param flags Flag that can take values of cv::ImreadModes
+     * @return automatically generated
+     */
+    public static Mat imread(String filename, int flags) {
+        return new Mat(imread_0(filename, flags));
+    }
+
+    /**
+     * Loads an image from a file.
+     *
+     *  imread
+     *
+     * The function imread loads an image from the specified file and returns it. If the image cannot be
+     * read (because of missing file, improper permissions, unsupported or invalid format), the function
+     * returns an empty matrix ( Mat::data==NULL ).
+     *
+     * Currently, the following file formats are supported:
+     *
+     * <ul>
+     *   <li>
+     *    Windows bitmaps - \*.bmp, \*.dib (always supported)
+     *   </li>
+     *   <li>
+     *    JPEG files - \*.jpeg, \*.jpg, \*.jpe (see the *Note* section)
+     *   </li>
+     *   <li>
+     *    JPEG 2000 files - \*.jp2 (see the *Note* section)
+     *   </li>
+     *   <li>
+     *    Portable Network Graphics - \*.png (see the *Note* section)
+     *   </li>
+     *   <li>
+     *    WebP - \*.webp (see the *Note* section)
+     *   </li>
+     *   <li>
+     *    Portable image format - \*.pbm, \*.pgm, \*.ppm \*.pxm, \*.pnm (always supported)
+     *   </li>
+     *   <li>
+     *    Sun rasters - \*.sr, \*.ras (always supported)
+     *   </li>
+     *   <li>
+     *    TIFF files - \*.tiff, \*.tif (see the *Note* section)
+     *   </li>
+     *   <li>
+     *    OpenEXR Image files - \*.exr (see the *Note* section)
+     *   </li>
+     *   <li>
+     *    Radiance HDR - \*.hdr, \*.pic (always supported)
+     *   </li>
+     *   <li>
+     *    Raster and Vector geospatial data supported by GDAL (see the *Note* section)
+     *   </li>
+     * </ul>
+     *
+     * <b>Note:</b>
+     * <ul>
+     *   <li>
+     *    The function determines the type of an image by the content, not by the file extension.
+     *   </li>
+     *   <li>
+     *    In the case of color images, the decoded images will have the channels stored in <b>B G R</b> order.
+     *   </li>
+     *   <li>
+     *    When using IMREAD_GRAYSCALE, the codec's internal grayscale conversion will be used, if available.
+     *     Results may differ to the output of cvtColor()
+     *   </li>
+     *   <li>
+     *    On Microsoft Windows\* OS and MacOSX\*, the codecs shipped with an OpenCV image (libjpeg,
+     *     libpng, libtiff, and libjasper) are used by default. So, OpenCV can always read JPEGs, PNGs,
+     *     and TIFFs. On MacOSX, there is also an option to use native MacOSX image readers. But beware
+     *     that currently these native image loaders give images with different pixel values because of
+     *     the color management embedded into MacOSX.
+     *   </li>
+     *   <li>
+     *    On Linux\*, BSD flavors and other Unix-like open-source operating systems, OpenCV looks for
+     *     codecs supplied with an OS image. Install the relevant packages (do not forget the development
+     *     files, for example, "libjpeg-dev", in Debian\* and Ubuntu\*) to get the codec support or turn
+     *     on the OPENCV_BUILD_3RDPARTY_LIBS flag in CMake.
+     *   </li>
+     *   <li>
+     *    In the case you set *WITH_GDAL* flag to true in CMake and REF: IMREAD_LOAD_GDAL to load the image,
+     *     then the [GDAL](http://www.gdal.org) driver will be used in order to decode the image, supporting
+     *     the following formats: [Raster](http://www.gdal.org/formats_list.html),
+     *     [Vector](http://www.gdal.org/ogr_formats.html).
+     *   </li>
+     *   <li>
+     *    If EXIF information is embedded in the image file, the EXIF orientation will be taken into account
+     *     and thus the image will be rotated accordingly except if the flags REF: IMREAD_IGNORE_ORIENTATION
+     *     or REF: IMREAD_UNCHANGED are passed.
+     *   </li>
+     *   <li>
+     *    By default number of pixels must be less than 2^30. Limit can be set using system
+     *     variable OPENCV_IO_MAX_IMAGE_PIXELS
+     *   </li>
+     * </ul>
+     *
+     * @param filename Name of file to be loaded.
+     * @return automatically generated
+     */
+    public static Mat imread(String filename) {
+        return new Mat(imread_1(filename));
+    }
+
+
+    //
+    // C++:  bool cv::imencode(String ext, Mat img, vector_uchar& buf, vector_int params = std::vector<int>())
+    //
+
+    /**
+     * Encodes an image into a memory buffer.
+     *
+     * The function imencode compresses the image and stores it in the memory buffer that is resized to fit the
+     * result. See cv::imwrite for the list of supported formats and flags description.
+     *
+     * @param ext File extension that defines the output format.
+     * @param img Image to be written.
+     * @param buf Output buffer resized to fit the compressed image.
+     * @param params Format-specific parameters. See cv::imwrite and cv::ImwriteFlags.
+     * @return automatically generated
+     */
+    public static boolean imencode(String ext, Mat img, MatOfByte buf, MatOfInt params) {
+        Mat buf_mat = buf;
+        Mat params_mat = params;
+        return imencode_0(ext, img.nativeObj, buf_mat.nativeObj, params_mat.nativeObj);
+    }
+
+    /**
+     * Encodes an image into a memory buffer.
+     *
+     * The function imencode compresses the image and stores it in the memory buffer that is resized to fit the
+     * result. See cv::imwrite for the list of supported formats and flags description.
+     *
+     * @param ext File extension that defines the output format.
+     * @param img Image to be written.
+     * @param buf Output buffer resized to fit the compressed image.
+     * @return automatically generated
+     */
+    public static boolean imencode(String ext, Mat img, MatOfByte buf) {
+        Mat buf_mat = buf;
+        return imencode_1(ext, img.nativeObj, buf_mat.nativeObj);
+    }
+
+
+    //
+    // C++:  bool cv::imreadmulti(String filename, vector_Mat& mats, int flags = IMREAD_ANYCOLOR)
+    //
+
+    /**
+     * Loads a multi-page image from a file.
+     *
+     * The function imreadmulti loads a multi-page image from the specified file into a vector of Mat objects.
+     * @param filename Name of file to be loaded.
+     * @param flags Flag that can take values of cv::ImreadModes, default with cv::IMREAD_ANYCOLOR.
+     * @param mats A vector of Mat objects holding each page, if more than one.
+     * SEE: cv::imread
+     * @return automatically generated
+     */
+    public static boolean imreadmulti(String filename, List<Mat> mats, int flags) {
+        Mat mats_mat = new Mat();
+        boolean retVal = imreadmulti_0(filename, mats_mat.nativeObj, flags);
+        Converters.Mat_to_vector_Mat(mats_mat, mats);
+        mats_mat.release();
+        return retVal;
+    }
+
+    /**
+     * Loads a multi-page image from a file.
+     *
+     * The function imreadmulti loads a multi-page image from the specified file into a vector of Mat objects.
+     * @param filename Name of file to be loaded.
+     * @param mats A vector of Mat objects holding each page, if more than one.
+     * SEE: cv::imread
+     * @return automatically generated
+     */
+    public static boolean imreadmulti(String filename, List<Mat> mats) {
+        Mat mats_mat = new Mat();
+        boolean retVal = imreadmulti_1(filename, mats_mat.nativeObj);
+        Converters.Mat_to_vector_Mat(mats_mat, mats);
+        mats_mat.release();
+        return retVal;
+    }
+
+
+    //
+    // C++:  bool cv::imwrite(String filename, Mat img, vector_int params = std::vector<int>())
+    //
+
+    /**
+     * Saves an image to a specified file.
+     *
+     * The function imwrite saves the image to the specified file. The image format is chosen based on the
+     * filename extension (see cv::imread for the list of extensions). In general, only 8-bit
+     * single-channel or 3-channel (with 'BGR' channel order) images
+     * can be saved using this function, with these exceptions:
+     *
+     * <ul>
+     *   <li>
+     *  16-bit unsigned (CV_16U) images can be saved in the case of PNG, JPEG 2000, and TIFF formats
+     *   </li>
+     *   <li>
+     *  32-bit float (CV_32F) images can be saved in TIFF, OpenEXR, and Radiance HDR formats; 3-channel
+     * (CV_32FC3) TIFF images will be saved using the LogLuv high dynamic range encoding (4 bytes per pixel)
+     *   </li>
+     *   <li>
+     *  PNG images with an alpha channel can be saved using this function. To do this, create
+     * 8-bit (or 16-bit) 4-channel image BGRA, where the alpha channel goes last. Fully transparent pixels
+     * should have alpha set to 0, fully opaque pixels should have alpha set to 255/65535 (see the code sample below).
+     *   </li>
+     *   <li>
+     *  Multiple images (vector of Mat) can be saved in TIFF format (see the code sample below).
+     *   </li>
+     * </ul>
+     *
+     * If the format, depth or channel order is different, use
+     * Mat::convertTo and cv::cvtColor to convert it before saving. Or, use the universal FileStorage I/O
+     * functions to save the image to XML or YAML format.
+     *
+     * The sample below shows how to create a BGRA image, how to set custom compression parameters and save it to a PNG file.
+     * It also demonstrates how to save multiple images in a TIFF file:
+     * INCLUDE: snippets/imgcodecs_imwrite.cpp
+     * @param filename Name of the file.
+     * @param img (Mat or vector of Mat) Image or Images to be saved.
+     * @param params Format-specific parameters encoded as pairs (paramId_1, paramValue_1, paramId_2, paramValue_2, ... .) see cv::ImwriteFlags
+     * @return automatically generated
+     */
+    public static boolean imwrite(String filename, Mat img, MatOfInt params) {
+        Mat params_mat = params;
+        return imwrite_0(filename, img.nativeObj, params_mat.nativeObj);
+    }
+
+    /**
+     * Saves an image to a specified file.
+     *
+     * The function imwrite saves the image to the specified file. The image format is chosen based on the
+     * filename extension (see cv::imread for the list of extensions). In general, only 8-bit
+     * single-channel or 3-channel (with 'BGR' channel order) images
+     * can be saved using this function, with these exceptions:
+     *
+     * <ul>
+     *   <li>
+     *  16-bit unsigned (CV_16U) images can be saved in the case of PNG, JPEG 2000, and TIFF formats
+     *   </li>
+     *   <li>
+     *  32-bit float (CV_32F) images can be saved in TIFF, OpenEXR, and Radiance HDR formats; 3-channel
+     * (CV_32FC3) TIFF images will be saved using the LogLuv high dynamic range encoding (4 bytes per pixel)
+     *   </li>
+     *   <li>
+     *  PNG images with an alpha channel can be saved using this function. To do this, create
+     * 8-bit (or 16-bit) 4-channel image BGRA, where the alpha channel goes last. Fully transparent pixels
+     * should have alpha set to 0, fully opaque pixels should have alpha set to 255/65535 (see the code sample below).
+     *   </li>
+     *   <li>
+     *  Multiple images (vector of Mat) can be saved in TIFF format (see the code sample below).
+     *   </li>
+     * </ul>
+     *
+     * If the format, depth or channel order is different, use
+     * Mat::convertTo and cv::cvtColor to convert it before saving. Or, use the universal FileStorage I/O
+     * functions to save the image to XML or YAML format.
+     *
+     * The sample below shows how to create a BGRA image, how to set custom compression parameters and save it to a PNG file.
+     * It also demonstrates how to save multiple images in a TIFF file:
+     * INCLUDE: snippets/imgcodecs_imwrite.cpp
+     * @param filename Name of the file.
+     * @param img (Mat or vector of Mat) Image or Images to be saved.
+     * @return automatically generated
+     */
+    public static boolean imwrite(String filename, Mat img) {
+        return imwrite_1(filename, img.nativeObj);
+    }
+
+
+
+
+    // C++:  Mat cv::imdecode(Mat buf, int flags)
+    private static native long imdecode_0(long buf_nativeObj, int flags);
+
+    // C++:  Mat cv::imread(String filename, int flags = IMREAD_COLOR)
+    private static native long imread_0(String filename, int flags);
+    private static native long imread_1(String filename);
+
+    // C++:  bool cv::imencode(String ext, Mat img, vector_uchar& buf, vector_int params = std::vector<int>())
+    private static native boolean imencode_0(String ext, long img_nativeObj, long buf_mat_nativeObj, long params_mat_nativeObj);
+    private static native boolean imencode_1(String ext, long img_nativeObj, long buf_mat_nativeObj);
+
+    // C++:  bool cv::imreadmulti(String filename, vector_Mat& mats, int flags = IMREAD_ANYCOLOR)
+    private static native boolean imreadmulti_0(String filename, long mats_mat_nativeObj, int flags);
+    private static native boolean imreadmulti_1(String filename, long mats_mat_nativeObj);
+
+    // C++:  bool cv::imwrite(String filename, Mat img, vector_int params = std::vector<int>())
+    private static native boolean imwrite_0(String filename, long img_nativeObj, long params_mat_nativeObj);
+    private static native boolean imwrite_1(String filename, long img_nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/objdetect/BaseCascadeClassifier.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/objdetect/BaseCascadeClassifier.java	(date 1605830247966)
+++ openCVLibrary3411/src/main/java/org/opencv/objdetect/BaseCascadeClassifier.java	(date 1605830247966)
@@ -0,0 +1,27 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.objdetect;
+
+import org.opencv.core.Algorithm;
+
+// C++: class BaseCascadeClassifier
+
+public class BaseCascadeClassifier extends Algorithm {
+
+    protected BaseCascadeClassifier(long addr) { super(addr); }
+
+    // internal usage only
+    public static BaseCascadeClassifier __fromPtr__(long addr) { return new BaseCascadeClassifier(addr); }
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/objdetect/CascadeClassifier.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/objdetect/CascadeClassifier.java	(date 1605830247969)
+++ openCVLibrary3411/src/main/java/org/opencv/objdetect/CascadeClassifier.java	(date 1605830247969)
@@ -0,0 +1,705 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.objdetect;
+
+import java.util.ArrayList;
+import java.util.List;
+import org.opencv.core.Mat;
+import org.opencv.core.MatOfDouble;
+import org.opencv.core.MatOfInt;
+import org.opencv.core.MatOfRect;
+import org.opencv.core.Size;
+import org.opencv.utils.Converters;
+
+// C++: class CascadeClassifier
+/**
+ * Cascade classifier class for object detection.
+ */
+public class CascadeClassifier {
+
+    protected final long nativeObj;
+    protected CascadeClassifier(long addr) { nativeObj = addr; }
+
+    public long getNativeObjAddr() { return nativeObj; }
+
+    // internal usage only
+    public static CascadeClassifier __fromPtr__(long addr) { return new CascadeClassifier(addr); }
+
+    //
+    // C++:   cv::CascadeClassifier::CascadeClassifier(String filename)
+    //
+
+    /**
+     * Loads a classifier from a file.
+     *
+     *     @param filename Name of the file from which the classifier is loaded.
+     */
+    public CascadeClassifier(String filename) {
+        nativeObj = CascadeClassifier_0(filename);
+    }
+
+
+    //
+    // C++:   cv::CascadeClassifier::CascadeClassifier()
+    //
+
+    public CascadeClassifier() {
+        nativeObj = CascadeClassifier_1();
+    }
+
+
+    //
+    // C++:  Size cv::CascadeClassifier::getOriginalWindowSize()
+    //
+
+    public Size getOriginalWindowSize() {
+        return new Size(getOriginalWindowSize_0(nativeObj));
+    }
+
+
+    //
+    // C++: static bool cv::CascadeClassifier::convert(String oldcascade, String newcascade)
+    //
+
+    public static boolean convert(String oldcascade, String newcascade) {
+        return convert_0(oldcascade, newcascade);
+    }
+
+
+    //
+    // C++:  bool cv::CascadeClassifier::empty()
+    //
+
+    /**
+     * Checks whether the classifier has been loaded.
+     * @return automatically generated
+     */
+    public boolean empty() {
+        return empty_0(nativeObj);
+    }
+
+
+    //
+    // C++:  bool cv::CascadeClassifier::isOldFormatCascade()
+    //
+
+    public boolean isOldFormatCascade() {
+        return isOldFormatCascade_0(nativeObj);
+    }
+
+
+    //
+    // C++:  bool cv::CascadeClassifier::load(String filename)
+    //
+
+    /**
+     * Loads a classifier from a file.
+     *
+     *     @param filename Name of the file from which the classifier is loaded. The file may contain an old
+     *     HAAR classifier trained by the haartraining application or a new cascade classifier trained by the
+     *     traincascade application.
+     * @return automatically generated
+     */
+    public boolean load(String filename) {
+        return load_0(nativeObj, filename);
+    }
+
+
+    //
+    // C++:  bool cv::CascadeClassifier::read(FileNode node)
+    //
+
+    // Unknown type 'FileNode' (I), skipping the function
+
+
+    //
+    // C++:  int cv::CascadeClassifier::getFeatureType()
+    //
+
+    public int getFeatureType() {
+        return getFeatureType_0(nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::CascadeClassifier::detectMultiScale(Mat image, vector_Rect& objects, double scaleFactor = 1.1, int minNeighbors = 3, int flags = 0, Size minSize = Size(), Size maxSize = Size())
+    //
+
+    /**
+     * Detects objects of different sizes in the input image. The detected objects are returned as a list
+     *     of rectangles.
+     *
+     *     @param image Matrix of the type CV_8U containing an image where objects are detected.
+     *     @param objects Vector of rectangles where each rectangle contains the detected object, the
+     *     rectangles may be partially outside the original image.
+     *     @param scaleFactor Parameter specifying how much the image size is reduced at each image scale.
+     *     @param minNeighbors Parameter specifying how many neighbors each candidate rectangle should have
+     *     to retain it.
+     *     @param flags Parameter with the same meaning for an old cascade as in the function
+     *     cvHaarDetectObjects. It is not used for a new cascade.
+     *     @param minSize Minimum possible object size. Objects smaller than that are ignored.
+     *     @param maxSize Maximum possible object size. Objects larger than that are ignored. If {@code maxSize == minSize} model is evaluated on single scale.
+     *
+     *     The function is parallelized with the TBB library.
+     *
+     *     <b>Note:</b>
+     * <ul>
+     *   <li>
+     *           (Python) A face detection example using cascade classifiers can be found at
+     *             opencv_source_code/samples/python/facedetect.py
+     *   </li>
+     * </ul>
+     */
+    public void detectMultiScale(Mat image, MatOfRect objects, double scaleFactor, int minNeighbors, int flags, Size minSize, Size maxSize) {
+        Mat objects_mat = objects;
+        detectMultiScale_0(nativeObj, image.nativeObj, objects_mat.nativeObj, scaleFactor, minNeighbors, flags, minSize.width, minSize.height, maxSize.width, maxSize.height);
+    }
+
+    /**
+     * Detects objects of different sizes in the input image. The detected objects are returned as a list
+     *     of rectangles.
+     *
+     *     @param image Matrix of the type CV_8U containing an image where objects are detected.
+     *     @param objects Vector of rectangles where each rectangle contains the detected object, the
+     *     rectangles may be partially outside the original image.
+     *     @param scaleFactor Parameter specifying how much the image size is reduced at each image scale.
+     *     @param minNeighbors Parameter specifying how many neighbors each candidate rectangle should have
+     *     to retain it.
+     *     @param flags Parameter with the same meaning for an old cascade as in the function
+     *     cvHaarDetectObjects. It is not used for a new cascade.
+     *     @param minSize Minimum possible object size. Objects smaller than that are ignored.
+     *
+     *     The function is parallelized with the TBB library.
+     *
+     *     <b>Note:</b>
+     * <ul>
+     *   <li>
+     *           (Python) A face detection example using cascade classifiers can be found at
+     *             opencv_source_code/samples/python/facedetect.py
+     *   </li>
+     * </ul>
+     */
+    public void detectMultiScale(Mat image, MatOfRect objects, double scaleFactor, int minNeighbors, int flags, Size minSize) {
+        Mat objects_mat = objects;
+        detectMultiScale_1(nativeObj, image.nativeObj, objects_mat.nativeObj, scaleFactor, minNeighbors, flags, minSize.width, minSize.height);
+    }
+
+    /**
+     * Detects objects of different sizes in the input image. The detected objects are returned as a list
+     *     of rectangles.
+     *
+     *     @param image Matrix of the type CV_8U containing an image where objects are detected.
+     *     @param objects Vector of rectangles where each rectangle contains the detected object, the
+     *     rectangles may be partially outside the original image.
+     *     @param scaleFactor Parameter specifying how much the image size is reduced at each image scale.
+     *     @param minNeighbors Parameter specifying how many neighbors each candidate rectangle should have
+     *     to retain it.
+     *     @param flags Parameter with the same meaning for an old cascade as in the function
+     *     cvHaarDetectObjects. It is not used for a new cascade.
+     *
+     *     The function is parallelized with the TBB library.
+     *
+     *     <b>Note:</b>
+     * <ul>
+     *   <li>
+     *           (Python) A face detection example using cascade classifiers can be found at
+     *             opencv_source_code/samples/python/facedetect.py
+     *   </li>
+     * </ul>
+     */
+    public void detectMultiScale(Mat image, MatOfRect objects, double scaleFactor, int minNeighbors, int flags) {
+        Mat objects_mat = objects;
+        detectMultiScale_2(nativeObj, image.nativeObj, objects_mat.nativeObj, scaleFactor, minNeighbors, flags);
+    }
+
+    /**
+     * Detects objects of different sizes in the input image. The detected objects are returned as a list
+     *     of rectangles.
+     *
+     *     @param image Matrix of the type CV_8U containing an image where objects are detected.
+     *     @param objects Vector of rectangles where each rectangle contains the detected object, the
+     *     rectangles may be partially outside the original image.
+     *     @param scaleFactor Parameter specifying how much the image size is reduced at each image scale.
+     *     @param minNeighbors Parameter specifying how many neighbors each candidate rectangle should have
+     *     to retain it.
+     *     cvHaarDetectObjects. It is not used for a new cascade.
+     *
+     *     The function is parallelized with the TBB library.
+     *
+     *     <b>Note:</b>
+     * <ul>
+     *   <li>
+     *           (Python) A face detection example using cascade classifiers can be found at
+     *             opencv_source_code/samples/python/facedetect.py
+     *   </li>
+     * </ul>
+     */
+    public void detectMultiScale(Mat image, MatOfRect objects, double scaleFactor, int minNeighbors) {
+        Mat objects_mat = objects;
+        detectMultiScale_3(nativeObj, image.nativeObj, objects_mat.nativeObj, scaleFactor, minNeighbors);
+    }
+
+    /**
+     * Detects objects of different sizes in the input image. The detected objects are returned as a list
+     *     of rectangles.
+     *
+     *     @param image Matrix of the type CV_8U containing an image where objects are detected.
+     *     @param objects Vector of rectangles where each rectangle contains the detected object, the
+     *     rectangles may be partially outside the original image.
+     *     @param scaleFactor Parameter specifying how much the image size is reduced at each image scale.
+     *     to retain it.
+     *     cvHaarDetectObjects. It is not used for a new cascade.
+     *
+     *     The function is parallelized with the TBB library.
+     *
+     *     <b>Note:</b>
+     * <ul>
+     *   <li>
+     *           (Python) A face detection example using cascade classifiers can be found at
+     *             opencv_source_code/samples/python/facedetect.py
+     *   </li>
+     * </ul>
+     */
+    public void detectMultiScale(Mat image, MatOfRect objects, double scaleFactor) {
+        Mat objects_mat = objects;
+        detectMultiScale_4(nativeObj, image.nativeObj, objects_mat.nativeObj, scaleFactor);
+    }
+
+    /**
+     * Detects objects of different sizes in the input image. The detected objects are returned as a list
+     *     of rectangles.
+     *
+     *     @param image Matrix of the type CV_8U containing an image where objects are detected.
+     *     @param objects Vector of rectangles where each rectangle contains the detected object, the
+     *     rectangles may be partially outside the original image.
+     *     to retain it.
+     *     cvHaarDetectObjects. It is not used for a new cascade.
+     *
+     *     The function is parallelized with the TBB library.
+     *
+     *     <b>Note:</b>
+     * <ul>
+     *   <li>
+     *           (Python) A face detection example using cascade classifiers can be found at
+     *             opencv_source_code/samples/python/facedetect.py
+     *   </li>
+     * </ul>
+     */
+    public void detectMultiScale(Mat image, MatOfRect objects) {
+        Mat objects_mat = objects;
+        detectMultiScale_5(nativeObj, image.nativeObj, objects_mat.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::CascadeClassifier::detectMultiScale(Mat image, vector_Rect& objects, vector_int& numDetections, double scaleFactor = 1.1, int minNeighbors = 3, int flags = 0, Size minSize = Size(), Size maxSize = Size())
+    //
+
+    /**
+     *
+     *     @param image Matrix of the type CV_8U containing an image where objects are detected.
+     *     @param objects Vector of rectangles where each rectangle contains the detected object, the
+     *     rectangles may be partially outside the original image.
+     *     @param numDetections Vector of detection numbers for the corresponding objects. An object's number
+     *     of detections is the number of neighboring positively classified rectangles that were joined
+     *     together to form the object.
+     *     @param scaleFactor Parameter specifying how much the image size is reduced at each image scale.
+     *     @param minNeighbors Parameter specifying how many neighbors each candidate rectangle should have
+     *     to retain it.
+     *     @param flags Parameter with the same meaning for an old cascade as in the function
+     *     cvHaarDetectObjects. It is not used for a new cascade.
+     *     @param minSize Minimum possible object size. Objects smaller than that are ignored.
+     *     @param maxSize Maximum possible object size. Objects larger than that are ignored. If {@code maxSize == minSize} model is evaluated on single scale.
+     */
+    public void detectMultiScale2(Mat image, MatOfRect objects, MatOfInt numDetections, double scaleFactor, int minNeighbors, int flags, Size minSize, Size maxSize) {
+        Mat objects_mat = objects;
+        Mat numDetections_mat = numDetections;
+        detectMultiScale2_0(nativeObj, image.nativeObj, objects_mat.nativeObj, numDetections_mat.nativeObj, scaleFactor, minNeighbors, flags, minSize.width, minSize.height, maxSize.width, maxSize.height);
+    }
+
+    /**
+     *
+     *     @param image Matrix of the type CV_8U containing an image where objects are detected.
+     *     @param objects Vector of rectangles where each rectangle contains the detected object, the
+     *     rectangles may be partially outside the original image.
+     *     @param numDetections Vector of detection numbers for the corresponding objects. An object's number
+     *     of detections is the number of neighboring positively classified rectangles that were joined
+     *     together to form the object.
+     *     @param scaleFactor Parameter specifying how much the image size is reduced at each image scale.
+     *     @param minNeighbors Parameter specifying how many neighbors each candidate rectangle should have
+     *     to retain it.
+     *     @param flags Parameter with the same meaning for an old cascade as in the function
+     *     cvHaarDetectObjects. It is not used for a new cascade.
+     *     @param minSize Minimum possible object size. Objects smaller than that are ignored.
+     */
+    public void detectMultiScale2(Mat image, MatOfRect objects, MatOfInt numDetections, double scaleFactor, int minNeighbors, int flags, Size minSize) {
+        Mat objects_mat = objects;
+        Mat numDetections_mat = numDetections;
+        detectMultiScale2_1(nativeObj, image.nativeObj, objects_mat.nativeObj, numDetections_mat.nativeObj, scaleFactor, minNeighbors, flags, minSize.width, minSize.height);
+    }
+
+    /**
+     *
+     *     @param image Matrix of the type CV_8U containing an image where objects are detected.
+     *     @param objects Vector of rectangles where each rectangle contains the detected object, the
+     *     rectangles may be partially outside the original image.
+     *     @param numDetections Vector of detection numbers for the corresponding objects. An object's number
+     *     of detections is the number of neighboring positively classified rectangles that were joined
+     *     together to form the object.
+     *     @param scaleFactor Parameter specifying how much the image size is reduced at each image scale.
+     *     @param minNeighbors Parameter specifying how many neighbors each candidate rectangle should have
+     *     to retain it.
+     *     @param flags Parameter with the same meaning for an old cascade as in the function
+     *     cvHaarDetectObjects. It is not used for a new cascade.
+     */
+    public void detectMultiScale2(Mat image, MatOfRect objects, MatOfInt numDetections, double scaleFactor, int minNeighbors, int flags) {
+        Mat objects_mat = objects;
+        Mat numDetections_mat = numDetections;
+        detectMultiScale2_2(nativeObj, image.nativeObj, objects_mat.nativeObj, numDetections_mat.nativeObj, scaleFactor, minNeighbors, flags);
+    }
+
+    /**
+     *
+     *     @param image Matrix of the type CV_8U containing an image where objects are detected.
+     *     @param objects Vector of rectangles where each rectangle contains the detected object, the
+     *     rectangles may be partially outside the original image.
+     *     @param numDetections Vector of detection numbers for the corresponding objects. An object's number
+     *     of detections is the number of neighboring positively classified rectangles that were joined
+     *     together to form the object.
+     *     @param scaleFactor Parameter specifying how much the image size is reduced at each image scale.
+     *     @param minNeighbors Parameter specifying how many neighbors each candidate rectangle should have
+     *     to retain it.
+     *     cvHaarDetectObjects. It is not used for a new cascade.
+     */
+    public void detectMultiScale2(Mat image, MatOfRect objects, MatOfInt numDetections, double scaleFactor, int minNeighbors) {
+        Mat objects_mat = objects;
+        Mat numDetections_mat = numDetections;
+        detectMultiScale2_3(nativeObj, image.nativeObj, objects_mat.nativeObj, numDetections_mat.nativeObj, scaleFactor, minNeighbors);
+    }
+
+    /**
+     *
+     *     @param image Matrix of the type CV_8U containing an image where objects are detected.
+     *     @param objects Vector of rectangles where each rectangle contains the detected object, the
+     *     rectangles may be partially outside the original image.
+     *     @param numDetections Vector of detection numbers for the corresponding objects. An object's number
+     *     of detections is the number of neighboring positively classified rectangles that were joined
+     *     together to form the object.
+     *     @param scaleFactor Parameter specifying how much the image size is reduced at each image scale.
+     *     to retain it.
+     *     cvHaarDetectObjects. It is not used for a new cascade.
+     */
+    public void detectMultiScale2(Mat image, MatOfRect objects, MatOfInt numDetections, double scaleFactor) {
+        Mat objects_mat = objects;
+        Mat numDetections_mat = numDetections;
+        detectMultiScale2_4(nativeObj, image.nativeObj, objects_mat.nativeObj, numDetections_mat.nativeObj, scaleFactor);
+    }
+
+    /**
+     *
+     *     @param image Matrix of the type CV_8U containing an image where objects are detected.
+     *     @param objects Vector of rectangles where each rectangle contains the detected object, the
+     *     rectangles may be partially outside the original image.
+     *     @param numDetections Vector of detection numbers for the corresponding objects. An object's number
+     *     of detections is the number of neighboring positively classified rectangles that were joined
+     *     together to form the object.
+     *     to retain it.
+     *     cvHaarDetectObjects. It is not used for a new cascade.
+     */
+    public void detectMultiScale2(Mat image, MatOfRect objects, MatOfInt numDetections) {
+        Mat objects_mat = objects;
+        Mat numDetections_mat = numDetections;
+        detectMultiScale2_5(nativeObj, image.nativeObj, objects_mat.nativeObj, numDetections_mat.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::CascadeClassifier::detectMultiScale(Mat image, vector_Rect& objects, vector_int& rejectLevels, vector_double& levelWeights, double scaleFactor = 1.1, int minNeighbors = 3, int flags = 0, Size minSize = Size(), Size maxSize = Size(), bool outputRejectLevels = false)
+    //
+
+    /**
+     *
+     *     This function allows you to retrieve the final stage decision certainty of classification.
+     *     For this, one needs to set {@code outputRejectLevels} on true and provide the {@code rejectLevels} and {@code levelWeights} parameter.
+     *     For each resulting detection, {@code levelWeights} will then contain the certainty of classification at the final stage.
+     *     This value can then be used to separate strong from weaker classifications.
+     *
+     *     A code sample on how to use it efficiently can be found below:
+     *     <code>
+     *     Mat img;
+     *     vector&lt;double&gt; weights;
+     *     vector&lt;int&gt; levels;
+     *     vector&lt;Rect&gt; detections;
+     *     CascadeClassifier model("/path/to/your/model.xml");
+     *     model.detectMultiScale(img, detections, levels, weights, 1.1, 3, 0, Size(), Size(), true);
+     *     cerr &lt;&lt; "Detection " &lt;&lt; detections[0] &lt;&lt; " with weight " &lt;&lt; weights[0] &lt;&lt; endl;
+     *     </code>
+     * @param image automatically generated
+     * @param objects automatically generated
+     * @param rejectLevels automatically generated
+     * @param levelWeights automatically generated
+     * @param scaleFactor automatically generated
+     * @param minNeighbors automatically generated
+     * @param flags automatically generated
+     * @param minSize automatically generated
+     * @param maxSize automatically generated
+     * @param outputRejectLevels automatically generated
+     */
+    public void detectMultiScale3(Mat image, MatOfRect objects, MatOfInt rejectLevels, MatOfDouble levelWeights, double scaleFactor, int minNeighbors, int flags, Size minSize, Size maxSize, boolean outputRejectLevels) {
+        Mat objects_mat = objects;
+        Mat rejectLevels_mat = rejectLevels;
+        Mat levelWeights_mat = levelWeights;
+        detectMultiScale3_0(nativeObj, image.nativeObj, objects_mat.nativeObj, rejectLevels_mat.nativeObj, levelWeights_mat.nativeObj, scaleFactor, minNeighbors, flags, minSize.width, minSize.height, maxSize.width, maxSize.height, outputRejectLevels);
+    }
+
+    /**
+     *
+     *     This function allows you to retrieve the final stage decision certainty of classification.
+     *     For this, one needs to set {@code outputRejectLevels} on true and provide the {@code rejectLevels} and {@code levelWeights} parameter.
+     *     For each resulting detection, {@code levelWeights} will then contain the certainty of classification at the final stage.
+     *     This value can then be used to separate strong from weaker classifications.
+     *
+     *     A code sample on how to use it efficiently can be found below:
+     *     <code>
+     *     Mat img;
+     *     vector&lt;double&gt; weights;
+     *     vector&lt;int&gt; levels;
+     *     vector&lt;Rect&gt; detections;
+     *     CascadeClassifier model("/path/to/your/model.xml");
+     *     model.detectMultiScale(img, detections, levels, weights, 1.1, 3, 0, Size(), Size(), true);
+     *     cerr &lt;&lt; "Detection " &lt;&lt; detections[0] &lt;&lt; " with weight " &lt;&lt; weights[0] &lt;&lt; endl;
+     *     </code>
+     * @param image automatically generated
+     * @param objects automatically generated
+     * @param rejectLevels automatically generated
+     * @param levelWeights automatically generated
+     * @param scaleFactor automatically generated
+     * @param minNeighbors automatically generated
+     * @param flags automatically generated
+     * @param minSize automatically generated
+     * @param maxSize automatically generated
+     */
+    public void detectMultiScale3(Mat image, MatOfRect objects, MatOfInt rejectLevels, MatOfDouble levelWeights, double scaleFactor, int minNeighbors, int flags, Size minSize, Size maxSize) {
+        Mat objects_mat = objects;
+        Mat rejectLevels_mat = rejectLevels;
+        Mat levelWeights_mat = levelWeights;
+        detectMultiScale3_1(nativeObj, image.nativeObj, objects_mat.nativeObj, rejectLevels_mat.nativeObj, levelWeights_mat.nativeObj, scaleFactor, minNeighbors, flags, minSize.width, minSize.height, maxSize.width, maxSize.height);
+    }
+
+    /**
+     *
+     *     This function allows you to retrieve the final stage decision certainty of classification.
+     *     For this, one needs to set {@code outputRejectLevels} on true and provide the {@code rejectLevels} and {@code levelWeights} parameter.
+     *     For each resulting detection, {@code levelWeights} will then contain the certainty of classification at the final stage.
+     *     This value can then be used to separate strong from weaker classifications.
+     *
+     *     A code sample on how to use it efficiently can be found below:
+     *     <code>
+     *     Mat img;
+     *     vector&lt;double&gt; weights;
+     *     vector&lt;int&gt; levels;
+     *     vector&lt;Rect&gt; detections;
+     *     CascadeClassifier model("/path/to/your/model.xml");
+     *     model.detectMultiScale(img, detections, levels, weights, 1.1, 3, 0, Size(), Size(), true);
+     *     cerr &lt;&lt; "Detection " &lt;&lt; detections[0] &lt;&lt; " with weight " &lt;&lt; weights[0] &lt;&lt; endl;
+     *     </code>
+     * @param image automatically generated
+     * @param objects automatically generated
+     * @param rejectLevels automatically generated
+     * @param levelWeights automatically generated
+     * @param scaleFactor automatically generated
+     * @param minNeighbors automatically generated
+     * @param flags automatically generated
+     * @param minSize automatically generated
+     */
+    public void detectMultiScale3(Mat image, MatOfRect objects, MatOfInt rejectLevels, MatOfDouble levelWeights, double scaleFactor, int minNeighbors, int flags, Size minSize) {
+        Mat objects_mat = objects;
+        Mat rejectLevels_mat = rejectLevels;
+        Mat levelWeights_mat = levelWeights;
+        detectMultiScale3_2(nativeObj, image.nativeObj, objects_mat.nativeObj, rejectLevels_mat.nativeObj, levelWeights_mat.nativeObj, scaleFactor, minNeighbors, flags, minSize.width, minSize.height);
+    }
+
+    /**
+     *
+     *     This function allows you to retrieve the final stage decision certainty of classification.
+     *     For this, one needs to set {@code outputRejectLevels} on true and provide the {@code rejectLevels} and {@code levelWeights} parameter.
+     *     For each resulting detection, {@code levelWeights} will then contain the certainty of classification at the final stage.
+     *     This value can then be used to separate strong from weaker classifications.
+     *
+     *     A code sample on how to use it efficiently can be found below:
+     *     <code>
+     *     Mat img;
+     *     vector&lt;double&gt; weights;
+     *     vector&lt;int&gt; levels;
+     *     vector&lt;Rect&gt; detections;
+     *     CascadeClassifier model("/path/to/your/model.xml");
+     *     model.detectMultiScale(img, detections, levels, weights, 1.1, 3, 0, Size(), Size(), true);
+     *     cerr &lt;&lt; "Detection " &lt;&lt; detections[0] &lt;&lt; " with weight " &lt;&lt; weights[0] &lt;&lt; endl;
+     *     </code>
+     * @param image automatically generated
+     * @param objects automatically generated
+     * @param rejectLevels automatically generated
+     * @param levelWeights automatically generated
+     * @param scaleFactor automatically generated
+     * @param minNeighbors automatically generated
+     * @param flags automatically generated
+     */
+    public void detectMultiScale3(Mat image, MatOfRect objects, MatOfInt rejectLevels, MatOfDouble levelWeights, double scaleFactor, int minNeighbors, int flags) {
+        Mat objects_mat = objects;
+        Mat rejectLevels_mat = rejectLevels;
+        Mat levelWeights_mat = levelWeights;
+        detectMultiScale3_3(nativeObj, image.nativeObj, objects_mat.nativeObj, rejectLevels_mat.nativeObj, levelWeights_mat.nativeObj, scaleFactor, minNeighbors, flags);
+    }
+
+    /**
+     *
+     *     This function allows you to retrieve the final stage decision certainty of classification.
+     *     For this, one needs to set {@code outputRejectLevels} on true and provide the {@code rejectLevels} and {@code levelWeights} parameter.
+     *     For each resulting detection, {@code levelWeights} will then contain the certainty of classification at the final stage.
+     *     This value can then be used to separate strong from weaker classifications.
+     *
+     *     A code sample on how to use it efficiently can be found below:
+     *     <code>
+     *     Mat img;
+     *     vector&lt;double&gt; weights;
+     *     vector&lt;int&gt; levels;
+     *     vector&lt;Rect&gt; detections;
+     *     CascadeClassifier model("/path/to/your/model.xml");
+     *     model.detectMultiScale(img, detections, levels, weights, 1.1, 3, 0, Size(), Size(), true);
+     *     cerr &lt;&lt; "Detection " &lt;&lt; detections[0] &lt;&lt; " with weight " &lt;&lt; weights[0] &lt;&lt; endl;
+     *     </code>
+     * @param image automatically generated
+     * @param objects automatically generated
+     * @param rejectLevels automatically generated
+     * @param levelWeights automatically generated
+     * @param scaleFactor automatically generated
+     * @param minNeighbors automatically generated
+     */
+    public void detectMultiScale3(Mat image, MatOfRect objects, MatOfInt rejectLevels, MatOfDouble levelWeights, double scaleFactor, int minNeighbors) {
+        Mat objects_mat = objects;
+        Mat rejectLevels_mat = rejectLevels;
+        Mat levelWeights_mat = levelWeights;
+        detectMultiScale3_4(nativeObj, image.nativeObj, objects_mat.nativeObj, rejectLevels_mat.nativeObj, levelWeights_mat.nativeObj, scaleFactor, minNeighbors);
+    }
+
+    /**
+     *
+     *     This function allows you to retrieve the final stage decision certainty of classification.
+     *     For this, one needs to set {@code outputRejectLevels} on true and provide the {@code rejectLevels} and {@code levelWeights} parameter.
+     *     For each resulting detection, {@code levelWeights} will then contain the certainty of classification at the final stage.
+     *     This value can then be used to separate strong from weaker classifications.
+     *
+     *     A code sample on how to use it efficiently can be found below:
+     *     <code>
+     *     Mat img;
+     *     vector&lt;double&gt; weights;
+     *     vector&lt;int&gt; levels;
+     *     vector&lt;Rect&gt; detections;
+     *     CascadeClassifier model("/path/to/your/model.xml");
+     *     model.detectMultiScale(img, detections, levels, weights, 1.1, 3, 0, Size(), Size(), true);
+     *     cerr &lt;&lt; "Detection " &lt;&lt; detections[0] &lt;&lt; " with weight " &lt;&lt; weights[0] &lt;&lt; endl;
+     *     </code>
+     * @param image automatically generated
+     * @param objects automatically generated
+     * @param rejectLevels automatically generated
+     * @param levelWeights automatically generated
+     * @param scaleFactor automatically generated
+     */
+    public void detectMultiScale3(Mat image, MatOfRect objects, MatOfInt rejectLevels, MatOfDouble levelWeights, double scaleFactor) {
+        Mat objects_mat = objects;
+        Mat rejectLevels_mat = rejectLevels;
+        Mat levelWeights_mat = levelWeights;
+        detectMultiScale3_5(nativeObj, image.nativeObj, objects_mat.nativeObj, rejectLevels_mat.nativeObj, levelWeights_mat.nativeObj, scaleFactor);
+    }
+
+    /**
+     *
+     *     This function allows you to retrieve the final stage decision certainty of classification.
+     *     For this, one needs to set {@code outputRejectLevels} on true and provide the {@code rejectLevels} and {@code levelWeights} parameter.
+     *     For each resulting detection, {@code levelWeights} will then contain the certainty of classification at the final stage.
+     *     This value can then be used to separate strong from weaker classifications.
+     *
+     *     A code sample on how to use it efficiently can be found below:
+     *     <code>
+     *     Mat img;
+     *     vector&lt;double&gt; weights;
+     *     vector&lt;int&gt; levels;
+     *     vector&lt;Rect&gt; detections;
+     *     CascadeClassifier model("/path/to/your/model.xml");
+     *     model.detectMultiScale(img, detections, levels, weights, 1.1, 3, 0, Size(), Size(), true);
+     *     cerr &lt;&lt; "Detection " &lt;&lt; detections[0] &lt;&lt; " with weight " &lt;&lt; weights[0] &lt;&lt; endl;
+     *     </code>
+     * @param image automatically generated
+     * @param objects automatically generated
+     * @param rejectLevels automatically generated
+     * @param levelWeights automatically generated
+     */
+    public void detectMultiScale3(Mat image, MatOfRect objects, MatOfInt rejectLevels, MatOfDouble levelWeights) {
+        Mat objects_mat = objects;
+        Mat rejectLevels_mat = rejectLevels;
+        Mat levelWeights_mat = levelWeights;
+        detectMultiScale3_6(nativeObj, image.nativeObj, objects_mat.nativeObj, rejectLevels_mat.nativeObj, levelWeights_mat.nativeObj);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++:   cv::CascadeClassifier::CascadeClassifier(String filename)
+    private static native long CascadeClassifier_0(String filename);
+
+    // C++:   cv::CascadeClassifier::CascadeClassifier()
+    private static native long CascadeClassifier_1();
+
+    // C++:  Size cv::CascadeClassifier::getOriginalWindowSize()
+    private static native double[] getOriginalWindowSize_0(long nativeObj);
+
+    // C++: static bool cv::CascadeClassifier::convert(String oldcascade, String newcascade)
+    private static native boolean convert_0(String oldcascade, String newcascade);
+
+    // C++:  bool cv::CascadeClassifier::empty()
+    private static native boolean empty_0(long nativeObj);
+
+    // C++:  bool cv::CascadeClassifier::isOldFormatCascade()
+    private static native boolean isOldFormatCascade_0(long nativeObj);
+
+    // C++:  bool cv::CascadeClassifier::load(String filename)
+    private static native boolean load_0(long nativeObj, String filename);
+
+    // C++:  int cv::CascadeClassifier::getFeatureType()
+    private static native int getFeatureType_0(long nativeObj);
+
+    // C++:  void cv::CascadeClassifier::detectMultiScale(Mat image, vector_Rect& objects, double scaleFactor = 1.1, int minNeighbors = 3, int flags = 0, Size minSize = Size(), Size maxSize = Size())
+    private static native void detectMultiScale_0(long nativeObj, long image_nativeObj, long objects_mat_nativeObj, double scaleFactor, int minNeighbors, int flags, double minSize_width, double minSize_height, double maxSize_width, double maxSize_height);
+    private static native void detectMultiScale_1(long nativeObj, long image_nativeObj, long objects_mat_nativeObj, double scaleFactor, int minNeighbors, int flags, double minSize_width, double minSize_height);
+    private static native void detectMultiScale_2(long nativeObj, long image_nativeObj, long objects_mat_nativeObj, double scaleFactor, int minNeighbors, int flags);
+    private static native void detectMultiScale_3(long nativeObj, long image_nativeObj, long objects_mat_nativeObj, double scaleFactor, int minNeighbors);
+    private static native void detectMultiScale_4(long nativeObj, long image_nativeObj, long objects_mat_nativeObj, double scaleFactor);
+    private static native void detectMultiScale_5(long nativeObj, long image_nativeObj, long objects_mat_nativeObj);
+
+    // C++:  void cv::CascadeClassifier::detectMultiScale(Mat image, vector_Rect& objects, vector_int& numDetections, double scaleFactor = 1.1, int minNeighbors = 3, int flags = 0, Size minSize = Size(), Size maxSize = Size())
+    private static native void detectMultiScale2_0(long nativeObj, long image_nativeObj, long objects_mat_nativeObj, long numDetections_mat_nativeObj, double scaleFactor, int minNeighbors, int flags, double minSize_width, double minSize_height, double maxSize_width, double maxSize_height);
+    private static native void detectMultiScale2_1(long nativeObj, long image_nativeObj, long objects_mat_nativeObj, long numDetections_mat_nativeObj, double scaleFactor, int minNeighbors, int flags, double minSize_width, double minSize_height);
+    private static native void detectMultiScale2_2(long nativeObj, long image_nativeObj, long objects_mat_nativeObj, long numDetections_mat_nativeObj, double scaleFactor, int minNeighbors, int flags);
+    private static native void detectMultiScale2_3(long nativeObj, long image_nativeObj, long objects_mat_nativeObj, long numDetections_mat_nativeObj, double scaleFactor, int minNeighbors);
+    private static native void detectMultiScale2_4(long nativeObj, long image_nativeObj, long objects_mat_nativeObj, long numDetections_mat_nativeObj, double scaleFactor);
+    private static native void detectMultiScale2_5(long nativeObj, long image_nativeObj, long objects_mat_nativeObj, long numDetections_mat_nativeObj);
+
+    // C++:  void cv::CascadeClassifier::detectMultiScale(Mat image, vector_Rect& objects, vector_int& rejectLevels, vector_double& levelWeights, double scaleFactor = 1.1, int minNeighbors = 3, int flags = 0, Size minSize = Size(), Size maxSize = Size(), bool outputRejectLevels = false)
+    private static native void detectMultiScale3_0(long nativeObj, long image_nativeObj, long objects_mat_nativeObj, long rejectLevels_mat_nativeObj, long levelWeights_mat_nativeObj, double scaleFactor, int minNeighbors, int flags, double minSize_width, double minSize_height, double maxSize_width, double maxSize_height, boolean outputRejectLevels);
+    private static native void detectMultiScale3_1(long nativeObj, long image_nativeObj, long objects_mat_nativeObj, long rejectLevels_mat_nativeObj, long levelWeights_mat_nativeObj, double scaleFactor, int minNeighbors, int flags, double minSize_width, double minSize_height, double maxSize_width, double maxSize_height);
+    private static native void detectMultiScale3_2(long nativeObj, long image_nativeObj, long objects_mat_nativeObj, long rejectLevels_mat_nativeObj, long levelWeights_mat_nativeObj, double scaleFactor, int minNeighbors, int flags, double minSize_width, double minSize_height);
+    private static native void detectMultiScale3_3(long nativeObj, long image_nativeObj, long objects_mat_nativeObj, long rejectLevels_mat_nativeObj, long levelWeights_mat_nativeObj, double scaleFactor, int minNeighbors, int flags);
+    private static native void detectMultiScale3_4(long nativeObj, long image_nativeObj, long objects_mat_nativeObj, long rejectLevels_mat_nativeObj, long levelWeights_mat_nativeObj, double scaleFactor, int minNeighbors);
+    private static native void detectMultiScale3_5(long nativeObj, long image_nativeObj, long objects_mat_nativeObj, long rejectLevels_mat_nativeObj, long levelWeights_mat_nativeObj, double scaleFactor);
+    private static native void detectMultiScale3_6(long nativeObj, long image_nativeObj, long objects_mat_nativeObj, long rejectLevels_mat_nativeObj, long levelWeights_mat_nativeObj);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/objdetect/HOGDescriptor.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/objdetect/HOGDescriptor.java	(date 1605830247983)
+++ openCVLibrary3411/src/main/java/org/opencv/objdetect/HOGDescriptor.java	(date 1605830247983)
@@ -0,0 +1,872 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.objdetect;
+
+import java.util.ArrayList;
+import java.util.List;
+import org.opencv.core.Mat;
+import org.opencv.core.MatOfDouble;
+import org.opencv.core.MatOfFloat;
+import org.opencv.core.MatOfPoint;
+import org.opencv.core.MatOfRect;
+import org.opencv.core.Size;
+import org.opencv.utils.Converters;
+
+// C++: class HOGDescriptor
+/**
+ * Implementation of HOG (Histogram of Oriented Gradients) descriptor and object detector.
+ *
+ * the HOG descriptor algorithm introduced by Navneet Dalal and Bill Triggs CITE: Dalal2005 .
+ *
+ * useful links:
+ *
+ * https://hal.inria.fr/inria-00548512/document/
+ *
+ * https://en.wikipedia.org/wiki/Histogram_of_oriented_gradients
+ *
+ * https://software.intel.com/en-us/ipp-dev-reference-histogram-of-oriented-gradients-hog-descriptor
+ *
+ * http://www.learnopencv.com/histogram-of-oriented-gradients
+ *
+ * http://www.learnopencv.com/handwritten-digits-classification-an-opencv-c-python-tutorial
+ */
+public class HOGDescriptor {
+
+    protected final long nativeObj;
+    protected HOGDescriptor(long addr) { nativeObj = addr; }
+
+    public long getNativeObjAddr() { return nativeObj; }
+
+    // internal usage only
+    public static HOGDescriptor __fromPtr__(long addr) { return new HOGDescriptor(addr); }
+
+    // C++: enum <unnamed>
+    public static final int
+            L2Hys = 0,
+            DEFAULT_NLEVELS = 64;
+
+
+    //
+    // C++:   cv::HOGDescriptor::HOGDescriptor(Size _winSize, Size _blockSize, Size _blockStride, Size _cellSize, int _nbins, int _derivAperture = 1, double _winSigma = -1, int _histogramNormType = HOGDescriptor::L2Hys, double _L2HysThreshold = 0.2, bool _gammaCorrection = false, int _nlevels = HOGDescriptor::DEFAULT_NLEVELS, bool _signedGradient = false)
+    //
+
+    /**
+     *
+     *     @param _winSize sets winSize with given value.
+     *     @param _blockSize sets blockSize with given value.
+     *     @param _blockStride sets blockStride with given value.
+     *     @param _cellSize sets cellSize with given value.
+     *     @param _nbins sets nbins with given value.
+     *     @param _derivAperture sets derivAperture with given value.
+     *     @param _winSigma sets winSigma with given value.
+     *     @param _histogramNormType sets histogramNormType with given value.
+     *     @param _L2HysThreshold sets L2HysThreshold with given value.
+     *     @param _gammaCorrection sets gammaCorrection with given value.
+     *     @param _nlevels sets nlevels with given value.
+     *     @param _signedGradient sets signedGradient with given value.
+     */
+    public HOGDescriptor(Size _winSize, Size _blockSize, Size _blockStride, Size _cellSize, int _nbins, int _derivAperture, double _winSigma, int _histogramNormType, double _L2HysThreshold, boolean _gammaCorrection, int _nlevels, boolean _signedGradient) {
+        nativeObj = HOGDescriptor_0(_winSize.width, _winSize.height, _blockSize.width, _blockSize.height, _blockStride.width, _blockStride.height, _cellSize.width, _cellSize.height, _nbins, _derivAperture, _winSigma, _histogramNormType, _L2HysThreshold, _gammaCorrection, _nlevels, _signedGradient);
+    }
+
+    /**
+     *
+     *     @param _winSize sets winSize with given value.
+     *     @param _blockSize sets blockSize with given value.
+     *     @param _blockStride sets blockStride with given value.
+     *     @param _cellSize sets cellSize with given value.
+     *     @param _nbins sets nbins with given value.
+     *     @param _derivAperture sets derivAperture with given value.
+     *     @param _winSigma sets winSigma with given value.
+     *     @param _histogramNormType sets histogramNormType with given value.
+     *     @param _L2HysThreshold sets L2HysThreshold with given value.
+     *     @param _gammaCorrection sets gammaCorrection with given value.
+     *     @param _nlevels sets nlevels with given value.
+     */
+    public HOGDescriptor(Size _winSize, Size _blockSize, Size _blockStride, Size _cellSize, int _nbins, int _derivAperture, double _winSigma, int _histogramNormType, double _L2HysThreshold, boolean _gammaCorrection, int _nlevels) {
+        nativeObj = HOGDescriptor_1(_winSize.width, _winSize.height, _blockSize.width, _blockSize.height, _blockStride.width, _blockStride.height, _cellSize.width, _cellSize.height, _nbins, _derivAperture, _winSigma, _histogramNormType, _L2HysThreshold, _gammaCorrection, _nlevels);
+    }
+
+    /**
+     *
+     *     @param _winSize sets winSize with given value.
+     *     @param _blockSize sets blockSize with given value.
+     *     @param _blockStride sets blockStride with given value.
+     *     @param _cellSize sets cellSize with given value.
+     *     @param _nbins sets nbins with given value.
+     *     @param _derivAperture sets derivAperture with given value.
+     *     @param _winSigma sets winSigma with given value.
+     *     @param _histogramNormType sets histogramNormType with given value.
+     *     @param _L2HysThreshold sets L2HysThreshold with given value.
+     *     @param _gammaCorrection sets gammaCorrection with given value.
+     */
+    public HOGDescriptor(Size _winSize, Size _blockSize, Size _blockStride, Size _cellSize, int _nbins, int _derivAperture, double _winSigma, int _histogramNormType, double _L2HysThreshold, boolean _gammaCorrection) {
+        nativeObj = HOGDescriptor_2(_winSize.width, _winSize.height, _blockSize.width, _blockSize.height, _blockStride.width, _blockStride.height, _cellSize.width, _cellSize.height, _nbins, _derivAperture, _winSigma, _histogramNormType, _L2HysThreshold, _gammaCorrection);
+    }
+
+    /**
+     *
+     *     @param _winSize sets winSize with given value.
+     *     @param _blockSize sets blockSize with given value.
+     *     @param _blockStride sets blockStride with given value.
+     *     @param _cellSize sets cellSize with given value.
+     *     @param _nbins sets nbins with given value.
+     *     @param _derivAperture sets derivAperture with given value.
+     *     @param _winSigma sets winSigma with given value.
+     *     @param _histogramNormType sets histogramNormType with given value.
+     *     @param _L2HysThreshold sets L2HysThreshold with given value.
+     */
+    public HOGDescriptor(Size _winSize, Size _blockSize, Size _blockStride, Size _cellSize, int _nbins, int _derivAperture, double _winSigma, int _histogramNormType, double _L2HysThreshold) {
+        nativeObj = HOGDescriptor_3(_winSize.width, _winSize.height, _blockSize.width, _blockSize.height, _blockStride.width, _blockStride.height, _cellSize.width, _cellSize.height, _nbins, _derivAperture, _winSigma, _histogramNormType, _L2HysThreshold);
+    }
+
+    /**
+     *
+     *     @param _winSize sets winSize with given value.
+     *     @param _blockSize sets blockSize with given value.
+     *     @param _blockStride sets blockStride with given value.
+     *     @param _cellSize sets cellSize with given value.
+     *     @param _nbins sets nbins with given value.
+     *     @param _derivAperture sets derivAperture with given value.
+     *     @param _winSigma sets winSigma with given value.
+     *     @param _histogramNormType sets histogramNormType with given value.
+     */
+    public HOGDescriptor(Size _winSize, Size _blockSize, Size _blockStride, Size _cellSize, int _nbins, int _derivAperture, double _winSigma, int _histogramNormType) {
+        nativeObj = HOGDescriptor_4(_winSize.width, _winSize.height, _blockSize.width, _blockSize.height, _blockStride.width, _blockStride.height, _cellSize.width, _cellSize.height, _nbins, _derivAperture, _winSigma, _histogramNormType);
+    }
+
+    /**
+     *
+     *     @param _winSize sets winSize with given value.
+     *     @param _blockSize sets blockSize with given value.
+     *     @param _blockStride sets blockStride with given value.
+     *     @param _cellSize sets cellSize with given value.
+     *     @param _nbins sets nbins with given value.
+     *     @param _derivAperture sets derivAperture with given value.
+     *     @param _winSigma sets winSigma with given value.
+     */
+    public HOGDescriptor(Size _winSize, Size _blockSize, Size _blockStride, Size _cellSize, int _nbins, int _derivAperture, double _winSigma) {
+        nativeObj = HOGDescriptor_5(_winSize.width, _winSize.height, _blockSize.width, _blockSize.height, _blockStride.width, _blockStride.height, _cellSize.width, _cellSize.height, _nbins, _derivAperture, _winSigma);
+    }
+
+    /**
+     *
+     *     @param _winSize sets winSize with given value.
+     *     @param _blockSize sets blockSize with given value.
+     *     @param _blockStride sets blockStride with given value.
+     *     @param _cellSize sets cellSize with given value.
+     *     @param _nbins sets nbins with given value.
+     *     @param _derivAperture sets derivAperture with given value.
+     */
+    public HOGDescriptor(Size _winSize, Size _blockSize, Size _blockStride, Size _cellSize, int _nbins, int _derivAperture) {
+        nativeObj = HOGDescriptor_6(_winSize.width, _winSize.height, _blockSize.width, _blockSize.height, _blockStride.width, _blockStride.height, _cellSize.width, _cellSize.height, _nbins, _derivAperture);
+    }
+
+    /**
+     *
+     *     @param _winSize sets winSize with given value.
+     *     @param _blockSize sets blockSize with given value.
+     *     @param _blockStride sets blockStride with given value.
+     *     @param _cellSize sets cellSize with given value.
+     *     @param _nbins sets nbins with given value.
+     */
+    public HOGDescriptor(Size _winSize, Size _blockSize, Size _blockStride, Size _cellSize, int _nbins) {
+        nativeObj = HOGDescriptor_7(_winSize.width, _winSize.height, _blockSize.width, _blockSize.height, _blockStride.width, _blockStride.height, _cellSize.width, _cellSize.height, _nbins);
+    }
+
+
+    //
+    // C++:   cv::HOGDescriptor::HOGDescriptor(String filename)
+    //
+
+    /**
+     *
+     *     @param filename the file name containing  HOGDescriptor properties and coefficients of the trained classifier
+     */
+    public HOGDescriptor(String filename) {
+        nativeObj = HOGDescriptor_8(filename);
+    }
+
+
+    //
+    // C++:   cv::HOGDescriptor::HOGDescriptor()
+    //
+
+    /**
+     * Creates the HOG descriptor and detector with default params.
+     *
+     *     aqual to HOGDescriptor(Size(64,128), Size(16,16), Size(8,8), Size(8,8), 9, 1 )
+     */
+    public HOGDescriptor() {
+        nativeObj = HOGDescriptor_9();
+    }
+
+
+    //
+    // C++:  bool cv::HOGDescriptor::checkDetectorSize()
+    //
+
+    /**
+     * Checks if detector size equal to descriptor size.
+     * @return automatically generated
+     */
+    public boolean checkDetectorSize() {
+        return checkDetectorSize_0(nativeObj);
+    }
+
+
+    //
+    // C++:  bool cv::HOGDescriptor::load(String filename, String objname = String())
+    //
+
+    /**
+     * loads coefficients for the linear SVM classifier from a file
+     *     @param filename Name of the file to read.
+     *     @param objname The optional name of the node to read (if empty, the first top-level node will be used).
+     * @return automatically generated
+     */
+    public boolean load(String filename, String objname) {
+        return load_0(nativeObj, filename, objname);
+    }
+
+    /**
+     * loads coefficients for the linear SVM classifier from a file
+     *     @param filename Name of the file to read.
+     * @return automatically generated
+     */
+    public boolean load(String filename) {
+        return load_1(nativeObj, filename);
+    }
+
+
+    //
+    // C++:  double cv::HOGDescriptor::getWinSigma()
+    //
+
+    /**
+     * Returns winSigma value
+     * @return automatically generated
+     */
+    public double getWinSigma() {
+        return getWinSigma_0(nativeObj);
+    }
+
+
+    //
+    // C++:  size_t cv::HOGDescriptor::getDescriptorSize()
+    //
+
+    /**
+     * Returns the number of coefficients required for the classification.
+     * @return automatically generated
+     */
+    public long getDescriptorSize() {
+        return getDescriptorSize_0(nativeObj);
+    }
+
+
+    //
+    // C++: static vector_float cv::HOGDescriptor::getDaimlerPeopleDetector()
+    //
+
+    /**
+     * Returns coefficients of the classifier trained for people detection (for 48x96 windows).
+     * @return automatically generated
+     */
+    public static MatOfFloat getDaimlerPeopleDetector() {
+        return MatOfFloat.fromNativeAddr(getDaimlerPeopleDetector_0());
+    }
+
+
+    //
+    // C++: static vector_float cv::HOGDescriptor::getDefaultPeopleDetector()
+    //
+
+    /**
+     * Returns coefficients of the classifier trained for people detection (for 64x128 windows).
+     * @return automatically generated
+     */
+    public static MatOfFloat getDefaultPeopleDetector() {
+        return MatOfFloat.fromNativeAddr(getDefaultPeopleDetector_0());
+    }
+
+
+    //
+    // C++:  void cv::HOGDescriptor::compute(Mat img, vector_float& descriptors, Size winStride = Size(), Size padding = Size(), vector_Point locations = std::vector<Point>())
+    //
+
+    /**
+     * Computes HOG descriptors of given image.
+     *     @param img Matrix of the type CV_8U containing an image where HOG features will be calculated.
+     *     @param descriptors Matrix of the type CV_32F
+     *     @param winStride Window stride. It must be a multiple of block stride.
+     *     @param padding Padding
+     *     @param locations Vector of Point
+     */
+    public void compute(Mat img, MatOfFloat descriptors, Size winStride, Size padding, MatOfPoint locations) {
+        Mat descriptors_mat = descriptors;
+        Mat locations_mat = locations;
+        compute_0(nativeObj, img.nativeObj, descriptors_mat.nativeObj, winStride.width, winStride.height, padding.width, padding.height, locations_mat.nativeObj);
+    }
+
+    /**
+     * Computes HOG descriptors of given image.
+     *     @param img Matrix of the type CV_8U containing an image where HOG features will be calculated.
+     *     @param descriptors Matrix of the type CV_32F
+     *     @param winStride Window stride. It must be a multiple of block stride.
+     *     @param padding Padding
+     */
+    public void compute(Mat img, MatOfFloat descriptors, Size winStride, Size padding) {
+        Mat descriptors_mat = descriptors;
+        compute_1(nativeObj, img.nativeObj, descriptors_mat.nativeObj, winStride.width, winStride.height, padding.width, padding.height);
+    }
+
+    /**
+     * Computes HOG descriptors of given image.
+     *     @param img Matrix of the type CV_8U containing an image where HOG features will be calculated.
+     *     @param descriptors Matrix of the type CV_32F
+     *     @param winStride Window stride. It must be a multiple of block stride.
+     */
+    public void compute(Mat img, MatOfFloat descriptors, Size winStride) {
+        Mat descriptors_mat = descriptors;
+        compute_2(nativeObj, img.nativeObj, descriptors_mat.nativeObj, winStride.width, winStride.height);
+    }
+
+    /**
+     * Computes HOG descriptors of given image.
+     *     @param img Matrix of the type CV_8U containing an image where HOG features will be calculated.
+     *     @param descriptors Matrix of the type CV_32F
+     */
+    public void compute(Mat img, MatOfFloat descriptors) {
+        Mat descriptors_mat = descriptors;
+        compute_3(nativeObj, img.nativeObj, descriptors_mat.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::HOGDescriptor::computeGradient(Mat img, Mat& grad, Mat& angleOfs, Size paddingTL = Size(), Size paddingBR = Size())
+    //
+
+    /**
+     *  Computes gradients and quantized gradient orientations.
+     *     @param img Matrix contains the image to be computed
+     *     @param grad Matrix of type CV_32FC2 contains computed gradients
+     *     @param angleOfs Matrix of type CV_8UC2 contains quantized gradient orientations
+     *     @param paddingTL Padding from top-left
+     *     @param paddingBR Padding from bottom-right
+     */
+    public void computeGradient(Mat img, Mat grad, Mat angleOfs, Size paddingTL, Size paddingBR) {
+        computeGradient_0(nativeObj, img.nativeObj, grad.nativeObj, angleOfs.nativeObj, paddingTL.width, paddingTL.height, paddingBR.width, paddingBR.height);
+    }
+
+    /**
+     *  Computes gradients and quantized gradient orientations.
+     *     @param img Matrix contains the image to be computed
+     *     @param grad Matrix of type CV_32FC2 contains computed gradients
+     *     @param angleOfs Matrix of type CV_8UC2 contains quantized gradient orientations
+     *     @param paddingTL Padding from top-left
+     */
+    public void computeGradient(Mat img, Mat grad, Mat angleOfs, Size paddingTL) {
+        computeGradient_1(nativeObj, img.nativeObj, grad.nativeObj, angleOfs.nativeObj, paddingTL.width, paddingTL.height);
+    }
+
+    /**
+     *  Computes gradients and quantized gradient orientations.
+     *     @param img Matrix contains the image to be computed
+     *     @param grad Matrix of type CV_32FC2 contains computed gradients
+     *     @param angleOfs Matrix of type CV_8UC2 contains quantized gradient orientations
+     */
+    public void computeGradient(Mat img, Mat grad, Mat angleOfs) {
+        computeGradient_2(nativeObj, img.nativeObj, grad.nativeObj, angleOfs.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::HOGDescriptor::detect(Mat img, vector_Point& foundLocations, vector_double& weights, double hitThreshold = 0, Size winStride = Size(), Size padding = Size(), vector_Point searchLocations = std::vector<Point>())
+    //
+
+    /**
+     * Performs object detection without a multi-scale window.
+     *     @param img Matrix of the type CV_8U or CV_8UC3 containing an image where objects are detected.
+     *     @param foundLocations Vector of point where each point contains left-top corner point of detected object boundaries.
+     *     @param weights Vector that will contain confidence values for each detected object.
+     *     @param hitThreshold Threshold for the distance between features and SVM classifying plane.
+     *     Usually it is 0 and should be specified in the detector coefficients (as the last free coefficient).
+     *     But if the free coefficient is omitted (which is allowed), you can specify it manually here.
+     *     @param winStride Window stride. It must be a multiple of block stride.
+     *     @param padding Padding
+     *     @param searchLocations Vector of Point includes set of requested locations to be evaluated.
+     */
+    public void detect(Mat img, MatOfPoint foundLocations, MatOfDouble weights, double hitThreshold, Size winStride, Size padding, MatOfPoint searchLocations) {
+        Mat foundLocations_mat = foundLocations;
+        Mat weights_mat = weights;
+        Mat searchLocations_mat = searchLocations;
+        detect_0(nativeObj, img.nativeObj, foundLocations_mat.nativeObj, weights_mat.nativeObj, hitThreshold, winStride.width, winStride.height, padding.width, padding.height, searchLocations_mat.nativeObj);
+    }
+
+    /**
+     * Performs object detection without a multi-scale window.
+     *     @param img Matrix of the type CV_8U or CV_8UC3 containing an image where objects are detected.
+     *     @param foundLocations Vector of point where each point contains left-top corner point of detected object boundaries.
+     *     @param weights Vector that will contain confidence values for each detected object.
+     *     @param hitThreshold Threshold for the distance between features and SVM classifying plane.
+     *     Usually it is 0 and should be specified in the detector coefficients (as the last free coefficient).
+     *     But if the free coefficient is omitted (which is allowed), you can specify it manually here.
+     *     @param winStride Window stride. It must be a multiple of block stride.
+     *     @param padding Padding
+     */
+    public void detect(Mat img, MatOfPoint foundLocations, MatOfDouble weights, double hitThreshold, Size winStride, Size padding) {
+        Mat foundLocations_mat = foundLocations;
+        Mat weights_mat = weights;
+        detect_1(nativeObj, img.nativeObj, foundLocations_mat.nativeObj, weights_mat.nativeObj, hitThreshold, winStride.width, winStride.height, padding.width, padding.height);
+    }
+
+    /**
+     * Performs object detection without a multi-scale window.
+     *     @param img Matrix of the type CV_8U or CV_8UC3 containing an image where objects are detected.
+     *     @param foundLocations Vector of point where each point contains left-top corner point of detected object boundaries.
+     *     @param weights Vector that will contain confidence values for each detected object.
+     *     @param hitThreshold Threshold for the distance between features and SVM classifying plane.
+     *     Usually it is 0 and should be specified in the detector coefficients (as the last free coefficient).
+     *     But if the free coefficient is omitted (which is allowed), you can specify it manually here.
+     *     @param winStride Window stride. It must be a multiple of block stride.
+     */
+    public void detect(Mat img, MatOfPoint foundLocations, MatOfDouble weights, double hitThreshold, Size winStride) {
+        Mat foundLocations_mat = foundLocations;
+        Mat weights_mat = weights;
+        detect_2(nativeObj, img.nativeObj, foundLocations_mat.nativeObj, weights_mat.nativeObj, hitThreshold, winStride.width, winStride.height);
+    }
+
+    /**
+     * Performs object detection without a multi-scale window.
+     *     @param img Matrix of the type CV_8U or CV_8UC3 containing an image where objects are detected.
+     *     @param foundLocations Vector of point where each point contains left-top corner point of detected object boundaries.
+     *     @param weights Vector that will contain confidence values for each detected object.
+     *     @param hitThreshold Threshold for the distance between features and SVM classifying plane.
+     *     Usually it is 0 and should be specified in the detector coefficients (as the last free coefficient).
+     *     But if the free coefficient is omitted (which is allowed), you can specify it manually here.
+     */
+    public void detect(Mat img, MatOfPoint foundLocations, MatOfDouble weights, double hitThreshold) {
+        Mat foundLocations_mat = foundLocations;
+        Mat weights_mat = weights;
+        detect_3(nativeObj, img.nativeObj, foundLocations_mat.nativeObj, weights_mat.nativeObj, hitThreshold);
+    }
+
+    /**
+     * Performs object detection without a multi-scale window.
+     *     @param img Matrix of the type CV_8U or CV_8UC3 containing an image where objects are detected.
+     *     @param foundLocations Vector of point where each point contains left-top corner point of detected object boundaries.
+     *     @param weights Vector that will contain confidence values for each detected object.
+     *     Usually it is 0 and should be specified in the detector coefficients (as the last free coefficient).
+     *     But if the free coefficient is omitted (which is allowed), you can specify it manually here.
+     */
+    public void detect(Mat img, MatOfPoint foundLocations, MatOfDouble weights) {
+        Mat foundLocations_mat = foundLocations;
+        Mat weights_mat = weights;
+        detect_4(nativeObj, img.nativeObj, foundLocations_mat.nativeObj, weights_mat.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::HOGDescriptor::detectMultiScale(Mat img, vector_Rect& foundLocations, vector_double& foundWeights, double hitThreshold = 0, Size winStride = Size(), Size padding = Size(), double scale = 1.05, double finalThreshold = 2.0, bool useMeanshiftGrouping = false)
+    //
+
+    /**
+     * Detects objects of different sizes in the input image. The detected objects are returned as a list
+     *     of rectangles.
+     *     @param img Matrix of the type CV_8U or CV_8UC3 containing an image where objects are detected.
+     *     @param foundLocations Vector of rectangles where each rectangle contains the detected object.
+     *     @param foundWeights Vector that will contain confidence values for each detected object.
+     *     @param hitThreshold Threshold for the distance between features and SVM classifying plane.
+     *     Usually it is 0 and should be specified in the detector coefficients (as the last free coefficient).
+     *     But if the free coefficient is omitted (which is allowed), you can specify it manually here.
+     *     @param winStride Window stride. It must be a multiple of block stride.
+     *     @param padding Padding
+     *     @param scale Coefficient of the detection window increase.
+     *     @param finalThreshold Final threshold
+     *     @param useMeanshiftGrouping indicates grouping algorithm
+     */
+    public void detectMultiScale(Mat img, MatOfRect foundLocations, MatOfDouble foundWeights, double hitThreshold, Size winStride, Size padding, double scale, double finalThreshold, boolean useMeanshiftGrouping) {
+        Mat foundLocations_mat = foundLocations;
+        Mat foundWeights_mat = foundWeights;
+        detectMultiScale_0(nativeObj, img.nativeObj, foundLocations_mat.nativeObj, foundWeights_mat.nativeObj, hitThreshold, winStride.width, winStride.height, padding.width, padding.height, scale, finalThreshold, useMeanshiftGrouping);
+    }
+
+    /**
+     * Detects objects of different sizes in the input image. The detected objects are returned as a list
+     *     of rectangles.
+     *     @param img Matrix of the type CV_8U or CV_8UC3 containing an image where objects are detected.
+     *     @param foundLocations Vector of rectangles where each rectangle contains the detected object.
+     *     @param foundWeights Vector that will contain confidence values for each detected object.
+     *     @param hitThreshold Threshold for the distance between features and SVM classifying plane.
+     *     Usually it is 0 and should be specified in the detector coefficients (as the last free coefficient).
+     *     But if the free coefficient is omitted (which is allowed), you can specify it manually here.
+     *     @param winStride Window stride. It must be a multiple of block stride.
+     *     @param padding Padding
+     *     @param scale Coefficient of the detection window increase.
+     *     @param finalThreshold Final threshold
+     */
+    public void detectMultiScale(Mat img, MatOfRect foundLocations, MatOfDouble foundWeights, double hitThreshold, Size winStride, Size padding, double scale, double finalThreshold) {
+        Mat foundLocations_mat = foundLocations;
+        Mat foundWeights_mat = foundWeights;
+        detectMultiScale_1(nativeObj, img.nativeObj, foundLocations_mat.nativeObj, foundWeights_mat.nativeObj, hitThreshold, winStride.width, winStride.height, padding.width, padding.height, scale, finalThreshold);
+    }
+
+    /**
+     * Detects objects of different sizes in the input image. The detected objects are returned as a list
+     *     of rectangles.
+     *     @param img Matrix of the type CV_8U or CV_8UC3 containing an image where objects are detected.
+     *     @param foundLocations Vector of rectangles where each rectangle contains the detected object.
+     *     @param foundWeights Vector that will contain confidence values for each detected object.
+     *     @param hitThreshold Threshold for the distance between features and SVM classifying plane.
+     *     Usually it is 0 and should be specified in the detector coefficients (as the last free coefficient).
+     *     But if the free coefficient is omitted (which is allowed), you can specify it manually here.
+     *     @param winStride Window stride. It must be a multiple of block stride.
+     *     @param padding Padding
+     *     @param scale Coefficient of the detection window increase.
+     */
+    public void detectMultiScale(Mat img, MatOfRect foundLocations, MatOfDouble foundWeights, double hitThreshold, Size winStride, Size padding, double scale) {
+        Mat foundLocations_mat = foundLocations;
+        Mat foundWeights_mat = foundWeights;
+        detectMultiScale_2(nativeObj, img.nativeObj, foundLocations_mat.nativeObj, foundWeights_mat.nativeObj, hitThreshold, winStride.width, winStride.height, padding.width, padding.height, scale);
+    }
+
+    /**
+     * Detects objects of different sizes in the input image. The detected objects are returned as a list
+     *     of rectangles.
+     *     @param img Matrix of the type CV_8U or CV_8UC3 containing an image where objects are detected.
+     *     @param foundLocations Vector of rectangles where each rectangle contains the detected object.
+     *     @param foundWeights Vector that will contain confidence values for each detected object.
+     *     @param hitThreshold Threshold for the distance between features and SVM classifying plane.
+     *     Usually it is 0 and should be specified in the detector coefficients (as the last free coefficient).
+     *     But if the free coefficient is omitted (which is allowed), you can specify it manually here.
+     *     @param winStride Window stride. It must be a multiple of block stride.
+     *     @param padding Padding
+     */
+    public void detectMultiScale(Mat img, MatOfRect foundLocations, MatOfDouble foundWeights, double hitThreshold, Size winStride, Size padding) {
+        Mat foundLocations_mat = foundLocations;
+        Mat foundWeights_mat = foundWeights;
+        detectMultiScale_3(nativeObj, img.nativeObj, foundLocations_mat.nativeObj, foundWeights_mat.nativeObj, hitThreshold, winStride.width, winStride.height, padding.width, padding.height);
+    }
+
+    /**
+     * Detects objects of different sizes in the input image. The detected objects are returned as a list
+     *     of rectangles.
+     *     @param img Matrix of the type CV_8U or CV_8UC3 containing an image where objects are detected.
+     *     @param foundLocations Vector of rectangles where each rectangle contains the detected object.
+     *     @param foundWeights Vector that will contain confidence values for each detected object.
+     *     @param hitThreshold Threshold for the distance between features and SVM classifying plane.
+     *     Usually it is 0 and should be specified in the detector coefficients (as the last free coefficient).
+     *     But if the free coefficient is omitted (which is allowed), you can specify it manually here.
+     *     @param winStride Window stride. It must be a multiple of block stride.
+     */
+    public void detectMultiScale(Mat img, MatOfRect foundLocations, MatOfDouble foundWeights, double hitThreshold, Size winStride) {
+        Mat foundLocations_mat = foundLocations;
+        Mat foundWeights_mat = foundWeights;
+        detectMultiScale_4(nativeObj, img.nativeObj, foundLocations_mat.nativeObj, foundWeights_mat.nativeObj, hitThreshold, winStride.width, winStride.height);
+    }
+
+    /**
+     * Detects objects of different sizes in the input image. The detected objects are returned as a list
+     *     of rectangles.
+     *     @param img Matrix of the type CV_8U or CV_8UC3 containing an image where objects are detected.
+     *     @param foundLocations Vector of rectangles where each rectangle contains the detected object.
+     *     @param foundWeights Vector that will contain confidence values for each detected object.
+     *     @param hitThreshold Threshold for the distance between features and SVM classifying plane.
+     *     Usually it is 0 and should be specified in the detector coefficients (as the last free coefficient).
+     *     But if the free coefficient is omitted (which is allowed), you can specify it manually here.
+     */
+    public void detectMultiScale(Mat img, MatOfRect foundLocations, MatOfDouble foundWeights, double hitThreshold) {
+        Mat foundLocations_mat = foundLocations;
+        Mat foundWeights_mat = foundWeights;
+        detectMultiScale_5(nativeObj, img.nativeObj, foundLocations_mat.nativeObj, foundWeights_mat.nativeObj, hitThreshold);
+    }
+
+    /**
+     * Detects objects of different sizes in the input image. The detected objects are returned as a list
+     *     of rectangles.
+     *     @param img Matrix of the type CV_8U or CV_8UC3 containing an image where objects are detected.
+     *     @param foundLocations Vector of rectangles where each rectangle contains the detected object.
+     *     @param foundWeights Vector that will contain confidence values for each detected object.
+     *     Usually it is 0 and should be specified in the detector coefficients (as the last free coefficient).
+     *     But if the free coefficient is omitted (which is allowed), you can specify it manually here.
+     */
+    public void detectMultiScale(Mat img, MatOfRect foundLocations, MatOfDouble foundWeights) {
+        Mat foundLocations_mat = foundLocations;
+        Mat foundWeights_mat = foundWeights;
+        detectMultiScale_6(nativeObj, img.nativeObj, foundLocations_mat.nativeObj, foundWeights_mat.nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::HOGDescriptor::save(String filename, String objname = String())
+    //
+
+    /**
+     * saves coefficients for the linear SVM classifier to a file
+     *     @param filename File name
+     *     @param objname Object name
+     */
+    public void save(String filename, String objname) {
+        save_0(nativeObj, filename, objname);
+    }
+
+    /**
+     * saves coefficients for the linear SVM classifier to a file
+     *     @param filename File name
+     */
+    public void save(String filename) {
+        save_1(nativeObj, filename);
+    }
+
+
+    //
+    // C++:  void cv::HOGDescriptor::setSVMDetector(Mat _svmdetector)
+    //
+
+    /**
+     * Sets coefficients for the linear SVM classifier.
+     *     @param _svmdetector coefficients for the linear SVM classifier.
+     */
+    public void setSVMDetector(Mat _svmdetector) {
+        setSVMDetector_0(nativeObj, _svmdetector.nativeObj);
+    }
+
+
+    //
+    // C++: Size HOGDescriptor::winSize
+    //
+
+    public Size get_winSize() {
+        return new Size(get_winSize_0(nativeObj));
+    }
+
+
+    //
+    // C++: Size HOGDescriptor::blockSize
+    //
+
+    public Size get_blockSize() {
+        return new Size(get_blockSize_0(nativeObj));
+    }
+
+
+    //
+    // C++: Size HOGDescriptor::blockStride
+    //
+
+    public Size get_blockStride() {
+        return new Size(get_blockStride_0(nativeObj));
+    }
+
+
+    //
+    // C++: Size HOGDescriptor::cellSize
+    //
+
+    public Size get_cellSize() {
+        return new Size(get_cellSize_0(nativeObj));
+    }
+
+
+    //
+    // C++: int HOGDescriptor::nbins
+    //
+
+    public int get_nbins() {
+        return get_nbins_0(nativeObj);
+    }
+
+
+    //
+    // C++: int HOGDescriptor::derivAperture
+    //
+
+    public int get_derivAperture() {
+        return get_derivAperture_0(nativeObj);
+    }
+
+
+    //
+    // C++: double HOGDescriptor::winSigma
+    //
+
+    public double get_winSigma() {
+        return get_winSigma_0(nativeObj);
+    }
+
+
+    //
+    // C++: int HOGDescriptor::histogramNormType
+    //
+
+    public int get_histogramNormType() {
+        return get_histogramNormType_0(nativeObj);
+    }
+
+
+    //
+    // C++: double HOGDescriptor::L2HysThreshold
+    //
+
+    public double get_L2HysThreshold() {
+        return get_L2HysThreshold_0(nativeObj);
+    }
+
+
+    //
+    // C++: bool HOGDescriptor::gammaCorrection
+    //
+
+    public boolean get_gammaCorrection() {
+        return get_gammaCorrection_0(nativeObj);
+    }
+
+
+    //
+    // C++: vector_float HOGDescriptor::svmDetector
+    //
+
+    public MatOfFloat get_svmDetector() {
+        return MatOfFloat.fromNativeAddr(get_svmDetector_0(nativeObj));
+    }
+
+
+    //
+    // C++: int HOGDescriptor::nlevels
+    //
+
+    public int get_nlevels() {
+        return get_nlevels_0(nativeObj);
+    }
+
+
+    //
+    // C++: bool HOGDescriptor::signedGradient
+    //
+
+    public boolean get_signedGradient() {
+        return get_signedGradient_0(nativeObj);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++:   cv::HOGDescriptor::HOGDescriptor(Size _winSize, Size _blockSize, Size _blockStride, Size _cellSize, int _nbins, int _derivAperture = 1, double _winSigma = -1, int _histogramNormType = HOGDescriptor::L2Hys, double _L2HysThreshold = 0.2, bool _gammaCorrection = false, int _nlevels = HOGDescriptor::DEFAULT_NLEVELS, bool _signedGradient = false)
+    private static native long HOGDescriptor_0(double _winSize_width, double _winSize_height, double _blockSize_width, double _blockSize_height, double _blockStride_width, double _blockStride_height, double _cellSize_width, double _cellSize_height, int _nbins, int _derivAperture, double _winSigma, int _histogramNormType, double _L2HysThreshold, boolean _gammaCorrection, int _nlevels, boolean _signedGradient);
+    private static native long HOGDescriptor_1(double _winSize_width, double _winSize_height, double _blockSize_width, double _blockSize_height, double _blockStride_width, double _blockStride_height, double _cellSize_width, double _cellSize_height, int _nbins, int _derivAperture, double _winSigma, int _histogramNormType, double _L2HysThreshold, boolean _gammaCorrection, int _nlevels);
+    private static native long HOGDescriptor_2(double _winSize_width, double _winSize_height, double _blockSize_width, double _blockSize_height, double _blockStride_width, double _blockStride_height, double _cellSize_width, double _cellSize_height, int _nbins, int _derivAperture, double _winSigma, int _histogramNormType, double _L2HysThreshold, boolean _gammaCorrection);
+    private static native long HOGDescriptor_3(double _winSize_width, double _winSize_height, double _blockSize_width, double _blockSize_height, double _blockStride_width, double _blockStride_height, double _cellSize_width, double _cellSize_height, int _nbins, int _derivAperture, double _winSigma, int _histogramNormType, double _L2HysThreshold);
+    private static native long HOGDescriptor_4(double _winSize_width, double _winSize_height, double _blockSize_width, double _blockSize_height, double _blockStride_width, double _blockStride_height, double _cellSize_width, double _cellSize_height, int _nbins, int _derivAperture, double _winSigma, int _histogramNormType);
+    private static native long HOGDescriptor_5(double _winSize_width, double _winSize_height, double _blockSize_width, double _blockSize_height, double _blockStride_width, double _blockStride_height, double _cellSize_width, double _cellSize_height, int _nbins, int _derivAperture, double _winSigma);
+    private static native long HOGDescriptor_6(double _winSize_width, double _winSize_height, double _blockSize_width, double _blockSize_height, double _blockStride_width, double _blockStride_height, double _cellSize_width, double _cellSize_height, int _nbins, int _derivAperture);
+    private static native long HOGDescriptor_7(double _winSize_width, double _winSize_height, double _blockSize_width, double _blockSize_height, double _blockStride_width, double _blockStride_height, double _cellSize_width, double _cellSize_height, int _nbins);
+
+    // C++:   cv::HOGDescriptor::HOGDescriptor(String filename)
+    private static native long HOGDescriptor_8(String filename);
+
+    // C++:   cv::HOGDescriptor::HOGDescriptor()
+    private static native long HOGDescriptor_9();
+
+    // C++:  bool cv::HOGDescriptor::checkDetectorSize()
+    private static native boolean checkDetectorSize_0(long nativeObj);
+
+    // C++:  bool cv::HOGDescriptor::load(String filename, String objname = String())
+    private static native boolean load_0(long nativeObj, String filename, String objname);
+    private static native boolean load_1(long nativeObj, String filename);
+
+    // C++:  double cv::HOGDescriptor::getWinSigma()
+    private static native double getWinSigma_0(long nativeObj);
+
+    // C++:  size_t cv::HOGDescriptor::getDescriptorSize()
+    private static native long getDescriptorSize_0(long nativeObj);
+
+    // C++: static vector_float cv::HOGDescriptor::getDaimlerPeopleDetector()
+    private static native long getDaimlerPeopleDetector_0();
+
+    // C++: static vector_float cv::HOGDescriptor::getDefaultPeopleDetector()
+    private static native long getDefaultPeopleDetector_0();
+
+    // C++:  void cv::HOGDescriptor::compute(Mat img, vector_float& descriptors, Size winStride = Size(), Size padding = Size(), vector_Point locations = std::vector<Point>())
+    private static native void compute_0(long nativeObj, long img_nativeObj, long descriptors_mat_nativeObj, double winStride_width, double winStride_height, double padding_width, double padding_height, long locations_mat_nativeObj);
+    private static native void compute_1(long nativeObj, long img_nativeObj, long descriptors_mat_nativeObj, double winStride_width, double winStride_height, double padding_width, double padding_height);
+    private static native void compute_2(long nativeObj, long img_nativeObj, long descriptors_mat_nativeObj, double winStride_width, double winStride_height);
+    private static native void compute_3(long nativeObj, long img_nativeObj, long descriptors_mat_nativeObj);
+
+    // C++:  void cv::HOGDescriptor::computeGradient(Mat img, Mat& grad, Mat& angleOfs, Size paddingTL = Size(), Size paddingBR = Size())
+    private static native void computeGradient_0(long nativeObj, long img_nativeObj, long grad_nativeObj, long angleOfs_nativeObj, double paddingTL_width, double paddingTL_height, double paddingBR_width, double paddingBR_height);
+    private static native void computeGradient_1(long nativeObj, long img_nativeObj, long grad_nativeObj, long angleOfs_nativeObj, double paddingTL_width, double paddingTL_height);
+    private static native void computeGradient_2(long nativeObj, long img_nativeObj, long grad_nativeObj, long angleOfs_nativeObj);
+
+    // C++:  void cv::HOGDescriptor::detect(Mat img, vector_Point& foundLocations, vector_double& weights, double hitThreshold = 0, Size winStride = Size(), Size padding = Size(), vector_Point searchLocations = std::vector<Point>())
+    private static native void detect_0(long nativeObj, long img_nativeObj, long foundLocations_mat_nativeObj, long weights_mat_nativeObj, double hitThreshold, double winStride_width, double winStride_height, double padding_width, double padding_height, long searchLocations_mat_nativeObj);
+    private static native void detect_1(long nativeObj, long img_nativeObj, long foundLocations_mat_nativeObj, long weights_mat_nativeObj, double hitThreshold, double winStride_width, double winStride_height, double padding_width, double padding_height);
+    private static native void detect_2(long nativeObj, long img_nativeObj, long foundLocations_mat_nativeObj, long weights_mat_nativeObj, double hitThreshold, double winStride_width, double winStride_height);
+    private static native void detect_3(long nativeObj, long img_nativeObj, long foundLocations_mat_nativeObj, long weights_mat_nativeObj, double hitThreshold);
+    private static native void detect_4(long nativeObj, long img_nativeObj, long foundLocations_mat_nativeObj, long weights_mat_nativeObj);
+
+    // C++:  void cv::HOGDescriptor::detectMultiScale(Mat img, vector_Rect& foundLocations, vector_double& foundWeights, double hitThreshold = 0, Size winStride = Size(), Size padding = Size(), double scale = 1.05, double finalThreshold = 2.0, bool useMeanshiftGrouping = false)
+    private static native void detectMultiScale_0(long nativeObj, long img_nativeObj, long foundLocations_mat_nativeObj, long foundWeights_mat_nativeObj, double hitThreshold, double winStride_width, double winStride_height, double padding_width, double padding_height, double scale, double finalThreshold, boolean useMeanshiftGrouping);
+    private static native void detectMultiScale_1(long nativeObj, long img_nativeObj, long foundLocations_mat_nativeObj, long foundWeights_mat_nativeObj, double hitThreshold, double winStride_width, double winStride_height, double padding_width, double padding_height, double scale, double finalThreshold);
+    private static native void detectMultiScale_2(long nativeObj, long img_nativeObj, long foundLocations_mat_nativeObj, long foundWeights_mat_nativeObj, double hitThreshold, double winStride_width, double winStride_height, double padding_width, double padding_height, double scale);
+    private static native void detectMultiScale_3(long nativeObj, long img_nativeObj, long foundLocations_mat_nativeObj, long foundWeights_mat_nativeObj, double hitThreshold, double winStride_width, double winStride_height, double padding_width, double padding_height);
+    private static native void detectMultiScale_4(long nativeObj, long img_nativeObj, long foundLocations_mat_nativeObj, long foundWeights_mat_nativeObj, double hitThreshold, double winStride_width, double winStride_height);
+    private static native void detectMultiScale_5(long nativeObj, long img_nativeObj, long foundLocations_mat_nativeObj, long foundWeights_mat_nativeObj, double hitThreshold);
+    private static native void detectMultiScale_6(long nativeObj, long img_nativeObj, long foundLocations_mat_nativeObj, long foundWeights_mat_nativeObj);
+
+    // C++:  void cv::HOGDescriptor::save(String filename, String objname = String())
+    private static native void save_0(long nativeObj, String filename, String objname);
+    private static native void save_1(long nativeObj, String filename);
+
+    // C++:  void cv::HOGDescriptor::setSVMDetector(Mat _svmdetector)
+    private static native void setSVMDetector_0(long nativeObj, long _svmdetector_nativeObj);
+
+    // C++: Size HOGDescriptor::winSize
+    private static native double[] get_winSize_0(long nativeObj);
+
+    // C++: Size HOGDescriptor::blockSize
+    private static native double[] get_blockSize_0(long nativeObj);
+
+    // C++: Size HOGDescriptor::blockStride
+    private static native double[] get_blockStride_0(long nativeObj);
+
+    // C++: Size HOGDescriptor::cellSize
+    private static native double[] get_cellSize_0(long nativeObj);
+
+    // C++: int HOGDescriptor::nbins
+    private static native int get_nbins_0(long nativeObj);
+
+    // C++: int HOGDescriptor::derivAperture
+    private static native int get_derivAperture_0(long nativeObj);
+
+    // C++: double HOGDescriptor::winSigma
+    private static native double get_winSigma_0(long nativeObj);
+
+    // C++: int HOGDescriptor::histogramNormType
+    private static native int get_histogramNormType_0(long nativeObj);
+
+    // C++: double HOGDescriptor::L2HysThreshold
+    private static native double get_L2HysThreshold_0(long nativeObj);
+
+    // C++: bool HOGDescriptor::gammaCorrection
+    private static native boolean get_gammaCorrection_0(long nativeObj);
+
+    // C++: vector_float HOGDescriptor::svmDetector
+    private static native long get_svmDetector_0(long nativeObj);
+
+    // C++: int HOGDescriptor::nlevels
+    private static native int get_nlevels_0(long nativeObj);
+
+    // C++: bool HOGDescriptor::signedGradient
+    private static native boolean get_signedGradient_0(long nativeObj);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/videoio/Videoio.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/videoio/Videoio.java	(date 1605830248363)
+++ openCVLibrary3411/src/main/java/org/opencv/videoio/Videoio.java	(date 1605830248363)
@@ -0,0 +1,750 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.videoio;
+
+
+
+// C++: class Videoio
+
+public class Videoio {
+
+    // C++: enum VideoWriterProperties
+    public static final int
+            VIDEOWRITER_PROP_QUALITY = 1,
+            VIDEOWRITER_PROP_FRAMEBYTES = 2,
+            VIDEOWRITER_PROP_NSTRIPES = 3;
+
+
+    // C++: enum <unnamed>
+    public static final int
+            CV_CAP_ANY = 0,
+            CV_CAP_MIL = 100,
+            CV_CAP_VFW = 200,
+            CV_CAP_V4L = 200,
+            CV_CAP_V4L2 = 200,
+            CV_CAP_FIREWARE = 300,
+            CV_CAP_FIREWIRE = 300,
+            CV_CAP_IEEE1394 = 300,
+            CV_CAP_DC1394 = 300,
+            CV_CAP_CMU1394 = 300,
+            CV_CAP_STEREO = 400,
+            CV_CAP_TYZX = 400,
+            CV_TYZX_LEFT = 400,
+            CV_TYZX_RIGHT = 401,
+            CV_TYZX_COLOR = 402,
+            CV_TYZX_Z = 403,
+            CV_CAP_QT = 500,
+            CV_CAP_UNICAP = 600,
+            CV_CAP_DSHOW = 700,
+            CV_CAP_MSMF = 1400,
+            CV_CAP_PVAPI = 800,
+            CV_CAP_OPENNI = 900,
+            CV_CAP_OPENNI_ASUS = 910,
+            CV_CAP_ANDROID = 1000,
+            CV_CAP_ANDROID_BACK = CV_CAP_ANDROID+99,
+            CV_CAP_ANDROID_FRONT = CV_CAP_ANDROID+98,
+            CV_CAP_XIAPI = 1100,
+            CV_CAP_AVFOUNDATION = 1200,
+            CV_CAP_GIGANETIX = 1300,
+            CV_CAP_INTELPERC = 1500,
+            CV_CAP_OPENNI2 = 1600,
+            CV_CAP_GPHOTO2 = 1700,
+            CV_CAP_GSTREAMER = 1800,
+            CV_CAP_FFMPEG = 1900,
+            CV_CAP_IMAGES = 2000,
+            CV_CAP_ARAVIS = 2100,
+            CV_CAP_PROP_DC1394_OFF = -4,
+            CV_CAP_PROP_DC1394_MODE_MANUAL = -3,
+            CV_CAP_PROP_DC1394_MODE_AUTO = -2,
+            CV_CAP_PROP_DC1394_MODE_ONE_PUSH_AUTO = -1,
+            CV_CAP_PROP_POS_MSEC = 0,
+            CV_CAP_PROP_POS_FRAMES = 1,
+            CV_CAP_PROP_POS_AVI_RATIO = 2,
+            CV_CAP_PROP_FRAME_WIDTH = 3,
+            CV_CAP_PROP_FRAME_HEIGHT = 4,
+            CV_CAP_PROP_FPS = 5,
+            CV_CAP_PROP_FOURCC = 6,
+            CV_CAP_PROP_FRAME_COUNT = 7,
+            CV_CAP_PROP_FORMAT = 8,
+            CV_CAP_PROP_MODE = 9,
+            CV_CAP_PROP_BRIGHTNESS = 10,
+            CV_CAP_PROP_CONTRAST = 11,
+            CV_CAP_PROP_SATURATION = 12,
+            CV_CAP_PROP_HUE = 13,
+            CV_CAP_PROP_GAIN = 14,
+            CV_CAP_PROP_EXPOSURE = 15,
+            CV_CAP_PROP_CONVERT_RGB = 16,
+            CV_CAP_PROP_WHITE_BALANCE_BLUE_U = 17,
+            CV_CAP_PROP_RECTIFICATION = 18,
+            CV_CAP_PROP_MONOCHROME = 19,
+            CV_CAP_PROP_SHARPNESS = 20,
+            CV_CAP_PROP_AUTO_EXPOSURE = 21,
+            CV_CAP_PROP_GAMMA = 22,
+            CV_CAP_PROP_TEMPERATURE = 23,
+            CV_CAP_PROP_TRIGGER = 24,
+            CV_CAP_PROP_TRIGGER_DELAY = 25,
+            CV_CAP_PROP_WHITE_BALANCE_RED_V = 26,
+            CV_CAP_PROP_ZOOM = 27,
+            CV_CAP_PROP_FOCUS = 28,
+            CV_CAP_PROP_GUID = 29,
+            CV_CAP_PROP_ISO_SPEED = 30,
+            CV_CAP_PROP_MAX_DC1394 = 31,
+            CV_CAP_PROP_BACKLIGHT = 32,
+            CV_CAP_PROP_PAN = 33,
+            CV_CAP_PROP_TILT = 34,
+            CV_CAP_PROP_ROLL = 35,
+            CV_CAP_PROP_IRIS = 36,
+            CV_CAP_PROP_SETTINGS = 37,
+            CV_CAP_PROP_BUFFERSIZE = 38,
+            CV_CAP_PROP_AUTOFOCUS = 39,
+            CV_CAP_PROP_SAR_NUM = 40,
+            CV_CAP_PROP_SAR_DEN = 41,
+            CV_CAP_PROP_AUTOGRAB = 1024,
+            CV_CAP_PROP_SUPPORTED_PREVIEW_SIZES_STRING = 1025,
+            CV_CAP_PROP_PREVIEW_FORMAT = 1026,
+            CV_CAP_OPENNI_DEPTH_GENERATOR = 1 << 31,
+            CV_CAP_OPENNI_IMAGE_GENERATOR = 1 << 30,
+            CV_CAP_OPENNI_IR_GENERATOR = 1 << 29,
+            CV_CAP_OPENNI_GENERATORS_MASK = CV_CAP_OPENNI_DEPTH_GENERATOR + CV_CAP_OPENNI_IMAGE_GENERATOR + CV_CAP_OPENNI_IR_GENERATOR,
+            CV_CAP_PROP_OPENNI_OUTPUT_MODE = 100,
+            CV_CAP_PROP_OPENNI_FRAME_MAX_DEPTH = 101,
+            CV_CAP_PROP_OPENNI_BASELINE = 102,
+            CV_CAP_PROP_OPENNI_FOCAL_LENGTH = 103,
+            CV_CAP_PROP_OPENNI_REGISTRATION = 104,
+            CV_CAP_PROP_OPENNI_REGISTRATION_ON = 104,
+            CV_CAP_PROP_OPENNI_APPROX_FRAME_SYNC = 105,
+            CV_CAP_PROP_OPENNI_MAX_BUFFER_SIZE = 106,
+            CV_CAP_PROP_OPENNI_CIRCLE_BUFFER = 107,
+            CV_CAP_PROP_OPENNI_MAX_TIME_DURATION = 108,
+            CV_CAP_PROP_OPENNI_GENERATOR_PRESENT = 109,
+            CV_CAP_PROP_OPENNI2_SYNC = 110,
+            CV_CAP_PROP_OPENNI2_MIRROR = 111,
+            CV_CAP_OPENNI_IMAGE_GENERATOR_PRESENT = CV_CAP_OPENNI_IMAGE_GENERATOR + CV_CAP_PROP_OPENNI_GENERATOR_PRESENT,
+            CV_CAP_OPENNI_IMAGE_GENERATOR_OUTPUT_MODE = CV_CAP_OPENNI_IMAGE_GENERATOR + CV_CAP_PROP_OPENNI_OUTPUT_MODE,
+            CV_CAP_OPENNI_DEPTH_GENERATOR_PRESENT = CV_CAP_OPENNI_DEPTH_GENERATOR + CV_CAP_PROP_OPENNI_GENERATOR_PRESENT,
+            CV_CAP_OPENNI_DEPTH_GENERATOR_BASELINE = CV_CAP_OPENNI_DEPTH_GENERATOR + CV_CAP_PROP_OPENNI_BASELINE,
+            CV_CAP_OPENNI_DEPTH_GENERATOR_FOCAL_LENGTH = CV_CAP_OPENNI_DEPTH_GENERATOR + CV_CAP_PROP_OPENNI_FOCAL_LENGTH,
+            CV_CAP_OPENNI_DEPTH_GENERATOR_REGISTRATION = CV_CAP_OPENNI_DEPTH_GENERATOR + CV_CAP_PROP_OPENNI_REGISTRATION,
+            CV_CAP_OPENNI_DEPTH_GENERATOR_REGISTRATION_ON = CV_CAP_OPENNI_DEPTH_GENERATOR + CV_CAP_PROP_OPENNI_REGISTRATION,
+            CV_CAP_OPENNI_IR_GENERATOR_PRESENT = CV_CAP_OPENNI_IR_GENERATOR + CV_CAP_PROP_OPENNI_GENERATOR_PRESENT,
+            CV_CAP_GSTREAMER_QUEUE_LENGTH = 200,
+            CV_CAP_PROP_PVAPI_MULTICASTIP = 300,
+            CV_CAP_PROP_PVAPI_FRAMESTARTTRIGGERMODE = 301,
+            CV_CAP_PROP_PVAPI_DECIMATIONHORIZONTAL = 302,
+            CV_CAP_PROP_PVAPI_DECIMATIONVERTICAL = 303,
+            CV_CAP_PROP_PVAPI_BINNINGX = 304,
+            CV_CAP_PROP_PVAPI_BINNINGY = 305,
+            CV_CAP_PROP_PVAPI_PIXELFORMAT = 306,
+            CV_CAP_PROP_XI_DOWNSAMPLING = 400,
+            CV_CAP_PROP_XI_DATA_FORMAT = 401,
+            CV_CAP_PROP_XI_OFFSET_X = 402,
+            CV_CAP_PROP_XI_OFFSET_Y = 403,
+            CV_CAP_PROP_XI_TRG_SOURCE = 404,
+            CV_CAP_PROP_XI_TRG_SOFTWARE = 405,
+            CV_CAP_PROP_XI_GPI_SELECTOR = 406,
+            CV_CAP_PROP_XI_GPI_MODE = 407,
+            CV_CAP_PROP_XI_GPI_LEVEL = 408,
+            CV_CAP_PROP_XI_GPO_SELECTOR = 409,
+            CV_CAP_PROP_XI_GPO_MODE = 410,
+            CV_CAP_PROP_XI_LED_SELECTOR = 411,
+            CV_CAP_PROP_XI_LED_MODE = 412,
+            CV_CAP_PROP_XI_MANUAL_WB = 413,
+            CV_CAP_PROP_XI_AUTO_WB = 414,
+            CV_CAP_PROP_XI_AEAG = 415,
+            CV_CAP_PROP_XI_EXP_PRIORITY = 416,
+            CV_CAP_PROP_XI_AE_MAX_LIMIT = 417,
+            CV_CAP_PROP_XI_AG_MAX_LIMIT = 418,
+            CV_CAP_PROP_XI_AEAG_LEVEL = 419,
+            CV_CAP_PROP_XI_TIMEOUT = 420,
+            CV_CAP_PROP_XI_EXPOSURE = 421,
+            CV_CAP_PROP_XI_EXPOSURE_BURST_COUNT = 422,
+            CV_CAP_PROP_XI_GAIN_SELECTOR = 423,
+            CV_CAP_PROP_XI_GAIN = 424,
+            CV_CAP_PROP_XI_DOWNSAMPLING_TYPE = 426,
+            CV_CAP_PROP_XI_BINNING_SELECTOR = 427,
+            CV_CAP_PROP_XI_BINNING_VERTICAL = 428,
+            CV_CAP_PROP_XI_BINNING_HORIZONTAL = 429,
+            CV_CAP_PROP_XI_BINNING_PATTERN = 430,
+            CV_CAP_PROP_XI_DECIMATION_SELECTOR = 431,
+            CV_CAP_PROP_XI_DECIMATION_VERTICAL = 432,
+            CV_CAP_PROP_XI_DECIMATION_HORIZONTAL = 433,
+            CV_CAP_PROP_XI_DECIMATION_PATTERN = 434,
+            CV_CAP_PROP_XI_TEST_PATTERN_GENERATOR_SELECTOR = 587,
+            CV_CAP_PROP_XI_TEST_PATTERN = 588,
+            CV_CAP_PROP_XI_IMAGE_DATA_FORMAT = 435,
+            CV_CAP_PROP_XI_SHUTTER_TYPE = 436,
+            CV_CAP_PROP_XI_SENSOR_TAPS = 437,
+            CV_CAP_PROP_XI_AEAG_ROI_OFFSET_X = 439,
+            CV_CAP_PROP_XI_AEAG_ROI_OFFSET_Y = 440,
+            CV_CAP_PROP_XI_AEAG_ROI_WIDTH = 441,
+            CV_CAP_PROP_XI_AEAG_ROI_HEIGHT = 442,
+            CV_CAP_PROP_XI_BPC = 445,
+            CV_CAP_PROP_XI_WB_KR = 448,
+            CV_CAP_PROP_XI_WB_KG = 449,
+            CV_CAP_PROP_XI_WB_KB = 450,
+            CV_CAP_PROP_XI_WIDTH = 451,
+            CV_CAP_PROP_XI_HEIGHT = 452,
+            CV_CAP_PROP_XI_REGION_SELECTOR = 589,
+            CV_CAP_PROP_XI_REGION_MODE = 595,
+            CV_CAP_PROP_XI_LIMIT_BANDWIDTH = 459,
+            CV_CAP_PROP_XI_SENSOR_DATA_BIT_DEPTH = 460,
+            CV_CAP_PROP_XI_OUTPUT_DATA_BIT_DEPTH = 461,
+            CV_CAP_PROP_XI_IMAGE_DATA_BIT_DEPTH = 462,
+            CV_CAP_PROP_XI_OUTPUT_DATA_PACKING = 463,
+            CV_CAP_PROP_XI_OUTPUT_DATA_PACKING_TYPE = 464,
+            CV_CAP_PROP_XI_IS_COOLED = 465,
+            CV_CAP_PROP_XI_COOLING = 466,
+            CV_CAP_PROP_XI_TARGET_TEMP = 467,
+            CV_CAP_PROP_XI_CHIP_TEMP = 468,
+            CV_CAP_PROP_XI_HOUS_TEMP = 469,
+            CV_CAP_PROP_XI_HOUS_BACK_SIDE_TEMP = 590,
+            CV_CAP_PROP_XI_SENSOR_BOARD_TEMP = 596,
+            CV_CAP_PROP_XI_CMS = 470,
+            CV_CAP_PROP_XI_APPLY_CMS = 471,
+            CV_CAP_PROP_XI_IMAGE_IS_COLOR = 474,
+            CV_CAP_PROP_XI_COLOR_FILTER_ARRAY = 475,
+            CV_CAP_PROP_XI_GAMMAY = 476,
+            CV_CAP_PROP_XI_GAMMAC = 477,
+            CV_CAP_PROP_XI_SHARPNESS = 478,
+            CV_CAP_PROP_XI_CC_MATRIX_00 = 479,
+            CV_CAP_PROP_XI_CC_MATRIX_01 = 480,
+            CV_CAP_PROP_XI_CC_MATRIX_02 = 481,
+            CV_CAP_PROP_XI_CC_MATRIX_03 = 482,
+            CV_CAP_PROP_XI_CC_MATRIX_10 = 483,
+            CV_CAP_PROP_XI_CC_MATRIX_11 = 484,
+            CV_CAP_PROP_XI_CC_MATRIX_12 = 485,
+            CV_CAP_PROP_XI_CC_MATRIX_13 = 486,
+            CV_CAP_PROP_XI_CC_MATRIX_20 = 487,
+            CV_CAP_PROP_XI_CC_MATRIX_21 = 488,
+            CV_CAP_PROP_XI_CC_MATRIX_22 = 489,
+            CV_CAP_PROP_XI_CC_MATRIX_23 = 490,
+            CV_CAP_PROP_XI_CC_MATRIX_30 = 491,
+            CV_CAP_PROP_XI_CC_MATRIX_31 = 492,
+            CV_CAP_PROP_XI_CC_MATRIX_32 = 493,
+            CV_CAP_PROP_XI_CC_MATRIX_33 = 494,
+            CV_CAP_PROP_XI_DEFAULT_CC_MATRIX = 495,
+            CV_CAP_PROP_XI_TRG_SELECTOR = 498,
+            CV_CAP_PROP_XI_ACQ_FRAME_BURST_COUNT = 499,
+            CV_CAP_PROP_XI_DEBOUNCE_EN = 507,
+            CV_CAP_PROP_XI_DEBOUNCE_T0 = 508,
+            CV_CAP_PROP_XI_DEBOUNCE_T1 = 509,
+            CV_CAP_PROP_XI_DEBOUNCE_POL = 510,
+            CV_CAP_PROP_XI_LENS_MODE = 511,
+            CV_CAP_PROP_XI_LENS_APERTURE_VALUE = 512,
+            CV_CAP_PROP_XI_LENS_FOCUS_MOVEMENT_VALUE = 513,
+            CV_CAP_PROP_XI_LENS_FOCUS_MOVE = 514,
+            CV_CAP_PROP_XI_LENS_FOCUS_DISTANCE = 515,
+            CV_CAP_PROP_XI_LENS_FOCAL_LENGTH = 516,
+            CV_CAP_PROP_XI_LENS_FEATURE_SELECTOR = 517,
+            CV_CAP_PROP_XI_LENS_FEATURE = 518,
+            CV_CAP_PROP_XI_DEVICE_MODEL_ID = 521,
+            CV_CAP_PROP_XI_DEVICE_SN = 522,
+            CV_CAP_PROP_XI_IMAGE_DATA_FORMAT_RGB32_ALPHA = 529,
+            CV_CAP_PROP_XI_IMAGE_PAYLOAD_SIZE = 530,
+            CV_CAP_PROP_XI_TRANSPORT_PIXEL_FORMAT = 531,
+            CV_CAP_PROP_XI_SENSOR_CLOCK_FREQ_HZ = 532,
+            CV_CAP_PROP_XI_SENSOR_CLOCK_FREQ_INDEX = 533,
+            CV_CAP_PROP_XI_SENSOR_OUTPUT_CHANNEL_COUNT = 534,
+            CV_CAP_PROP_XI_FRAMERATE = 535,
+            CV_CAP_PROP_XI_COUNTER_SELECTOR = 536,
+            CV_CAP_PROP_XI_COUNTER_VALUE = 537,
+            CV_CAP_PROP_XI_ACQ_TIMING_MODE = 538,
+            CV_CAP_PROP_XI_AVAILABLE_BANDWIDTH = 539,
+            CV_CAP_PROP_XI_BUFFER_POLICY = 540,
+            CV_CAP_PROP_XI_LUT_EN = 541,
+            CV_CAP_PROP_XI_LUT_INDEX = 542,
+            CV_CAP_PROP_XI_LUT_VALUE = 543,
+            CV_CAP_PROP_XI_TRG_DELAY = 544,
+            CV_CAP_PROP_XI_TS_RST_MODE = 545,
+            CV_CAP_PROP_XI_TS_RST_SOURCE = 546,
+            CV_CAP_PROP_XI_IS_DEVICE_EXIST = 547,
+            CV_CAP_PROP_XI_ACQ_BUFFER_SIZE = 548,
+            CV_CAP_PROP_XI_ACQ_BUFFER_SIZE_UNIT = 549,
+            CV_CAP_PROP_XI_ACQ_TRANSPORT_BUFFER_SIZE = 550,
+            CV_CAP_PROP_XI_BUFFERS_QUEUE_SIZE = 551,
+            CV_CAP_PROP_XI_ACQ_TRANSPORT_BUFFER_COMMIT = 552,
+            CV_CAP_PROP_XI_RECENT_FRAME = 553,
+            CV_CAP_PROP_XI_DEVICE_RESET = 554,
+            CV_CAP_PROP_XI_COLUMN_FPN_CORRECTION = 555,
+            CV_CAP_PROP_XI_ROW_FPN_CORRECTION = 591,
+            CV_CAP_PROP_XI_SENSOR_MODE = 558,
+            CV_CAP_PROP_XI_HDR = 559,
+            CV_CAP_PROP_XI_HDR_KNEEPOINT_COUNT = 560,
+            CV_CAP_PROP_XI_HDR_T1 = 561,
+            CV_CAP_PROP_XI_HDR_T2 = 562,
+            CV_CAP_PROP_XI_KNEEPOINT1 = 563,
+            CV_CAP_PROP_XI_KNEEPOINT2 = 564,
+            CV_CAP_PROP_XI_IMAGE_BLACK_LEVEL = 565,
+            CV_CAP_PROP_XI_HW_REVISION = 571,
+            CV_CAP_PROP_XI_DEBUG_LEVEL = 572,
+            CV_CAP_PROP_XI_AUTO_BANDWIDTH_CALCULATION = 573,
+            CV_CAP_PROP_XI_FFS_FILE_ID = 594,
+            CV_CAP_PROP_XI_FFS_FILE_SIZE = 580,
+            CV_CAP_PROP_XI_FREE_FFS_SIZE = 581,
+            CV_CAP_PROP_XI_USED_FFS_SIZE = 582,
+            CV_CAP_PROP_XI_FFS_ACCESS_KEY = 583,
+            CV_CAP_PROP_XI_SENSOR_FEATURE_SELECTOR = 585,
+            CV_CAP_PROP_XI_SENSOR_FEATURE_VALUE = 586,
+            CV_CAP_PROP_ANDROID_FLASH_MODE = 8001,
+            CV_CAP_PROP_ANDROID_FOCUS_MODE = 8002,
+            CV_CAP_PROP_ANDROID_WHITE_BALANCE = 8003,
+            CV_CAP_PROP_ANDROID_ANTIBANDING = 8004,
+            CV_CAP_PROP_ANDROID_FOCAL_LENGTH = 8005,
+            CV_CAP_PROP_ANDROID_FOCUS_DISTANCE_NEAR = 8006,
+            CV_CAP_PROP_ANDROID_FOCUS_DISTANCE_OPTIMAL = 8007,
+            CV_CAP_PROP_ANDROID_FOCUS_DISTANCE_FAR = 8008,
+            CV_CAP_PROP_ANDROID_EXPOSE_LOCK = 8009,
+            CV_CAP_PROP_ANDROID_WHITEBALANCE_LOCK = 8010,
+            CV_CAP_PROP_IOS_DEVICE_FOCUS = 9001,
+            CV_CAP_PROP_IOS_DEVICE_EXPOSURE = 9002,
+            CV_CAP_PROP_IOS_DEVICE_FLASH = 9003,
+            CV_CAP_PROP_IOS_DEVICE_WHITEBALANCE = 9004,
+            CV_CAP_PROP_IOS_DEVICE_TORCH = 9005,
+            CV_CAP_PROP_GIGA_FRAME_OFFSET_X = 10001,
+            CV_CAP_PROP_GIGA_FRAME_OFFSET_Y = 10002,
+            CV_CAP_PROP_GIGA_FRAME_WIDTH_MAX = 10003,
+            CV_CAP_PROP_GIGA_FRAME_HEIGH_MAX = 10004,
+            CV_CAP_PROP_GIGA_FRAME_SENS_WIDTH = 10005,
+            CV_CAP_PROP_GIGA_FRAME_SENS_HEIGH = 10006,
+            CV_CAP_PROP_INTELPERC_PROFILE_COUNT = 11001,
+            CV_CAP_PROP_INTELPERC_PROFILE_IDX = 11002,
+            CV_CAP_PROP_INTELPERC_DEPTH_LOW_CONFIDENCE_VALUE = 11003,
+            CV_CAP_PROP_INTELPERC_DEPTH_SATURATION_VALUE = 11004,
+            CV_CAP_PROP_INTELPERC_DEPTH_CONFIDENCE_THRESHOLD = 11005,
+            CV_CAP_PROP_INTELPERC_DEPTH_FOCAL_LENGTH_HORZ = 11006,
+            CV_CAP_PROP_INTELPERC_DEPTH_FOCAL_LENGTH_VERT = 11007,
+            CV_CAP_INTELPERC_DEPTH_GENERATOR = 1 << 29,
+            CV_CAP_INTELPERC_IMAGE_GENERATOR = 1 << 28,
+            CV_CAP_INTELPERC_GENERATORS_MASK = CV_CAP_INTELPERC_DEPTH_GENERATOR + CV_CAP_INTELPERC_IMAGE_GENERATOR,
+            CV_CAP_MODE_BGR = 0,
+            CV_CAP_MODE_RGB = 1,
+            CV_CAP_MODE_GRAY = 2,
+            CV_CAP_MODE_YUYV = 3,
+            CV_CAP_OPENNI_DEPTH_MAP = 0,
+            CV_CAP_OPENNI_POINT_CLOUD_MAP = 1,
+            CV_CAP_OPENNI_DISPARITY_MAP = 2,
+            CV_CAP_OPENNI_DISPARITY_MAP_32F = 3,
+            CV_CAP_OPENNI_VALID_DEPTH_MASK = 4,
+            CV_CAP_OPENNI_BGR_IMAGE = 5,
+            CV_CAP_OPENNI_GRAY_IMAGE = 6,
+            CV_CAP_OPENNI_IR_IMAGE = 7,
+            CV_CAP_OPENNI_VGA_30HZ = 0,
+            CV_CAP_OPENNI_SXGA_15HZ = 1,
+            CV_CAP_OPENNI_SXGA_30HZ = 2,
+            CV_CAP_OPENNI_QVGA_30HZ = 3,
+            CV_CAP_OPENNI_QVGA_60HZ = 4,
+            CV_CAP_INTELPERC_DEPTH_MAP = 0,
+            CV_CAP_INTELPERC_UVDEPTH_MAP = 1,
+            CV_CAP_INTELPERC_IR_MAP = 2,
+            CV_CAP_INTELPERC_IMAGE = 3,
+            CV_CAP_PROP_GPHOTO2_PREVIEW = 17001,
+            CV_CAP_PROP_GPHOTO2_WIDGET_ENUMERATE = 17002,
+            CV_CAP_PROP_GPHOTO2_RELOAD_CONFIG = 17003,
+            CV_CAP_PROP_GPHOTO2_RELOAD_ON_CHANGE = 17004,
+            CV_CAP_PROP_GPHOTO2_COLLECT_MSGS = 17005,
+            CV_CAP_PROP_GPHOTO2_FLUSH_MSGS = 17006,
+            CV_CAP_PROP_SPEED = 17007,
+            CV_CAP_PROP_APERTURE = 17008,
+            CV_CAP_PROP_EXPOSUREPROGRAM = 17009,
+            CV_CAP_PROP_VIEWFINDER = 17010,
+            CAP_PROP_DC1394_OFF = -4,
+            CAP_PROP_DC1394_MODE_MANUAL = -3,
+            CAP_PROP_DC1394_MODE_AUTO = -2,
+            CAP_PROP_DC1394_MODE_ONE_PUSH_AUTO = -1,
+            CAP_PROP_DC1394_MAX = 31,
+            CAP_OPENNI_DEPTH_GENERATOR = 1 << 31,
+            CAP_OPENNI_IMAGE_GENERATOR = 1 << 30,
+            CAP_OPENNI_IR_GENERATOR = 1 << 29,
+            CAP_OPENNI_GENERATORS_MASK = CAP_OPENNI_DEPTH_GENERATOR + CAP_OPENNI_IMAGE_GENERATOR + CAP_OPENNI_IR_GENERATOR,
+            CAP_PROP_OPENNI_OUTPUT_MODE = 100,
+            CAP_PROP_OPENNI_FRAME_MAX_DEPTH = 101,
+            CAP_PROP_OPENNI_BASELINE = 102,
+            CAP_PROP_OPENNI_FOCAL_LENGTH = 103,
+            CAP_PROP_OPENNI_REGISTRATION = 104,
+            CAP_PROP_OPENNI_REGISTRATION_ON = 104,
+            CAP_PROP_OPENNI_APPROX_FRAME_SYNC = 105,
+            CAP_PROP_OPENNI_MAX_BUFFER_SIZE = 106,
+            CAP_PROP_OPENNI_CIRCLE_BUFFER = 107,
+            CAP_PROP_OPENNI_MAX_TIME_DURATION = 108,
+            CAP_PROP_OPENNI_GENERATOR_PRESENT = 109,
+            CAP_PROP_OPENNI2_SYNC = 110,
+            CAP_PROP_OPENNI2_MIRROR = 111,
+            CAP_OPENNI_IMAGE_GENERATOR_PRESENT = CAP_OPENNI_IMAGE_GENERATOR + CAP_PROP_OPENNI_GENERATOR_PRESENT,
+            CAP_OPENNI_IMAGE_GENERATOR_OUTPUT_MODE = CAP_OPENNI_IMAGE_GENERATOR + CAP_PROP_OPENNI_OUTPUT_MODE,
+            CAP_OPENNI_DEPTH_GENERATOR_PRESENT = CAP_OPENNI_DEPTH_GENERATOR + CAP_PROP_OPENNI_GENERATOR_PRESENT,
+            CAP_OPENNI_DEPTH_GENERATOR_BASELINE = CAP_OPENNI_DEPTH_GENERATOR + CAP_PROP_OPENNI_BASELINE,
+            CAP_OPENNI_DEPTH_GENERATOR_FOCAL_LENGTH = CAP_OPENNI_DEPTH_GENERATOR + CAP_PROP_OPENNI_FOCAL_LENGTH,
+            CAP_OPENNI_DEPTH_GENERATOR_REGISTRATION = CAP_OPENNI_DEPTH_GENERATOR + CAP_PROP_OPENNI_REGISTRATION,
+            CAP_OPENNI_DEPTH_GENERATOR_REGISTRATION_ON = CAP_OPENNI_DEPTH_GENERATOR + CAP_PROP_OPENNI_REGISTRATION,
+            CAP_OPENNI_IR_GENERATOR_PRESENT = CAP_OPENNI_IR_GENERATOR + CAP_PROP_OPENNI_GENERATOR_PRESENT,
+            CAP_OPENNI_DEPTH_MAP = 0,
+            CAP_OPENNI_POINT_CLOUD_MAP = 1,
+            CAP_OPENNI_DISPARITY_MAP = 2,
+            CAP_OPENNI_DISPARITY_MAP_32F = 3,
+            CAP_OPENNI_VALID_DEPTH_MASK = 4,
+            CAP_OPENNI_BGR_IMAGE = 5,
+            CAP_OPENNI_GRAY_IMAGE = 6,
+            CAP_OPENNI_IR_IMAGE = 7,
+            CAP_OPENNI_VGA_30HZ = 0,
+            CAP_OPENNI_SXGA_15HZ = 1,
+            CAP_OPENNI_SXGA_30HZ = 2,
+            CAP_OPENNI_QVGA_30HZ = 3,
+            CAP_OPENNI_QVGA_60HZ = 4,
+            CAP_PROP_GSTREAMER_QUEUE_LENGTH = 200,
+            CAP_PROP_PVAPI_MULTICASTIP = 300,
+            CAP_PROP_PVAPI_FRAMESTARTTRIGGERMODE = 301,
+            CAP_PROP_PVAPI_DECIMATIONHORIZONTAL = 302,
+            CAP_PROP_PVAPI_DECIMATIONVERTICAL = 303,
+            CAP_PROP_PVAPI_BINNINGX = 304,
+            CAP_PROP_PVAPI_BINNINGY = 305,
+            CAP_PROP_PVAPI_PIXELFORMAT = 306,
+            CAP_PVAPI_FSTRIGMODE_FREERUN = 0,
+            CAP_PVAPI_FSTRIGMODE_SYNCIN1 = 1,
+            CAP_PVAPI_FSTRIGMODE_SYNCIN2 = 2,
+            CAP_PVAPI_FSTRIGMODE_FIXEDRATE = 3,
+            CAP_PVAPI_FSTRIGMODE_SOFTWARE = 4,
+            CAP_PVAPI_DECIMATION_OFF = 1,
+            CAP_PVAPI_DECIMATION_2OUTOF4 = 2,
+            CAP_PVAPI_DECIMATION_2OUTOF8 = 4,
+            CAP_PVAPI_DECIMATION_2OUTOF16 = 8,
+            CAP_PVAPI_PIXELFORMAT_MONO8 = 1,
+            CAP_PVAPI_PIXELFORMAT_MONO16 = 2,
+            CAP_PVAPI_PIXELFORMAT_BAYER8 = 3,
+            CAP_PVAPI_PIXELFORMAT_BAYER16 = 4,
+            CAP_PVAPI_PIXELFORMAT_RGB24 = 5,
+            CAP_PVAPI_PIXELFORMAT_BGR24 = 6,
+            CAP_PVAPI_PIXELFORMAT_RGBA32 = 7,
+            CAP_PVAPI_PIXELFORMAT_BGRA32 = 8,
+            CAP_PROP_XI_DOWNSAMPLING = 400,
+            CAP_PROP_XI_DATA_FORMAT = 401,
+            CAP_PROP_XI_OFFSET_X = 402,
+            CAP_PROP_XI_OFFSET_Y = 403,
+            CAP_PROP_XI_TRG_SOURCE = 404,
+            CAP_PROP_XI_TRG_SOFTWARE = 405,
+            CAP_PROP_XI_GPI_SELECTOR = 406,
+            CAP_PROP_XI_GPI_MODE = 407,
+            CAP_PROP_XI_GPI_LEVEL = 408,
+            CAP_PROP_XI_GPO_SELECTOR = 409,
+            CAP_PROP_XI_GPO_MODE = 410,
+            CAP_PROP_XI_LED_SELECTOR = 411,
+            CAP_PROP_XI_LED_MODE = 412,
+            CAP_PROP_XI_MANUAL_WB = 413,
+            CAP_PROP_XI_AUTO_WB = 414,
+            CAP_PROP_XI_AEAG = 415,
+            CAP_PROP_XI_EXP_PRIORITY = 416,
+            CAP_PROP_XI_AE_MAX_LIMIT = 417,
+            CAP_PROP_XI_AG_MAX_LIMIT = 418,
+            CAP_PROP_XI_AEAG_LEVEL = 419,
+            CAP_PROP_XI_TIMEOUT = 420,
+            CAP_PROP_XI_EXPOSURE = 421,
+            CAP_PROP_XI_EXPOSURE_BURST_COUNT = 422,
+            CAP_PROP_XI_GAIN_SELECTOR = 423,
+            CAP_PROP_XI_GAIN = 424,
+            CAP_PROP_XI_DOWNSAMPLING_TYPE = 426,
+            CAP_PROP_XI_BINNING_SELECTOR = 427,
+            CAP_PROP_XI_BINNING_VERTICAL = 428,
+            CAP_PROP_XI_BINNING_HORIZONTAL = 429,
+            CAP_PROP_XI_BINNING_PATTERN = 430,
+            CAP_PROP_XI_DECIMATION_SELECTOR = 431,
+            CAP_PROP_XI_DECIMATION_VERTICAL = 432,
+            CAP_PROP_XI_DECIMATION_HORIZONTAL = 433,
+            CAP_PROP_XI_DECIMATION_PATTERN = 434,
+            CAP_PROP_XI_TEST_PATTERN_GENERATOR_SELECTOR = 587,
+            CAP_PROP_XI_TEST_PATTERN = 588,
+            CAP_PROP_XI_IMAGE_DATA_FORMAT = 435,
+            CAP_PROP_XI_SHUTTER_TYPE = 436,
+            CAP_PROP_XI_SENSOR_TAPS = 437,
+            CAP_PROP_XI_AEAG_ROI_OFFSET_X = 439,
+            CAP_PROP_XI_AEAG_ROI_OFFSET_Y = 440,
+            CAP_PROP_XI_AEAG_ROI_WIDTH = 441,
+            CAP_PROP_XI_AEAG_ROI_HEIGHT = 442,
+            CAP_PROP_XI_BPC = 445,
+            CAP_PROP_XI_WB_KR = 448,
+            CAP_PROP_XI_WB_KG = 449,
+            CAP_PROP_XI_WB_KB = 450,
+            CAP_PROP_XI_WIDTH = 451,
+            CAP_PROP_XI_HEIGHT = 452,
+            CAP_PROP_XI_REGION_SELECTOR = 589,
+            CAP_PROP_XI_REGION_MODE = 595,
+            CAP_PROP_XI_LIMIT_BANDWIDTH = 459,
+            CAP_PROP_XI_SENSOR_DATA_BIT_DEPTH = 460,
+            CAP_PROP_XI_OUTPUT_DATA_BIT_DEPTH = 461,
+            CAP_PROP_XI_IMAGE_DATA_BIT_DEPTH = 462,
+            CAP_PROP_XI_OUTPUT_DATA_PACKING = 463,
+            CAP_PROP_XI_OUTPUT_DATA_PACKING_TYPE = 464,
+            CAP_PROP_XI_IS_COOLED = 465,
+            CAP_PROP_XI_COOLING = 466,
+            CAP_PROP_XI_TARGET_TEMP = 467,
+            CAP_PROP_XI_CHIP_TEMP = 468,
+            CAP_PROP_XI_HOUS_TEMP = 469,
+            CAP_PROP_XI_HOUS_BACK_SIDE_TEMP = 590,
+            CAP_PROP_XI_SENSOR_BOARD_TEMP = 596,
+            CAP_PROP_XI_CMS = 470,
+            CAP_PROP_XI_APPLY_CMS = 471,
+            CAP_PROP_XI_IMAGE_IS_COLOR = 474,
+            CAP_PROP_XI_COLOR_FILTER_ARRAY = 475,
+            CAP_PROP_XI_GAMMAY = 476,
+            CAP_PROP_XI_GAMMAC = 477,
+            CAP_PROP_XI_SHARPNESS = 478,
+            CAP_PROP_XI_CC_MATRIX_00 = 479,
+            CAP_PROP_XI_CC_MATRIX_01 = 480,
+            CAP_PROP_XI_CC_MATRIX_02 = 481,
+            CAP_PROP_XI_CC_MATRIX_03 = 482,
+            CAP_PROP_XI_CC_MATRIX_10 = 483,
+            CAP_PROP_XI_CC_MATRIX_11 = 484,
+            CAP_PROP_XI_CC_MATRIX_12 = 485,
+            CAP_PROP_XI_CC_MATRIX_13 = 486,
+            CAP_PROP_XI_CC_MATRIX_20 = 487,
+            CAP_PROP_XI_CC_MATRIX_21 = 488,
+            CAP_PROP_XI_CC_MATRIX_22 = 489,
+            CAP_PROP_XI_CC_MATRIX_23 = 490,
+            CAP_PROP_XI_CC_MATRIX_30 = 491,
+            CAP_PROP_XI_CC_MATRIX_31 = 492,
+            CAP_PROP_XI_CC_MATRIX_32 = 493,
+            CAP_PROP_XI_CC_MATRIX_33 = 494,
+            CAP_PROP_XI_DEFAULT_CC_MATRIX = 495,
+            CAP_PROP_XI_TRG_SELECTOR = 498,
+            CAP_PROP_XI_ACQ_FRAME_BURST_COUNT = 499,
+            CAP_PROP_XI_DEBOUNCE_EN = 507,
+            CAP_PROP_XI_DEBOUNCE_T0 = 508,
+            CAP_PROP_XI_DEBOUNCE_T1 = 509,
+            CAP_PROP_XI_DEBOUNCE_POL = 510,
+            CAP_PROP_XI_LENS_MODE = 511,
+            CAP_PROP_XI_LENS_APERTURE_VALUE = 512,
+            CAP_PROP_XI_LENS_FOCUS_MOVEMENT_VALUE = 513,
+            CAP_PROP_XI_LENS_FOCUS_MOVE = 514,
+            CAP_PROP_XI_LENS_FOCUS_DISTANCE = 515,
+            CAP_PROP_XI_LENS_FOCAL_LENGTH = 516,
+            CAP_PROP_XI_LENS_FEATURE_SELECTOR = 517,
+            CAP_PROP_XI_LENS_FEATURE = 518,
+            CAP_PROP_XI_DEVICE_MODEL_ID = 521,
+            CAP_PROP_XI_DEVICE_SN = 522,
+            CAP_PROP_XI_IMAGE_DATA_FORMAT_RGB32_ALPHA = 529,
+            CAP_PROP_XI_IMAGE_PAYLOAD_SIZE = 530,
+            CAP_PROP_XI_TRANSPORT_PIXEL_FORMAT = 531,
+            CAP_PROP_XI_SENSOR_CLOCK_FREQ_HZ = 532,
+            CAP_PROP_XI_SENSOR_CLOCK_FREQ_INDEX = 533,
+            CAP_PROP_XI_SENSOR_OUTPUT_CHANNEL_COUNT = 534,
+            CAP_PROP_XI_FRAMERATE = 535,
+            CAP_PROP_XI_COUNTER_SELECTOR = 536,
+            CAP_PROP_XI_COUNTER_VALUE = 537,
+            CAP_PROP_XI_ACQ_TIMING_MODE = 538,
+            CAP_PROP_XI_AVAILABLE_BANDWIDTH = 539,
+            CAP_PROP_XI_BUFFER_POLICY = 540,
+            CAP_PROP_XI_LUT_EN = 541,
+            CAP_PROP_XI_LUT_INDEX = 542,
+            CAP_PROP_XI_LUT_VALUE = 543,
+            CAP_PROP_XI_TRG_DELAY = 544,
+            CAP_PROP_XI_TS_RST_MODE = 545,
+            CAP_PROP_XI_TS_RST_SOURCE = 546,
+            CAP_PROP_XI_IS_DEVICE_EXIST = 547,
+            CAP_PROP_XI_ACQ_BUFFER_SIZE = 548,
+            CAP_PROP_XI_ACQ_BUFFER_SIZE_UNIT = 549,
+            CAP_PROP_XI_ACQ_TRANSPORT_BUFFER_SIZE = 550,
+            CAP_PROP_XI_BUFFERS_QUEUE_SIZE = 551,
+            CAP_PROP_XI_ACQ_TRANSPORT_BUFFER_COMMIT = 552,
+            CAP_PROP_XI_RECENT_FRAME = 553,
+            CAP_PROP_XI_DEVICE_RESET = 554,
+            CAP_PROP_XI_COLUMN_FPN_CORRECTION = 555,
+            CAP_PROP_XI_ROW_FPN_CORRECTION = 591,
+            CAP_PROP_XI_SENSOR_MODE = 558,
+            CAP_PROP_XI_HDR = 559,
+            CAP_PROP_XI_HDR_KNEEPOINT_COUNT = 560,
+            CAP_PROP_XI_HDR_T1 = 561,
+            CAP_PROP_XI_HDR_T2 = 562,
+            CAP_PROP_XI_KNEEPOINT1 = 563,
+            CAP_PROP_XI_KNEEPOINT2 = 564,
+            CAP_PROP_XI_IMAGE_BLACK_LEVEL = 565,
+            CAP_PROP_XI_HW_REVISION = 571,
+            CAP_PROP_XI_DEBUG_LEVEL = 572,
+            CAP_PROP_XI_AUTO_BANDWIDTH_CALCULATION = 573,
+            CAP_PROP_XI_FFS_FILE_ID = 594,
+            CAP_PROP_XI_FFS_FILE_SIZE = 580,
+            CAP_PROP_XI_FREE_FFS_SIZE = 581,
+            CAP_PROP_XI_USED_FFS_SIZE = 582,
+            CAP_PROP_XI_FFS_ACCESS_KEY = 583,
+            CAP_PROP_XI_SENSOR_FEATURE_SELECTOR = 585,
+            CAP_PROP_XI_SENSOR_FEATURE_VALUE = 586,
+            CAP_PROP_IOS_DEVICE_FOCUS = 9001,
+            CAP_PROP_IOS_DEVICE_EXPOSURE = 9002,
+            CAP_PROP_IOS_DEVICE_FLASH = 9003,
+            CAP_PROP_IOS_DEVICE_WHITEBALANCE = 9004,
+            CAP_PROP_IOS_DEVICE_TORCH = 9005,
+            CAP_PROP_GIGA_FRAME_OFFSET_X = 10001,
+            CAP_PROP_GIGA_FRAME_OFFSET_Y = 10002,
+            CAP_PROP_GIGA_FRAME_WIDTH_MAX = 10003,
+            CAP_PROP_GIGA_FRAME_HEIGH_MAX = 10004,
+            CAP_PROP_GIGA_FRAME_SENS_WIDTH = 10005,
+            CAP_PROP_GIGA_FRAME_SENS_HEIGH = 10006,
+            CAP_PROP_INTELPERC_PROFILE_COUNT = 11001,
+            CAP_PROP_INTELPERC_PROFILE_IDX = 11002,
+            CAP_PROP_INTELPERC_DEPTH_LOW_CONFIDENCE_VALUE = 11003,
+            CAP_PROP_INTELPERC_DEPTH_SATURATION_VALUE = 11004,
+            CAP_PROP_INTELPERC_DEPTH_CONFIDENCE_THRESHOLD = 11005,
+            CAP_PROP_INTELPERC_DEPTH_FOCAL_LENGTH_HORZ = 11006,
+            CAP_PROP_INTELPERC_DEPTH_FOCAL_LENGTH_VERT = 11007,
+            CAP_INTELPERC_DEPTH_GENERATOR = 1 << 29,
+            CAP_INTELPERC_IMAGE_GENERATOR = 1 << 28,
+            CAP_INTELPERC_GENERATORS_MASK = CAP_INTELPERC_DEPTH_GENERATOR + CAP_INTELPERC_IMAGE_GENERATOR,
+            CAP_INTELPERC_DEPTH_MAP = 0,
+            CAP_INTELPERC_UVDEPTH_MAP = 1,
+            CAP_INTELPERC_IR_MAP = 2,
+            CAP_INTELPERC_IMAGE = 3,
+            CAP_PROP_GPHOTO2_PREVIEW = 17001,
+            CAP_PROP_GPHOTO2_WIDGET_ENUMERATE = 17002,
+            CAP_PROP_GPHOTO2_RELOAD_CONFIG = 17003,
+            CAP_PROP_GPHOTO2_RELOAD_ON_CHANGE = 17004,
+            CAP_PROP_GPHOTO2_COLLECT_MSGS = 17005,
+            CAP_PROP_GPHOTO2_FLUSH_MSGS = 17006,
+            CAP_PROP_SPEED = 17007,
+            CAP_PROP_APERTURE = 17008,
+            CAP_PROP_EXPOSUREPROGRAM = 17009,
+            CAP_PROP_VIEWFINDER = 17010,
+            CAP_PROP_IMAGES_BASE = 18000,
+            CAP_PROP_IMAGES_LAST = 19000;
+
+
+    // C++: enum VideoCaptureProperties
+    public static final int
+            CAP_PROP_POS_MSEC = 0,
+            CAP_PROP_POS_FRAMES = 1,
+            CAP_PROP_POS_AVI_RATIO = 2,
+            CAP_PROP_FRAME_WIDTH = 3,
+            CAP_PROP_FRAME_HEIGHT = 4,
+            CAP_PROP_FPS = 5,
+            CAP_PROP_FOURCC = 6,
+            CAP_PROP_FRAME_COUNT = 7,
+            CAP_PROP_FORMAT = 8,
+            CAP_PROP_MODE = 9,
+            CAP_PROP_BRIGHTNESS = 10,
+            CAP_PROP_CONTRAST = 11,
+            CAP_PROP_SATURATION = 12,
+            CAP_PROP_HUE = 13,
+            CAP_PROP_GAIN = 14,
+            CAP_PROP_EXPOSURE = 15,
+            CAP_PROP_CONVERT_RGB = 16,
+            CAP_PROP_WHITE_BALANCE_BLUE_U = 17,
+            CAP_PROP_RECTIFICATION = 18,
+            CAP_PROP_MONOCHROME = 19,
+            CAP_PROP_SHARPNESS = 20,
+            CAP_PROP_AUTO_EXPOSURE = 21,
+            CAP_PROP_GAMMA = 22,
+            CAP_PROP_TEMPERATURE = 23,
+            CAP_PROP_TRIGGER = 24,
+            CAP_PROP_TRIGGER_DELAY = 25,
+            CAP_PROP_WHITE_BALANCE_RED_V = 26,
+            CAP_PROP_ZOOM = 27,
+            CAP_PROP_FOCUS = 28,
+            CAP_PROP_GUID = 29,
+            CAP_PROP_ISO_SPEED = 30,
+            CAP_PROP_BACKLIGHT = 32,
+            CAP_PROP_PAN = 33,
+            CAP_PROP_TILT = 34,
+            CAP_PROP_ROLL = 35,
+            CAP_PROP_IRIS = 36,
+            CAP_PROP_SETTINGS = 37,
+            CAP_PROP_BUFFERSIZE = 38,
+            CAP_PROP_AUTOFOCUS = 39,
+            CAP_PROP_SAR_NUM = 40,
+            CAP_PROP_SAR_DEN = 41,
+            CAP_PROP_BACKEND = 42,
+            CAP_PROP_CHANNEL = 43,
+            CAP_PROP_AUTO_WB = 44,
+            CAP_PROP_WB_TEMPERATURE = 45,
+            CAP_PROP_CODEC_PIXEL_FORMAT = 46,
+            CAP_PROP_BITRATE = 47;
+
+
+    // C++: enum VideoCaptureAPIs
+    public static final int
+            CAP_ANY = 0,
+            CAP_VFW = 200,
+            CAP_V4L = 200,
+            CAP_V4L2 = 200,
+            CAP_FIREWIRE = 300,
+            CAP_FIREWARE = 300,
+            CAP_IEEE1394 = 300,
+            CAP_DC1394 = 300,
+            CAP_CMU1394 = 300,
+            CAP_QT = 500,
+            CAP_UNICAP = 600,
+            CAP_DSHOW = 700,
+            CAP_PVAPI = 800,
+            CAP_OPENNI = 900,
+            CAP_OPENNI_ASUS = 910,
+            CAP_ANDROID = 1000,
+            CAP_XIAPI = 1100,
+            CAP_AVFOUNDATION = 1200,
+            CAP_GIGANETIX = 1300,
+            CAP_MSMF = 1400,
+            CAP_WINRT = 1410,
+            CAP_INTELPERC = 1500,
+            CAP_OPENNI2 = 1600,
+            CAP_OPENNI2_ASUS = 1610,
+            CAP_GPHOTO2 = 1700,
+            CAP_GSTREAMER = 1800,
+            CAP_FFMPEG = 1900,
+            CAP_IMAGES = 2000,
+            CAP_ARAVIS = 2100,
+            CAP_OPENCV_MJPEG = 2200,
+            CAP_INTEL_MFX = 2300,
+            CAP_XINE = 2400;
+
+
+    // C++: enum VideoCaptureModes
+    public static final int
+            CAP_MODE_BGR = 0,
+            CAP_MODE_RGB = 1,
+            CAP_MODE_GRAY = 2,
+            CAP_MODE_YUYV = 3;
+
+
+    //
+    // C++:  String cv::videoio_registry::getBackendName(VideoCaptureAPIs api)
+    //
+
+    /**
+     * Returns backend API name or "unknown"
+     * @param api backend ID (#VideoCaptureAPIs)
+     * @return automatically generated
+     */
+    public static String getBackendName(int api) {
+        return getBackendName_0(api);
+    }
+
+
+    //
+    // C++:  vector_VideoCaptureAPIs cv::videoio_registry::getBackends()
+    //
+
+    // Return type 'vector_VideoCaptureAPIs' is not supported, skipping the function
+
+
+    //
+    // C++:  vector_VideoCaptureAPIs cv::videoio_registry::getCameraBackends()
+    //
+
+    // Return type 'vector_VideoCaptureAPIs' is not supported, skipping the function
+
+
+    //
+    // C++:  vector_VideoCaptureAPIs cv::videoio_registry::getStreamBackends()
+    //
+
+    // Return type 'vector_VideoCaptureAPIs' is not supported, skipping the function
+
+
+    //
+    // C++:  vector_VideoCaptureAPIs cv::videoio_registry::getWriterBackends()
+    //
+
+    // Return type 'vector_VideoCaptureAPIs' is not supported, skipping the function
+
+
+
+
+    // C++:  String cv::videoio_registry::getBackendName(VideoCaptureAPIs api)
+    private static native String getBackendName_0(int api);
+
+}
Index: openCVLibrary3411/src/main/java/org/opencv/videoio/VideoWriter.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/videoio/VideoWriter.java	(date 1605830248365)
+++ openCVLibrary3411/src/main/java/org/opencv/videoio/VideoWriter.java	(date 1605830248365)
@@ -0,0 +1,380 @@
+//
+// This file is auto-generated. Please don't modify it!
+//
+package org.opencv.videoio;
+
+import org.opencv.core.Mat;
+import org.opencv.core.Size;
+
+// C++: class VideoWriter
+/**
+ * Video writer class.
+ *
+ * The class provides C++ API for writing video files or image sequences.
+ */
+public class VideoWriter {
+
+    protected final long nativeObj;
+    protected VideoWriter(long addr) { nativeObj = addr; }
+
+    public long getNativeObjAddr() { return nativeObj; }
+
+    // internal usage only
+    public static VideoWriter __fromPtr__(long addr) { return new VideoWriter(addr); }
+
+    //
+    // C++:   cv::VideoWriter::VideoWriter(String filename, int apiPreference, int fourcc, double fps, Size frameSize, bool isColor = true)
+    //
+
+    /**
+     *
+     *     The {@code apiPreference} parameter allows to specify API backends to use. Can be used to enforce a specific reader implementation
+     *     if multiple are available: e.g. cv::CAP_FFMPEG or cv::CAP_GSTREAMER.
+     * @param filename automatically generated
+     * @param apiPreference automatically generated
+     * @param fourcc automatically generated
+     * @param fps automatically generated
+     * @param frameSize automatically generated
+     * @param isColor automatically generated
+     */
+    public VideoWriter(String filename, int apiPreference, int fourcc, double fps, Size frameSize, boolean isColor) {
+        nativeObj = VideoWriter_0(filename, apiPreference, fourcc, fps, frameSize.width, frameSize.height, isColor);
+    }
+
+    /**
+     *
+     *     The {@code apiPreference} parameter allows to specify API backends to use. Can be used to enforce a specific reader implementation
+     *     if multiple are available: e.g. cv::CAP_FFMPEG or cv::CAP_GSTREAMER.
+     * @param filename automatically generated
+     * @param apiPreference automatically generated
+     * @param fourcc automatically generated
+     * @param fps automatically generated
+     * @param frameSize automatically generated
+     */
+    public VideoWriter(String filename, int apiPreference, int fourcc, double fps, Size frameSize) {
+        nativeObj = VideoWriter_1(filename, apiPreference, fourcc, fps, frameSize.width, frameSize.height);
+    }
+
+
+    //
+    // C++:   cv::VideoWriter::VideoWriter(String filename, int fourcc, double fps, Size frameSize, bool isColor = true)
+    //
+
+    /**
+     *
+     *     @param filename Name of the output video file.
+     *     @param fourcc 4-character code of codec used to compress the frames. For example,
+     *     VideoWriter::fourcc('P','I','M','1') is a MPEG-1 codec, VideoWriter::fourcc('M','J','P','G') is a
+     *     motion-jpeg codec etc. List of codes can be obtained at [Video Codecs by
+     *     FOURCC](http://www.fourcc.org/codecs.php) page. FFMPEG backend with MP4 container natively uses
+     *     other values as fourcc code: see [ObjectType](http://mp4ra.org/#/codecs),
+     *     so you may receive a warning message from OpenCV about fourcc code conversion.
+     *     @param fps Framerate of the created video stream.
+     *     @param frameSize Size of the video frames.
+     *     @param isColor If it is not zero, the encoder will expect and encode color frames, otherwise it
+     *     will work with grayscale frames (the flag is currently supported on Windows only).
+     *
+     *     <b>Tips</b>:
+     * <ul>
+     *   <li>
+     *      With some backends {@code fourcc=-1} pops up the codec selection dialog from the system.
+     *   </li>
+     *   <li>
+     *      To save image sequence use a proper filename (eg. {@code img_%02d.jpg}) and {@code fourcc=0}
+     *       OR {@code fps=0}. Use uncompressed image format (eg. {@code img_%02d.BMP}) to save raw frames.
+     *   </li>
+     *   <li>
+     *      Most codecs are lossy. If you want lossless video file you need to use a lossless codecs
+     *       (eg. FFMPEG FFV1, Huffman HFYU, Lagarith LAGS, etc...)
+     *   </li>
+     *   <li>
+     *      If FFMPEG is enabled, using {@code codec=0; fps=0;} you can create an uncompressed (raw) video file.
+     *   </li>
+     * </ul>
+     */
+    public VideoWriter(String filename, int fourcc, double fps, Size frameSize, boolean isColor) {
+        nativeObj = VideoWriter_2(filename, fourcc, fps, frameSize.width, frameSize.height, isColor);
+    }
+
+    /**
+     *
+     *     @param filename Name of the output video file.
+     *     @param fourcc 4-character code of codec used to compress the frames. For example,
+     *     VideoWriter::fourcc('P','I','M','1') is a MPEG-1 codec, VideoWriter::fourcc('M','J','P','G') is a
+     *     motion-jpeg codec etc. List of codes can be obtained at [Video Codecs by
+     *     FOURCC](http://www.fourcc.org/codecs.php) page. FFMPEG backend with MP4 container natively uses
+     *     other values as fourcc code: see [ObjectType](http://mp4ra.org/#/codecs),
+     *     so you may receive a warning message from OpenCV about fourcc code conversion.
+     *     @param fps Framerate of the created video stream.
+     *     @param frameSize Size of the video frames.
+     *     will work with grayscale frames (the flag is currently supported on Windows only).
+     *
+     *     <b>Tips</b>:
+     * <ul>
+     *   <li>
+     *      With some backends {@code fourcc=-1} pops up the codec selection dialog from the system.
+     *   </li>
+     *   <li>
+     *      To save image sequence use a proper filename (eg. {@code img_%02d.jpg}) and {@code fourcc=0}
+     *       OR {@code fps=0}. Use uncompressed image format (eg. {@code img_%02d.BMP}) to save raw frames.
+     *   </li>
+     *   <li>
+     *      Most codecs are lossy. If you want lossless video file you need to use a lossless codecs
+     *       (eg. FFMPEG FFV1, Huffman HFYU, Lagarith LAGS, etc...)
+     *   </li>
+     *   <li>
+     *      If FFMPEG is enabled, using {@code codec=0; fps=0;} you can create an uncompressed (raw) video file.
+     *   </li>
+     * </ul>
+     */
+    public VideoWriter(String filename, int fourcc, double fps, Size frameSize) {
+        nativeObj = VideoWriter_3(filename, fourcc, fps, frameSize.width, frameSize.height);
+    }
+
+
+    //
+    // C++:   cv::VideoWriter::VideoWriter()
+    //
+
+    /**
+     * Default constructors
+     *
+     *     The constructors/functions initialize video writers.
+     * <ul>
+     *   <li>
+     *        On Linux FFMPEG is used to write videos;
+     *   </li>
+     *   <li>
+     *        On Windows FFMPEG or VFW is used;
+     *   </li>
+     *   <li>
+     *        On MacOSX QTKit is used.
+     *   </li>
+     * </ul>
+     */
+    public VideoWriter() {
+        nativeObj = VideoWriter_4();
+    }
+
+
+    //
+    // C++:  String cv::VideoWriter::getBackendName()
+    //
+
+    /**
+     * Returns used backend API name
+     *
+     *      <b>Note:</b> Stream should be opened.
+     * @return automatically generated
+     */
+    public String getBackendName() {
+        return getBackendName_0(nativeObj);
+    }
+
+
+    //
+    // C++:  bool cv::VideoWriter::isOpened()
+    //
+
+    /**
+     * Returns true if video writer has been successfully initialized.
+     * @return automatically generated
+     */
+    public boolean isOpened() {
+        return isOpened_0(nativeObj);
+    }
+
+
+    //
+    // C++:  bool cv::VideoWriter::open(String filename, int apiPreference, int fourcc, double fps, Size frameSize, bool isColor = true)
+    //
+
+    public boolean open(String filename, int apiPreference, int fourcc, double fps, Size frameSize, boolean isColor) {
+        return open_0(nativeObj, filename, apiPreference, fourcc, fps, frameSize.width, frameSize.height, isColor);
+    }
+
+    public boolean open(String filename, int apiPreference, int fourcc, double fps, Size frameSize) {
+        return open_1(nativeObj, filename, apiPreference, fourcc, fps, frameSize.width, frameSize.height);
+    }
+
+
+    //
+    // C++:  bool cv::VideoWriter::open(String filename, int fourcc, double fps, Size frameSize, bool isColor = true)
+    //
+
+    /**
+     * Initializes or reinitializes video writer.
+     *
+     *     The method opens video writer. Parameters are the same as in the constructor
+     *     VideoWriter::VideoWriter.
+     *     @return {@code true} if video writer has been successfully initialized
+     *
+     *     The method first calls VideoWriter::release to close the already opened file.
+     * @param filename automatically generated
+     * @param fourcc automatically generated
+     * @param fps automatically generated
+     * @param frameSize automatically generated
+     * @param isColor automatically generated
+     */
+    public boolean open(String filename, int fourcc, double fps, Size frameSize, boolean isColor) {
+        return open_2(nativeObj, filename, fourcc, fps, frameSize.width, frameSize.height, isColor);
+    }
+
+    /**
+     * Initializes or reinitializes video writer.
+     *
+     *     The method opens video writer. Parameters are the same as in the constructor
+     *     VideoWriter::VideoWriter.
+     *     @return {@code true} if video writer has been successfully initialized
+     *
+     *     The method first calls VideoWriter::release to close the already opened file.
+     * @param filename automatically generated
+     * @param fourcc automatically generated
+     * @param fps automatically generated
+     * @param frameSize automatically generated
+     */
+    public boolean open(String filename, int fourcc, double fps, Size frameSize) {
+        return open_3(nativeObj, filename, fourcc, fps, frameSize.width, frameSize.height);
+    }
+
+
+    //
+    // C++:  bool cv::VideoWriter::set(int propId, double value)
+    //
+
+    /**
+     * Sets a property in the VideoWriter.
+     *
+     *      @param propId Property identifier from cv::VideoWriterProperties (eg. cv::VIDEOWRITER_PROP_QUALITY)
+     *      or one of REF: videoio_flags_others
+     *
+     *      @param value Value of the property.
+     *      @return  {@code true} if the property is supported by the backend used by the VideoWriter instance.
+     */
+    public boolean set(int propId, double value) {
+        return set_0(nativeObj, propId, value);
+    }
+
+
+    //
+    // C++:  double cv::VideoWriter::get(int propId)
+    //
+
+    /**
+     * Returns the specified VideoWriter property
+     *
+     *      @param propId Property identifier from cv::VideoWriterProperties (eg. cv::VIDEOWRITER_PROP_QUALITY)
+     *      or one of REF: videoio_flags_others
+     *
+     *      @return Value for the specified property. Value 0 is returned when querying a property that is
+     *      not supported by the backend used by the VideoWriter instance.
+     */
+    public double get(int propId) {
+        return get_0(nativeObj, propId);
+    }
+
+
+    //
+    // C++: static int cv::VideoWriter::fourcc(char c1, char c2, char c3, char c4)
+    //
+
+    /**
+     * Concatenates 4 chars to a fourcc code
+     *
+     *     @return a fourcc code
+     *
+     *     This static method constructs the fourcc code of the codec to be used in the constructor
+     *     VideoWriter::VideoWriter or VideoWriter::open.
+     * @param c1 automatically generated
+     * @param c2 automatically generated
+     * @param c3 automatically generated
+     * @param c4 automatically generated
+     */
+    public static int fourcc(char c1, char c2, char c3, char c4) {
+        return fourcc_0(c1, c2, c3, c4);
+    }
+
+
+    //
+    // C++:  void cv::VideoWriter::release()
+    //
+
+    /**
+     * Closes the video writer.
+     *
+     *     The method is automatically called by subsequent VideoWriter::open and by the VideoWriter
+     *     destructor.
+     */
+    public void release() {
+        release_0(nativeObj);
+    }
+
+
+    //
+    // C++:  void cv::VideoWriter::write(Mat image)
+    //
+
+    /**
+     * Writes the next video frame
+     *
+     *     @param image The written frame. In general, color images are expected in BGR format.
+     *
+     *     The function/method writes the specified image to video file. It must have the same size as has
+     *     been specified when opening the video writer.
+     */
+    public void write(Mat image) {
+        write_0(nativeObj, image.nativeObj);
+    }
+
+
+    @Override
+    protected void finalize() throws Throwable {
+        delete(nativeObj);
+    }
+
+
+
+    // C++:   cv::VideoWriter::VideoWriter(String filename, int apiPreference, int fourcc, double fps, Size frameSize, bool isColor = true)
+    private static native long VideoWriter_0(String filename, int apiPreference, int fourcc, double fps, double frameSize_width, double frameSize_height, boolean isColor);
+    private static native long VideoWriter_1(String filename, int apiPreference, int fourcc, double fps, double frameSize_width, double frameSize_height);
+
+    // C++:   cv::VideoWriter::VideoWriter(String filename, int fourcc, double fps, Size frameSize, bool isColor = true)
+    private static native long VideoWriter_2(String filename, int fourcc, double fps, double frameSize_width, double frameSize_height, boolean isColor);
+    private static native long VideoWriter_3(String filename, int fourcc, double fps, double frameSize_width, double frameSize_height);
+
+    // C++:   cv::VideoWriter::VideoWriter()
+    private static native long VideoWriter_4();
+
+    // C++:  String cv::VideoWriter::getBackendName()
+    private static native String getBackendName_0(long nativeObj);
+
+    // C++:  bool cv::VideoWriter::isOpened()
+    private static native boolean isOpened_0(long nativeObj);
+
+    // C++:  bool cv::VideoWriter::open(String filename, int apiPreference, int fourcc, double fps, Size frameSize, bool isColor = true)
+    private static native boolean open_0(long nativeObj, String filename, int apiPreference, int fourcc, double fps, double frameSize_width, double frameSize_height, boolean isColor);
+    private static native boolean open_1(long nativeObj, String filename, int apiPreference, int fourcc, double fps, double frameSize_width, double frameSize_height);
+
+    // C++:  bool cv::VideoWriter::open(String filename, int fourcc, double fps, Size frameSize, bool isColor = true)
+    private static native boolean open_2(long nativeObj, String filename, int fourcc, double fps, double frameSize_width, double frameSize_height, boolean isColor);
+    private static native boolean open_3(long nativeObj, String filename, int fourcc, double fps, double frameSize_width, double frameSize_height);
+
+    // C++:  bool cv::VideoWriter::set(int propId, double value)
+    private static native boolean set_0(long nativeObj, int propId, double value);
+
+    // C++:  double cv::VideoWriter::get(int propId)
+    private static native double get_0(long nativeObj, int propId);
+
+    // C++: static int cv::VideoWriter::fourcc(char c1, char c2, char c3, char c4)
+    private static native int fourcc_0(char c1, char c2, char c3, char c4);
+
+    // C++:  void cv::VideoWriter::release()
+    private static native void release_0(long nativeObj);
+
+    // C++:  void cv::VideoWriter::write(Mat image)
+    private static native void write_0(long nativeObj, long image_nativeObj);
+
+    // native support for java finalize()
+    private static native void delete(long nativeObj);
+
+}
Index: app/src/main/java/com/maksym/findthis/Components/MatchObjectsCallback.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- app/src/main/java/com/maksym/findthis/Components/MatchObjectsCallback.java	(revision cc086539aa8e7367bf4575d32b97aa049d6faad9)
+++ app/src/main/java/com/maksym/findthis/Components/MatchObjectsCallback.java	(date 1607965448230)
@@ -3,5 +3,5 @@
 import org.opencv.core.Mat;
 
 public interface MatchObjectsCallback {
-    void matchObjectsCallback(Mat homography, boolean found);
+    void matchObjectsCallback(float[] sceneCorners, Mat frame, boolean found);
 }
Index: app/src/main/java/com/maksym/findthis/Utils/ImageEditor.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- app/src/main/java/com/maksym/findthis/Utils/ImageEditor.java	(date 1607956866069)
+++ app/src/main/java/com/maksym/findthis/Utils/ImageEditor.java	(date 1607956866069)
@@ -0,0 +1,48 @@
+package com.maksym.findthis.Utils;
+
+import android.graphics.Bitmap;
+
+import org.opencv.android.Utils;
+import org.opencv.core.Core;
+import org.opencv.core.CvType;
+import org.opencv.core.Mat;
+import org.opencv.core.Point;
+import org.opencv.core.Scalar;
+import org.opencv.imgproc.Imgproc;
+
+public class ImageEditor {
+    public static Bitmap crpoHomography(Bitmap input, Mat homography){
+        Mat imgObject = new Mat();
+        Utils.bitmapToMat(input, imgObject);
+
+        //-- Get the corners from the image_1 ( the object to be "detected" )
+        Mat objCorners = new Mat(4, 1, CvType.CV_32FC2), sceneCorners = new Mat();
+        float[] objCornersData = new float[(int) (objCorners.total() * objCorners.channels())];
+        objCorners.get(0, 0, objCornersData);
+        objCornersData[0] = 0;
+        objCornersData[1] = 0;
+        objCornersData[2] = imgObject.cols();
+        objCornersData[3] = 0;
+        objCornersData[4] = imgObject.cols();
+        objCornersData[5] = imgObject.rows();
+        objCornersData[6] = 0;
+        objCornersData[7] = imgObject.rows();
+        objCorners.put(0, 0, objCornersData);
+        Core.perspectiveTransform(objCorners, sceneCorners, homography);
+
+        float[] sceneCornersData = new float[(int) (sceneCorners.total() * sceneCorners.channels())];
+        sceneCorners.get(0, 0, sceneCornersData);
+//
+//        //-- Draw lines between the corners (the mapped object in the scene - image_2 )
+//        Imgproc.line(imgMatches, new Point(sceneCornersData[0] + imgObject.cols(), sceneCornersData[1]),
+//                new Point(sceneCornersData[2] + imgObject.cols(), sceneCornersData[3]), new Scalar(0, 255, 0), 4);
+//        Imgproc.line(imgMatches, new Point(sceneCornersData[2] + imgObject.cols(), sceneCornersData[3]),
+//                new Point(sceneCornersData[4] + imgObject.cols(), sceneCornersData[5]), new Scalar(0, 255, 0), 4);
+//        Imgproc.line(imgMatches, new Point(sceneCornersData[4] + imgObject.cols(), sceneCornersData[5]),
+//                new Point(sceneCornersData[6] + imgObject.cols(), sceneCornersData[7]), new Scalar(0, 255, 0), 4);
+//        Imgproc.line(imgMatches, new Point(sceneCornersData[6] + imgObject.cols(), sceneCornersData[7]),
+//                new Point(sceneCornersData[0] + imgObject.cols(), sceneCornersData[1]), new Scalar(0, 255, 0), 4);
+
+        return null;
+    }
+}
Index: app/src/main/java/com/maksym/findthis/OpenCVmagic/MatchObjectThread.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- app/src/main/java/com/maksym/findthis/OpenCVmagic/MatchObjectThread.java	(revision cc086539aa8e7367bf4575d32b97aa049d6faad9)
+++ app/src/main/java/com/maksym/findthis/OpenCVmagic/MatchObjectThread.java	(date 1607969071408)
@@ -6,15 +6,21 @@
 import com.maksym.findthis.Utils.Constants;
 
 import org.opencv.calib3d.Calib3d;
+import org.opencv.core.Core;
+import org.opencv.core.CvType;
 import org.opencv.core.DMatch;
 import org.opencv.core.KeyPoint;
 import org.opencv.core.Mat;
+import org.opencv.core.MatOfByte;
 import org.opencv.core.MatOfDMatch;
 import org.opencv.core.MatOfKeyPoint;
 import org.opencv.core.MatOfPoint2f;
 import org.opencv.core.Point;
+import org.opencv.core.Scalar;
 import org.opencv.features2d.DescriptorMatcher;
 import org.opencv.features2d.Feature2D;
+import org.opencv.features2d.Features2d;
+import org.opencv.imgproc.Imgproc;
 
 import java.util.ArrayList;
 import java.util.List;
@@ -72,7 +78,7 @@
 
 //        //-- Draw matches
 //        Mat imgMatches = new Mat();
-//        Features2d.drawMatches(imgObject, keypointsObject, imgScene, keypointsScene, goodMatches, imgMatches, Scalar.all(-1),
+//        Features2d.drawMatches(object, keypointsObject, objectInScene, keypointsScene, goodMatches, imgMatches, Scalar.all(-1),
 //                Scalar.all(-1), new MatOfByte(), Features2d.NOT_DRAW_SINGLE_POINTS);
 
             //-- Localize the object
@@ -91,15 +97,51 @@
             objMat.fromList(obj);
             sceneMat.fromList(scene);
             double ransacReprojThreshold = 3.0;
-            Mat H = Calib3d.findHomography(objMat, sceneMat, Calib3d.RANSAC, ransacReprojThreshold);
+            Mat H = Calib3d.findHomography(objMat, sceneMat, Calib3d.RANSAC, ransacReprojThreshold);////// TODO estimate fundamental matrix instead
+//            Mat fundamental = Calib3d.findFundamentalMat(objMat, sceneMat);
+
+            //-- Get the corners from the image_1 ( the object to be "detected" )
+            Mat objCorners = new Mat(4, 1, CvType.CV_32FC2), sceneCorners = new Mat();
+            float[] objCornersData = new float[(int) (objCorners.total() * objCorners.channels())];
+            objCorners.get(0, 0, objCornersData);
+            objCornersData[0] = 0;
+            objCornersData[1] = 0;
+            objCornersData[2] = object.cols();
+            objCornersData[3] = 0;
+            objCornersData[4] = object.cols();
+            objCornersData[5] = object.rows();
+            objCornersData[6] = 0;
+            objCornersData[7] = object.rows();
+            objCorners.put(0, 0, objCornersData);
+            Core.perspectiveTransform(objCorners, sceneCorners, H);
+            float[] sceneCornersData = new float[(int) (sceneCorners.total() * sceneCorners.channels())];
+            sceneCorners.get(0, 0, sceneCornersData);
+//            //-- Draw lines between the corners (the mapped object in the scene - image_2 )
+//            Imgproc.line(objectInScene, new Point(sceneCornersData[0] + object.cols(), sceneCornersData[1]),
+//                    new Point(sceneCornersData[2] + object.cols(), sceneCornersData[3]), new Scalar(0, 255, 0), 4);
+//            Imgproc.line(objectInScene, new Point(sceneCornersData[2] + object.cols(), sceneCornersData[3]),
+//                    new Point(sceneCornersData[4] + object.cols(), sceneCornersData[5]), new Scalar(0, 255, 0), 4);
+//            Imgproc.line(objectInScene, new Point(sceneCornersData[4] + object.cols(), sceneCornersData[5]),
+//                    new Point(sceneCornersData[6] + object.cols(), sceneCornersData[7]), new Scalar(0, 255, 0), 4);
+//            Imgproc.line(objectInScene, new Point(sceneCornersData[6] + object.cols(), sceneCornersData[7]),
+//                    new Point(sceneCornersData[0] + object.cols(), sceneCornersData[1]), new Scalar(0, 255, 0), 4);
+
 
             Log.d(TAG, "Done calculations!");
             Log.d(TAG, "good matches size: " + goodMatches.toList().size());
+            Log.d(TAG, "scene corners data: "+sceneCornersData[0]);
+            Log.d(TAG, "scene corners data: "+sceneCornersData[1]);
+            Log.d(TAG, "scene corners data: "+sceneCornersData[2]);
+            Log.d(TAG, "scene corners data: "+sceneCornersData[3]);
+            Log.d(TAG, "scene corners data: "+sceneCornersData[4]);
+            Log.d(TAG, "scene corners data: "+sceneCornersData[5]);
+            Log.d(TAG, "scene corners data: "+sceneCornersData[6]);
+            Log.d(TAG, "scene corners data: "+sceneCornersData[7]);
 
-            callback.matchObjectsCallback(H, true);
+            callback.matchObjectsCallback(sceneCornersData, objectInScene, true);
         }
         else {
-            callback.matchObjectsCallback(null, false);
+            callback.matchObjectsCallback(null, null, false);
         }
     }
 }
Index: openCVLibrary3411/src/main/java/org/opencv/core/MatOfKeyPoint.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/core/MatOfKeyPoint.java	(date 1605830247397)
+++ openCVLibrary3411/src/main/java/org/opencv/core/MatOfKeyPoint.java	(date 1605830247397)
@@ -0,0 +1,86 @@
+package org.opencv.core;
+
+import java.util.Arrays;
+import java.util.List;
+
+import org.opencv.core.KeyPoint;
+
+public class MatOfKeyPoint extends Mat {
+    // 32FC7
+    private static final int _depth = CvType.CV_32F;
+    private static final int _channels = 7;
+
+    public MatOfKeyPoint() {
+        super();
+    }
+
+    protected MatOfKeyPoint(long addr) {
+        super(addr);
+        if( !empty() && checkVector(_channels, _depth) < 0 )
+            throw new IllegalArgumentException("Incompatible Mat");
+        //FIXME: do we need release() here?
+    }
+
+    public static MatOfKeyPoint fromNativeAddr(long addr) {
+        return new MatOfKeyPoint(addr);
+    }
+
+    public MatOfKeyPoint(Mat m) {
+        super(m, Range.all());
+        if( !empty() && checkVector(_channels, _depth) < 0 )
+            throw new IllegalArgumentException("Incompatible Mat");
+        //FIXME: do we need release() here?
+    }
+
+    public MatOfKeyPoint(KeyPoint...a) {
+        super();
+        fromArray(a);
+    }
+
+    public void alloc(int elemNumber) {
+        if(elemNumber>0)
+            super.create(elemNumber, 1, CvType.makeType(_depth, _channels));
+    }
+
+    public void fromArray(KeyPoint...a) {
+        if(a==null || a.length==0)
+            return;
+        int num = a.length;
+        alloc(num);
+        float buff[] = new float[num * _channels];
+        for(int i=0; i<num; i++) {
+            KeyPoint kp = a[i];
+            buff[_channels*i+0] = (float) kp.pt.x;
+            buff[_channels*i+1] = (float) kp.pt.y;
+            buff[_channels*i+2] = kp.size;
+            buff[_channels*i+3] = kp.angle;
+            buff[_channels*i+4] = kp.response;
+            buff[_channels*i+5] = kp.octave;
+            buff[_channels*i+6] = kp.class_id;
+        }
+        put(0, 0, buff); //TODO: check ret val!
+    }
+
+    public KeyPoint[] toArray() {
+        int num = (int) total();
+        KeyPoint[] a = new KeyPoint[num];
+        if(num == 0)
+            return a;
+        float buff[] = new float[num * _channels];
+        get(0, 0, buff); //TODO: check ret val!
+        for(int i=0; i<num; i++)
+            a[i] = new KeyPoint( buff[_channels*i+0], buff[_channels*i+1], buff[_channels*i+2], buff[_channels*i+3],
+                                 buff[_channels*i+4], (int) buff[_channels*i+5], (int) buff[_channels*i+6] );
+        return a;
+    }
+
+    public void fromList(List<KeyPoint> lkp) {
+        KeyPoint akp[] = lkp.toArray(new KeyPoint[0]);
+        fromArray(akp);
+    }
+
+    public List<KeyPoint> toList() {
+        KeyPoint[] akp = toArray();
+        return Arrays.asList(akp);
+    }
+}
Index: openCVLibrary3411/src/main/java/org/opencv/core/MatOfPoint.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/core/MatOfPoint.java	(date 1605830247408)
+++ openCVLibrary3411/src/main/java/org/opencv/core/MatOfPoint.java	(date 1605830247408)
@@ -0,0 +1,78 @@
+package org.opencv.core;
+
+import java.util.Arrays;
+import java.util.List;
+
+public class MatOfPoint extends Mat {
+    // 32SC2
+    private static final int _depth = CvType.CV_32S;
+    private static final int _channels = 2;
+
+    public MatOfPoint() {
+        super();
+    }
+
+    protected MatOfPoint(long addr) {
+        super(addr);
+        if( !empty() && checkVector(_channels, _depth) < 0 )
+            throw new IllegalArgumentException("Incompatible Mat");
+        //FIXME: do we need release() here?
+    }
+
+    public static MatOfPoint fromNativeAddr(long addr) {
+        return new MatOfPoint(addr);
+    }
+
+    public MatOfPoint(Mat m) {
+        super(m, Range.all());
+        if( !empty() && checkVector(_channels, _depth) < 0 )
+            throw new IllegalArgumentException("Incompatible Mat");
+        //FIXME: do we need release() here?
+    }
+
+    public MatOfPoint(Point...a) {
+        super();
+        fromArray(a);
+    }
+
+    public void alloc(int elemNumber) {
+        if(elemNumber>0)
+            super.create(elemNumber, 1, CvType.makeType(_depth, _channels));
+    }
+
+    public void fromArray(Point...a) {
+        if(a==null || a.length==0)
+            return;
+        int num = a.length;
+        alloc(num);
+        int buff[] = new int[num * _channels];
+        for(int i=0; i<num; i++) {
+            Point p = a[i];
+            buff[_channels*i+0] = (int) p.x;
+            buff[_channels*i+1] = (int) p.y;
+        }
+        put(0, 0, buff); //TODO: check ret val!
+    }
+
+    public Point[] toArray() {
+        int num = (int) total();
+        Point[] ap = new Point[num];
+        if(num == 0)
+            return ap;
+        int buff[] = new int[num * _channels];
+        get(0, 0, buff); //TODO: check ret val!
+        for(int i=0; i<num; i++)
+            ap[i] = new Point(buff[i*_channels], buff[i*_channels+1]);
+        return ap;
+    }
+
+    public void fromList(List<Point> lp) {
+        Point ap[] = lp.toArray(new Point[0]);
+        fromArray(ap);
+    }
+
+    public List<Point> toList() {
+        Point[] ap = toArray();
+        return Arrays.asList(ap);
+    }
+}
Index: openCVLibrary3411/src/main/java/org/opencv/core/MatOfInt4.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/core/MatOfInt4.java	(date 1605830247358)
+++ openCVLibrary3411/src/main/java/org/opencv/core/MatOfInt4.java	(date 1605830247358)
@@ -0,0 +1,80 @@
+package org.opencv.core;
+
+import java.util.Arrays;
+import java.util.List;
+
+
+public class MatOfInt4 extends Mat {
+    // 32SC4
+    private static final int _depth = CvType.CV_32S;
+    private static final int _channels = 4;
+
+    public MatOfInt4() {
+        super();
+    }
+
+    protected MatOfInt4(long addr) {
+        super(addr);
+        if( !empty() && checkVector(_channels, _depth) < 0 )
+            throw new IllegalArgumentException("Incompatible Mat");
+        //FIXME: do we need release() here?
+    }
+
+    public static MatOfInt4 fromNativeAddr(long addr) {
+        return new MatOfInt4(addr);
+    }
+
+    public MatOfInt4(Mat m) {
+        super(m, Range.all());
+        if( !empty() && checkVector(_channels, _depth) < 0 )
+            throw new IllegalArgumentException("Incompatible Mat");
+        //FIXME: do we need release() here?
+    }
+
+    public MatOfInt4(int...a) {
+        super();
+        fromArray(a);
+    }
+
+    public void alloc(int elemNumber) {
+        if(elemNumber>0)
+            super.create(elemNumber, 1, CvType.makeType(_depth, _channels));
+    }
+
+    public void fromArray(int...a) {
+        if(a==null || a.length==0)
+            return;
+        int num = a.length / _channels;
+        alloc(num);
+        put(0, 0, a); //TODO: check ret val!
+    }
+
+    public int[] toArray() {
+        int num = checkVector(_channels, _depth);
+        if(num < 0)
+            throw new RuntimeException("Native Mat has unexpected type or size: " + toString());
+        int[] a = new int[num * _channels];
+        if(num == 0)
+            return a;
+        get(0, 0, a); //TODO: check ret val!
+        return a;
+    }
+
+    public void fromList(List<Integer> lb) {
+        if(lb==null || lb.size()==0)
+            return;
+        Integer ab[] = lb.toArray(new Integer[0]);
+        int a[] = new int[ab.length];
+        for(int i=0; i<ab.length; i++)
+            a[i] = ab[i];
+        fromArray(a);
+    }
+
+    public List<Integer> toList() {
+        int[] a = toArray();
+        Integer ab[] = new Integer[a.length];
+        for(int i=0; i<a.length; i++)
+            ab[i] = a[i];
+        return Arrays.asList(ab);
+    }
+}
Index: openCVLibrary3411/src/main/java/org/opencv/core/MatOfFloat4.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/core/MatOfFloat4.java	(date 1605830247353)
+++ openCVLibrary3411/src/main/java/org/opencv/core/MatOfFloat4.java	(date 1605830247353)
@@ -0,0 +1,79 @@
+package org.opencv.core;
+
+import java.util.Arrays;
+import java.util.List;
+
+public class MatOfFloat4 extends Mat {
+    // 32FC4
+    private static final int _depth = CvType.CV_32F;
+    private static final int _channels = 4;
+
+    public MatOfFloat4() {
+        super();
+    }
+
+    protected MatOfFloat4(long addr) {
+        super(addr);
+        if( !empty() && checkVector(_channels, _depth) < 0 )
+            throw new IllegalArgumentException("Incompatible Mat");
+        //FIXME: do we need release() here?
+    }
+
+    public static MatOfFloat4 fromNativeAddr(long addr) {
+        return new MatOfFloat4(addr);
+    }
+
+    public MatOfFloat4(Mat m) {
+        super(m, Range.all());
+        if( !empty() && checkVector(_channels, _depth) < 0 )
+            throw new IllegalArgumentException("Incompatible Mat");
+        //FIXME: do we need release() here?
+    }
+
+    public MatOfFloat4(float...a) {
+        super();
+        fromArray(a);
+    }
+
+    public void alloc(int elemNumber) {
+        if(elemNumber>0)
+            super.create(elemNumber, 1, CvType.makeType(_depth, _channels));
+    }
+
+    public void fromArray(float...a) {
+        if(a==null || a.length==0)
+            return;
+        int num = a.length / _channels;
+        alloc(num);
+        put(0, 0, a); //TODO: check ret val!
+    }
+
+    public float[] toArray() {
+        int num = checkVector(_channels, _depth);
+        if(num < 0)
+            throw new RuntimeException("Native Mat has unexpected type or size: " + toString());
+        float[] a = new float[num * _channels];
+        if(num == 0)
+            return a;
+        get(0, 0, a); //TODO: check ret val!
+        return a;
+    }
+
+    public void fromList(List<Float> lb) {
+        if(lb==null || lb.size()==0)
+            return;
+        Float ab[] = lb.toArray(new Float[0]);
+        float a[] = new float[ab.length];
+        for(int i=0; i<ab.length; i++)
+            a[i] = ab[i];
+        fromArray(a);
+    }
+
+    public List<Float> toList() {
+        float[] a = toArray();
+        Float ab[] = new Float[a.length];
+        for(int i=0; i<a.length; i++)
+            ab[i] = a[i];
+        return Arrays.asList(ab);
+    }
+}
Index: openCVLibrary3411/src/main/java/org/opencv/core/MatOfFloat6.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/core/MatOfFloat6.java	(date 1605830247355)
+++ openCVLibrary3411/src/main/java/org/opencv/core/MatOfFloat6.java	(date 1605830247355)
@@ -0,0 +1,79 @@
+package org.opencv.core;
+
+import java.util.Arrays;
+import java.util.List;
+
+public class MatOfFloat6 extends Mat {
+    // 32FC6
+    private static final int _depth = CvType.CV_32F;
+    private static final int _channels = 6;
+
+    public MatOfFloat6() {
+        super();
+    }
+
+    protected MatOfFloat6(long addr) {
+        super(addr);
+        if( !empty() && checkVector(_channels, _depth) < 0 )
+            throw new IllegalArgumentException("Incompatible Mat");
+        //FIXME: do we need release() here?
+    }
+
+    public static MatOfFloat6 fromNativeAddr(long addr) {
+        return new MatOfFloat6(addr);
+    }
+
+    public MatOfFloat6(Mat m) {
+        super(m, Range.all());
+        if( !empty() && checkVector(_channels, _depth) < 0 )
+            throw new IllegalArgumentException("Incompatible Mat");
+        //FIXME: do we need release() here?
+    }
+
+    public MatOfFloat6(float...a) {
+        super();
+        fromArray(a);
+    }
+
+    public void alloc(int elemNumber) {
+        if(elemNumber>0)
+            super.create(elemNumber, 1, CvType.makeType(_depth, _channels));
+    }
+
+    public void fromArray(float...a) {
+        if(a==null || a.length==0)
+            return;
+        int num = a.length / _channels;
+        alloc(num);
+        put(0, 0, a); //TODO: check ret val!
+    }
+
+    public float[] toArray() {
+        int num = checkVector(_channels, _depth);
+        if(num < 0)
+            throw new RuntimeException("Native Mat has unexpected type or size: " + toString());
+        float[] a = new float[num * _channels];
+        if(num == 0)
+            return a;
+        get(0, 0, a); //TODO: check ret val!
+        return a;
+    }
+
+    public void fromList(List<Float> lb) {
+        if(lb==null || lb.size()==0)
+            return;
+        Float ab[] = lb.toArray(new Float[0]);
+        float a[] = new float[ab.length];
+        for(int i=0; i<ab.length; i++)
+            a[i] = ab[i];
+        fromArray(a);
+    }
+
+    public List<Float> toList() {
+        float[] a = toArray();
+        Float ab[] = new Float[a.length];
+        for(int i=0; i<a.length; i++)
+            ab[i] = a[i];
+        return Arrays.asList(ab);
+    }
+}
Index: openCVLibrary3411/src/main/java/org/opencv/core/MatOfInt.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/core/MatOfInt.java	(date 1605830247357)
+++ openCVLibrary3411/src/main/java/org/opencv/core/MatOfInt.java	(date 1605830247357)
@@ -0,0 +1,80 @@
+package org.opencv.core;
+
+import java.util.Arrays;
+import java.util.List;
+
+
+public class MatOfInt extends Mat {
+    // 32SC1
+    private static final int _depth = CvType.CV_32S;
+    private static final int _channels = 1;
+
+    public MatOfInt() {
+        super();
+    }
+
+    protected MatOfInt(long addr) {
+        super(addr);
+        if( !empty() && checkVector(_channels, _depth) < 0 )
+            throw new IllegalArgumentException("Incompatible Mat");
+        //FIXME: do we need release() here?
+    }
+
+    public static MatOfInt fromNativeAddr(long addr) {
+        return new MatOfInt(addr);
+    }
+
+    public MatOfInt(Mat m) {
+        super(m, Range.all());
+        if( !empty() && checkVector(_channels, _depth) < 0 )
+            throw new IllegalArgumentException("Incompatible Mat");
+        //FIXME: do we need release() here?
+    }
+
+    public MatOfInt(int...a) {
+        super();
+        fromArray(a);
+    }
+
+    public void alloc(int elemNumber) {
+        if(elemNumber>0)
+            super.create(elemNumber, 1, CvType.makeType(_depth, _channels));
+    }
+
+    public void fromArray(int...a) {
+        if(a==null || a.length==0)
+            return;
+        int num = a.length / _channels;
+        alloc(num);
+        put(0, 0, a); //TODO: check ret val!
+    }
+
+    public int[] toArray() {
+        int num = checkVector(_channels, _depth);
+        if(num < 0)
+            throw new RuntimeException("Native Mat has unexpected type or size: " + toString());
+        int[] a = new int[num * _channels];
+        if(num == 0)
+            return a;
+        get(0, 0, a); //TODO: check ret val!
+        return a;
+    }
+
+    public void fromList(List<Integer> lb) {
+        if(lb==null || lb.size()==0)
+            return;
+        Integer ab[] = lb.toArray(new Integer[0]);
+        int a[] = new int[ab.length];
+        for(int i=0; i<ab.length; i++)
+            a[i] = ab[i];
+        fromArray(a);
+    }
+
+    public List<Integer> toList() {
+        int[] a = toArray();
+        Integer ab[] = new Integer[a.length];
+        for(int i=0; i<a.length; i++)
+            ab[i] = a[i];
+        return Arrays.asList(ab);
+    }
+}
Index: openCVLibrary3411/src/main/java/org/opencv/core/MatOfFloat.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- openCVLibrary3411/src/main/java/org/opencv/core/MatOfFloat.java	(date 1605830247351)
+++ openCVLibrary3411/src/main/java/org/opencv/core/MatOfFloat.java	(date 1605830247351)
@@ -0,0 +1,79 @@
+package org.opencv.core;
+
+import java.util.Arrays;
+import java.util.List;
+
+public class MatOfFloat extends Mat {
+    // 32FC1
+    private static final int _depth = CvType.CV_32F;
+    private static final int _channels = 1;
+
+    public MatOfFloat() {
+        super();
+    }
+
+    protected MatOfFloat(long addr) {
+        super(addr);
+        if( !empty() && checkVector(_channels, _depth) < 0 )
+            throw new IllegalArgumentException("Incompatible Mat");
+        //FIXME: do we need release() here?
+    }
+
+    public static MatOfFloat fromNativeAddr(long addr) {
+        return new MatOfFloat(addr);
+    }
+
+    public MatOfFloat(Mat m) {
+        super(m, Range.all());
+        if( !empty() && checkVector(_channels, _depth) < 0 )
+            throw new IllegalArgumentException("Incompatible Mat");
+        //FIXME: do we need release() here?
+    }
+
+    public MatOfFloat(float...a) {
+        super();
+        fromArray(a);
+    }
+
+    public void alloc(int elemNumber) {
+        if(elemNumber>0)
+            super.create(elemNumber, 1, CvType.makeType(_depth, _channels));
+    }
+
+    public void fromArray(float...a) {
+        if(a==null || a.length==0)
+            return;
+        int num = a.length / _channels;
+        alloc(num);
+        put(0, 0, a); //TODO: check ret val!
+    }
+
+    public float[] toArray() {
+        int num = checkVector(_channels, _depth);
+        if(num < 0)
+            throw new RuntimeException("Native Mat has unexpected type or size: " + toString());
+        float[] a = new float[num * _channels];
+        if(num == 0)
+            return a;
+        get(0, 0, a); //TODO: check ret val!
+        return a;
+    }
+
+    public void fromList(List<Float> lb) {
+        if(lb==null || lb.size()==0)
+            return;
+        Float ab[] = lb.toArray(new Float[0]);
+        float a[] = new float[ab.length];
+        for(int i=0; i<ab.length; i++)
+            a[i] = ab[i];
+        fromArray(a);
+    }
+
+    public List<Float> toList() {
+        float[] a = toArray();
+        Float ab[] = new Float[a.length];
+        for(int i=0; i<a.length; i++)
+            ab[i] = a[i];
+        return Arrays.asList(ab);
+    }
+}
